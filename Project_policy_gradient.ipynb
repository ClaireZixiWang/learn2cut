{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_policy_gradient.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "039c9c17bafc40e0827108fbca81c032": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc4e8b39d254488b8effcefa54538fdf",
              "IPY_MODEL_c9951c0aa6104832b2076847a5b32354"
            ],
            "layout": "IPY_MODEL_de1d993f297d4379890e58b111f43372"
          }
        },
        "bc4e8b39d254488b8effcefa54538fdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50fa5111f16b426f877f98ac67e06cc0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2225fdcbbb2f4de2ad8d8b17984b8d62",
            "value": "1.555 MB of 1.555 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "c9951c0aa6104832b2076847a5b32354": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef0c3d8046e348818bb8e82a6692a1b8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6cd1de4bb69d4c4c95db3ac2853cbebd",
            "value": 1
          }
        },
        "de1d993f297d4379890e58b111f43372": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50fa5111f16b426f877f98ac67e06cc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2225fdcbbb2f4de2ad8d8b17984b8d62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef0c3d8046e348818bb8e82a6692a1b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cd1de4bb69d4c4c95db3ac2853cbebd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ClaireZixiWang/learn2cut/blob/main/Project_policy_gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## See README.md file for further details about the project and the environment.\n",
        "\n",
        "### State-Action Description\n",
        "\n",
        "### State\n",
        "State s is an array with give components\n",
        "\n",
        "* s[0]:  constraint matrix $A$of the current LP ($\\max  -c^Tx \\text{ s.t. }Ax \\le  b$) . Dimension is $m \\times n$. See by printing s[0].shape. Here $n$ is the (fixed) number of variables. For instances of size 60 by 60 used in the above command, $n$ will remain fixed as 60. And $m$ is the current number of constraints. Initially, $m$ is to the number of constraints in the IP instance. (For instances generated with --num-c=60, $m$ is 60 at the first step).  But $m$ will increase by one in every step of the episode as one new constraint (cut) is added on taking an action.\n",
        "* s[1]: rhs $b$ for the current LP ($Ax\\le b$). Dimension same as the number $m$ in matrix A.\n",
        "* s[2]: coefficient vector $c$ from the LP objective ($-c^Tx$). Dimension same as the number of variables, i.e., $n$.\n",
        "* s[3],  s[4]: Gomory cuts available in the current round of Gomory's cutting plane algorithm. Each cut $i$ is of the form $D_i x\\le d_i$.   s[3] gives the matrix $D$ (of dimension $k \\times n$) of cuts and s[4] gives the rhs $d$ (of dimension $k$). The number of cuts $k$ available in each round changes, you can find it out by printing the size of last component of state, i.e., s[4].size or s[-1].size.\n",
        "\n",
        "### Actions\n",
        "There are k=s[4].size actions available in each state $s$, with $i^{th}$ action corresponding to the $i^{th}$ cut with inequality $D_i x\\le d_i$ in $s[3], s[4]$."
      ],
      "metadata": {
        "id": "5TN-sMTvcG_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***QUESTIONS***:\n",
        "1. By \"current\" LP, you mean the LP that the agent was running in the last state? As in, the LP with all the added constraints? \n",
        "  * ==> I think so.\n",
        "1. What do you mean Gomory cuts *available*? As in, after doing Simplex methods, the *variables* that you can choose to cut?\n",
        "  * Yes I think so.\n",
        "2. Isn't the number of variables (n) changing? in the C-G cutting plane method?\n",
        "  * No, as the spec says, **$n$ is the fixed number of variables**.\n",
        "  * If you look that cuttng plane lecture notes, you can see that after each step, the dummy variable is not added in the constraint. They are merely there for the sake of the LP solver (simplex method), but not really relevant for us.\n",
        "    * This is not correct, I think they are still very much relevant, it's just that I think among the 60 variables a lot of them are space holders for dummy variables so that our $n$ is fixed, so that we don't have to worry about using LSTM. Since each time the sequence [a, b] will be of size n+1. And we can just use a fixed-input-size network to do that.\n",
        "    * But still need to verify with the TA about the place holder understanding.\n",
        "3. dimension of s[3] and s[4]? Where is the \"available all\" stored? In which dimension?\n",
        "  * Each row of D is an \"available cut\". Therefore each $D_i x\\le d_i$ is an \"available\" cut in CG method solved from the simplex method.\n",
        "4. pointing towards the slides: why does the number of constraints m increase 1 in each step, if you can choose *multiple* cuts in one step? (OR in the algorithm we just choose one cut each time? or is that a more vanilla version to start, but to expand on multiple cuts a time later?)\n",
        "5. What do you mean by each \"instance\"?"
      ],
      "metadata": {
        "id": "XJE0bz30UL1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYROdPaaZOVa",
        "outputId": "291d8ce1-b3a6-4ac1-d53d-3259800f850f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -i https://pypi.gurobi.com gurobipy"
      ],
      "metadata": {
        "id": "xSXTKB2zurrt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92303613-9be5-4d87-ce11-5639fa8407af"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.gurobi.com\n",
            "Requirement already satisfied: gurobipy in /usr/local/lib/python3.7/dist-packages (9.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qqq"
      ],
      "metadata": {
        "id": "YULy9ymNvDxN"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/IEOR_RL/Project_learn2cut\n",
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "lTnvB0_iZUrX",
        "outputId": "e8195a22-3dc7-4ddd-9312-5ba1bca14f9a"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive/IEOR_RL/Project_learn2cut'\n",
            "/content/drive/MyDrive/IEOR_RL/Project_learn2cut\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/IEOR_RL/Project_learn2cut'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import time"
      ],
      "metadata": {
        "id": "Q8PiSPj5us0O"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(4, 40),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(40, 20), \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(20, 10)\n",
        "        )\n",
        "\n",
        "datapoint = torch.FloatTensor([\n",
        "                               [[1,2,3,4],\n",
        "                                [2,3,4,5]],\n",
        "                               [[3,4,5,6],\n",
        "                                [4,5,6,7]]\n",
        "                              ])\n"
      ],
      "metadata": {
        "id": "ojB8UvBAPAWk"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(datapoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r_nraniPaKl",
        "outputId": "78bfc415-7880-423a-fe24-dadb1b7fbce6"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.9311e-01, -1.0979e-01,  3.6028e-01,  1.5639e-01, -1.3580e-01,\n",
              "           2.8566e-01, -4.9615e-02, -1.2735e-01,  2.2602e-01,  4.9194e-03],\n",
              "         [-1.6585e-01, -5.1356e-02,  4.7905e-01,  1.6484e-01, -1.7511e-01,\n",
              "           3.8126e-01, -1.6237e-01, -1.2102e-01,  2.7125e-01,  3.5351e-02]],\n",
              "\n",
              "        [[-1.4058e-01, -1.9938e-05,  5.9489e-01,  1.7617e-01, -2.1856e-01,\n",
              "           4.6711e-01, -2.6529e-01, -1.2780e-01,  3.2186e-01,  6.4014e-02],\n",
              "         [-1.1566e-01,  5.1480e-02,  7.0998e-01,  1.8796e-01, -2.6163e-01,\n",
              "           5.5308e-01, -3.6769e-01, -1.3332e-01,  3.7285e-01,  9.2075e-02]]],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Code for Policy Model\n",
        "\n",
        "class Policy(object):\n",
        "\n",
        "    # inputsize = n+1 = 61\n",
        "    def __init__(self, lr, input_size, attention_size=10, temperature=1) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_size, 40),\n",
        "            # torch.nn.Sigmoid(),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(40, 30), \n",
        "            # torch.nn.Sigmoid(),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(30, 20), \n",
        "            # torch.nn.Sigmoid(),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(20, attention_size)\n",
        "        )\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "        # RECORD HYPER-PARAMS\n",
        "        self.input_size = input_size\n",
        "        self.attention_size = attention_size\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def compute_logits_big_batch(self, obs_matrix, act_matrix, batchsize):\n",
        "        '''\n",
        "        Function that takes in a batch of observations and computes the action logits for each of observations in this batch\n",
        "        Args: obs_matrix: np.array(m * batchsize, n+1) # TODO maybe change this to tensor\n",
        "              act_matrix: np.array(k * batchsize, n+1)\n",
        "        Return: batch_logit: tensor(batchsize, k)\n",
        "        '''\n",
        "\n",
        "        # Get the batch result\n",
        "\n",
        "        # transform to tensor\n",
        "        obs_attention = self.model(torch.FloatTensor(obs_matrix)) # tensor(m * batchsize, u)\n",
        "        act_attention = self.model(torch.FloatTensor(act_matrix)) # tensor(k * batchsize, u)\n",
        "\n",
        "        assert obs_attention.shape == (obs_matrix.shape[0], self.attention_size)\n",
        "        assert act_attention.shape == (act_matrix.shape[0], self.attention_size)\n",
        "\n",
        "        # split a batch of output of size tensor(m * batchsize, u) into (batchsize, m, u)\n",
        "        batch_obs_output = torch.reshape(obs_attention, (batchsize, obs_attention.shape[0]/batchsize, obs_attention.shape[1]))\n",
        "        \n",
        "        # split a batch of output of size tensor(k * batchsize, u) into (batchsize, k, u)\n",
        "        batch_act_output = torch.reshape(act_attention, (batchsize, act_attention.shape[0]/batchsize, act_attention.shape[1]))\n",
        "\n",
        "        # To do batch matrix multiplication, transpose (batchsize, k, u) into (batchsize, u, k)\n",
        "        # (batchsize, m, u) @ (batchsize, u, k) =  (batchsize, m, k)\n",
        "        # (batchsize, m, k) == mean across all observation ==> (batchsize, k)\n",
        "        batch_logit = torch.bmm(batch_obs_output, batch_act_output.transpose(0, 2, 1)).mean(dim=1)\n",
        "\n",
        "        assert batch_logit.shape == (batchsize, act_matrix.shape[0]/batchsize)\n",
        "\n",
        "        return batch_logit\n",
        "\n",
        "    def compute_batch_selected_prob(self, batch_probs):\n",
        "        '''\n",
        "        Function that takes in a batch of probabilities and return the max probability for each \"datapoint\" in the batch\n",
        "        Args:    batch_logit: tensor(batchsize, k)\n",
        "        Return:  batch_selected_prob: tensor(batchsize,)\n",
        "        '''\n",
        "        # TODO\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def compute_one_step_logits(self, obs, act):\n",
        "        '''\n",
        "        Function that takes in ONE observation and action space, computes the action logits\n",
        "        Args:   obs_matrix: np.array(m, n+1)\n",
        "                act_matrix: np.array(k, n+1)\n",
        "        Return: logit: tensor(k)\n",
        "        '''\n",
        "\n",
        "        obs_attention = self.model(torch.FloatTensor(obs)) #-> (m, 10)\n",
        "        act_attention = self.model(torch.FloatTensor(act)) #-> (k, 10)\n",
        "        # print(\"DEBUGGING: obs_attention looks like:\", obs_attention)\n",
        "        # print(\"DEBUGGING: act_attention looks like:\", act_attention)\n",
        "\n",
        "        # attention matrix multiplication & mean to get the score\n",
        "        logits = torch.mm(obs_attention, act_attention.transpose(1, 0)).mean(dim=0)\n",
        "        # print(\"DEBUGGING: logits looks like:\", logits)\n",
        "\n",
        "        # print(\"DEBUGGING: act.shape =\", act_attention.shape)\n",
        "        # print(\"DEBUGGING: logits.shape =\", logits.shape)\n",
        "        assert logits.shape[0] == act_attention.shape[0]\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def predict_prob(self, obs, act):\n",
        "        # Function that uses softmax to transform logits to probabilities\n",
        "        # TODO: What shape should obs and act take?\n",
        "        # Args:   obs_matrix: np.array(m, n+1)\n",
        "        #         act_matrix: np.array(k, n+1)\n",
        "        # Return: probs: tensor(k)\n",
        "\n",
        "        logits = self.compute_one_step_logits(obs, act)\n",
        "\n",
        "        # TODO: make temperature a argument in the function, so that it's depended on the steps\n",
        "        probs = torch.nn.functional.softmax(logits/self.temperature, dim=0)\n",
        "        return probs\n",
        "\n",
        "    def selected_prob(self, probs, action):\n",
        "\n",
        "\n",
        "        # TODO: do I need this function?\n",
        "        one_hot = torch.zeros()\n",
        "        return probs.max()\n",
        "\n",
        "    def choose_action(self, obs, act):\n",
        "\n",
        "        # TODO: is this returning a number?\n",
        "        return torch.argmax(self.predict_prob(obs, act)).item()\n",
        "\n",
        "\n",
        "    def train(self, obs_matrix, act_matrix, actions, Qs):\n",
        "        \"\"\"\n",
        "        Args: obs_matrix: np.array(batchsize * m, n+1) => changed to [np.array(m, n+1)] * batchsize, note that m are varied!\n",
        "              act_matrix: np.array(batchsize * k, n+1)  \n",
        "              actions: => [[action number]] * batchsize      \n",
        "              Qs: np.array(batchsize, )\n",
        "        \"\"\"\n",
        "        # Convert numpy array to tensor\n",
        "\n",
        "        # use compute_batch_prob to compute the batch logits for every datapoint in batch.\n",
        "        # use compute_batch_selected_prob the compute the max probabilty for every datapoint in batch.\n",
        "        start = time.time()\n",
        "        print(\"DEBUGGING: I'm inside the training now!\")\n",
        "        \n",
        "        Qs = torch.FloatTensor(Qs)\n",
        "\n",
        "\n",
        "        # Try using a for loop first, see whether it really cost too much time & whether it works at all\n",
        "        prob_selected = torch.zeros(len(actions))\n",
        "        for i in range(len(obs_matrix)):\n",
        "            #TODO: adjust the temperatures based on trajectory steps\n",
        "            probs = self.predict_prob(obs_matrix[i], act_matrix[i])\n",
        "            one_hot = torch.zeros_like(probs)\n",
        "            one_hot[actions[i][0]] = 1\n",
        "            prob_selected[i] = torch.sum(probs * one_hot)\n",
        "\n",
        "\n",
        "        # For robustness add in noise for prob_selected # TODO: why do this?\n",
        "\n",
        "        # define loss function as in lab 4\n",
        "        # TODO define loss function as described in the text above\n",
        "        loss = - (torch.sum(Qs * torch.log(prob_selected)) / (len(obs_matrix) + 1))\n",
        "        print(\"DEBUGGING: the loss =\", loss)\n",
        "\n",
        "        print(\"DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\")\n",
        "        print(\"   First layer:\")\n",
        "        print(policy.model[0].weight.grad)\n",
        "        print(\"   Last layer:\")\n",
        "        print(policy.model[6].weight.grad)\n",
        "\n",
        "        # backward pass\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "\n",
        "        print(\"DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\")\n",
        "        print(\"   First layer:\")\n",
        "        print(policy.model[0].weight.grad)\n",
        "        print(\"   Last layer:\")\n",
        "        print(policy.model[6].weight.grad)\n",
        "\n",
        "        # step\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "        print(\"DEBUGGING: training for one iteration takes %f min:\" % ((time.time() - start)/60))\n",
        "\n",
        "\n",
        "        # return detached loss (why?)\n",
        "        return loss.detach().cpu().data.numpy()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HViRnY1ssGfc"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchsummary import summary\n",
        "# summary(policy.model, (61,))\n",
        "# print(policy.model)"
      ],
      "metadata": {
        "id": "OGm4an0pWPHr"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(policy.model[6].weight.grad)"
      ],
      "metadata": {
        "id": "dfkXKSJWW6OR"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP model for policy model:\n",
        "#   model.forward\n",
        "#   model.train --> What is in this function? what are the function arguments?\n",
        "# Baseline function b(s) ==> Okay maybe we stil need the V model as a proper baseline\n",
        "\n",
        "\n",
        "# Q value model ==> Discard this right now, just do vanilla policy gradient\n",
        "#   Can I just use the one in Lab4? What does it mean? what does the states and actions mean? --> Print out the s, r to check\n",
        "#   What is a Q-value in our set-up?\n",
        "#   How do I used this? \n",
        "#   (What's the baseline function??)"
      ],
      "metadata": {
        "id": "ACiQz-gESgOO"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "2xI0riE6md5V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "b7c51d16-eb9d-4d8f-ec25-d50cc340bc3c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/My Drive/IEOR_RL/Project_learn2cut/wandb/run-20220508_235644-3o250wsg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/ieor4575-spring2022/finalproject/runs/3o250wsg\" target=\"_blank\">true-wood-2736</a></strong> to <a href=\"https://wandb.ai/ieor4575-spring2022/finalproject\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# import gymenv_v2\n",
        "from gymenv_v2 import make_multiple_env\n",
        "\n",
        "\n",
        "import wandb\n",
        "wandb.login()\n",
        "run=wandb.init(project=\"finalproject\", entity=\"ieor4575-spring2022\", tags=[\"training-easy\"])\n",
        "#run=wandb.init(project=\"finalproject\", entity=\"ieor-4575\", tags=[\"training-hard\"])\n",
        "#run=wandb.init(project=\"finalproject\", entity=\"ieor-4575\", tags=[\"test\"])\n",
        "\n",
        "### TRAINING\n",
        "\n",
        "# Setup: You may generate your own instances on which you train the cutting agent.\n",
        "custom_config = {\n",
        "    \"load_dir\"        : 'instances/randomip_n60_m60',   # this is the location of the randomly generated instances (you may specify a different directory)\n",
        "    \"idx_list\"        : list(range(20)),                # take the first 20 instances from the directory\n",
        "    \"timelimit\"       : 50,                             # the maximum horizon length is 50\n",
        "    \"reward_type\"     : 'obj'                           # DO NOT CHANGE reward_type\n",
        "}\n",
        "\n",
        "# Easy Setup: Use the following environment settings. We will evaluate your agent with the same easy config below:\n",
        "easy_config = {\n",
        "    \"load_dir\"        : 'instances/train_10_n60_m60',\n",
        "    \"idx_list\"        : list(range(5)),\n",
        "    \"timelimit\"       : 50,\n",
        "    \"reward_type\"     : 'obj'\n",
        "}\n",
        "\n",
        "# Hard Setup: Use the following environment settings. We will evaluate your agent with the same hard config below:\n",
        "hard_config = {\n",
        "    \"load_dir\"        : 'instances/train_100_n60_m60',\n",
        "    \"idx_list\"        : list(range(99)),\n",
        "    \"timelimit\"       : 50,\n",
        "    \"reward_type\"     : 'obj'\n",
        "}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._C import dtype\n",
        "def discounted_rewards(r, gamma):\n",
        "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "    discounted_r = np.zeros_like(r, dtype=float)\n",
        "    running_sum = 0\n",
        "    for i in reversed(range(0,len(r))):\n",
        "        discounted_r[i] = running_sum * gamma + r[i]\n",
        "        running_sum = discounted_r[i]\n",
        "    return list(discounted_r)"
      ],
      "metadata": {
        "id": "COyIO0TEDRjt"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = make_multiple_env(**easy_config) \n",
        "s = env.reset()   # samples a RANDOM INSTANCE every time env.reset() is called\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFhvgi_q53V4",
        "outputId": "0606fe3f-257d-4be0-c297-94ca468c570e"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading training instances, dir instances/train_10_n60_m60 idx 0\n",
            "loading training instances, dir instances/train_10_n60_m60 idx 1\n",
            "loading training instances, dir instances/train_10_n60_m60 idx 2\n",
            "loading training instances, dir instances/train_10_n60_m60 idx 3\n",
            "loading training instances, dir instances/train_10_n60_m60 idx 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "A, b, c0, cuts_a, cuts_b = s\n",
        "concat = np.hstack((s[0], np.expand_dims(s[1], axis=1)))\n",
        "concat\n",
        "# np.linalg.norm(concat, axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJXouQZx9DQ7",
        "outputId": "c074b7ea-b6a0-450b-82e1-8e79f11718d2"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  3,   4,   0, ...,   0,   1, 543],\n",
              "       [  4,   3,   1, ...,   0,   1, 558],\n",
              "       [  2,   0,   1, ...,   4,   2, 555],\n",
              "       ...,\n",
              "       [  4,   1,   1, ...,   4,   0, 588],\n",
              "       [  2,   3,   1, ...,   0,   2, 553],\n",
              "       [  0,   2,   3, ...,   4,   0, 576]])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.expand_dims(s[1], axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSUhT5go6czx",
        "outputId": "06211843-65f8-4e21-9d00-cba5faa3c64b"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[543],\n",
              "       [558],\n",
              "       [555],\n",
              "       [563],\n",
              "       [551],\n",
              "       [577],\n",
              "       [584],\n",
              "       [570],\n",
              "       [580],\n",
              "       [568],\n",
              "       [598],\n",
              "       [557],\n",
              "       [550],\n",
              "       [572],\n",
              "       [551],\n",
              "       [566],\n",
              "       [543],\n",
              "       [549],\n",
              "       [551],\n",
              "       [566],\n",
              "       [583],\n",
              "       [556],\n",
              "       [577],\n",
              "       [540],\n",
              "       [551],\n",
              "       [553],\n",
              "       [587],\n",
              "       [594],\n",
              "       [542],\n",
              "       [578],\n",
              "       [569],\n",
              "       [567],\n",
              "       [548],\n",
              "       [543],\n",
              "       [577],\n",
              "       [563],\n",
              "       [575],\n",
              "       [555],\n",
              "       [574],\n",
              "       [582],\n",
              "       [576],\n",
              "       [569],\n",
              "       [562],\n",
              "       [583],\n",
              "       [542],\n",
              "       [595],\n",
              "       [554],\n",
              "       [587],\n",
              "       [561],\n",
              "       [577],\n",
              "       [558],\n",
              "       [544],\n",
              "       [553],\n",
              "       [555],\n",
              "       [573],\n",
              "       [564],\n",
              "       [585],\n",
              "       [588],\n",
              "       [553],\n",
              "       [576]])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.hstack((s[0], np.expand_dims(s[1], axis=1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjIckPCE7n2z",
        "outputId": "6f993745-884a-4117-a01f-366091cee044"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  3,   4,   0, ...,   0,   1, 543],\n",
              "       [  4,   3,   1, ...,   0,   1, 558],\n",
              "       [  2,   0,   1, ...,   4,   2, 555],\n",
              "       ...,\n",
              "       [  4,   1,   1, ...,   4,   0, 588],\n",
              "       [  2,   3,   1, ...,   0,   2, 553],\n",
              "       [  0,   2,   3, ...,   4,   0, 576]])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([\n",
        "              [1,2,3,4],\n",
        "              [3,3,3,3],\n",
        "              [2,2,2,2],\n",
        "              [1,1,1,1]\n",
        "])\n",
        "print(a.mean(axis=0, keepdims=True))\n",
        "a = a - a.mean(axis=0, keepdims=True)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMUMT8x_A8I0",
        "outputId": "d722a82f-3472-4036-b49e-2ed8993d3a9c"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.75 2.   2.25 2.5 ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.75,  0.  ,  0.75,  1.5 ],\n",
              "       [ 1.25,  1.  ,  0.75,  0.5 ],\n",
              "       [ 0.25,  0.  , -0.25, -0.5 ],\n",
              "       [-0.75, -1.  , -1.25, -1.5 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "empty = [np.array([1,2]), np.array([3,4])]\n",
        "a = []\n",
        "b = a + empty + empty\n",
        "\n",
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBFhGXfSJz3l",
        "outputId": "39d0b793-1760-468d-acd3-cd4a69c8b281"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([1, 2]), array([3, 4]), array([1, 2]), array([3, 4])]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "empty = [np.array([10,20])]\n",
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJY9NttjZiM7",
        "outputId": "f86aa45b-5c38-4252-9e2c-a0fd2bc7e15e"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([1, 2]), array([3, 4]), array([1, 2]), array([3, 4])]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%wandb\n",
        "# create env\n",
        "env = make_multiple_env(**easy_config) \n",
        "# Parameter initialization\n",
        "numtrajs = 4  # num of trajecories from the current policy to collect in each iteration\n",
        "lr_pg = 1e-2  # learning rate for PG\n",
        "attention_size = 10\n",
        "iterations = 50\n",
        "discount_gamma = .99\n",
        "\n",
        "# Network initialize\n",
        "policy = Policy(lr_pg, 61, attention_size, 0.25)\n",
        "\n",
        "#To record training reward for logging and plotting purposes\n",
        "rrecord = []\n",
        "\n",
        "VAL_ALL = []  # Monte carlo value predictions of ALL trajectories (to compute baseline, and policy gradient) => 2 d list (numtrajs, #steps per traj)\n",
        "\n",
        "# For every training iteration, we roll out 5 random instances using the policy network\n",
        "# collect observation and action matrices from these 5 roll-outs, consider this a BATCH\n",
        "# train the network using this BATCH\n",
        "for ite in range(iterations):\n",
        "\n",
        "    print(\"==========================================================================================================\")\n",
        "    print(\"Outer iteration no\", ite)\n",
        "\n",
        "    # To record traectories generated from current policy\n",
        "    OBS_MAT = []  # observations: [obs_matrices_batch_traj1, obs_matrices_batch_traj2, ....]\n",
        "    ACT_MAT = []  # actions\n",
        "    ADS = []  # advantages (to compute policy gradient)\n",
        "    VAL = []  # Monte carlo value predictions (to compute baseline, and policy gradient) => 2 d list (numtrajs, #steps per traj)\n",
        "    traj_returns = []  # a list of 5 numbers, each number represents the RETURN of that roll-out\n",
        "    actions = []\n",
        "    \n",
        "    # collect some trajectories\n",
        "    for num in range(numtrajs):\n",
        "        print(\"-------------------------------------------------------------------------------------------\")\n",
        "        print(\"Running trajectories:\", num)\n",
        "        # Initialize a list of obs_matrices and act_matrices, to store all the obs_matrix and act_matrix in the trajectory\n",
        "        obs_matrices = []  # states: [obs_matrix_state1, obs_matrix_state2, ...]\n",
        "        act_matrices = []  # actions matrices\n",
        "        \n",
        "        # this is used to collect all the immedaite rewards in this trajectory\n",
        "        rews = []  # instant rewards\n",
        "        rel_rews = []\n",
        "\n",
        "        # gym loop\n",
        "        s = env.reset()   # samples a RANDOM INSTANCE every time env.reset() is called\n",
        "        done = False\n",
        "\n",
        "        # TODO: wandb logging -> what reward average should we log??\n",
        "        t = 0 # TODO: how is this used?\n",
        "        repisode = 0  # TODO: how is this used?\n",
        "\n",
        "        inner_step = 1\n",
        "        \n",
        "\n",
        "        # roll out ONE policy\n",
        "        while not done:\n",
        "\n",
        "            # TODO compute the running average RETURN as basline\n",
        "            A, b, c0, cuts_a, cuts_b = s\n",
        "\n",
        "            # choose the action according to the model output probabilities\n",
        "\n",
        "            # Concat [A,b] and [cuts_a, cuts_b]\n",
        "            assert A.shape[0] == b.shape[0]\n",
        "            assert cuts_a.shape[0] == cuts_b.shape[0]\n",
        "\n",
        "            obs_matrix = np.hstack((A, np.expand_dims(b, axis=1)))\n",
        "            act_matrix = np.hstack((cuts_a, np.expand_dims(cuts_b, axis=1)))\n",
        "\n",
        "            assert obs_matrix.shape == (A.shape[0], A.shape[1]+1)\n",
        "            assert act_matrix.shape == (cuts_a.shape[0], cuts_a.shape[1]+1)\n",
        "\n",
        "            # Normalize on a row (MIGHT NOT NEED THIS)\n",
        "            \n",
        "            # The reason we want to normalize a row: we want the numeric space of the model input to be just between 0, 1\n",
        "            # Right now I'm normalizing such that the largest number has value 1 --> each row divided by the largest num in that row\n",
        "            #   => This would result in b vector always be 1 (does it make sense?)\n",
        "            # another option is to normalize such that the SUM of the row is 1\n",
        "            #   => I think this makes more sense, consider this differentiates the max among datapoints\n",
        "            #   => According to prof in OH this might cause some information loss\n",
        "            \n",
        "            obs_matrix = obs_matrix / obs_matrix.max(axis=1, keepdims=True) * 100\n",
        "            act_matrix = act_matrix / act_matrix.max(axis=1, keepdims=True) * 100\n",
        "            \n",
        "            # print(\"DEBUGGING: obs_matrix.shape = \", obs_matrix.shape)\n",
        "            # print(\"DEBUGGING: act_matrix.shape = \", act_matrix.shape)\n",
        "\n",
        "            action_prob = policy.predict_prob(obs_matrix, act_matrix)\n",
        "            action = random.choices(range(0, len(cuts_b)), action_prob) # this returns a list\n",
        "            actions.append(action)\n",
        "\n",
        "            if  inner_step %10 == 0:\n",
        "                # TODO: print the logits as well as well.\n",
        "                print(\"DEBUGGING: the action_prob is:\", action_prob)\n",
        "                print(\"DEBUGGING: the actual action to take is:\", action)\n",
        "                logits_dbg = policy.compute_one_step_logits(obs_matrix, act_matrix)\n",
        "                print(\"DEBUGGING: logits looks like:\", logits_dbg)\n",
        "\n",
        "\n",
        "\n",
        "            # take the action in the environment\n",
        "            # TODO: why does the environment.step function takes in a list?\n",
        "            # TODO: remember to go in the environment to change the returned r to the NORMALIZED r!!\n",
        "            s, r, done, _ = env.step(action)\n",
        "            \n",
        "            # Record the observed immediate reward & observed matrices along the trajectory\n",
        "            abs_reward, rel_reward = r\n",
        "            # print(\"DEBUGGING: rel_reward looks like:\", rel_reward)\n",
        "            rews.append(abs_reward) # rews = list(len of trajectory)\n",
        "            rel_rews.append(rel_reward * 10 * inner_step)\n",
        "            obs_matrices.append(obs_matrix)\n",
        "            act_matrices.append(act_matrix)\n",
        "\n",
        "            inner_step += 1\n",
        "\n",
        "            # TODO: do we need to also record the one step of observation and action where the environment terminates?\n",
        "            #   ==> RN I'm thinking maybe don't need to\n",
        "        print(\"-------------------------------------------------------------------------------------------\")\n",
        "        #Below is for logging training performance\n",
        "        print(\"DEBUGGING: the total abs reward of the trajectory =\", np.sum(rews), \"and immediate abs rewards look like:\", rews)\n",
        "        print(\"DEBUGGING: the total relative reward of the trajectory =\", np.sum(rel_rews), \"and immediate relative rewards look like:\", rel_rews)\n",
        "\n",
        "        rrecord.append(np.sum(rews))\n",
        "\n",
        "        # After the policy roll out for this trajectory,\n",
        "        # compute the monte-carlo RETURN of this trajectory (i.e. discounted sum of rewards), add to big list\n",
        "        # TODO: one of the next steps could be: to make the basline state-dependent\n",
        "        v_hat = discounted_rewards(rel_rews, discount_gamma) # This is a list\n",
        "        traj_returns.append(v_hat[0])\n",
        "        # OBS_MAT.append(np.concatenate(obs_matrices, axis=1))\n",
        "        # ACT_MAT.append(np.concatenate(act_matrices, aixs=1))\n",
        "        VAL.append(v_hat) # VAL -> 2d list\n",
        "        VAL_ALL.append(v_hat) # VAL_ALL -> 2d list\n",
        "        OBS_MAT += obs_matrices\n",
        "        ACT_MAT += act_matrices\n",
        "\n",
        "        # TODO: do I need to specify batchsize somewhere?\n",
        "\n",
        "    print(\"+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\")\n",
        "    \n",
        "    # After collecting 5 (or however many) trajectories,\n",
        "    print(\"DEBUGGING: OBS_MAT has %d number of matrices\" % len(OBS_MAT))\n",
        "    print(\"DEBUGGING: ACT_MAT has %d number of matrices\" % len(ACT_MAT))\n",
        "    print(\"DEBUGGING: VAL looks like:\", VAL)\n",
        "    # print(\"DEBUGGING: OBS_MAT looks like:\", OBS_MAT)\n",
        "    # print(\"DEBUGGING: ACT_MAT looks like:\", ACT_MAT)\n",
        "    print(\"DEBUGGING: traj_returns =\", traj_returns)\n",
        "    print(\"DEBUGGING: actions =\", actions)\n",
        "    print(\"DEBUGGING: actions length =\", len(actions))\n",
        "\n",
        "    ## For debugging purposes, let's look at what does the model output\n",
        "    print(\"DEBUGGING: what does the model output in this round of roll-out?\")\n",
        "    obs_attention_dbg = policy.model(torch.FloatTensor(obs_matrix))\n",
        "    act_attention_dbg = policy.model(torch.FloatTensor(act_matrix))\n",
        "    logits_dbg = policy.compute_one_step_logits(obs_matrix, act_matrix)\n",
        "    print(\"DEBUGGING: obs_attention looks like:\", obs_attention_dbg)\n",
        "    print(\"DEBUGGING: act_attention looks like:\", act_attention_dbg)\n",
        "    print(\"DEBUGGING: logits looks like:\", logits_dbg)\n",
        "\n",
        "\n",
        "    assert len(traj_returns) == numtrajs\n",
        "    VAL = np.array(VAL)\n",
        "    VAL_ALL_np = np.array(VAL_ALL)\n",
        "    # 1. calculate the baseline: average return of the trajectories\n",
        "    # TODO: potentially can make this into *running average*, take into account of all the previous trajectories as well\n",
        "    # TODO: potentially make the baseline the average *VALUE* of every *state*, \n",
        "    #       but I'm not sure whether that \"state\" is useful in this concept, since the first *step*\n",
        "    #       doesn't really mean the same thing among instances\n",
        "    #       I think prof says it makes sense in the OH, also it would make more sense if you engineered the reward\n",
        "    baseline = np.mean(traj_returns)\n",
        "    baseline_2 = VAL.mean(axis=0, keepdims=True) # This is NOT a good baseline, this is only the mean of that episode, NOT running mean\n",
        "    baseline_3 = VAL_ALL_np.mean(axis=0, keepdims=True)\n",
        "    # assert baseline_2.shape == VAL.shape\n",
        "\n",
        "    # 2. Update the policy\n",
        "    ADS = (VAL - baseline_2).flatten()\n",
        "    print(\"DEBUGGING: baseline2 looks like:\", baseline_2)\n",
        "    print(\"DEBUGGING: baseline2 looks like:\", baseline)\n",
        "    print(\"DEBUGGING: ADS looks like:\", ADS)\n",
        "\n",
        "\n",
        "    # Train the agent using the batch\n",
        "    # obs_batch = np.concatenate(OBS_MAT)\n",
        "    # act_batch = np.concatenate(ACT_MAT)\n",
        "\n",
        "    assert ADS.shape[0] == len(actions)\n",
        "\n",
        "    # scaling up the rewards to artificially make bigger loss, improve learning\n",
        "\n",
        "    loss = policy.train(OBS_MAT, ACT_MAT, actions, ADS)\n",
        "\n",
        "    fixedWindow=50\n",
        "    movingAverage=0\n",
        "    if len(rrecord) >= fixedWindow:\n",
        "        movingAverage=np.mean(rrecord[len(rrecord)-fixedWindow:len(rrecord)-1])\n",
        "\n",
        "    # TODO: wandb logging\n",
        "    wandb.log({ \"training reward\" : rrecord[-1], \"training reward moving average\" : movingAverage, \"training loss\": loss})\n",
        "    #make sure to use the correct tag in wandb.init in the initialization on top\n"
      ],
      "metadata": {
        "id": "aeQHQnp1-8fR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8041acc9-5a7f-40e2-a136-de23bc7a6f35"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<wandb.jupyter.IFrame at 0x7f7879ab6890>"
            ],
            "text/html": [
              "<iframe src=\"https://wandb.ai/ieor4575-spring2022/finalproject/runs/3o250wsg?jupyter=true\" style=\"border:none;width:100%;height:420px;\"></iframe>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "        23.7486, 23.7542, 23.7418, 23.7451, 23.7442, 23.7545, 23.7531, 23.7589,\n",
            "        23.7495, 23.7504, 23.7540, 23.7500, 23.7525, 23.7506, 23.7528, 23.7518,\n",
            "        23.7528, 23.7544, 23.7595, 23.7525, 23.7626, 23.7529, 23.7524, 23.7468,\n",
            "        23.7472, 23.7486, 23.7476, 23.7574, 23.7508, 23.7568, 23.7448, 23.7484,\n",
            "        23.7522, 23.7455, 23.7521, 23.7527, 23.7578, 23.7551, 23.7256, 23.7489,\n",
            "        23.7421, 23.7532, 23.7486, 23.7508, 23.7547, 23.7479, 23.7485, 23.7509,\n",
            "        23.7502, 23.7517, 23.7512, 23.7535, 23.7545, 23.7443, 23.7521, 23.7510,\n",
            "        23.7556, 23.7502, 23.7550, 23.7478, 23.7488, 23.7523, 23.7546, 23.7523,\n",
            "        23.7544, 23.7233], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.6072478073253933 and immediate abs rewards look like: [0.1100722757364565, 0.4971755315671089, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 2.2737367544323206e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 1.5916157281026244e-12, 1.1368683772161603e-12, 4.547473508864641e-13, 0.0, 2.2737367544323206e-13, 1.1368683772161603e-12, 1.1368683772161603e-12, 2.2737367544323206e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 4.547473508864641e-13, 2.2737367544323206e-13, 6.821210263296962e-13, 4.547473508864641e-13, 6.821210263296962e-13, 4.547473508864641e-13, 2.2737367544323206e-13, 6.821210263296962e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 1.3642420526593924e-12, 1.5916157281026244e-12, 2.2737367544323206e-13, 2.2737367544323206e-13, 6.821210263296962e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 2.2737367544323206e-13, 0.0, 2.2737367544323206e-13, 4.547473508864641e-13, 0.0, 2.2737367544323206e-13, 0.0, 0.0]\n",
            "DEBUGGING: the total relative reward of the trajectory = 3.1484395400531566 and immediate relative rewards look like: [0.3051419852930756, 2.843297553007788, 9.094947017730662e-12, 6.0632980118186026e-12, 0.0, 4.547473508864641e-12, 5.305385760342483e-12, 6.063298011819521e-12, 6.821210263297478e-12, 1.5158245029548802e-11, 1.6674069532506213e-11, 3.637978807091713e-11, 6.897001488446798e-11, 5.305385760340875e-11, 2.2737367544326654e-11, 0.0, 1.2884508275116483e-11, 6.821210263296445e-11, 7.200166389037865e-11, 1.5158245029547655e-11, 1.5916157281026244e-11, 1.667406953250242e-11, 1.7431981783978483e-11, 3.6379788070914377e-11, 1.894780628693744e-11, 5.91171556152493e-11, 4.092726157977867e-11, 6.36646291241098e-11, 4.395891058568487e-11, 2.2737367544323203e-11, 7.048583938739659e-11, 2.4253192047281764e-11, 2.501110429875742e-11, 2.5769016550232966e-11, 1.591615728102745e-10, 1.9099388737224258e-10, 2.8042753304669542e-11, 2.880066555614491e-11, 8.86757334228605e-11, 1.2126596023636286e-10, 6.21488046211548e-11, 0.0, 3.259022681352746e-11, 0.0, 3.410605131648481e-11, 6.972792713591922e-11, 0.0, 3.637978807091988e-11, 0.0, 0.0]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 200 number of matrices\n",
            "DEBUGGING: ACT_MAT has 200 number of matrices\n",
            "DEBUGGING: VAL looks like: [[2.489997471179552, 2.50094477292038, 2.5158865949297287, 2.5166614061325374, 2.541730117382544, 2.5596122262252083, 2.534526639334779, 2.3904638366505315, 2.0327480874976867, 1.9265000177392215, 1.907194993240632, 1.740303905273345, 1.718932724365462, 1.726749751203701, 1.5251644850453676, 1.4048935441684356, 1.4115079354756004, 1.3547954915406304, 1.3178905920743202, 1.1917265356317104, 1.1560686320770057, 1.1653048825222347, 0.9469468869241514, 0.9354858226498038, 0.8315294312624263, 0.8365168104222097, 0.8213534098079402, 0.827812195878816, 0.833097009991051, 0.8294409804566779, 0.7338902836074037, 0.6953640630902095, 0.6816345461467018, 0.686100712788575, 0.6247574954936125, 0.6301744842930811, 0.6337360909872447, 0.5869605751938891, 0.5266345886068599, 0.5115307154691628, 0.40719065638108093, 0.3067029437818131, 0.3047889454225941, 0.26188006977079653, 0.25648922056183704, 0.10915741708410578, 0.08966315911003131, 0.08830489209145097, 0.01564239048578427, 0.0017706099276010856], [5.163046708266186, 5.210900609133369, 1.481409238871282e-09, 1.4963729685568505e-09, 1.4931142166882705e-09, 1.4967126595112239e-09, 1.484270523694986e-09, 1.467109302154493e-09, 1.4819285880348414e-09, 1.455556895409151e-09, 1.4702594903122738e-09, 1.4345831128432177e-09, 1.421513405848515e-09, 1.4060149778190788e-09, 1.4202171493122008e-09, 1.4345627770830312e-09, 1.44905331018488e-09, 1.4636902123079595e-09, 1.437134293664826e-09, 1.4080134296268708e-09, 1.4222357875018896e-09, 1.4366018055574644e-09, 1.4511129349065297e-09, 1.4657706413197271e-09, 1.4805764053734618e-09, 1.4955317225994563e-09, 1.5106381036358146e-09, 1.5258970743796106e-09, 1.5413101761410208e-09, 1.3570657358640494e-09, 1.1640701292575767e-09, 1.1758284133914916e-09, 1.1877054680722137e-09, 1.048120042706748e-09, 9.025312559649291e-10, 9.116477332979082e-10, 9.208562962605133e-10, 9.301578750106195e-10, 8.522786649921124e-10, 6.817446445922739e-10, 5.967628024393957e-10, 5.086258540481521e-10, 4.1730192965860236e-10, 4.215171006652549e-10, 4.257748491568231e-10, 4.3007560520891226e-10, 4.344198032413255e-10, 3.3086280382121864e-10, 2.23963070311558e-10, 1.1368683772159018e-10], [3.2598660656636342, 3.2902013328330453, 3.095212792712145, 2.898809322639077, 2.5532235680640682, 2.5594266999921063, 2.5292699399107663, 2.473518195162642, 2.2199560847122983, 2.2029135207806885, 2.2100787997706046, 2.1264619600421963, 2.0861394778948963, 1.7936443602514869, 1.7987385070641575, 1.804389232364777, 1.8030852843351899, 1.723528827535578, 1.691542713212981, 1.478537185922835, 1.3355989915078297, 1.2750878369357246, 1.2851721017489204, 1.2867894763868957, 1.2452304260426812, 0.9931454768408577, 0.9981723133576491, 1.0054787482089502, 1.0114045830483458, 1.0214639213661134, 1.0298972552561638, 1.0398489199805983, 1.049978317688995, 1.0521581376864324, 1.0551636691642838, 1.0633334489131605, 1.0199133481473717, 1.0237801692984982, 0.983031692607452, 0.9379611576526664, 0.9192318051136094, 0.8503138531992646, 0.7863639797680886, 0.18408301976232347, 0.15498812931154843, 0.1536091291330613, 0.1366072352981448, 0.09504118612775166, 0.0757149216878872, 0.05342987233299592], [3.1200065640850525, 2.8432975543353303, 1.3409515008698508e-09, 1.3453096503556768e-09, 1.3527740932766244e-09, 1.3664384780571965e-09, 1.3756474793417494e-09, 1.3841839329105121e-09, 1.3920410453522146e-09, 1.39921195463527e-09, 1.3980340501067888e-09, 1.3953131116911946e-09, 1.372659922848765e-09, 1.3168584928932295e-09, 1.2765703386765866e-09, 1.2664979506386464e-09, 1.279290859230956e-09, 1.2791983342988278e-09, 1.2232184158241044e-09, 1.1628452039734604e-09, 1.1592797565090028e-09, 1.1549127264929056e-09, 1.1497360171317205e-09, 1.143741449846204e-09, 1.1185471331063532e-09, 1.1107063907266827e-09, 1.0622113485974075e-09, 1.031600087896595e-09, 9.77712584618672e-10, 9.431855293262497e-10, 9.297456179615419e-10, 8.679391702769145e-10, 8.522080588178108e-10, 8.355524793121752e-10, 8.179630936989315e-10, 6.654560817057142e-10, 4.792547417509814e-10, 4.557696852993049e-10, 4.3128183812440406e-10, 3.4606677242580156e-10, 2.2707152746407948e-10, 1.6658860893224717e-10, 1.6827132215378503e-10, 1.3705161145480563e-10, 1.3843597116647033e-10, 1.0538375742422779e-10, 3.601599019021068e-11, 3.637978807091988e-11, 0.0, 0.0]]\n",
            "DEBUGGING: traj_returns = [2.489997471179552, 5.163046708266186, 3.2598660656636342, 3.1200065640850525]\n",
            "DEBUGGING: actions = [[24], [53], [20], [29], [60], [52], [40], [40], [31], [51], [46], [63], [35], [3], [17], [72], [18], [48], [24], [7], [74], [4], [53], [34], [18], [66], [26], [74], [33], [76], [85], [90], [27], [65], [11], [68], [71], [27], [71], [35], [10], [2], [38], [50], [83], [42], [70], [24], [91], [99], [54], [0], [17], [59], [54], [1], [35], [11], [22], [33], [12], [44], [62], [41], [46], [67], [14], [59], [4], [29], [46], [58], [19], [68], [63], [38], [52], [5], [20], [47], [46], [8], [77], [59], [50], [10], [68], [49], [34], [20], [38], [20], [90], [33], [49], [60], [68], [85], [93], [101], [45], [42], [7], [46], [29], [22], [44], [14], [58], [32], [5], [54], [50], [66], [44], [62], [7], [12], [53], [31], [71], [46], [29], [27], [28], [42], [13], [45], [7], [6], [57], [70], [32], [76], [59], [73], [73], [19], [32], [14], [73], [32], [14], [61], [37], [6], [23], [85], [81], [36], [17], [0], [44], [7], [48], [7], [0], [2], [53], [25], [9], [33], [42], [68], [12], [17], [4], [31], [21], [68], [54], [50], [66], [0], [9], [70], [18], [67], [57], [37], [77], [78], [50], [83], [68], [24], [77], [93], [69], [76], [37], [2], [80], [17], [101], [98], [50], [24], [26], [69]]\n",
            "DEBUGGING: actions length = 200\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[2.6854, 1.2738, 0.3162,  ..., 2.0979, 0.5664, 2.3026],\n",
            "        [2.6808, 1.2762, 0.3169,  ..., 2.0977, 0.5678, 2.3011],\n",
            "        [2.6685, 1.2474, 0.3129,  ..., 2.0777, 0.5381, 2.2722],\n",
            "        ...,\n",
            "        [2.7066, 1.2941, 0.3177,  ..., 2.1177, 0.5857, 2.3290],\n",
            "        [2.7073, 1.2946, 0.3178,  ..., 2.1183, 0.5863, 2.3297],\n",
            "        [2.7072, 1.2947, 0.3178,  ..., 2.1183, 0.5863, 2.3298]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[2.7073, 1.2948, 0.3178,  ..., 2.1183, 0.5864, 2.3300],\n",
            "        [2.7072, 1.2946, 0.3178,  ..., 2.1181, 0.5862, 2.3298],\n",
            "        [2.7078, 1.2952, 0.3179,  ..., 2.1188, 0.5868, 2.3305],\n",
            "        ...,\n",
            "        [2.7074, 1.2948, 0.3179,  ..., 2.1183, 0.5865, 2.3302],\n",
            "        [2.7076, 1.2950, 0.3179,  ..., 2.1185, 0.5867, 2.3304],\n",
            "        [2.7050, 1.2920, 0.3176,  ..., 2.1156, 0.5841, 2.3271]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([23.7514, 23.7497, 23.7570, 23.7527, 23.7501, 23.7519, 23.7469, 23.7524,\n",
            "        23.7546, 23.7509, 23.7520, 23.7532, 23.7467, 23.7591, 23.7498, 23.7622,\n",
            "        23.7503, 23.7396, 23.7531, 23.7585, 23.7491, 23.7722, 23.7560, 23.7515,\n",
            "        23.7492, 23.7493, 23.7446, 23.7529, 23.7439, 23.7540, 23.7574, 23.7492,\n",
            "        23.7540, 23.7565, 23.7522, 23.7501, 23.7575, 23.7490, 23.7560, 23.7573,\n",
            "        23.7486, 23.7542, 23.7418, 23.7451, 23.7442, 23.7545, 23.7531, 23.7589,\n",
            "        23.7495, 23.7504, 23.7540, 23.7500, 23.7525, 23.7506, 23.7528, 23.7518,\n",
            "        23.7528, 23.7544, 23.7595, 23.7525, 23.7626, 23.7529, 23.7524, 23.7468,\n",
            "        23.7472, 23.7486, 23.7476, 23.7574, 23.7508, 23.7568, 23.7448, 23.7484,\n",
            "        23.7522, 23.7455, 23.7521, 23.7527, 23.7578, 23.7551, 23.7256, 23.7489,\n",
            "        23.7421, 23.7532, 23.7486, 23.7508, 23.7547, 23.7479, 23.7485, 23.7509,\n",
            "        23.7502, 23.7517, 23.7512, 23.7535, 23.7545, 23.7443, 23.7521, 23.7510,\n",
            "        23.7556, 23.7502, 23.7550, 23.7478, 23.7488, 23.7523, 23.7546, 23.7523,\n",
            "        23.7544, 23.7233], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[3.5082292  3.46133607 1.40277485 1.35386768 1.27373842 1.27975973\n",
            "  1.26594915 1.21599551 1.06317604 1.03235339 1.02931845 0.96669147\n",
            "  0.95126805 0.88009853 0.83097575 0.80232069 0.80364831 0.76958108\n",
            "  0.75235833 0.66756593 0.62291691 0.61009818 0.55802975 0.55556883\n",
            "  0.51918996 0.45741557 0.45488143 0.45832274 0.4611254  0.46272623\n",
            "  0.44094689 0.43380325 0.43290322 0.43456471 0.41998029 0.42337698\n",
            "  0.41341236 0.40268519 0.37741657 0.36237297 0.33160562 0.2892542\n",
            "  0.27278823 0.11149077 0.10286934 0.06569164 0.0565676  0.04583652\n",
            "  0.02283933 0.01380012]]\n",
            "DEBUGGING: baseline2 looks like: 3.5082292022986064\n",
            "DEBUGGING: ADS looks like: [-1.01823173 -0.96039129  1.11311175  1.16279372  1.2679917   1.27985249\n",
            "  1.26857749  1.17446833  0.96957204  0.89414663  0.87787654  0.77361244\n",
            "  0.76766467  0.84665122  0.69418874  0.60257285  0.60785963  0.58521441\n",
            "  0.56553227  0.5241606   0.53315173  0.5552067   0.38891714  0.379917\n",
            "  0.31233947  0.37910124  0.36647198  0.36948946  0.37197161  0.36671475\n",
            "  0.2929434   0.26156082  0.24873133  0.251536    0.2047772   0.2067975\n",
            "  0.22032373  0.18427539  0.14921802  0.14915775  0.07558504  0.01744874\n",
            "  0.03200071  0.1503893   0.15361988  0.04346578  0.03309556  0.04246837\n",
            " -0.00719694 -0.01202951  1.65481751  1.74956454 -1.40277485 -1.35386768\n",
            " -1.27373842 -1.27975973 -1.26594914 -1.21599551 -1.06317604 -1.03235338\n",
            " -1.02931845 -0.96669147 -0.95126805 -0.88009853 -0.83097575 -0.80232069\n",
            " -0.8036483  -0.76958108 -0.75235833 -0.66756593 -0.62291691 -0.61009818\n",
            " -0.55802975 -0.55556882 -0.51918996 -0.45741557 -0.45488143 -0.45832274\n",
            " -0.4611254  -0.46272622 -0.44094688 -0.43380325 -0.43290322 -0.43456471\n",
            " -0.41998029 -0.42337698 -0.41341236 -0.40268519 -0.37741657 -0.36237297\n",
            " -0.33160561 -0.2892542  -0.27278823 -0.11149077 -0.10286934 -0.06569164\n",
            " -0.0565676  -0.04583652 -0.02283933 -0.01380012 -0.24836314 -0.17113473\n",
            "  1.69243795  1.54494164  1.27948515  1.27966697  1.26332079  1.25752269\n",
            "  1.15678004  1.17056014  1.18076035  1.15977049  1.13487143  0.91354583\n",
            "  0.96776276  1.00206854  0.99943698  0.95394775  0.93918439  0.81097125\n",
            "  0.71268208  0.66498966  0.72714235  0.73122065  0.72604046  0.5357299\n",
            "  0.54329088  0.54715601  0.55027918  0.5587377   0.58895037  0.60604567\n",
            "  0.6170751   0.61759342  0.63518338  0.63995647  0.60650099  0.62109498\n",
            "  0.60561512  0.57558819  0.58762619  0.56105965  0.51357575  0.07259225\n",
            "  0.05211879  0.08791749  0.08003964  0.04920467  0.05287559  0.03962975\n",
            " -0.38822264 -0.61803851 -1.40277485 -1.35386768 -1.27373842 -1.27975973\n",
            " -1.26594914 -1.21599551 -1.06317604 -1.03235338 -1.02931845 -0.96669147\n",
            " -0.95126805 -0.88009853 -0.83097575 -0.80232069 -0.8036483  -0.76958108\n",
            " -0.75235833 -0.66756593 -0.62291691 -0.61009818 -0.55802975 -0.55556882\n",
            " -0.51918996 -0.45741557 -0.45488143 -0.45832274 -0.4611254  -0.46272623\n",
            " -0.44094688 -0.43380325 -0.43290322 -0.43456471 -0.41998029 -0.42337698\n",
            " -0.41341236 -0.40268519 -0.37741657 -0.36237297 -0.33160562 -0.2892542\n",
            " -0.27278823 -0.11149077 -0.10286934 -0.06569164 -0.0565676  -0.04583652\n",
            " -0.02283933 -0.01380012]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(0.0139, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 3.1516e-02,  1.4327e-02, -1.0041e-03,  ..., -3.9302e-03,\n",
            "          2.8231e-02,  7.9270e-02],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 4.2378e-02,  1.9318e-02, -1.2456e-03,  ..., -5.2202e-03,\n",
            "          3.7972e-02,  1.2291e-01],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-5.7162e-03, -2.6357e-03,  9.8562e-05,  ...,  6.6859e-04,\n",
            "         -5.1298e-03, -2.5506e-02]])\n",
            "   Last layer:\n",
            "tensor([[ 0.0000e+00, -1.7435e-02,  0.0000e+00,  2.7282e-02,  0.0000e+00,\n",
            "          1.1010e-05,  0.0000e+00,  3.4400e-02,  1.4336e-02,  0.0000e+00,\n",
            "          2.7209e-03,  0.0000e+00,  0.0000e+00, -1.2989e-03,  0.0000e+00,\n",
            "         -2.2621e-03,  0.0000e+00,  2.4990e-02,  1.4361e-02,  0.0000e+00],\n",
            "        [ 0.0000e+00, -6.7985e-03,  0.0000e+00,  1.8206e-02,  0.0000e+00,\n",
            "          1.3050e-05,  0.0000e+00,  3.0348e-02,  1.3807e-02,  0.0000e+00,\n",
            "          7.1282e-03,  0.0000e+00,  0.0000e+00,  2.1265e-03,  0.0000e+00,\n",
            "          4.6934e-03,  0.0000e+00,  2.2299e-02,  1.4003e-02,  0.0000e+00],\n",
            "        [ 0.0000e+00, -2.1009e-03,  0.0000e+00,  3.4986e-03,  0.0000e+00,\n",
            "          2.0602e-06,  0.0000e+00,  4.6278e-03,  1.9566e-03,  0.0000e+00,\n",
            "          5.0247e-04,  0.0000e+00,  0.0000e+00, -8.1567e-05,  0.0000e+00,\n",
            "         -1.1006e-04,  0.0000e+00,  3.3657e-03,  1.9660e-03,  0.0000e+00],\n",
            "        [ 0.0000e+00, -4.2993e-03,  0.0000e+00,  5.9450e-03,  0.0000e+00,\n",
            "          4.1963e-06,  0.0000e+00,  6.7720e-03,  2.6680e-03,  0.0000e+00,\n",
            "          5.9981e-05,  0.0000e+00,  0.0000e+00, -5.8712e-04,  0.0000e+00,\n",
            "         -1.0991e-03,  0.0000e+00,  4.8703e-03,  2.6972e-03,  0.0000e+00],\n",
            "        [ 0.0000e+00, -2.6038e-03,  0.0000e+00,  5.3349e-03,  0.0000e+00,\n",
            "          1.3688e-06,  0.0000e+00,  7.9304e-03,  3.5186e-03,  0.0000e+00,\n",
            "          1.4011e-03,  0.0000e+00,  0.0000e+00,  2.4222e-04,  0.0000e+00,\n",
            "          5.6533e-04,  0.0000e+00,  5.8139e-03,  3.5279e-03,  0.0000e+00],\n",
            "        [ 0.0000e+00,  2.8625e-04,  0.0000e+00, -5.9512e-03,  0.0000e+00,\n",
            "         -6.2894e-06,  0.0000e+00, -1.2873e-02, -6.2153e-03,  0.0000e+00,\n",
            "         -4.4560e-03,  0.0000e+00,  0.0000e+00, -1.8920e-03,  0.0000e+00,\n",
            "         -4.0120e-03,  0.0000e+00, -9.5405e-03, -6.3427e-03,  0.0000e+00],\n",
            "        [ 0.0000e+00, -1.3225e-02,  0.0000e+00,  1.6454e-02,  0.0000e+00,\n",
            "          6.1622e-06,  0.0000e+00,  1.6650e-02,  6.2416e-03,  0.0000e+00,\n",
            "         -1.3218e-03,  0.0000e+00,  0.0000e+00, -2.4541e-03,  0.0000e+00,\n",
            "         -4.7960e-03,  0.0000e+00,  1.1930e-02,  6.2189e-03,  0.0000e+00],\n",
            "        [ 0.0000e+00, -1.2566e-02,  0.0000e+00,  2.3487e-02,  0.0000e+00,\n",
            "          1.2455e-05,  0.0000e+00,  3.3340e-02,  1.4472e-02,  0.0000e+00,\n",
            "          5.0193e-03,  0.0000e+00,  0.0000e+00,  3.9458e-04,  0.0000e+00,\n",
            "          1.1867e-03,  0.0000e+00,  2.4348e-02,  1.4597e-02,  0.0000e+00],\n",
            "        [ 0.0000e+00, -2.8919e-03,  0.0000e+00,  1.3244e-02,  0.0000e+00,\n",
            "          1.2745e-05,  0.0000e+00,  2.5239e-02,  1.1860e-02,  0.0000e+00,\n",
            "          7.4667e-03,  0.0000e+00,  0.0000e+00,  2.8224e-03,  0.0000e+00,\n",
            "          6.0557e-03,  0.0000e+00,  1.8633e-02,  1.2056e-02,  0.0000e+00],\n",
            "        [ 0.0000e+00, -1.4327e-02,  0.0000e+00,  2.9073e-02,  0.0000e+00,\n",
            "          1.5197e-05,  0.0000e+00,  4.3144e-02,  1.9027e-02,  0.0000e+00,\n",
            "          7.5719e-03,  0.0000e+00,  0.0000e+00,  1.2495e-03,  0.0000e+00,\n",
            "          3.0210e-03,  0.0000e+00,  3.1580e-02,  1.9161e-02,  0.0000e+00]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 1.7434e-03,  4.5702e-04,  4.3806e-04,  ...,  1.5912e-03,\n",
            "         -1.7317e-03,  1.8677e-02],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 2.3525e-03,  6.1917e-04,  5.9392e-04,  ...,  2.1475e-03,\n",
            "         -2.3279e-03,  2.6411e-02],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-2.4219e-04, -6.8556e-05, -6.5540e-05,  ..., -2.2225e-04,\n",
            "          2.2690e-04, -4.5594e-03]])\n",
            "   Last layer:\n",
            "tensor([[ 0.0000e+00, -1.3084e-03,  0.0000e+00,  5.3738e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  8.6797e-03,  1.9355e-03,  0.0000e+00,\n",
            "          2.5416e-03,  0.0000e+00,  0.0000e+00,  7.2174e-04,  0.0000e+00,\n",
            "          3.4863e-03,  0.0000e+00,  5.3457e-03,  5.8730e-03,  0.0000e+00],\n",
            "        [ 0.0000e+00, -2.2501e-04,  0.0000e+00,  3.3804e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  6.7522e-03,  2.2420e-03,  0.0000e+00,\n",
            "          2.6075e-03,  0.0000e+00,  0.0000e+00,  8.5750e-04,  0.0000e+00,\n",
            "          3.0118e-03,  0.0000e+00,  4.5232e-03,  4.1149e-03,  0.0000e+00],\n",
            "        [ 0.0000e+00, -1.4663e-04,  0.0000e+00,  6.4938e-04,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  1.0734e-03,  2.5362e-04,  0.0000e+00,\n",
            "          3.2607e-04,  0.0000e+00,  0.0000e+00,  9.4995e-05,  0.0000e+00,\n",
            "          4.3670e-04,  0.0000e+00,  6.6795e-04,  7.1756e-04,  0.0000e+00],\n",
            "        [ 0.0000e+00, -1.0038e-04,  0.0000e+00,  1.7170e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  3.4595e-03,  1.1629e-03,  0.0000e+00,\n",
            "          1.3475e-03,  0.0000e+00,  0.0000e+00,  4.4571e-04,  0.0000e+00,\n",
            "          1.5479e-03,  0.0000e+00,  2.3241e-03,  2.0995e-03,  0.0000e+00],\n",
            "        [ 0.0000e+00, -2.8363e-04,  0.0000e+00,  6.8945e-04,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  8.6381e-04,  4.9536e-05,  0.0000e+00,\n",
            "          1.3130e-04,  0.0000e+00,  0.0000e+00,  1.3972e-05,  0.0000e+00,\n",
            "          2.8977e-04,  0.0000e+00,  4.6147e-04,  6.7247e-04,  0.0000e+00],\n",
            "        [ 0.0000e+00, -1.4133e-04,  0.0000e+00, -1.0072e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -2.4622e-03, -1.0256e-03,  0.0000e+00,\n",
            "         -1.1281e-03,  0.0000e+00,  0.0000e+00, -3.9662e-04,  0.0000e+00,\n",
            "         -1.1818e-03,  0.0000e+00, -1.7522e-03, -1.3724e-03,  0.0000e+00],\n",
            "        [ 0.0000e+00, -8.8549e-04,  0.0000e+00,  4.0029e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  6.6563e-03,  1.5946e-03,  0.0000e+00,\n",
            "          2.0422e-03,  0.0000e+00,  0.0000e+00,  5.9863e-04,  0.0000e+00,\n",
            "          2.7169e-03,  0.0000e+00,  4.1535e-03,  4.4362e-03,  0.0000e+00],\n",
            "        [ 0.0000e+00, -7.6987e-04,  0.0000e+00,  4.7231e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  8.4495e-03,  2.3525e-03,  0.0000e+00,\n",
            "          2.8745e-03,  0.0000e+00,  0.0000e+00,  8.9122e-04,  0.0000e+00,\n",
            "          3.5837e-03,  0.0000e+00,  5.4357e-03,  5.4285e-03,  0.0000e+00],\n",
            "        [ 0.0000e+00,  1.3856e-04,  0.0000e+00,  2.0200e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  4.6258e-03,  1.8076e-03,  0.0000e+00,\n",
            "          2.0192e-03,  0.0000e+00,  0.0000e+00,  6.9630e-04,  0.0000e+00,\n",
            "          2.1743e-03,  0.0000e+00,  3.2338e-03,  2.6512e-03,  0.0000e+00],\n",
            "        [ 0.0000e+00, -1.0015e-03,  0.0000e+00,  4.8688e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  8.2621e-03,  2.0681e-03,  0.0000e+00,\n",
            "          2.6135e-03,  0.0000e+00,  0.0000e+00,  7.7700e-04,  0.0000e+00,\n",
            "          3.4120e-03,  0.0000e+00,  5.2007e-03,  5.4509e-03,  0.0000e+00]])\n",
            "DEBUGGING: training for one iteration takes 0.004549 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 18\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0246, 0.0139, 0.0142, 0.0140, 0.0143, 0.0145, 0.0149, 0.0158, 0.0148,\n",
            "        0.0130, 0.0151, 0.0143, 0.0147, 0.0145, 0.0144, 0.0141, 0.0137, 0.0142,\n",
            "        0.0142, 0.0145, 0.0148, 0.0143, 0.0153, 0.0148, 0.0145, 0.0148, 0.0148,\n",
            "        0.0149, 0.0142, 0.0144, 0.0141, 0.0144, 0.0138, 0.0146, 0.0150, 0.0132,\n",
            "        0.0136, 0.0145, 0.0143, 0.0147, 0.0144, 0.0143, 0.0148, 0.0143, 0.0151,\n",
            "        0.0143, 0.0144, 0.0136, 0.0144, 0.0147, 0.0141, 0.0143, 0.0139, 0.0139,\n",
            "        0.0149, 0.0141, 0.0148, 0.0142, 0.0140, 0.0137, 0.0138, 0.0146, 0.0144,\n",
            "        0.0139, 0.0145, 0.0137, 0.0148, 0.0141, 0.0136],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [20]\n",
            "DEBUGGING: logits looks like: tensor([21.4647, 21.3231, 21.3279, 21.3237, 21.3298, 21.3329, 21.3404, 21.3541,\n",
            "        21.3382, 21.3047, 21.3432, 21.3299, 21.3356, 21.3328, 21.3306, 21.3252,\n",
            "        21.3185, 21.3270, 21.3282, 21.3322, 21.3377, 21.3287, 21.3456, 21.3374,\n",
            "        21.3328, 21.3386, 21.3380, 21.3393, 21.3282, 21.3315, 21.3257, 21.3306,\n",
            "        21.3205, 21.3343, 21.3407, 21.3085, 21.3170, 21.3331, 21.3298, 21.3365,\n",
            "        21.3317, 21.3290, 21.3377, 21.3302, 21.3423, 21.3288, 21.3311, 21.3171,\n",
            "        21.3318, 21.3361, 21.3268, 21.3298, 21.3220, 21.3230, 21.3401, 21.3262,\n",
            "        21.3386, 21.3279, 21.3245, 21.3183, 21.3213, 21.3351, 21.3304, 21.3221,\n",
            "        21.3331, 21.3180, 21.3380, 21.3256, 21.3166], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0216, 0.0129, 0.0126, 0.0124, 0.0122, 0.0123, 0.0123, 0.0125, 0.0124,\n",
            "        0.0119, 0.0126, 0.0122, 0.0127, 0.0124, 0.0122, 0.0124, 0.0124, 0.0125,\n",
            "        0.0125, 0.0124, 0.0128, 0.0123, 0.0125, 0.0120, 0.0123, 0.0122, 0.0124,\n",
            "        0.0124, 0.0123, 0.0145, 0.0121, 0.0126, 0.0125, 0.0120, 0.0124, 0.0124,\n",
            "        0.0122, 0.0126, 0.0122, 0.0129, 0.0125, 0.0130, 0.0125, 0.0124, 0.0121,\n",
            "        0.0121, 0.0123, 0.0128, 0.0122, 0.0123, 0.0125, 0.0127, 0.0116, 0.0126,\n",
            "        0.0122, 0.0117, 0.0127, 0.0123, 0.0123, 0.0126, 0.0127, 0.0126, 0.0120,\n",
            "        0.0123, 0.0123, 0.0121, 0.0128, 0.0121, 0.0126, 0.0123, 0.0122, 0.0122,\n",
            "        0.0119, 0.0118, 0.0119, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [13]\n",
            "DEBUGGING: logits looks like: tensor([21.4575, 21.3287, 21.3231, 21.3185, 21.3153, 21.3168, 21.3176, 21.3209,\n",
            "        21.3187, 21.3094, 21.3224, 21.3155, 21.3247, 21.3188, 21.3158, 21.3196,\n",
            "        21.3194, 21.3204, 21.3217, 21.3195, 21.3275, 21.3165, 21.3218, 21.3112,\n",
            "        21.3169, 21.3149, 21.3195, 21.3198, 21.3178, 21.3583, 21.3135, 21.3228,\n",
            "        21.3206, 21.3113, 21.3195, 21.3195, 21.3148, 21.3229, 21.3144, 21.3298,\n",
            "        21.3207, 21.3317, 21.3212, 21.3195, 21.3130, 21.3137, 21.3168, 21.3272,\n",
            "        21.3146, 21.3179, 21.3209, 21.3261, 21.3025, 21.3241, 21.3145, 21.3044,\n",
            "        21.3247, 21.3169, 21.3169, 21.3227, 21.3249, 21.3240, 21.3108, 21.3182,\n",
            "        21.3170, 21.3141, 21.3282, 21.3134, 21.3230, 21.3182, 21.3151, 21.3152,\n",
            "        21.3084, 21.3078, 21.3091, 21.3198, 21.3184, 21.3185, 21.3192, 21.3185],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0194, 0.0111, 0.0110, 0.0110, 0.0111, 0.0110, 0.0111, 0.0110, 0.0111,\n",
            "        0.0109, 0.0111, 0.0110, 0.0111, 0.0110, 0.0111, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0111, 0.0110, 0.0110, 0.0111, 0.0110, 0.0110, 0.0111,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0109, 0.0110, 0.0110,\n",
            "        0.0111, 0.0109, 0.0109, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0111, 0.0111, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0111, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0111,\n",
            "        0.0110, 0.0112, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0109, 0.0110, 0.0110,\n",
            "        0.0111, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0111],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [76]\n",
            "DEBUGGING: logits looks like: tensor([21.4512, 21.3104, 21.3082, 21.3089, 21.3103, 21.3097, 21.3110, 21.3092,\n",
            "        21.3103, 21.3057, 21.3120, 21.3088, 21.3105, 21.3086, 21.3106, 21.3085,\n",
            "        21.3099, 21.3081, 21.3097, 21.3097, 21.3103, 21.3085, 21.3099, 21.3108,\n",
            "        21.3096, 21.3089, 21.3101, 21.3093, 21.3091, 21.3099, 21.3093, 21.3084,\n",
            "        21.3090, 21.3061, 21.3087, 21.3096, 21.3106, 21.3074, 21.3077, 21.3094,\n",
            "        21.3087, 21.3094, 21.3099, 21.3086, 21.3095, 21.3092, 21.3098, 21.3087,\n",
            "        21.3095, 21.3104, 21.3101, 21.3093, 21.3096, 21.3089, 21.3100, 21.3103,\n",
            "        21.3088, 21.3085, 21.3092, 21.3095, 21.3083, 21.3088, 21.3102, 21.3083,\n",
            "        21.3124, 21.3090, 21.3099, 21.3097, 21.3086, 21.3085, 21.3091, 21.3101,\n",
            "        21.3093, 21.3088, 21.3080, 21.3095, 21.3100, 21.3098, 21.3078, 21.3095,\n",
            "        21.3098, 21.3101, 21.3094, 21.3086, 21.3094, 21.3095, 21.3091, 21.3090,\n",
            "        21.3089, 21.3104], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0176, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0101, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0101, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0101, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0101, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [71]\n",
            "DEBUGGING: logits looks like: tensor([21.4464, 21.3057, 21.3058, 21.3057, 21.3056, 21.3058, 21.3058, 21.3056,\n",
            "        21.3056, 21.3053, 21.3065, 21.3057, 21.3057, 21.3056, 21.3057, 21.3057,\n",
            "        21.3057, 21.3060, 21.3055, 21.3057, 21.3054, 21.3054, 21.3059, 21.3055,\n",
            "        21.3059, 21.3058, 21.3053, 21.3057, 21.3056, 21.3053, 21.3057, 21.3057,\n",
            "        21.3063, 21.3059, 21.3058, 21.3058, 21.3057, 21.3052, 21.3055, 21.3057,\n",
            "        21.3053, 21.3050, 21.3062, 21.3055, 21.3056, 21.3055, 21.3058, 21.3056,\n",
            "        21.3054, 21.3058, 21.3055, 21.3054, 21.3053, 21.3055, 21.3059, 21.3065,\n",
            "        21.3059, 21.3056, 21.3057, 21.3057, 21.3058, 21.3063, 21.3059, 21.3056,\n",
            "        21.3059, 21.3056, 21.3055, 21.3058, 21.3056, 21.3058, 21.3054, 21.3054,\n",
            "        21.3059, 21.3051, 21.3059, 21.3057, 21.3053, 21.3058, 21.3056, 21.3060,\n",
            "        21.3059, 21.3056, 21.3067, 21.3057, 21.3054, 21.3056, 21.3057, 21.3060,\n",
            "        21.3055, 21.3053, 21.3059, 21.3055, 21.3053, 21.3059, 21.3059, 21.3055,\n",
            "        21.3057, 21.3054, 21.3060], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0162, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0091,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [42]\n",
            "DEBUGGING: logits looks like: tensor([21.4424, 21.3018, 21.3018, 21.3021, 21.3022, 21.3014, 21.3017, 21.3017,\n",
            "        21.3018, 21.3015, 21.3021, 21.3010, 21.3016, 21.3012, 21.3020, 21.3018,\n",
            "        21.3015, 21.3011, 21.3021, 21.3015, 21.3015, 21.3015, 21.3015, 21.3015,\n",
            "        21.3009, 21.3018, 21.3018, 21.3013, 21.3012, 21.3017, 21.3019, 21.3013,\n",
            "        21.3022, 21.3015, 21.3018, 21.3020, 21.3019, 21.3018, 21.3019, 21.3012,\n",
            "        21.3017, 21.3017, 21.3018, 21.3012, 21.3001, 21.3017, 21.3005, 21.3019,\n",
            "        21.3017, 21.3012, 21.3014, 21.3014, 21.3018, 21.3017, 21.3017, 21.3015,\n",
            "        21.3013, 21.3013, 21.3014, 21.3021, 21.3021, 21.3018, 21.3017, 21.3017,\n",
            "        21.3017, 21.3017, 21.3015, 21.3016, 21.3016, 21.3017, 21.3012, 21.3015,\n",
            "        21.3016, 21.3016, 21.3020, 21.3015, 21.3015, 21.3018, 21.3014, 21.3012,\n",
            "        21.3015, 21.3021, 21.3013, 21.3018, 21.3024, 21.3016, 21.3017, 21.3013,\n",
            "        21.3012, 21.3018, 21.3014, 21.3018, 21.3015, 21.3006, 21.3019, 21.3019,\n",
            "        21.3007, 21.3018, 21.3018, 21.3016, 21.3020, 21.3017, 21.3005, 21.3014,\n",
            "        21.3017, 21.3008, 21.3021, 21.3017], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.13340626068020356 and immediate abs rewards look like: [0.02136458493760074, 0.029428839689899178, 0.004145160439747997, 0.009380703599163098, 0.012635715997930674, 0.0017380872536705283, 0.006922610827132303, 0.01989724057739295, 0.0006191073980517103, 3.4727491311059566e-05, 0.001243805911144591, 0.0012929214149153267, 0.0001491880698267778, 5.9990422869304894e-05, 0.002604495938612672, 5.572824693445e-05, 0.000351256632256991, 0.01047218602934663, 0.0005332905921022757, 0.0014240461559893447, 0.002807337948524946, 2.021175578192924e-05, 0.00016462518578919116, 1.7480176211392973e-05, 0.0010211282228738128, 0.0008812249143375084, 0.0019433355237197247, 2.4337205104529858e-05, 5.090208196634194e-05, 0.00030061149345783633, 8.061646394708077e-05, 0.00010626719858919387, 0.00033338494949930464, 7.965372788021341e-07, 0.00048521000644541346, 2.0591022803273518e-05, 5.8001489378511906e-05, 6.7646524257725105e-06, 1.6426936781499535e-05, 0.0004902025598312321, 3.05319158542261e-05, 4.1482539472781355e-05, 1.419829459337052e-06, 1.0797839422593825e-06, 2.7979618607787415e-05, 1.0900121196755208e-05, 1.988328176594223e-05, 1.5020728824310936e-05, 6.754280866516638e-05, 7.276101769093657e-06]\n",
            "DEBUGGING: the total relative reward of the trajectory = 2.6730228421322213 and immediate relative rewards look like: [0.057613420551555806, 0.15963998929000478, 0.034000210189639835, 0.10270861908344381, 0.17337957108306254, 0.02871834852304813, 0.1335096059532308, 0.43939641271867835, 0.015465860543521262, 0.0009640812462685946, 0.0379830392347976, 0.043087150464689236, 0.005388002784563361, 0.0023333415960725586, 0.1085401821786074, 0.002479050287457228, 0.016602372583866508, 0.5241420110496227, 0.02825680842849606, 0.07943725686093452, 0.1644965137888297, 0.0012416801932906899, 0.010573276656298173, 0.0011715544962654133, 0.07128987538985407, 0.06400170492005333, 0.1466053776358404, 0.0019050350008806682, 0.004126774368302241, 0.02521216825095469, 0.0069872341855510365, 0.009507774213001814, 0.030761144030904526, 7.57300182329325e-05, 0.047487677302514235, 0.0020731106562425863, 0.006001854106409226, 0.0007189202110808717, 0.0017917343670863562, 0.054839058810945224, 0.0035014818919202966, 0.004873402768603541, 0.0001707762048171725, 0.00013289616025110633, 0.003521902486739749, 0.001402541048213107, 0.0026140486813227464, 0.0020167980131227414, 0.009257786962436102, 0.0010176746606960034]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0213, 0.0143, 0.0179, 0.0160, 0.0126, 0.0194, 0.0138, 0.0128, 0.0135,\n",
            "        0.0130, 0.0098, 0.0137, 0.0138, 0.0161, 0.0127, 0.0123, 0.0132, 0.0132,\n",
            "        0.0121, 0.0125, 0.0135, 0.0238, 0.0148, 0.0151, 0.0136, 0.0178, 0.0138,\n",
            "        0.0147, 0.0143, 0.0147, 0.0146, 0.0153, 0.0143, 0.0128, 0.0143, 0.0136,\n",
            "        0.0131, 0.0131, 0.0136, 0.0140, 0.0137, 0.0130, 0.0143, 0.0141, 0.0200,\n",
            "        0.0143, 0.0154, 0.0125, 0.0127, 0.0186, 0.0154, 0.0145, 0.0162, 0.0123,\n",
            "        0.0130, 0.0141, 0.0215, 0.0154, 0.0131, 0.0126, 0.0141, 0.0155, 0.0137,\n",
            "        0.0110, 0.0177, 0.0149, 0.0127, 0.0249], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [18]\n",
            "DEBUGGING: logits looks like: tensor([21.4555, 21.3553, 21.4116, 21.3836, 21.3239, 21.4320, 21.3463, 21.3292,\n",
            "        21.3418, 21.3326, 21.2617, 21.3456, 21.3475, 21.3861, 21.3262, 21.3187,\n",
            "        21.3363, 21.3356, 21.3149, 21.3228, 21.3421, 21.4834, 21.3652, 21.3695,\n",
            "        21.3432, 21.4104, 21.3479, 21.3623, 21.3555, 21.3623, 21.3610, 21.3731,\n",
            "        21.3559, 21.3285, 21.3556, 21.3441, 21.3337, 21.3338, 21.3430, 21.3509,\n",
            "        21.3455, 21.3331, 21.3562, 21.3532, 21.4398, 21.3554, 21.3745, 21.3214,\n",
            "        21.3266, 21.4215, 21.3752, 21.3600, 21.3872, 21.3181, 21.3325, 21.3517,\n",
            "        21.4583, 21.3746, 21.3345, 21.3241, 21.3531, 21.3758, 21.3462, 21.2915,\n",
            "        21.4099, 21.3663, 21.3262, 21.4944], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0179, 0.0128, 0.0126, 0.0129, 0.0125, 0.0126, 0.0129, 0.0129, 0.0126,\n",
            "        0.0127, 0.0124, 0.0124, 0.0127, 0.0126, 0.0128, 0.0125, 0.0122, 0.0136,\n",
            "        0.0129, 0.0128, 0.0126, 0.0128, 0.0125, 0.0126, 0.0126, 0.0130, 0.0129,\n",
            "        0.0126, 0.0129, 0.0126, 0.0127, 0.0130, 0.0126, 0.0124, 0.0129, 0.0135,\n",
            "        0.0131, 0.0129, 0.0135, 0.0130, 0.0127, 0.0127, 0.0129, 0.0127, 0.0134,\n",
            "        0.0125, 0.0131, 0.0127, 0.0129, 0.0126, 0.0124, 0.0123, 0.0130, 0.0125,\n",
            "        0.0127, 0.0126, 0.0134, 0.0130, 0.0130, 0.0126, 0.0123, 0.0127, 0.0130,\n",
            "        0.0130, 0.0127, 0.0130, 0.0125, 0.0131, 0.0124, 0.0126, 0.0126, 0.0130,\n",
            "        0.0127, 0.0126, 0.0123, 0.0126, 0.0126, 0.0129],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [44]\n",
            "DEBUGGING: logits looks like: tensor([21.4237, 21.3403, 21.3358, 21.3412, 21.3333, 21.3354, 21.3415, 21.3415,\n",
            "        21.3359, 21.3370, 21.3313, 21.3310, 21.3379, 21.3359, 21.3397, 21.3337,\n",
            "        21.3286, 21.3547, 21.3416, 21.3395, 21.3361, 21.3394, 21.3349, 21.3357,\n",
            "        21.3362, 21.3430, 21.3420, 21.3350, 21.3411, 21.3360, 21.3385, 21.3432,\n",
            "        21.3364, 21.3320, 21.3419, 21.3527, 21.3459, 21.3423, 21.3529, 21.3443,\n",
            "        21.3374, 21.3373, 21.3422, 21.3372, 21.3506, 21.3346, 21.3459, 21.3380,\n",
            "        21.3413, 21.3365, 21.3309, 21.3307, 21.3436, 21.3342, 21.3370, 21.3367,\n",
            "        21.3504, 21.3438, 21.3432, 21.3349, 21.3307, 21.3372, 21.3439, 21.3431,\n",
            "        21.3375, 21.3444, 21.3347, 21.3462, 21.3319, 21.3368, 21.3362, 21.3432,\n",
            "        21.3373, 21.3359, 21.3291, 21.3352, 21.3362, 21.3409],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0153, 0.0115, 0.0113, 0.0114, 0.0115, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0115, 0.0116, 0.0116, 0.0113, 0.0114, 0.0115, 0.0116,\n",
            "        0.0114, 0.0114, 0.0113, 0.0113, 0.0117, 0.0117, 0.0114, 0.0112, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0110, 0.0114, 0.0117, 0.0114, 0.0115,\n",
            "        0.0115, 0.0116, 0.0115, 0.0113, 0.0115, 0.0114, 0.0115, 0.0115, 0.0115,\n",
            "        0.0116, 0.0115, 0.0115, 0.0114, 0.0115, 0.0114, 0.0112, 0.0115, 0.0114,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0114, 0.0114, 0.0113, 0.0116, 0.0121,\n",
            "        0.0113, 0.0113, 0.0113, 0.0116, 0.0114, 0.0114, 0.0114, 0.0116, 0.0116,\n",
            "        0.0115, 0.0114, 0.0115, 0.0114, 0.0115, 0.0113, 0.0115, 0.0113, 0.0114,\n",
            "        0.0114, 0.0116, 0.0113, 0.0116, 0.0114, 0.0116],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [39]\n",
            "DEBUGGING: logits looks like: tensor([21.4120, 21.3410, 21.3369, 21.3393, 21.3397, 21.3392, 21.3379, 21.3385,\n",
            "        21.3376, 21.3387, 21.3385, 21.3402, 21.3426, 21.3419, 21.3371, 21.3378,\n",
            "        21.3408, 21.3420, 21.3390, 21.3379, 21.3365, 21.3359, 21.3444, 21.3445,\n",
            "        21.3378, 21.3349, 21.3388, 21.3387, 21.3384, 21.3389, 21.3393, 21.3300,\n",
            "        21.3373, 21.3440, 21.3373, 21.3400, 21.3408, 21.3416, 21.3402, 21.3367,\n",
            "        21.3415, 21.3385, 21.3406, 21.3401, 21.3410, 21.3437, 21.3408, 21.3398,\n",
            "        21.3377, 21.3402, 21.3377, 21.3342, 21.3395, 21.3389, 21.3414, 21.3400,\n",
            "        21.3404, 21.3411, 21.3380, 21.3391, 21.3357, 21.3431, 21.3533, 21.3371,\n",
            "        21.3360, 21.3353, 21.3425, 21.3389, 21.3382, 21.3373, 21.3426, 21.3421,\n",
            "        21.3398, 21.3389, 21.3394, 21.3386, 21.3409, 21.3352, 21.3399, 21.3367,\n",
            "        21.3376, 21.3393, 21.3416, 21.3361, 21.3422, 21.3377, 21.3426],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0146, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0103, 0.0101, 0.0101,\n",
            "        0.0104, 0.0102, 0.0100, 0.0101, 0.0100, 0.0100, 0.0106, 0.0103, 0.0100,\n",
            "        0.0101, 0.0101, 0.0103, 0.0101, 0.0104, 0.0101, 0.0103, 0.0102, 0.0100,\n",
            "        0.0103, 0.0101, 0.0101, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0101,\n",
            "        0.0102, 0.0101, 0.0102, 0.0104, 0.0102, 0.0105, 0.0100, 0.0101, 0.0102,\n",
            "        0.0101, 0.0103, 0.0103, 0.0101, 0.0102, 0.0100, 0.0100, 0.0101, 0.0102,\n",
            "        0.0100, 0.0100, 0.0102, 0.0101, 0.0103, 0.0101, 0.0102, 0.0101, 0.0104,\n",
            "        0.0101, 0.0104, 0.0102, 0.0101, 0.0103, 0.0101, 0.0100, 0.0102, 0.0101,\n",
            "        0.0102, 0.0101, 0.0101, 0.0101, 0.0102, 0.0101, 0.0103, 0.0102, 0.0102,\n",
            "        0.0101, 0.0100, 0.0102, 0.0101, 0.0101, 0.0101, 0.0101, 0.0102, 0.0103,\n",
            "        0.0101, 0.0101, 0.0102, 0.0102, 0.0101, 0.0101, 0.0102, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [18]\n",
            "DEBUGGING: logits looks like: tensor([21.4243, 21.3321, 21.3307, 21.3315, 21.3312, 21.3309, 21.3362, 21.3330,\n",
            "        21.3309, 21.3393, 21.3334, 21.3304, 21.3316, 21.3291, 21.3290, 21.3431,\n",
            "        21.3368, 21.3302, 21.3315, 21.3311, 21.3360, 21.3318, 21.3379, 21.3330,\n",
            "        21.3378, 21.3336, 21.3303, 21.3360, 21.3314, 21.3314, 21.3342, 21.3350,\n",
            "        21.3332, 21.3350, 21.3343, 21.3318, 21.3342, 21.3320, 21.3339, 21.3401,\n",
            "        21.3341, 21.3405, 21.3298, 21.3308, 21.3333, 21.3315, 21.3360, 21.3359,\n",
            "        21.3320, 21.3344, 21.3293, 21.3298, 21.3327, 21.3344, 21.3301, 21.3290,\n",
            "        21.3354, 21.3330, 21.3358, 21.3321, 21.3332, 21.3316, 21.3384, 21.3327,\n",
            "        21.3391, 21.3354, 21.3308, 21.3361, 21.3320, 21.3292, 21.3344, 21.3317,\n",
            "        21.3334, 21.3322, 21.3322, 21.3324, 21.3354, 21.3307, 21.3362, 21.3340,\n",
            "        21.3332, 21.3316, 21.3305, 21.3336, 21.3317, 21.3310, 21.3313, 21.3316,\n",
            "        21.3336, 21.3365, 21.3329, 21.3327, 21.3337, 21.3333, 21.3329, 21.3320,\n",
            "        21.3341, 21.3320], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0137, 0.0094, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095,\n",
            "        0.0095, 0.0095, 0.0094, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095,\n",
            "        0.0095, 0.0095, 0.0095, 0.0095, 0.0094, 0.0095, 0.0095, 0.0095, 0.0094,\n",
            "        0.0096, 0.0096, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095,\n",
            "        0.0095, 0.0095, 0.0094, 0.0094, 0.0094, 0.0095, 0.0095, 0.0094, 0.0095,\n",
            "        0.0094, 0.0094, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095, 0.0098,\n",
            "        0.0095, 0.0094, 0.0095, 0.0094, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095,\n",
            "        0.0095, 0.0094, 0.0094, 0.0094, 0.0095, 0.0095, 0.0094, 0.0094, 0.0095,\n",
            "        0.0095, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095,\n",
            "        0.0095, 0.0095, 0.0095, 0.0096, 0.0094, 0.0095, 0.0095, 0.0094, 0.0095,\n",
            "        0.0095, 0.0095, 0.0095, 0.0095, 0.0095, 0.0094, 0.0095, 0.0095, 0.0095,\n",
            "        0.0095, 0.0095, 0.0100, 0.0095, 0.0095, 0.0095],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [11]\n",
            "DEBUGGING: logits looks like: tensor([21.4171, 21.3236, 21.3247, 21.3241, 21.3247, 21.3242, 21.3249, 21.3237,\n",
            "        21.3243, 21.3237, 21.3237, 21.3232, 21.3254, 21.3244, 21.3255, 21.3243,\n",
            "        21.3255, 21.3253, 21.3244, 21.3255, 21.3240, 21.3247, 21.3236, 21.3254,\n",
            "        21.3247, 21.3246, 21.3224, 21.3288, 21.3263, 21.3244, 21.3247, 21.3251,\n",
            "        21.3245, 21.3250, 21.3243, 21.3237, 21.3240, 21.3237, 21.3235, 21.3232,\n",
            "        21.3230, 21.3247, 21.3242, 21.3230, 21.3247, 21.3230, 21.3234, 21.3241,\n",
            "        21.3240, 21.3242, 21.3255, 21.3240, 21.3246, 21.3317, 21.3245, 21.3233,\n",
            "        21.3242, 21.3231, 21.3242, 21.3245, 21.3244, 21.3240, 21.3249, 21.3250,\n",
            "        21.3233, 21.3234, 21.3232, 21.3247, 21.3242, 21.3234, 21.3233, 21.3237,\n",
            "        21.3243, 21.3257, 21.3239, 21.3246, 21.3251, 21.3246, 21.3237, 21.3243,\n",
            "        21.3250, 21.3240, 21.3249, 21.3240, 21.3267, 21.3233, 21.3242, 21.3244,\n",
            "        21.3231, 21.3240, 21.3248, 21.3249, 21.3255, 21.3247, 21.3240, 21.3232,\n",
            "        21.3240, 21.3239, 21.3246, 21.3238, 21.3241, 21.3369, 21.3249, 21.3247,\n",
            "        21.3251], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.03828272001919686 and immediate abs rewards look like: [0.002620732086143107, 0.00461282885635228, 0.0017830048968789924, 0.004446293651653832, 6.358134805850568e-05, 0.0006762300831724133, 0.00502852575846191, 4.2341741846030345e-05, 0.001641454616219562, 9.66023785622383e-05, 0.0005317076465871651, 0.0033879439943120815, 0.0009533080410619732, 0.00026072900618601125, 0.00016792053293102072, 0.0004943709700455656, 0.0003414342336327536, 0.0007319550481952319, 0.001045948154569487, 0.000967139163549291, 0.00039063166150299367, 0.0018740343234640022, 4.331335730967112e-06, 4.6609907258243766e-06, 0.00033391217584721744, 0.0011091602450505889, 3.770171224459773e-05, 4.355793680588249e-05, 0.0003020592002940248, 4.4053815145161934e-05, 0.0019408553648645466, 0.00019565825323297759, 3.700878642121097e-05, 6.12991889283876e-05, 9.364211837237235e-05, 2.414718437648844e-06, 0.0003080784754274646, 0.00011034414046662278, 0.00013008372343392693, 0.00010424627907923423, 5.075572698842734e-05, 0.00016700293690519175, 1.1957972674281336e-06, 0.0005588111962424591, 1.0378545084677171e-05, 6.0568506341951434e-05, 2.9677232305402867e-05, 0.00037195665845501935, 2.1519081201404333e-06, 8.434857591055334e-06]\n",
            "DEBUGGING: the total relative reward of the trajectory = 1.7759114578873032 and immediate relative rewards look like: [0.009685443813022044, 0.03412833294782969, 0.019821350832276426, 0.06594845556796447, 0.0011807633084947452, 0.015070203155443414, 0.13077393262069298, 0.0012608205512798574, 0.05498861812722562, 0.0035979461997983275, 0.02178452337209579, 0.1514558164113081, 0.04622677324053665, 0.013620367143673403, 0.009399586443237002, 0.029519829310931535, 0.02166591412771595, 0.04918507085204009, 0.0742093849482536, 0.07225762734003732, 0.030655499758416173, 0.15409384394238246, 0.0003725967388439235, 0.0004183883174544285, 0.03122217043769379, 0.10787298856109369, 0.0038093434538042934, 0.004564116816507269, 0.032781480608526696, 0.004946435768344858, 0.22519026972101483, 0.02345084521160669, 0.0045746825415505935, 0.007806958011341807, 0.012277146681327358, 0.0003256434923968649, 0.04270088519849786, 0.0157092985771658, 0.019007694453533553, 0.015623689396155941, 0.007797383875790135, 0.026282196914611755, 0.00019268213752989438, 0.09213686123964125, 0.0017504736575956836, 0.010442704153626083, 0.005228045906412446, 0.06692009425096754, 0.00039527862090613715, 0.0015809991287063045]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0145, 0.0145, 0.0149, 0.0158, 0.0152, 0.0151, 0.0144, 0.0152, 0.0139,\n",
            "        0.0146, 0.0160, 0.0155, 0.0154, 0.0155, 0.0162, 0.0146, 0.0151, 0.0151,\n",
            "        0.0146, 0.0159, 0.0155, 0.0164, 0.0159, 0.0176, 0.0153, 0.0149, 0.0156,\n",
            "        0.0151, 0.0151, 0.0153, 0.0153, 0.0151, 0.0147, 0.0160, 0.0145, 0.0150,\n",
            "        0.0152, 0.0145, 0.0150, 0.0150, 0.0149, 0.0150, 0.0155, 0.0157, 0.0158,\n",
            "        0.0140, 0.0151, 0.0149, 0.0155, 0.0145, 0.0148, 0.0147, 0.0149, 0.0150,\n",
            "        0.0146, 0.0151, 0.0146, 0.0150, 0.0139, 0.0156, 0.0149, 0.0158, 0.0156,\n",
            "        0.0158, 0.0145, 0.0153], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [8]\n",
            "DEBUGGING: logits looks like: tensor([21.4983, 21.4992, 21.5051, 21.5197, 21.5102, 21.5091, 21.4964, 21.5099,\n",
            "        21.4885, 21.5013, 21.5228, 21.5151, 21.5138, 21.5147, 21.5272, 21.5002,\n",
            "        21.5086, 21.5083, 21.5002, 21.5222, 21.5148, 21.5291, 21.5221, 21.5467,\n",
            "        21.5130, 21.5056, 21.5170, 21.5091, 21.5085, 21.5128, 21.5129, 21.5095,\n",
            "        21.5025, 21.5234, 21.4990, 21.5075, 21.5103, 21.4995, 21.5078, 21.5077,\n",
            "        21.5061, 21.5072, 21.5155, 21.5184, 21.5196, 21.4909, 21.5094, 21.5053,\n",
            "        21.5154, 21.4994, 21.5045, 21.5027, 21.5056, 21.5070, 21.5008, 21.5091,\n",
            "        21.5013, 21.5079, 21.4878, 21.5176, 21.5059, 21.5208, 21.5170, 21.5200,\n",
            "        21.4994, 21.5118], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0171, 0.0135, 0.0129, 0.0129, 0.0124, 0.0130, 0.0131, 0.0132, 0.0129,\n",
            "        0.0135, 0.0129, 0.0130, 0.0131, 0.0133, 0.0133, 0.0132, 0.0127, 0.0130,\n",
            "        0.0131, 0.0132, 0.0118, 0.0133, 0.0126, 0.0126, 0.0125, 0.0132, 0.0132,\n",
            "        0.0127, 0.0128, 0.0129, 0.0123, 0.0128, 0.0130, 0.0125, 0.0128, 0.0130,\n",
            "        0.0132, 0.0127, 0.0129, 0.0128, 0.0129, 0.0132, 0.0128, 0.0132, 0.0130,\n",
            "        0.0126, 0.0129, 0.0126, 0.0131, 0.0129, 0.0131, 0.0128, 0.0128, 0.0127,\n",
            "        0.0141, 0.0128, 0.0129, 0.0128, 0.0129, 0.0130, 0.0132, 0.0136, 0.0129,\n",
            "        0.0128, 0.0129, 0.0130, 0.0129, 0.0119, 0.0131, 0.0130, 0.0132, 0.0123,\n",
            "        0.0127, 0.0131, 0.0133, 0.0129, 0.0129], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [57]\n",
            "DEBUGGING: logits looks like: tensor([21.6007, 21.5409, 21.5308, 21.5302, 21.5211, 21.5327, 21.5343, 21.5352,\n",
            "        21.5296, 21.5421, 21.5304, 21.5314, 21.5341, 21.5372, 21.5380, 21.5363,\n",
            "        21.5267, 21.5328, 21.5335, 21.5364, 21.5080, 21.5372, 21.5246, 21.5246,\n",
            "        21.5223, 21.5358, 21.5363, 21.5258, 21.5290, 21.5305, 21.5192, 21.5280,\n",
            "        21.5318, 21.5228, 21.5283, 21.5312, 21.5361, 21.5266, 21.5300, 21.5290,\n",
            "        21.5298, 21.5351, 21.5275, 21.5353, 21.5330, 21.5236, 21.5307, 21.5248,\n",
            "        21.5336, 21.5307, 21.5342, 21.5281, 21.5276, 21.5271, 21.5519, 21.5285,\n",
            "        21.5303, 21.5291, 21.5296, 21.5323, 21.5366, 21.5434, 21.5301, 21.5284,\n",
            "        21.5295, 21.5323, 21.5303, 21.5097, 21.5342, 21.5317, 21.5360, 21.5178,\n",
            "        21.5269, 21.5343, 21.5373, 21.5309, 21.5303], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0148, 0.0116, 0.0109, 0.0115, 0.0113, 0.0109, 0.0113, 0.0113, 0.0117,\n",
            "        0.0112, 0.0110, 0.0112, 0.0112, 0.0119, 0.0111, 0.0119, 0.0114, 0.0113,\n",
            "        0.0109, 0.0113, 0.0113, 0.0115, 0.0121, 0.0115, 0.0114, 0.0121, 0.0110,\n",
            "        0.0111, 0.0116, 0.0108, 0.0111, 0.0116, 0.0109, 0.0111, 0.0110, 0.0109,\n",
            "        0.0111, 0.0113, 0.0117, 0.0113, 0.0117, 0.0111, 0.0111, 0.0112, 0.0113,\n",
            "        0.0133, 0.0117, 0.0111, 0.0112, 0.0111, 0.0114, 0.0110, 0.0112, 0.0109,\n",
            "        0.0111, 0.0120, 0.0114, 0.0114, 0.0113, 0.0114, 0.0114, 0.0114, 0.0116,\n",
            "        0.0111, 0.0113, 0.0116, 0.0111, 0.0111, 0.0113, 0.0114, 0.0111, 0.0114,\n",
            "        0.0115, 0.0113, 0.0118, 0.0114, 0.0110, 0.0113, 0.0112, 0.0113, 0.0111,\n",
            "        0.0105, 0.0113, 0.0111, 0.0112, 0.0113, 0.0111, 0.0114],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [24]\n",
            "DEBUGGING: logits looks like: tensor([21.6078, 21.5469, 21.5317, 21.5446, 21.5391, 21.5308, 21.5397, 21.5400,\n",
            "        21.5485, 21.5366, 21.5338, 21.5376, 21.5373, 21.5528, 21.5358, 21.5528,\n",
            "        21.5416, 21.5390, 21.5313, 21.5407, 21.5392, 21.5437, 21.5564, 21.5432,\n",
            "        21.5427, 21.5568, 21.5333, 21.5362, 21.5453, 21.5276, 21.5357, 21.5471,\n",
            "        21.5302, 21.5363, 21.5322, 21.5302, 21.5362, 21.5404, 21.5481, 21.5391,\n",
            "        21.5474, 21.5352, 21.5360, 21.5377, 21.5392, 21.5801, 21.5494, 21.5364,\n",
            "        21.5386, 21.5361, 21.5420, 21.5328, 21.5374, 21.5303, 21.5360, 21.5546,\n",
            "        21.5429, 21.5415, 21.5399, 21.5422, 21.5410, 21.5420, 21.5452, 21.5356,\n",
            "        21.5405, 21.5473, 21.5364, 21.5364, 21.5396, 21.5420, 21.5363, 21.5409,\n",
            "        21.5444, 21.5400, 21.5496, 21.5428, 21.5335, 21.5396, 21.5377, 21.5394,\n",
            "        21.5362, 21.5213, 21.5409, 21.5347, 21.5380, 21.5406, 21.5355, 21.5413],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0135, 0.0100, 0.0101, 0.0102, 0.0097, 0.0099, 0.0101, 0.0101, 0.0102,\n",
            "        0.0101, 0.0105, 0.0101, 0.0100, 0.0101, 0.0100, 0.0100, 0.0100, 0.0102,\n",
            "        0.0101, 0.0102, 0.0100, 0.0100, 0.0101, 0.0100, 0.0101, 0.0100, 0.0101,\n",
            "        0.0101, 0.0100, 0.0101, 0.0101, 0.0102, 0.0101, 0.0100, 0.0100, 0.0101,\n",
            "        0.0101, 0.0101, 0.0100, 0.0101, 0.0100, 0.0101, 0.0100, 0.0100, 0.0101,\n",
            "        0.0100, 0.0101, 0.0100, 0.0101, 0.0101, 0.0100, 0.0101, 0.0100, 0.0100,\n",
            "        0.0101, 0.0100, 0.0101, 0.0099, 0.0100, 0.0100, 0.0100, 0.0101, 0.0100,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0100, 0.0100,\n",
            "        0.0101, 0.0101, 0.0100, 0.0101, 0.0100, 0.0100, 0.0101, 0.0102, 0.0101,\n",
            "        0.0101, 0.0101, 0.0100, 0.0099, 0.0100, 0.0102, 0.0101, 0.0101, 0.0099,\n",
            "        0.0099, 0.0101, 0.0101, 0.0101, 0.0100, 0.0100, 0.0101, 0.0100, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [9]\n",
            "DEBUGGING: logits looks like: tensor([21.6133, 21.5395, 21.5403, 21.5446, 21.5316, 21.5374, 21.5421, 21.5410,\n",
            "        21.5428, 21.5406, 21.5507, 21.5406, 21.5382, 21.5421, 21.5386, 21.5388,\n",
            "        21.5378, 21.5426, 21.5421, 21.5429, 21.5396, 21.5383, 21.5407, 21.5399,\n",
            "        21.5404, 21.5394, 21.5413, 21.5424, 21.5396, 21.5421, 21.5420, 21.5436,\n",
            "        21.5422, 21.5394, 21.5393, 21.5406, 21.5423, 21.5425, 21.5387, 21.5410,\n",
            "        21.5389, 21.5406, 21.5398, 21.5397, 21.5417, 21.5393, 21.5412, 21.5389,\n",
            "        21.5423, 21.5418, 21.5393, 21.5412, 21.5396, 21.5391, 21.5412, 21.5397,\n",
            "        21.5403, 21.5365, 21.5394, 21.5399, 21.5395, 21.5413, 21.5395, 21.5405,\n",
            "        21.5410, 21.5404, 21.5408, 21.5409, 21.5409, 21.5414, 21.5399, 21.5390,\n",
            "        21.5411, 21.5408, 21.5400, 21.5409, 21.5388, 21.5400, 21.5414, 21.5438,\n",
            "        21.5411, 21.5402, 21.5409, 21.5397, 21.5374, 21.5388, 21.5429, 21.5420,\n",
            "        21.5401, 21.5374, 21.5366, 21.5408, 21.5412, 21.5407, 21.5386, 21.5393,\n",
            "        21.5422, 21.5396, 21.5407], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0121, 0.0091, 0.0090, 0.0090, 0.0091, 0.0090, 0.0090, 0.0090, 0.0091,\n",
            "        0.0090, 0.0091, 0.0090, 0.0092, 0.0091, 0.0091, 0.0090, 0.0091, 0.0090,\n",
            "        0.0091, 0.0090, 0.0090, 0.0092, 0.0091, 0.0093, 0.0091, 0.0091, 0.0091,\n",
            "        0.0090, 0.0090, 0.0090, 0.0091, 0.0090, 0.0090, 0.0091, 0.0090, 0.0091,\n",
            "        0.0090, 0.0091, 0.0090, 0.0091, 0.0090, 0.0091, 0.0091, 0.0090, 0.0090,\n",
            "        0.0091, 0.0091, 0.0090, 0.0091, 0.0089, 0.0091, 0.0090, 0.0090, 0.0090,\n",
            "        0.0090, 0.0091, 0.0091, 0.0090, 0.0091, 0.0090, 0.0091, 0.0090, 0.0090,\n",
            "        0.0090, 0.0091, 0.0091, 0.0091, 0.0091, 0.0090, 0.0091, 0.0089, 0.0090,\n",
            "        0.0091, 0.0091, 0.0091, 0.0090, 0.0091, 0.0091, 0.0090, 0.0090, 0.0092,\n",
            "        0.0090, 0.0090, 0.0091, 0.0091, 0.0091, 0.0090, 0.0091, 0.0091, 0.0091,\n",
            "        0.0090, 0.0091, 0.0091, 0.0091, 0.0090, 0.0090, 0.0091, 0.0091, 0.0091,\n",
            "        0.0090, 0.0091, 0.0091, 0.0092, 0.0090, 0.0091, 0.0091, 0.0091, 0.0091,\n",
            "        0.0091, 0.0091], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [5]\n",
            "DEBUGGING: logits looks like: tensor([21.6179, 21.5461, 21.5456, 21.5458, 21.5469, 21.5456, 21.5458, 21.5451,\n",
            "        21.5464, 21.5456, 21.5466, 21.5440, 21.5505, 21.5475, 21.5470, 21.5456,\n",
            "        21.5477, 21.5458, 21.5460, 21.5456, 21.5454, 21.5496, 21.5474, 21.5517,\n",
            "        21.5462, 21.5472, 21.5474, 21.5450, 21.5457, 21.5458, 21.5460, 21.5459,\n",
            "        21.5458, 21.5462, 21.5458, 21.5460, 21.5455, 21.5486, 21.5457, 21.5463,\n",
            "        21.5454, 21.5467, 21.5460, 21.5455, 21.5449, 21.5465, 21.5462, 21.5455,\n",
            "        21.5468, 21.5426, 21.5461, 21.5458, 21.5458, 21.5450, 21.5441, 21.5485,\n",
            "        21.5465, 21.5458, 21.5465, 21.5451, 21.5484, 21.5451, 21.5452, 21.5457,\n",
            "        21.5463, 21.5465, 21.5467, 21.5467, 21.5458, 21.5464, 21.5429, 21.5458,\n",
            "        21.5465, 21.5467, 21.5464, 21.5458, 21.5486, 21.5460, 21.5456, 21.5451,\n",
            "        21.5497, 21.5457, 21.5459, 21.5466, 21.5461, 21.5473, 21.5457, 21.5463,\n",
            "        21.5461, 21.5474, 21.5457, 21.5469, 21.5464, 21.5473, 21.5458, 21.5457,\n",
            "        21.5468, 21.5464, 21.5462, 21.5453, 21.5470, 21.5480, 21.5492, 21.5455,\n",
            "        21.5466, 21.5469, 21.5461, 21.5465, 21.5465, 21.5467],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.1380999226548738 and immediate abs rewards look like: [0.049325372636303655, 0.0012093512409592222, 0.0014095062006163062, 0.010917331839664257, 0.001656916854699375, 0.014627836031650077, 0.02121819320245777, 0.0001551160826238629, 0.0009117444310504652, 0.004491461550969689, 0.0008453631348857016, 2.1114600713190157e-05, 0.00014488620581687428, 0.000503832019148831, 0.0004074042590218596, 0.009055798268946091, 0.00013101443983032368, 6.433149792428594e-06, 8.676461629875121e-05, 0.0005387482970036217, 0.00047129578933891025, 0.001159931315214635, 0.0007012721562205115, 0.0018732379262473842, 0.0003906093302248337, 3.0155805688991677e-05, 4.712808640761068e-05, 0.0008359682492482534, 0.002305088584762416, 0.0001984723726309312, 0.000967103826951643, 1.5026550499896985e-05, 0.0016314009985762823, 4.236020595271839e-05, 0.0005702029666281305, 0.00173502342613574, 0.0007561935094599903, 0.000960809086336667, 0.0009644472902436974, 0.0011058197483180265, 3.6229987017577514e-05, 0.00012519143956524204, 0.000761804040394054, 5.780889978268533e-05, 0.001114649649025523, 0.0003848463561553217, 8.471824912703596e-06, 0.00034065897307300474, 0.0008438151480731904, 7.100493348843884e-07]\n",
            "DEBUGGING: the total relative reward of the trajectory = 4.239934577371429 and immediate relative rewards look like: [0.15904683921820995, 0.007925012452804267, 0.013860467283763781, 0.14320785054316365, 0.027265828165956404, 0.2890122158931208, 0.49146014014779965, 0.004135124124717025, 0.02734511868463197, 0.14972138776720978, 0.03104437113161361, 0.0008461231555385116, 0.006289888043419445, 0.02355631409177507, 0.020411900417643375, 0.4840301802462279, 0.007462934316032813, 0.00038802237386079645, 0.0055240523679916725, 0.03610688879672876, 0.0331715243235631, 0.08554135695909228, 0.054088446725331585, 0.15079828614647064, 0.0327753854416278, 0.0026318819821333343, 0.004271398522210774, 0.0785744243067091, 0.22446100419084092, 0.020008423696389086, 0.10075238163550027, 0.0016164823643380019, 0.18098332170200765, 0.0048443902176410324, 0.06712833003764397, 0.2101355222927037, 0.09418459518632577, 0.12293526143992986, 0.12668914298126138, 0.1490327542818975, 0.005006696553521051, 0.0177226419063755, 0.11041665648718339, 0.008575940892360967, 0.16911952914981265, 0.05971058440200353, 0.0013431895930883832, 0.05516006891875225, 0.139494485052395, 0.00011981076010822782]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 3\n",
            "DEBUGGING: the action_prob is: tensor([0.0269, 0.0156, 0.0132, 0.0145, 0.0136, 0.0151, 0.0140, 0.0147, 0.0142,\n",
            "        0.0147, 0.0144, 0.0145, 0.0150, 0.0143, 0.0147, 0.0125, 0.0151, 0.0147,\n",
            "        0.0148, 0.0142, 0.0136, 0.0143, 0.0161, 0.0149, 0.0154, 0.0149, 0.0149,\n",
            "        0.0147, 0.0155, 0.0154, 0.0155, 0.0142, 0.0155, 0.0149, 0.0135, 0.0151,\n",
            "        0.0145, 0.0139, 0.0140, 0.0144, 0.0150, 0.0143, 0.0142, 0.0140, 0.0134,\n",
            "        0.0149, 0.0146, 0.0136, 0.0143, 0.0147, 0.0135, 0.0126, 0.0140, 0.0141,\n",
            "        0.0148, 0.0140, 0.0141, 0.0151, 0.0134, 0.0146, 0.0149, 0.0145, 0.0153,\n",
            "        0.0165, 0.0144, 0.0145, 0.0157, 0.0145], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [49]\n",
            "DEBUGGING: logits looks like: tensor([21.5256, 21.3900, 21.3471, 21.3721, 21.3561, 21.3809, 21.3630, 21.3751,\n",
            "        21.3670, 21.3744, 21.3699, 21.3718, 21.3799, 21.3687, 21.3751, 21.3352,\n",
            "        21.3819, 21.3752, 21.3769, 21.3668, 21.3563, 21.3677, 21.3973, 21.3783,\n",
            "        21.3869, 21.3782, 21.3781, 21.3749, 21.3873, 21.3863, 21.3876, 21.3669,\n",
            "        21.3878, 21.3774, 21.3528, 21.3823, 21.3717, 21.3614, 21.3635, 21.3697,\n",
            "        21.3799, 21.3683, 21.3668, 21.3626, 21.3514, 21.3788, 21.3724, 21.3560,\n",
            "        21.3688, 21.3755, 21.3529, 21.3355, 21.3628, 21.3647, 21.3772, 21.3633,\n",
            "        21.3647, 21.3809, 21.3509, 21.3725, 21.3787, 21.3706, 21.3853, 21.4044,\n",
            "        21.3702, 21.3722, 21.3909, 21.3721], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0221, 0.0122, 0.0124, 0.0126, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124,\n",
            "        0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0123,\n",
            "        0.0123, 0.0124, 0.0123, 0.0124, 0.0124, 0.0123, 0.0123, 0.0125, 0.0124,\n",
            "        0.0124, 0.0123, 0.0122, 0.0124, 0.0124, 0.0124, 0.0123, 0.0123, 0.0124,\n",
            "        0.0123, 0.0124, 0.0123, 0.0125, 0.0124, 0.0124, 0.0123, 0.0124, 0.0123,\n",
            "        0.0124, 0.0125, 0.0124, 0.0124, 0.0124, 0.0122, 0.0124, 0.0125, 0.0125,\n",
            "        0.0124, 0.0124, 0.0124, 0.0124, 0.0122, 0.0123, 0.0124, 0.0124, 0.0125,\n",
            "        0.0124, 0.0124, 0.0125, 0.0124, 0.0124, 0.0124, 0.0123, 0.0124, 0.0124,\n",
            "        0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0125, 0.0123],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [42]\n",
            "DEBUGGING: logits looks like: tensor([21.5123, 21.3640, 21.3680, 21.3719, 21.3687, 21.3687, 21.3679, 21.3679,\n",
            "        21.3679, 21.3685, 21.3680, 21.3674, 21.3674, 21.3688, 21.3680, 21.3676,\n",
            "        21.3674, 21.3668, 21.3670, 21.3677, 21.3671, 21.3687, 21.3672, 21.3662,\n",
            "        21.3665, 21.3693, 21.3686, 21.3681, 21.3655, 21.3651, 21.3684, 21.3679,\n",
            "        21.3685, 21.3671, 21.3657, 21.3689, 21.3655, 21.3678, 21.3671, 21.3696,\n",
            "        21.3675, 21.3687, 21.3661, 21.3679, 21.3655, 21.3686, 21.3696, 21.3675,\n",
            "        21.3677, 21.3680, 21.3649, 21.3675, 21.3697, 21.3709, 21.3684, 21.3681,\n",
            "        21.3676, 21.3682, 21.3645, 21.3670, 21.3686, 21.3673, 21.3698, 21.3689,\n",
            "        21.3674, 21.3696, 21.3688, 21.3678, 21.3680, 21.3656, 21.3682, 21.3692,\n",
            "        21.3673, 21.3673, 21.3675, 21.3678, 21.3680, 21.3688, 21.3698, 21.3665],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0210, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118,\n",
            "        0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118,\n",
            "        0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0117, 0.0118, 0.0118,\n",
            "        0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118,\n",
            "        0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118,\n",
            "        0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118,\n",
            "        0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118,\n",
            "        0.0118, 0.0117, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118,\n",
            "        0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118,\n",
            "        0.0118, 0.0118, 0.0118], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [56]\n",
            "DEBUGGING: logits looks like: tensor([21.5126, 21.3686, 21.3687, 21.3698, 21.3689, 21.3688, 21.3687, 21.3688,\n",
            "        21.3692, 21.3689, 21.3690, 21.3691, 21.3685, 21.3688, 21.3687, 21.3691,\n",
            "        21.3690, 21.3689, 21.3691, 21.3682, 21.3689, 21.3693, 21.3685, 21.3690,\n",
            "        21.3674, 21.3683, 21.3687, 21.3693, 21.3697, 21.3680, 21.3695, 21.3700,\n",
            "        21.3682, 21.3688, 21.3687, 21.3690, 21.3687, 21.3684, 21.3683, 21.3688,\n",
            "        21.3683, 21.3695, 21.3693, 21.3694, 21.3689, 21.3685, 21.3697, 21.3699,\n",
            "        21.3679, 21.3687, 21.3684, 21.3692, 21.3685, 21.3688, 21.3697, 21.3687,\n",
            "        21.3691, 21.3688, 21.3687, 21.3695, 21.3682, 21.3690, 21.3688, 21.3685,\n",
            "        21.3676, 21.3699, 21.3691, 21.3692, 21.3687, 21.3680, 21.3689, 21.3692,\n",
            "        21.3693, 21.3685, 21.3687, 21.3684, 21.3690, 21.3690, 21.3679, 21.3690,\n",
            "        21.3682, 21.3691, 21.3679, 21.3691], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0129, 0.0094, 0.0104, 0.0103, 0.0110, 0.0096, 0.0104, 0.0107, 0.0091,\n",
            "        0.0108, 0.0113, 0.0116, 0.0106, 0.0110, 0.0093, 0.0112, 0.0102, 0.0081,\n",
            "        0.0099, 0.0102, 0.0101, 0.0103, 0.0112, 0.0097, 0.0102, 0.0089, 0.0089,\n",
            "        0.0091, 0.0088, 0.0100, 0.0094, 0.0105, 0.0107, 0.0094, 0.0099, 0.0102,\n",
            "        0.0096, 0.0125, 0.0095, 0.0122, 0.0088, 0.0106, 0.0096, 0.0105, 0.0095,\n",
            "        0.0122, 0.0116, 0.0099, 0.0098, 0.0107, 0.0096, 0.0104, 0.0107, 0.0096,\n",
            "        0.0102, 0.0095, 0.0134, 0.0091, 0.0105, 0.0111, 0.0098, 0.0095, 0.0105,\n",
            "        0.0109, 0.0095, 0.0090, 0.0140, 0.0106, 0.0099, 0.0102, 0.0097, 0.0097,\n",
            "        0.0094, 0.0105, 0.0095, 0.0104, 0.0091, 0.0118, 0.0110, 0.0096, 0.0094,\n",
            "        0.0104, 0.0118, 0.0099, 0.0098, 0.0119, 0.0116, 0.0093, 0.0103, 0.0140,\n",
            "        0.0094, 0.0101, 0.0109, 0.0088, 0.0107, 0.0103, 0.0110],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [41]\n",
            "DEBUGGING: logits looks like: tensor([21.4256, 21.3464, 21.3725, 21.3685, 21.3849, 21.3523, 21.3710, 21.3793,\n",
            "        21.3374, 21.3814, 21.3918, 21.3996, 21.3764, 21.3855, 21.3430, 21.3894,\n",
            "        21.3664, 21.3093, 21.3590, 21.3679, 21.3637, 21.3686, 21.3909, 21.3544,\n",
            "        21.3677, 21.3337, 21.3328, 21.3376, 21.3305, 21.3622, 21.3472, 21.3754,\n",
            "        21.3792, 21.3458, 21.3588, 21.3673, 21.3529, 21.4172, 21.3505, 21.4112,\n",
            "        21.3304, 21.3766, 21.3526, 21.3737, 21.3499, 21.4125, 21.3987, 21.3606,\n",
            "        21.3562, 21.3788, 21.3510, 21.3727, 21.3782, 21.3510, 21.3662, 21.3502,\n",
            "        21.4346, 21.3380, 21.3751, 21.3883, 21.3567, 21.3488, 21.3735, 21.3842,\n",
            "        21.3504, 21.3348, 21.4466, 21.3775, 21.3603, 21.3661, 21.3547, 21.3533,\n",
            "        21.3460, 21.3734, 21.3493, 21.3718, 21.3380, 21.4028, 21.3866, 21.3513,\n",
            "        21.3470, 21.3708, 21.4029, 21.3607, 21.3570, 21.4057, 21.3985, 21.3436,\n",
            "        21.3685, 21.4459, 21.3465, 21.3641, 21.3828, 21.3305, 21.3779, 21.3695,\n",
            "        21.3864], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0142, 0.0087, 0.0114, 0.0089, 0.0091, 0.0102, 0.0111, 0.0087, 0.0088,\n",
            "        0.0115, 0.0096, 0.0096, 0.0102, 0.0095, 0.0101, 0.0100, 0.0072, 0.0090,\n",
            "        0.0092, 0.0107, 0.0099, 0.0092, 0.0088, 0.0106, 0.0103, 0.0102, 0.0090,\n",
            "        0.0085, 0.0087, 0.0087, 0.0087, 0.0089, 0.0078, 0.0095, 0.0102, 0.0099,\n",
            "        0.0093, 0.0082, 0.0090, 0.0098, 0.0085, 0.0107, 0.0092, 0.0095, 0.0100,\n",
            "        0.0091, 0.0089, 0.0109, 0.0098, 0.0094, 0.0080, 0.0095, 0.0088, 0.0112,\n",
            "        0.0111, 0.0082, 0.0095, 0.0092, 0.0083, 0.0080, 0.0104, 0.0087, 0.0091,\n",
            "        0.0096, 0.0091, 0.0089, 0.0091, 0.0090, 0.0105, 0.0092, 0.0102, 0.0107,\n",
            "        0.0098, 0.0117, 0.0092, 0.0087, 0.0093, 0.0100, 0.0098, 0.0089, 0.0095,\n",
            "        0.0093, 0.0093, 0.0095, 0.0100, 0.0092, 0.0089, 0.0090, 0.0092, 0.0087,\n",
            "        0.0098, 0.0084, 0.0083, 0.0096, 0.0102, 0.0099, 0.0095, 0.0085, 0.0087,\n",
            "        0.0087, 0.0097, 0.0092, 0.0089, 0.0096, 0.0080, 0.0081],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [104]\n",
            "DEBUGGING: logits looks like: tensor([21.4987, 21.3763, 21.4435, 21.3814, 21.3866, 21.4165, 21.4358, 21.3741,\n",
            "        21.3795, 21.4453, 21.4005, 21.3991, 21.4142, 21.3971, 21.4125, 21.4100,\n",
            "        21.3284, 21.3831, 21.3889, 21.4262, 21.4077, 21.3883, 21.3795, 21.4261,\n",
            "        21.4183, 21.4150, 21.3831, 21.3705, 21.3759, 21.3744, 21.3763, 21.3802,\n",
            "        21.3483, 21.3967, 21.4145, 21.4081, 21.3911, 21.3596, 21.3851, 21.4043,\n",
            "        21.3707, 21.4277, 21.3901, 21.3963, 21.4115, 21.3874, 21.3799, 21.4312,\n",
            "        21.4042, 21.3949, 21.3539, 21.3982, 21.3770, 21.4383, 21.4368, 21.3594,\n",
            "        21.3969, 21.3908, 21.3648, 21.3558, 21.4208, 21.3757, 21.3874, 21.3996,\n",
            "        21.3860, 21.3810, 21.3878, 21.3830, 21.4219, 21.3892, 21.4156, 21.4271,\n",
            "        21.4059, 21.4491, 21.3894, 21.3765, 21.3934, 21.4101, 21.4051, 21.3809,\n",
            "        21.3973, 21.3916, 21.3928, 21.3986, 21.4098, 21.3894, 21.3825, 21.3830,\n",
            "        21.3895, 21.3767, 21.4042, 21.3672, 21.3626, 21.3992, 21.4165, 21.4089,\n",
            "        21.3968, 21.3700, 21.3755, 21.3759, 21.4029, 21.3886, 21.3809, 21.4014,\n",
            "        21.3542, 21.3568], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.886741328977223 and immediate abs rewards look like: [0.047758116060776956, 0.005690713145668269, 0.025661467008831096, 0.012067834270510502, 0.01190216860959481, 0.010554556221450184, 0.012092806605778605, 0.005722791986045195, 0.008459431081064395, 0.0008774477428232785, 0.005455464959140954, 0.0006802140683248581, 0.00196110842307462, 4.6385594032472e-07, 0.0010807759599629208, 0.00011641808259810205, 0.00016912557839532383, 1.8565496702649398e-05, 0.0006353590483740845, 0.0002058927761936502, 0.0002540602413318993, 0.00010499443396838615, 7.478377028746763e-06, 5.5116258863563417e-05, 8.257281479018275e-05, 0.0005181883093428041, 0.0005880514841010154, 0.0006865950231258466, 5.755711299570976e-06, 2.8213527457410237e-05, 2.1985656530887354e-06, 0.00012906231722809025, 9.355472229799489e-06, 0.0001093017308448907, 0.0002994039614350186, 0.7327502597631792, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0]\n",
            "DEBUGGING: the total relative reward of the trajectory = 99.16708617468598 and immediate relative rewards look like: [0.16543954105431735, 0.04008979778243492, 0.2717135665761481, 0.17192906232637806, 0.21287604258127507, 0.22749658327290168, 0.30525205953904416, 0.16581712881297156, 0.27632247621700023, 0.03194399659088522, 0.21854013079869036, 0.029784977903574594, 0.09305146016363088, 2.3719217874832387e-05, 0.059212884054080345, 0.00680614468515724, 0.010506001127485242, 0.0012211953586846662, 0.04411454799290114, 0.01505154099200599, 0.019502870937268768, 0.008444459133805253, 0.0006288321591055447, 0.004836060345489588, 0.0075472126661599755, 0.04925876958570544, 0.05806093203741922, 0.07031645569672931, 0.0006106668819233831, 0.003096612946189917, 0.0002493525982271402, 0.015109933432843399, 0.0011295703276651674, 0.013596936349831439, 0.03834229135794909, 96.52916236028409, 8.412825991401499e-11, 0.0, 0.0, 9.094947017729282e-11, 9.322320693170395e-11, 0.0, 9.777068044058979e-11, 1.0004441719499936e-10, 0.0, 1.0459189070388675e-10, 1.0686562745829477e-10, 1.0913936421275139e-10, 1.1141310096715838e-10, 0.0]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 200 number of matrices\n",
            "DEBUGGING: ACT_MAT has 200 number of matrices\n",
            "DEBUGGING: VAL looks like: [[2.332087637508743, 2.297448703997159, 2.1594027421284383, 2.1468712443826248, 2.0648107326254355, 1.9105365268104777, 1.9008264427145753, 1.7851685219811562, 1.359365766931796, 1.3574746529174493, 1.3702126986577583, 1.3456865246696574, 1.315756943641382, 1.3236049907644634, 1.334617827442819, 1.238462267943648, 1.248467896622415, 1.244308610139948, 0.727441009182147, 0.7062466674279303, 0.6331408187545412, 0.47337808582395097, 0.4769054602329902, 0.4710426096734263, 0.47461722745167767, 0.40740136571901375, 0.3468683442413742, 0.20228582485407456, 0.20240483823554942, 0.20028087259317895, 0.17683707509315583, 0.1715654958662675, 0.163694668336632, 0.1342762871775025, 0.13555611834269654, 0.0889580212527094, 0.08776253595602708, 0.08258654732284632, 0.08269457284016711, 0.08172003886169774, 0.027152505101770207, 0.02388992243419183, 0.019208605722816453, 0.0192301308262619, 0.019290136026273528, 0.015927508625791696, 0.014671684421796557, 0.012179430040882636, 0.010265284876525145, 0.0010176746606960034], [1.4400343829975635, 1.4447969082672136, 1.4249177528478625, 1.4192892949652385, 1.3670109488861353, 1.3796264500784248, 1.3783396433565467, 1.2601673845816705, 1.2716227919498897, 1.2289234079016809, 1.237702486567558, 1.2281999628236995, 1.0876203499115065, 1.0519127037080505, 1.048780137943815, 1.049879344950079, 1.0306661774132801, 1.0191921851369334, 0.9798051659443368, 0.9147432131273567, 0.8509955411993126, 0.828626304485754, 0.6813459197407794, 0.6878518414160965, 0.6943772253521636, 0.6698535908226968, 0.5676571740016193, 0.569543263179611, 0.5706860064273775, 0.5433379048675261, 0.5438297667668497, 0.3218580778240756, 0.30142144708330193, 0.29984521670883973, 0.2949881400984828, 0.2855666600173287, 0.28812223891407257, 0.2479003572884593, 0.2345364229407005, 0.2177057863506737, 0.20412333025708865, 0.1983090367487864, 0.1737644846809845, 0.17532505307419655, 0.0840284766005609, 0.08310909388178304, 0.07340039366480501, 0.06886095733170966, 0.0019604677583253786, 0.0015809991287063045], [3.418610009201091, 3.2924880504877585, 3.3177404424595496, 3.3372525001775615, 3.226307726903432, 3.231355453270177, 2.972063876138441, 2.505660339384486, 2.5267931467270395, 2.5246949778206136, 2.398963222276165, 2.391837223378335, 2.4151425254775725, 2.43318448225672, 2.433967846631258, 2.4379352992056713, 1.9736415343024678, 1.986038989885288, 2.0057080479913405, 2.020387874367019, 2.004324227848778, 1.9910633368941566, 1.924769676702085, 1.8895769999765186, 1.7563421351818667, 1.7409765148891303, 1.7559036696030272, 1.769325526344259, 1.7078293959975253, 1.498351910915843, 1.49327624971662, 1.406589765738505, 1.4191649327011786, 1.2506884959587585, 1.2584283896374926, 1.2033333935352006, 1.003230172972219, 0.9182278563493872, 0.8033258534438963, 0.6834714247097322, 0.5398370408361967, 0.5402326709926016, 0.5277879081679052, 0.4215871229098201, 0.4171830121388476, 0.25056917473639895, 0.1927864548832277, 0.19337703564660538, 0.13961309770490216, 0.00011981076010822782], [70.33327350096711, 70.87659995950787, 71.55203046638934, 72.0003201008214, 72.55393034191415, 73.07177201952817, 73.58007619823765, 74.01497387747334, 74.59510782692966, 75.06948015223502, 75.7954910663072, 76.34035448031163, 77.08138333576572, 77.76599179353747, 78.55148290335312, 79.2851212316152, 80.07910614841418, 80.87737388614819, 81.69308352605002, 82.47370603844153, 83.29157019944397, 84.11319932172394, 84.95429784100014, 85.81178687761721, 86.67368769421385, 87.54155604196737, 88.37605785089058, 89.21009789783147, 90.04018327488359, 90.94906324040572, 91.86461275500962, 92.792286265062, 93.71431952689815, 94.65978783491968, 95.60221302885844, 96.52916236111162, 8.358887588890211e-10, 7.59354039368693e-10, 7.67024282190599e-10, 7.747720022127263e-10, 6.907298303388217e-10, 6.035420438455735e-10, 6.096384281268419e-10, 5.170381289760122e-10, 4.212057694757705e-10, 4.25460373207849e-10, 3.2410957828683055e-10, 2.1943833417023817e-10, 1.1141310096715838e-10, 0.0]]\n",
            "DEBUGGING: traj_returns = [2.332087637508743, 1.4400343829975635, 3.418610009201091, 70.33327350096711]\n",
            "DEBUGGING: actions = [[46], [43], [54], [7], [44], [20], [26], [26], [19], [20], [36], [8], [40], [44], [17], [51], [22], [36], [3], [13], [40], [64], [66], [38], [51], [66], [35], [41], [1], [76], [43], [43], [2], [69], [56], [86], [26], [64], [17], [71], [78], [41], [37], [80], [7], [43], [12], [4], [102], [42], [27], [50], [14], [14], [57], [11], [47], [4], [15], [18], [16], [10], [57], [67], [9], [10], [71], [45], [25], [44], [6], [52], [62], [20], [20], [1], [40], [34], [27], [39], [68], [29], [40], [84], [45], [3], [24], [28], [93], [18], [95], [90], [85], [56], [59], [90], [67], [80], [64], [11], [9], [27], [9], [11], [3], [20], [60], [43], [36], [8], [3], [21], [44], [56], [6], [65], [45], [7], [55], [57], [4], [45], [32], [13], [39], [72], [2], [26], [27], [24], [47], [31], [21], [9], [15], [12], [44], [62], [59], [9], [27], [57], [40], [57], [11], [71], [45], [30], [15], [5], [55], [8], [16], [56], [50], [51], [43], [36], [3], [49], [19], [19], [30], [73], [8], [27], [21], [33], [1], [42], [77], [54], [16], [9], [80], [56], [48], [32], [9], [56], [36], [63], [39], [70], [17], [0], [87], [60], [28], [41], [30], [34], [7], [26], [15], [26], [35], [59], [99], [104]]\n",
            "DEBUGGING: actions length = 200\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[2.5546, 1.1058, 0.3051,  ..., 1.9769, 0.2971, 2.0194],\n",
            "        [2.5644, 1.1260, 0.3016,  ..., 1.9964, 0.3251, 2.0362],\n",
            "        [2.6126, 1.1589, 0.2913,  ..., 2.0347, 0.3583, 2.0993],\n",
            "        ...,\n",
            "        [2.6128, 1.1670, 0.3014,  ..., 2.0424, 0.3580, 2.0914],\n",
            "        [2.6169, 1.1719, 0.3016,  ..., 2.0474, 0.3618, 2.0960],\n",
            "        [2.6150, 1.1694, 0.3012,  ..., 2.0449, 0.3597, 2.0937]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[2.6245, 1.1808, 0.3026,  ..., 2.0567, 0.3689, 2.1044],\n",
            "        [2.6137, 1.1680, 0.3010,  ..., 2.0436, 0.3582, 2.0921],\n",
            "        [2.6197, 1.1750, 0.3021,  ..., 2.0506, 0.3645, 2.0994],\n",
            "        ...,\n",
            "        [2.6158, 1.1707, 0.3012,  ..., 2.0462, 0.3605, 2.0949],\n",
            "        [2.6117, 1.1661, 0.3014,  ..., 2.0411, 0.3566, 2.0905],\n",
            "        [2.6119, 1.1661, 0.3009,  ..., 2.0415, 0.3567, 2.0902]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([21.4987, 21.3763, 21.4435, 21.3814, 21.3866, 21.4165, 21.4358, 21.3741,\n",
            "        21.3795, 21.4453, 21.4005, 21.3991, 21.4142, 21.3971, 21.4125, 21.4100,\n",
            "        21.3284, 21.3831, 21.3889, 21.4262, 21.4077, 21.3883, 21.3795, 21.4261,\n",
            "        21.4183, 21.4150, 21.3831, 21.3705, 21.3759, 21.3744, 21.3763, 21.3802,\n",
            "        21.3483, 21.3967, 21.4145, 21.4081, 21.3911, 21.3596, 21.3851, 21.4043,\n",
            "        21.3707, 21.4277, 21.3901, 21.3963, 21.4115, 21.3874, 21.3799, 21.4312,\n",
            "        21.4042, 21.3949, 21.3539, 21.3982, 21.3770, 21.4383, 21.4368, 21.3594,\n",
            "        21.3969, 21.3908, 21.3648, 21.3558, 21.4208, 21.3757, 21.3874, 21.3996,\n",
            "        21.3860, 21.3810, 21.3878, 21.3830, 21.4219, 21.3892, 21.4156, 21.4271,\n",
            "        21.4059, 21.4491, 21.3894, 21.3765, 21.3934, 21.4101, 21.4051, 21.3809,\n",
            "        21.3973, 21.3916, 21.3928, 21.3986, 21.4098, 21.3894, 21.3825, 21.3830,\n",
            "        21.3895, 21.3767, 21.4042, 21.3672, 21.3626, 21.3992, 21.4165, 21.4089,\n",
            "        21.3968, 21.3700, 21.3755, 21.3759, 21.4029, 21.3886, 21.3809, 21.4014,\n",
            "        21.3542, 21.3568], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[1.93810014e+01 1.94778334e+01 1.96135229e+01 1.97259333e+01\n",
            "  1.98030149e+01 1.98983226e+01 1.99578265e+01 1.98914925e+01\n",
            "  1.99382224e+01 2.00451433e+01 2.02005924e+01 2.03265195e+01\n",
            "  2.04749758e+01 2.06436735e+01 2.08422122e+01 2.10028495e+01\n",
            "  2.10829704e+01 2.12817284e+01 2.13515094e+01 2.15287709e+01\n",
            "  2.16950077e+01 2.18515668e+01 2.20093297e+01 2.22150646e+01\n",
            "  2.23997561e+01 2.25899469e+01 2.27616218e+01 2.29378131e+01\n",
            "  2.31302759e+01 2.32977585e+01 2.35196390e+01 2.36730749e+01\n",
            "  2.38996501e+01 2.40861495e+01 2.43227964e+01 2.45267551e+01\n",
            "  3.44778737e-01 3.12178690e-01 2.80139212e-01 2.45724313e-01\n",
            "  1.92778219e-01 1.90607908e-01 1.80190250e-01 1.54035577e-01\n",
            "  1.30125406e-01 8.74014444e-02 7.02146333e-02 6.86043558e-02\n",
            "  3.79597126e-02 6.79621137e-04]]\n",
            "DEBUGGING: baseline2 looks like: 19.38100138266863\n",
            "DEBUGGING: ADS looks like: [-1.70489137e+01 -1.71803847e+01 -1.74541201e+01 -1.75790620e+01\n",
            " -1.77382042e+01 -1.79877861e+01 -1.80570001e+01 -1.81063240e+01\n",
            " -1.85788566e+01 -1.86876686e+01 -1.88303797e+01 -1.89808330e+01\n",
            " -1.91592188e+01 -1.93200685e+01 -1.95075944e+01 -1.97643873e+01\n",
            " -1.98345025e+01 -2.00374198e+01 -2.06240684e+01 -2.08225243e+01\n",
            " -2.10618669e+01 -2.13781887e+01 -2.15324243e+01 -2.17440220e+01\n",
            " -2.19251388e+01 -2.21825455e+01 -2.24147534e+01 -2.27355273e+01\n",
            " -2.29278710e+01 -2.30974776e+01 -2.33428019e+01 -2.35015094e+01\n",
            " -2.37359555e+01 -2.39518732e+01 -2.41872403e+01 -2.44377971e+01\n",
            " -2.57016201e-01 -2.29592143e-01 -1.97444640e-01 -1.64004274e-01\n",
            " -1.65625714e-01 -1.66717985e-01 -1.60981644e-01 -1.34805446e-01\n",
            " -1.10835270e-01 -7.14739358e-02 -5.55429489e-02 -5.64249258e-02\n",
            " -2.76944277e-02  3.38053523e-04 -1.79409670e+01 -1.80330365e+01\n",
            " -1.81886051e+01 -1.83066440e+01 -1.84360040e+01 -1.85186962e+01\n",
            " -1.85794869e+01 -1.86313251e+01 -1.86665996e+01 -1.88162199e+01\n",
            " -1.89628899e+01 -1.90983196e+01 -1.93873554e+01 -1.95917608e+01\n",
            " -1.97934320e+01 -1.99529702e+01 -2.00523043e+01 -2.02625362e+01\n",
            " -2.03717043e+01 -2.06140277e+01 -2.08440122e+01 -2.10229405e+01\n",
            " -2.13279838e+01 -2.15272127e+01 -2.17053788e+01 -2.19200933e+01\n",
            " -2.21939646e+01 -2.23682699e+01 -2.25595899e+01 -2.27544206e+01\n",
            " -2.29758092e+01 -2.33512168e+01 -2.35982287e+01 -2.37863042e+01\n",
            " -2.40278083e+01 -2.42411884e+01 -5.66564983e-02 -6.42783331e-02\n",
            " -4.56027896e-02 -2.80185263e-02  1.13451110e-02  7.70112905e-03\n",
            " -6.42576511e-03  2.12894762e-02 -4.60969297e-02 -4.29235054e-03\n",
            "  3.18576034e-03  2.56601522e-04 -3.59992449e-02  9.01377991e-04\n",
            " -1.59623914e+01 -1.61853454e+01 -1.62957824e+01 -1.63886808e+01\n",
            " -1.65767072e+01 -1.66669672e+01 -1.69857627e+01 -1.73858322e+01\n",
            " -1.74114292e+01 -1.75204483e+01 -1.78016291e+01 -1.79346823e+01\n",
            " -1.80598333e+01 -1.82104890e+01 -1.84082443e+01 -1.85649142e+01\n",
            " -1.91093289e+01 -1.92956894e+01 -1.93458014e+01 -1.95083831e+01\n",
            " -1.96906835e+01 -1.98605034e+01 -2.00845600e+01 -2.03254876e+01\n",
            " -2.06434139e+01 -2.08489704e+01 -2.10057181e+01 -2.11684876e+01\n",
            " -2.14224465e+01 -2.17994066e+01 -2.20263627e+01 -2.22664851e+01\n",
            " -2.24804852e+01 -2.28354610e+01 -2.30643680e+01 -2.33234217e+01\n",
            "  6.58451436e-01  6.06049166e-01  5.23186641e-01  4.37747112e-01\n",
            "  3.47058822e-01  3.49624763e-01  3.47597658e-01  2.67551546e-01\n",
            "  2.87057606e-01  1.63167730e-01  1.22571822e-01  1.24772680e-01\n",
            "  1.01653385e-01 -5.59810377e-04  5.09522721e+01  5.13987666e+01\n",
            "  5.19385076e+01  5.22743868e+01  5.27509154e+01  5.31734494e+01\n",
            "  5.36222497e+01  5.41234813e+01  5.46568854e+01  5.50243369e+01\n",
            "  5.55948987e+01  5.60138349e+01  5.66064075e+01  5.71223183e+01\n",
            "  5.77092707e+01  5.82822717e+01  5.89961357e+01  5.95956455e+01\n",
            "  6.03415741e+01  6.09449351e+01  6.15965625e+01  6.22616326e+01\n",
            "  6.29449681e+01  6.35967223e+01  6.42739316e+01  6.49516092e+01\n",
            "  6.56144361e+01  6.62722848e+01  6.69099074e+01  6.76513048e+01\n",
            "  6.83449738e+01  6.91192114e+01  6.98146694e+01  7.05736384e+01\n",
            "  7.12794166e+01  7.20024073e+01 -3.44778736e-01 -3.12178690e-01\n",
            " -2.80139212e-01 -2.45724312e-01 -1.92778219e-01 -1.90607907e-01\n",
            " -1.80190249e-01 -1.54035576e-01 -1.30125406e-01 -8.74014440e-02\n",
            " -7.02146330e-02 -6.86043556e-02 -3.79597125e-02 -6.79621137e-04]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(-0.3494, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 1.7434e-03,  4.5702e-04,  4.3806e-04,  ...,  1.5912e-03,\n",
            "         -1.7317e-03,  1.8677e-02],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 2.3525e-03,  6.1917e-04,  5.9392e-04,  ...,  2.1475e-03,\n",
            "         -2.3279e-03,  2.6411e-02],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-2.4219e-04, -6.8556e-05, -6.5540e-05,  ..., -2.2225e-04,\n",
            "          2.2690e-04, -4.5594e-03]])\n",
            "   Last layer:\n",
            "tensor([[ 0.0000e+00, -1.3084e-03,  0.0000e+00,  5.3738e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  8.6797e-03,  1.9355e-03,  0.0000e+00,\n",
            "          2.5416e-03,  0.0000e+00,  0.0000e+00,  7.2174e-04,  0.0000e+00,\n",
            "          3.4863e-03,  0.0000e+00,  5.3457e-03,  5.8730e-03,  0.0000e+00],\n",
            "        [ 0.0000e+00, -2.2501e-04,  0.0000e+00,  3.3804e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  6.7522e-03,  2.2420e-03,  0.0000e+00,\n",
            "          2.6075e-03,  0.0000e+00,  0.0000e+00,  8.5750e-04,  0.0000e+00,\n",
            "          3.0118e-03,  0.0000e+00,  4.5232e-03,  4.1149e-03,  0.0000e+00],\n",
            "        [ 0.0000e+00, -1.4663e-04,  0.0000e+00,  6.4938e-04,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  1.0734e-03,  2.5362e-04,  0.0000e+00,\n",
            "          3.2607e-04,  0.0000e+00,  0.0000e+00,  9.4995e-05,  0.0000e+00,\n",
            "          4.3670e-04,  0.0000e+00,  6.6795e-04,  7.1756e-04,  0.0000e+00],\n",
            "        [ 0.0000e+00, -1.0038e-04,  0.0000e+00,  1.7170e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  3.4595e-03,  1.1629e-03,  0.0000e+00,\n",
            "          1.3475e-03,  0.0000e+00,  0.0000e+00,  4.4571e-04,  0.0000e+00,\n",
            "          1.5479e-03,  0.0000e+00,  2.3241e-03,  2.0995e-03,  0.0000e+00],\n",
            "        [ 0.0000e+00, -2.8363e-04,  0.0000e+00,  6.8945e-04,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  8.6381e-04,  4.9536e-05,  0.0000e+00,\n",
            "          1.3130e-04,  0.0000e+00,  0.0000e+00,  1.3972e-05,  0.0000e+00,\n",
            "          2.8977e-04,  0.0000e+00,  4.6147e-04,  6.7247e-04,  0.0000e+00],\n",
            "        [ 0.0000e+00, -1.4133e-04,  0.0000e+00, -1.0072e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -2.4622e-03, -1.0256e-03,  0.0000e+00,\n",
            "         -1.1281e-03,  0.0000e+00,  0.0000e+00, -3.9662e-04,  0.0000e+00,\n",
            "         -1.1818e-03,  0.0000e+00, -1.7522e-03, -1.3724e-03,  0.0000e+00],\n",
            "        [ 0.0000e+00, -8.8549e-04,  0.0000e+00,  4.0029e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  6.6563e-03,  1.5946e-03,  0.0000e+00,\n",
            "          2.0422e-03,  0.0000e+00,  0.0000e+00,  5.9863e-04,  0.0000e+00,\n",
            "          2.7169e-03,  0.0000e+00,  4.1535e-03,  4.4362e-03,  0.0000e+00],\n",
            "        [ 0.0000e+00, -7.6987e-04,  0.0000e+00,  4.7231e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  8.4495e-03,  2.3525e-03,  0.0000e+00,\n",
            "          2.8745e-03,  0.0000e+00,  0.0000e+00,  8.9122e-04,  0.0000e+00,\n",
            "          3.5837e-03,  0.0000e+00,  5.4357e-03,  5.4285e-03,  0.0000e+00],\n",
            "        [ 0.0000e+00,  1.3856e-04,  0.0000e+00,  2.0200e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  4.6258e-03,  1.8076e-03,  0.0000e+00,\n",
            "          2.0192e-03,  0.0000e+00,  0.0000e+00,  6.9630e-04,  0.0000e+00,\n",
            "          2.1743e-03,  0.0000e+00,  3.2338e-03,  2.6512e-03,  0.0000e+00],\n",
            "        [ 0.0000e+00, -1.0015e-03,  0.0000e+00,  4.8688e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  8.2621e-03,  2.0681e-03,  0.0000e+00,\n",
            "          2.6135e-03,  0.0000e+00,  0.0000e+00,  7.7700e-04,  0.0000e+00,\n",
            "          3.4120e-03,  0.0000e+00,  5.2007e-03,  5.4509e-03,  0.0000e+00]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-2.1296e-02, -2.8833e-03, -1.7090e-02,  ..., -1.5477e-02,\n",
            "         -2.5327e-02, -6.0155e-01],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [-2.8940e-02, -4.0907e-03, -2.3298e-02,  ..., -2.1089e-02,\n",
            "         -3.4353e-02, -8.8975e-01],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 2.2561e-03,  5.5118e-04,  1.9096e-03,  ...,  1.7094e-03,\n",
            "          2.6085e-03,  1.4805e-01]])\n",
            "   Last layer:\n",
            "tensor([[ 0.0000,  0.0578,  0.0000, -0.1587,  0.0000,  0.0000,  0.0000, -0.2813,\n",
            "         -0.0827,  0.0000, -0.0786,  0.0000,  0.0000, -0.0223,  0.0000, -0.0828,\n",
            "          0.0000, -0.1778, -0.1816,  0.0000],\n",
            "        [ 0.0000,  0.0127,  0.0000, -0.0962,  0.0000,  0.0000,  0.0000, -0.2152,\n",
            "         -0.0814,  0.0000, -0.0901,  0.0000,  0.0000, -0.0259,  0.0000, -0.0888,\n",
            "          0.0000, -0.1474, -0.1244,  0.0000],\n",
            "        [ 0.0000,  0.0062,  0.0000, -0.0191,  0.0000,  0.0000,  0.0000, -0.0353,\n",
            "         -0.0110,  0.0000, -0.0109,  0.0000,  0.0000, -0.0031,  0.0000, -0.0113,\n",
            "          0.0000, -0.0227, -0.0223,  0.0000],\n",
            "        [ 0.0000,  0.0106,  0.0000, -0.0453,  0.0000,  0.0000,  0.0000, -0.0921,\n",
            "         -0.0319,  0.0000, -0.0337,  0.0000,  0.0000, -0.0097,  0.0000, -0.0339,\n",
            "          0.0000, -0.0613, -0.0556,  0.0000],\n",
            "        [ 0.0000,  0.0098,  0.0000, -0.0210,  0.0000,  0.0000,  0.0000, -0.0330,\n",
            "         -0.0080,  0.0000, -0.0064,  0.0000,  0.0000, -0.0018,  0.0000, -0.0073,\n",
            "          0.0000, -0.0198, -0.0227,  0.0000],\n",
            "        [ 0.0000,  0.0029,  0.0000,  0.0263,  0.0000,  0.0000,  0.0000,  0.0716,\n",
            "          0.0312,  0.0000,  0.0368,  0.0000,  0.0000,  0.0107,  0.0000,  0.0353,\n",
            "          0.0000,  0.0516,  0.0381,  0.0000],\n",
            "        [ 0.0000,  0.0438,  0.0000, -0.1180,  0.0000,  0.0000,  0.0000, -0.2073,\n",
            "         -0.0603,  0.0000, -0.0568,  0.0000,  0.0000, -0.0161,  0.0000, -0.0600,\n",
            "          0.0000, -0.1306, -0.1344,  0.0000],\n",
            "        [ 0.0000,  0.0383,  0.0000, -0.1375,  0.0000,  0.0000,  0.0000, -0.2673,\n",
            "         -0.0882,  0.0000, -0.0905,  0.0000,  0.0000, -0.0259,  0.0000, -0.0921,\n",
            "          0.0000, -0.1750, -0.1649,  0.0000],\n",
            "        [ 0.0000, -0.0082,  0.0000, -0.0526,  0.0000,  0.0000,  0.0000, -0.1481,\n",
            "         -0.0657,  0.0000, -0.0781,  0.0000,  0.0000, -0.0226,  0.0000, -0.0748,\n",
            "          0.0000, -0.1075, -0.0778,  0.0000],\n",
            "        [ 0.0000,  0.0382,  0.0000, -0.1423,  0.0000,  0.0000,  0.0000, -0.2795,\n",
            "         -0.0932,  0.0000, -0.0964,  0.0000,  0.0000, -0.0276,  0.0000, -0.0978,\n",
            "          0.0000, -0.1836, -0.1716,  0.0000]])\n",
            "DEBUGGING: training for one iteration takes 0.004387 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 19\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0203, 0.0141, 0.0143, 0.0143, 0.0139, 0.0147, 0.0140, 0.0144, 0.0139,\n",
            "        0.0142, 0.0139, 0.0140, 0.0139, 0.0144, 0.0139, 0.0143, 0.0141, 0.0140,\n",
            "        0.0142, 0.0144, 0.0142, 0.0145, 0.0139, 0.0139, 0.0145, 0.0146, 0.0145,\n",
            "        0.0144, 0.0140, 0.0145, 0.0142, 0.0139, 0.0146, 0.0142, 0.0149, 0.0141,\n",
            "        0.0142, 0.0143, 0.0141, 0.0143, 0.0141, 0.0144, 0.0140, 0.0145, 0.0145,\n",
            "        0.0144, 0.0141, 0.0138, 0.0139, 0.0140, 0.0140, 0.0143, 0.0140, 0.0141,\n",
            "        0.0139, 0.0144, 0.0144, 0.0145, 0.0142, 0.0146, 0.0141, 0.0140, 0.0142,\n",
            "        0.0141, 0.0141, 0.0143, 0.0141, 0.0135, 0.0144, 0.0142],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [11]\n",
            "DEBUGGING: logits looks like: tensor([25.1767, 25.0843, 25.0884, 25.0886, 25.0813, 25.0958, 25.0834, 25.0910,\n",
            "        25.0823, 25.0869, 25.0818, 25.0827, 25.0809, 25.0896, 25.0809, 25.0887,\n",
            "        25.0849, 25.0834, 25.0867, 25.0900, 25.0863, 25.0915, 25.0822, 25.0815,\n",
            "        25.0928, 25.0938, 25.0919, 25.0906, 25.0829, 25.0916, 25.0877, 25.0824,\n",
            "        25.0930, 25.0864, 25.0986, 25.0845, 25.0870, 25.0888, 25.0844, 25.0878,\n",
            "        25.0845, 25.0910, 25.0835, 25.0919, 25.0928, 25.0910, 25.0858, 25.0801,\n",
            "        25.0812, 25.0838, 25.0841, 25.0885, 25.0838, 25.0846, 25.0822, 25.0912,\n",
            "        25.0905, 25.0918, 25.0862, 25.0938, 25.0844, 25.0836, 25.0862, 25.0859,\n",
            "        25.0855, 25.0878, 25.0857, 25.0744, 25.0898, 25.0869],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0185, 0.0126, 0.0127, 0.0127, 0.0128, 0.0128, 0.0128, 0.0127, 0.0127,\n",
            "        0.0128, 0.0127, 0.0128, 0.0126, 0.0127, 0.0128, 0.0127, 0.0128, 0.0127,\n",
            "        0.0128, 0.0127, 0.0128, 0.0127, 0.0129, 0.0127, 0.0128, 0.0128, 0.0127,\n",
            "        0.0127, 0.0128, 0.0128, 0.0126, 0.0128, 0.0127, 0.0128, 0.0127, 0.0128,\n",
            "        0.0127, 0.0125, 0.0129, 0.0128, 0.0131, 0.0128, 0.0127, 0.0129, 0.0128,\n",
            "        0.0128, 0.0127, 0.0127, 0.0127, 0.0127, 0.0128, 0.0128, 0.0128, 0.0127,\n",
            "        0.0126, 0.0127, 0.0126, 0.0127, 0.0129, 0.0127, 0.0128, 0.0127, 0.0125,\n",
            "        0.0128, 0.0128, 0.0127, 0.0127, 0.0127, 0.0128, 0.0127, 0.0128, 0.0127,\n",
            "        0.0127, 0.0127, 0.0128, 0.0129, 0.0127, 0.0128],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [46]\n",
            "DEBUGGING: logits looks like: tensor([25.2012, 25.1060, 25.1084, 25.1071, 25.1089, 25.1095, 25.1094, 25.1074,\n",
            "        25.1072, 25.1091, 25.1082, 25.1089, 25.1054, 25.1080, 25.1085, 25.1084,\n",
            "        25.1094, 25.1082, 25.1090, 25.1077, 25.1100, 25.1084, 25.1108, 25.1078,\n",
            "        25.1089, 25.1088, 25.1067, 25.1080, 25.1085, 25.1098, 25.1064, 25.1084,\n",
            "        25.1076, 25.1095, 25.1079, 25.1094, 25.1079, 25.1038, 25.1115, 25.1088,\n",
            "        25.1147, 25.1087, 25.1078, 25.1107, 25.1093, 25.1095, 25.1073, 25.1077,\n",
            "        25.1083, 25.1084, 25.1086, 25.1090, 25.1090, 25.1071, 25.1062, 25.1080,\n",
            "        25.1046, 25.1074, 25.1110, 25.1071, 25.1089, 25.1081, 25.1038, 25.1085,\n",
            "        25.1090, 25.1075, 25.1083, 25.1072, 25.1098, 25.1073, 25.1097, 25.1071,\n",
            "        25.1075, 25.1083, 25.1096, 25.1120, 25.1079, 25.1097],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0122, 0.0116, 0.0119, 0.0121, 0.0109, 0.0109, 0.0116, 0.0120, 0.0118,\n",
            "        0.0121, 0.0122, 0.0127, 0.0117, 0.0119, 0.0125, 0.0113, 0.0118, 0.0116,\n",
            "        0.0122, 0.0123, 0.0117, 0.0119, 0.0119, 0.0121, 0.0118, 0.0113, 0.0116,\n",
            "        0.0116, 0.0117, 0.0122, 0.0120, 0.0109, 0.0121, 0.0116, 0.0113, 0.0118,\n",
            "        0.0114, 0.0120, 0.0107, 0.0123, 0.0125, 0.0120, 0.0116, 0.0114, 0.0125,\n",
            "        0.0110, 0.0113, 0.0116, 0.0119, 0.0116, 0.0115, 0.0115, 0.0109, 0.0123,\n",
            "        0.0123, 0.0120, 0.0120, 0.0116, 0.0106, 0.0117, 0.0108, 0.0121, 0.0109,\n",
            "        0.0122, 0.0124, 0.0116, 0.0118, 0.0119, 0.0114, 0.0124, 0.0119, 0.0119,\n",
            "        0.0119, 0.0118, 0.0108, 0.0118, 0.0118, 0.0126, 0.0117, 0.0118, 0.0120,\n",
            "        0.0116, 0.0113, 0.0117, 0.0123], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [82]\n",
            "DEBUGGING: logits looks like: tensor([25.1582, 25.1448, 25.1509, 25.1554, 25.1280, 25.1296, 25.1452, 25.1534,\n",
            "        25.1498, 25.1552, 25.1572, 25.1680, 25.1478, 25.1518, 25.1637, 25.1375,\n",
            "        25.1486, 25.1451, 25.1579, 25.1583, 25.1465, 25.1510, 25.1503, 25.1546,\n",
            "        25.1494, 25.1380, 25.1453, 25.1451, 25.1478, 25.1570, 25.1541, 25.1286,\n",
            "        25.1548, 25.1452, 25.1380, 25.1492, 25.1413, 25.1540, 25.1244, 25.1589,\n",
            "        25.1639, 25.1531, 25.1457, 25.1393, 25.1638, 25.1313, 25.1388, 25.1451,\n",
            "        25.1509, 25.1444, 25.1423, 25.1421, 25.1301, 25.1587, 25.1596, 25.1525,\n",
            "        25.1532, 25.1449, 25.1210, 25.1478, 25.1263, 25.1553, 25.1301, 25.1575,\n",
            "        25.1604, 25.1456, 25.1496, 25.1502, 25.1407, 25.1616, 25.1515, 25.1516,\n",
            "        25.1519, 25.1481, 25.1268, 25.1496, 25.1498, 25.1648, 25.1469, 25.1488,\n",
            "        25.1527, 25.1457, 25.1378, 25.1468, 25.1588], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0103, 0.0103, 0.0104, 0.0104, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0104, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0104, 0.0103,\n",
            "        0.0105, 0.0102, 0.0106, 0.0103, 0.0102, 0.0103, 0.0103, 0.0104, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0104, 0.0103, 0.0103, 0.0103, 0.0104, 0.0103,\n",
            "        0.0104, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0102, 0.0102, 0.0103,\n",
            "        0.0104, 0.0103, 0.0104, 0.0103, 0.0104, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0102, 0.0104, 0.0103, 0.0104, 0.0103, 0.0104, 0.0102, 0.0102,\n",
            "        0.0103, 0.0102, 0.0103, 0.0103, 0.0103, 0.0103, 0.0104, 0.0102, 0.0103,\n",
            "        0.0104, 0.0102, 0.0103, 0.0103, 0.0102, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0102, 0.0103, 0.0104, 0.0103, 0.0104, 0.0104, 0.0104, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0102, 0.0102, 0.0103],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [94]\n",
            "DEBUGGING: logits looks like: tensor([25.1403, 25.1410, 25.1416, 25.1419, 25.1405, 25.1400, 25.1394, 25.1413,\n",
            "        25.1396, 25.1389, 25.1420, 25.1400, 25.1405, 25.1398, 25.1412, 25.1393,\n",
            "        25.1418, 25.1408, 25.1441, 25.1385, 25.1473, 25.1402, 25.1382, 25.1392,\n",
            "        25.1409, 25.1431, 25.1402, 25.1394, 25.1403, 25.1399, 25.1416, 25.1393,\n",
            "        25.1391, 25.1406, 25.1418, 25.1396, 25.1419, 25.1403, 25.1390, 25.1398,\n",
            "        25.1404, 25.1402, 25.1383, 25.1385, 25.1402, 25.1413, 25.1391, 25.1426,\n",
            "        25.1395, 25.1424, 25.1411, 25.1406, 25.1407, 25.1398, 25.1407, 25.1387,\n",
            "        25.1414, 25.1409, 25.1416, 25.1409, 25.1416, 25.1378, 25.1381, 25.1404,\n",
            "        25.1383, 25.1404, 25.1395, 25.1405, 25.1397, 25.1413, 25.1388, 25.1389,\n",
            "        25.1420, 25.1384, 25.1410, 25.1400, 25.1386, 25.1394, 25.1399, 25.1406,\n",
            "        25.1389, 25.1400, 25.1380, 25.1402, 25.1424, 25.1392, 25.1418, 25.1421,\n",
            "        25.1421, 25.1413, 25.1400, 25.1395, 25.1394, 25.1389, 25.1386, 25.1388,\n",
            "        25.1402], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0095, 0.0097, 0.0095, 0.0094, 0.0094, 0.0096, 0.0094, 0.0098, 0.0087,\n",
            "        0.0092, 0.0099, 0.0092, 0.0096, 0.0094, 0.0094, 0.0094, 0.0094, 0.0096,\n",
            "        0.0088, 0.0093, 0.0090, 0.0099, 0.0092, 0.0085, 0.0100, 0.0100, 0.0090,\n",
            "        0.0098, 0.0093, 0.0090, 0.0094, 0.0096, 0.0091, 0.0096, 0.0090, 0.0089,\n",
            "        0.0088, 0.0098, 0.0091, 0.0091, 0.0098, 0.0095, 0.0078, 0.0099, 0.0092,\n",
            "        0.0094, 0.0088, 0.0096, 0.0093, 0.0092, 0.0085, 0.0094, 0.0097, 0.0091,\n",
            "        0.0087, 0.0097, 0.0089, 0.0093, 0.0090, 0.0093, 0.0092, 0.0080, 0.0090,\n",
            "        0.0095, 0.0095, 0.0097, 0.0093, 0.0095, 0.0089, 0.0085, 0.0098, 0.0090,\n",
            "        0.0090, 0.0091, 0.0095, 0.0083, 0.0096, 0.0092, 0.0092, 0.0093, 0.0094,\n",
            "        0.0091, 0.0097, 0.0092, 0.0092, 0.0091, 0.0093, 0.0097, 0.0091, 0.0085,\n",
            "        0.0090, 0.0080, 0.0091, 0.0096, 0.0094, 0.0089, 0.0093, 0.0094, 0.0097,\n",
            "        0.0102, 0.0091, 0.0088, 0.0097, 0.0096, 0.0091, 0.0097, 0.0084, 0.0093],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [12]\n",
            "DEBUGGING: logits looks like: tensor([25.1893, 25.1930, 25.1872, 25.1858, 25.1864, 25.1922, 25.1846, 25.1972,\n",
            "        25.1671, 25.1794, 25.1982, 25.1797, 25.1916, 25.1864, 25.1853, 25.1866,\n",
            "        25.1848, 25.1924, 25.1687, 25.1832, 25.1737, 25.1977, 25.1805, 25.1616,\n",
            "        25.2004, 25.2001, 25.1756, 25.1963, 25.1830, 25.1758, 25.1858, 25.1899,\n",
            "        25.1767, 25.1914, 25.1752, 25.1730, 25.1707, 25.1952, 25.1776, 25.1791,\n",
            "        25.1963, 25.1877, 25.1378, 25.1992, 25.1813, 25.1852, 25.1699, 25.1919,\n",
            "        25.1845, 25.1802, 25.1596, 25.1849, 25.1938, 25.1769, 25.1672, 25.1942,\n",
            "        25.1726, 25.1845, 25.1742, 25.1818, 25.1804, 25.1450, 25.1763, 25.1876,\n",
            "        25.1874, 25.1942, 25.1819, 25.1898, 25.1715, 25.1604, 25.1967, 25.1736,\n",
            "        25.1762, 25.1776, 25.1880, 25.1551, 25.1918, 25.1808, 25.1814, 25.1822,\n",
            "        25.1870, 25.1789, 25.1935, 25.1798, 25.1803, 25.1766, 25.1836, 25.1940,\n",
            "        25.1788, 25.1611, 25.1762, 25.1460, 25.1777, 25.1918, 25.1853, 25.1709,\n",
            "        25.1823, 25.1852, 25.1944, 25.2071, 25.1769, 25.1689, 25.1949, 25.1913,\n",
            "        25.1770, 25.1933, 25.1586, 25.1825], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.607247807314252 and immediate abs rewards look like: [0.034180376574113325, 0.041621466182050426, 0.02617224323694245, 0.022342098561239254, 0.0005575066625169711, 0.00293490732610735, 0.005470721440815396, 0.005088094354960049, 0.00032876294790185057, 0.00024231236329796957, 0.00023323366531258216, 0.0005810555837797438, 0.0030423270652590872, 0.002836331595062802, 0.00028157685346741346, 0.0017896414167353214, 0.0015260354991823988, 0.002847518977432628, 0.0005109734852339898, 0.0021354631503527344, 0.001221170597091259, 0.0017406261702035408, 0.007577339489216683, 0.4419860241050628, 4.547473508864641e-13, 2.2737367544323206e-13, 4.547473508864641e-13, 6.821210263296962e-13, 0.0, 4.547473508864641e-13, 6.821210263296962e-13, 0.0, 9.094947017729282e-13, 6.821210263296962e-13, 2.2737367544323206e-13, 0.0, 2.2737367544323206e-13, 2.2737367544323206e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 6.821210263296962e-13, 1.1368683772161603e-12, 1.1368683772161603e-12, 4.547473508864641e-13, 4.547473508864641e-13, 2.2737367544323206e-13, 0.0, 2.2737367544323206e-13, 4.547473508864641e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 33.34127409331306 and immediate relative rewards look like: [0.09475472271384545, 0.23297330368914185, 0.22233592273271652, 0.2549541101502279, 0.008003410960979205, 0.05056735252912718, 0.11006098335988396, 0.11717076590723498, 0.008529745035654363, 0.006985981157121176, 0.007397178519747479, 0.020105288455319097, 0.11406001025060236, 0.11461736024144331, 0.0122013890358505, 0.0827260706666718, 0.07498848071126014, 0.14822168378715017, 0.028098448794508374, 0.12362795556930935, 0.07427775714234747, 0.11095451417217406, 0.5052199072244467, 30.818441749135992, 3.789561257387488e-11, 1.9705718538411953e-11, 4.0927261579781764e-11, 6.366462912409533e-11, 0.0, 4.5474735088649856e-11, 7.048583938739659e-11, 0.0, 1.0004441719503728e-10, 7.730704965068719e-11, 2.6526928801712415e-11, 0.0, 2.8042753304665286e-11, 2.8800665556140544e-11, 5.911715561524034e-11, 6.063298011818603e-11, 0.0, 9.549694368615746e-11, 1.629511340676126e-10, 1.6674069532506213e-10, 6.821210263295411e-11, 6.972792713591922e-11, 3.562187581944239e-11, 0.0, 3.7137700322394565e-11, 7.579122514774975e-11]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0172, 0.0144, 0.0148, 0.0160, 0.0146, 0.0154, 0.0152, 0.0100, 0.0164,\n",
            "        0.0154, 0.0144, 0.0144, 0.0138, 0.0149, 0.0187, 0.0147, 0.0126, 0.0159,\n",
            "        0.0156, 0.0168, 0.0116, 0.0144, 0.0144, 0.0148, 0.0168, 0.0145, 0.0134,\n",
            "        0.0145, 0.0149, 0.0148, 0.0168, 0.0153, 0.0149, 0.0173, 0.0140, 0.0147,\n",
            "        0.0175, 0.0155, 0.0137, 0.0142, 0.0159, 0.0137, 0.0143, 0.0142, 0.0154,\n",
            "        0.0142, 0.0154, 0.0140, 0.0152, 0.0165, 0.0131, 0.0152, 0.0151, 0.0151,\n",
            "        0.0147, 0.0116, 0.0113, 0.0144, 0.0161, 0.0145, 0.0165, 0.0168, 0.0149,\n",
            "        0.0146, 0.0145, 0.0177, 0.0154], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [52]\n",
            "DEBUGGING: logits looks like: tensor([25.2989, 25.2546, 25.2608, 25.2809, 25.2586, 25.2719, 25.2685, 25.1637,\n",
            "        25.2868, 25.2719, 25.2537, 25.2546, 25.2437, 25.2623, 25.3193, 25.2596,\n",
            "        25.2207, 25.2792, 25.2751, 25.2927, 25.2002, 25.2544, 25.2548, 25.2612,\n",
            "        25.2934, 25.2566, 25.2362, 25.2567, 25.2631, 25.2610, 25.2935, 25.2691,\n",
            "        25.2633, 25.3000, 25.2466, 25.2589, 25.3030, 25.2720, 25.2426, 25.2515,\n",
            "        25.2799, 25.2422, 25.2530, 25.2513, 25.2719, 25.2515, 25.2713, 25.2470,\n",
            "        25.2682, 25.2882, 25.2313, 25.2685, 25.2662, 25.2663, 25.2590, 25.2009,\n",
            "        25.1932, 25.2551, 25.2829, 25.2569, 25.2886, 25.2928, 25.2637, 25.2573,\n",
            "        25.2569, 25.3061, 25.2718], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0154, 0.0112, 0.0125, 0.0131, 0.0123, 0.0124, 0.0129, 0.0129, 0.0125,\n",
            "        0.0128, 0.0125, 0.0126, 0.0118, 0.0131, 0.0125, 0.0128, 0.0126, 0.0126,\n",
            "        0.0125, 0.0125, 0.0126, 0.0127, 0.0128, 0.0126, 0.0132, 0.0128, 0.0120,\n",
            "        0.0129, 0.0125, 0.0126, 0.0127, 0.0127, 0.0127, 0.0128, 0.0123, 0.0121,\n",
            "        0.0129, 0.0126, 0.0127, 0.0122, 0.0129, 0.0127, 0.0127, 0.0126, 0.0128,\n",
            "        0.0127, 0.0125, 0.0124, 0.0126, 0.0125, 0.0133, 0.0131, 0.0126, 0.0126,\n",
            "        0.0127, 0.0125, 0.0130, 0.0125, 0.0121, 0.0134, 0.0122, 0.0124, 0.0125,\n",
            "        0.0126, 0.0126, 0.0127, 0.0127, 0.0130, 0.0125, 0.0125, 0.0122, 0.0127,\n",
            "        0.0127, 0.0127, 0.0124, 0.0124, 0.0133, 0.0128, 0.0128],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [8]\n",
            "DEBUGGING: logits looks like: tensor([25.3501, 25.2694, 25.2962, 25.3095, 25.2940, 25.2955, 25.3060, 25.3052,\n",
            "        25.2981, 25.3026, 25.2964, 25.2996, 25.2824, 25.3082, 25.2975, 25.3039,\n",
            "        25.3001, 25.2992, 25.2978, 25.2971, 25.3000, 25.3019, 25.3025, 25.2986,\n",
            "        25.3115, 25.3032, 25.2863, 25.3053, 25.2981, 25.2998, 25.3017, 25.3002,\n",
            "        25.3018, 25.3021, 25.2922, 25.2900, 25.3058, 25.2994, 25.3006, 25.2916,\n",
            "        25.3056, 25.3003, 25.3017, 25.2998, 25.3025, 25.3013, 25.2978, 25.2956,\n",
            "        25.2995, 25.2974, 25.3118, 25.3089, 25.2988, 25.2999, 25.3003, 25.2967,\n",
            "        25.3061, 25.2972, 25.2890, 25.3150, 25.2916, 25.2958, 25.2964, 25.2987,\n",
            "        25.2985, 25.3007, 25.3009, 25.3068, 25.2962, 25.2970, 25.2905, 25.3004,\n",
            "        25.3016, 25.3002, 25.2961, 25.2956, 25.3130, 25.3025, 25.3035],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0136, 0.0112, 0.0110, 0.0112, 0.0112, 0.0113, 0.0113, 0.0112, 0.0113,\n",
            "        0.0111, 0.0112, 0.0112, 0.0111, 0.0113, 0.0111, 0.0113, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0111, 0.0112, 0.0112, 0.0111,\n",
            "        0.0112, 0.0112, 0.0113, 0.0113, 0.0111, 0.0111, 0.0112, 0.0113, 0.0113,\n",
            "        0.0111, 0.0114, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0113, 0.0112,\n",
            "        0.0113, 0.0112, 0.0112, 0.0112, 0.0113, 0.0111, 0.0112, 0.0111, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0113, 0.0112, 0.0112,\n",
            "        0.0111, 0.0111, 0.0113, 0.0112, 0.0112, 0.0112, 0.0113, 0.0112, 0.0112,\n",
            "        0.0113, 0.0112, 0.0113, 0.0113, 0.0113, 0.0111, 0.0110, 0.0112, 0.0112,\n",
            "        0.0113, 0.0113, 0.0112, 0.0111, 0.0112, 0.0112, 0.0112, 0.0111],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [18]\n",
            "DEBUGGING: logits looks like: tensor([25.3624, 25.3133, 25.3104, 25.3134, 25.3127, 25.3149, 25.3159, 25.3141,\n",
            "        25.3160, 25.3118, 25.3137, 25.3139, 25.3119, 25.3149, 25.3120, 25.3153,\n",
            "        25.3140, 25.3144, 25.3148, 25.3141, 25.3128, 25.3138, 25.3139, 25.3121,\n",
            "        25.3142, 25.3145, 25.3121, 25.3139, 25.3134, 25.3160, 25.3150, 25.3125,\n",
            "        25.3113, 25.3148, 25.3159, 25.3170, 25.3121, 25.3188, 25.3128, 25.3135,\n",
            "        25.3141, 25.3136, 25.3142, 25.3155, 25.3137, 25.3157, 25.3142, 25.3127,\n",
            "        25.3127, 25.3152, 25.3114, 25.3148, 25.3123, 25.3137, 25.3143, 25.3134,\n",
            "        25.3144, 25.3132, 25.3142, 25.3139, 25.3162, 25.3144, 25.3131, 25.3125,\n",
            "        25.3117, 25.3166, 25.3148, 25.3140, 25.3142, 25.3151, 25.3139, 25.3128,\n",
            "        25.3151, 25.3128, 25.3154, 25.3166, 25.3165, 25.3125, 25.3101, 25.3140,\n",
            "        25.3146, 25.3150, 25.3158, 25.3146, 25.3120, 25.3142, 25.3148, 25.3143,\n",
            "        25.3121], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0125, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0102, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0102, 0.0103, 0.0103, 0.0103, 0.0103, 0.0102,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0102, 0.0104, 0.0103, 0.0102, 0.0104, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0102,\n",
            "        0.0103, 0.0103, 0.0102, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0104, 0.0102, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [57]\n",
            "DEBUGGING: logits looks like: tensor([25.3724, 25.3230, 25.3232, 25.3233, 25.3234, 25.3233, 25.3249, 25.3232,\n",
            "        25.3237, 25.3230, 25.3235, 25.3242, 25.3230, 25.3223, 25.3235, 25.3231,\n",
            "        25.3235, 25.3237, 25.3232, 25.3229, 25.3231, 25.3223, 25.3233, 25.3231,\n",
            "        25.3236, 25.3234, 25.3226, 25.3245, 25.3232, 25.3238, 25.3241, 25.3241,\n",
            "        25.3231, 25.3240, 25.3228, 25.3234, 25.3230, 25.3235, 25.3238, 25.3235,\n",
            "        25.3234, 25.3235, 25.3237, 25.3235, 25.3237, 25.3244, 25.3216, 25.3253,\n",
            "        25.3237, 25.3227, 25.3253, 25.3239, 25.3232, 25.3234, 25.3232, 25.3233,\n",
            "        25.3241, 25.3234, 25.3232, 25.3248, 25.3231, 25.3232, 25.3240, 25.3239,\n",
            "        25.3247, 25.3229, 25.3237, 25.3239, 25.3246, 25.3234, 25.3241, 25.3232,\n",
            "        25.3237, 25.3234, 25.3245, 25.3232, 25.3236, 25.3227, 25.3236, 25.3242,\n",
            "        25.3227, 25.3238, 25.3236, 25.3226, 25.3246, 25.3249, 25.3238, 25.3237,\n",
            "        25.3238, 25.3227, 25.3266, 25.3226, 25.3233, 25.3228, 25.3239, 25.3234,\n",
            "        25.3230], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0117, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096,\n",
            "        0.0095, 0.0095, 0.0096, 0.0096, 0.0097, 0.0096, 0.0096, 0.0096, 0.0096,\n",
            "        0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096,\n",
            "        0.0095, 0.0096, 0.0096, 0.0096, 0.0096, 0.0097, 0.0096, 0.0096, 0.0096,\n",
            "        0.0096, 0.0096, 0.0095, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096,\n",
            "        0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096,\n",
            "        0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096,\n",
            "        0.0096, 0.0097, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0097,\n",
            "        0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096,\n",
            "        0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096,\n",
            "        0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096,\n",
            "        0.0096, 0.0095, 0.0096, 0.0096, 0.0096], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [71]\n",
            "DEBUGGING: logits looks like: tensor([25.3805, 25.3310, 25.3312, 25.3317, 25.3313, 25.3299, 25.3309, 25.3299,\n",
            "        25.3313, 25.3290, 25.3291, 25.3311, 25.3297, 25.3321, 25.3302, 25.3310,\n",
            "        25.3308, 25.3306, 25.3296, 25.3308, 25.3308, 25.3313, 25.3310, 25.3308,\n",
            "        25.3315, 25.3297, 25.3310, 25.3294, 25.3301, 25.3305, 25.3312, 25.3310,\n",
            "        25.3323, 25.3317, 25.3297, 25.3307, 25.3303, 25.3311, 25.3279, 25.3306,\n",
            "        25.3316, 25.3296, 25.3299, 25.3316, 25.3306, 25.3310, 25.3308, 25.3304,\n",
            "        25.3302, 25.3307, 25.3308, 25.3304, 25.3309, 25.3313, 25.3306, 25.3311,\n",
            "        25.3308, 25.3309, 25.3309, 25.3313, 25.3309, 25.3306, 25.3297, 25.3316,\n",
            "        25.3322, 25.3312, 25.3310, 25.3311, 25.3304, 25.3295, 25.3301, 25.3323,\n",
            "        25.3304, 25.3296, 25.3304, 25.3303, 25.3305, 25.3300, 25.3306, 25.3308,\n",
            "        25.3306, 25.3308, 25.3311, 25.3310, 25.3299, 25.3300, 25.3309, 25.3308,\n",
            "        25.3316, 25.3303, 25.3308, 25.3305, 25.3311, 25.3308, 25.3309, 25.3297,\n",
            "        25.3302, 25.3304, 25.3300, 25.3297, 25.3293, 25.3314, 25.3306, 25.3304],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.1492805349516857 and immediate abs rewards look like: [0.01472099603006427, 0.0034223400507471524, 0.00422702223113447, 0.0023727202460577246, 0.0007456652915607265, 0.011327275363782974, 0.002529502608922485, 0.040865983781259274, 0.0030479320816994004, 0.011097868791239307, 0.0003778725886149914, 0.007265723128512036, 0.012283266491522227, 0.008838156090860139, 0.0001701568039607082, 0.0010955417264995049, 0.005759741694419063, 0.0008221702182709123, 0.0006160763582556683, 0.0034059369445458287, 0.008479287267164182, 0.001093743947421899, 7.457203037120053e-05, 0.0010798890266414674, 9.281004122385639e-05, 0.000616610085216962, 1.8878463833971182e-05, 3.0312052331282757e-05, 8.15418566162407e-05, 0.0001290894347221183, 0.0002762836879810493, 7.561372012787615e-05, 2.1136441318958532e-05, 1.5817915937077487e-05, 3.448973166086944e-05, 1.5922123566269875e-06, 1.7534171092847828e-06, 0.00012510674059740268, 0.0003228598925488768, 1.5182924926193664e-05, 6.673226380371489e-07, 0.0002939429468824528, 7.653589818801265e-06, 0.001208914186918264, 3.832570200756891e-06, 4.4072257878724486e-05, 1.3803527508571278e-05, 3.980771543865558e-05, 1.9361753402336035e-05, 6.99596689628379e-05]\n",
            "DEBUGGING: the total relative reward of the trajectory = 5.147893592098378 and immediate relative rewards look like: [0.04746700863243581, 0.022175539647923813, 0.04112999260443419, 0.03082514997859436, 0.012118459411375375, 0.22096091556967637, 0.05777956188883287, 1.0677058997767503, 0.09079935290182707, 0.3677163305405329, 0.013823286865816907, 0.2899932684652232, 0.5323972992751057, 0.4142386008370633, 0.008570148705339566, 0.05886016820708634, 0.3289154828586039, 0.04980898829184658, 0.039407765624423954, 0.2293772557477181, 0.6002895308720957, 0.08135114822272, 0.005800825077075978, 0.08765713844303841, 0.00785037754079737, 0.05424418375989881, 0.0017250045312437571, 0.0028723408308524655, 0.008002867880099991, 0.013106628579202235, 0.028987779640218477, 0.0081901000848729, 0.002360997617172969, 0.001820459527293566, 0.004086138029642801, 0.000194027699401002, 0.00021960763833486405, 0.01609255645999286, 0.042624356203583715, 0.0020560895771818783, 9.262933424681687e-05, 0.041796629388322404, 0.001114309379725791, 0.18010319761305216, 0.0005841894494954426, 0.006867122158013508, 0.0021975869491121687, 0.006472449139596691, 0.003213710676664708, 0.011849133964817031]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0278, 0.0132, 0.0145, 0.0140, 0.0140, 0.0155, 0.0144, 0.0127, 0.0150,\n",
            "        0.0145, 0.0155, 0.0151, 0.0144, 0.0149, 0.0151, 0.0145, 0.0141, 0.0138,\n",
            "        0.0141, 0.0149, 0.0138, 0.0141, 0.0137, 0.0146, 0.0159, 0.0130, 0.0140,\n",
            "        0.0153, 0.0139, 0.0155, 0.0142, 0.0141, 0.0140, 0.0137, 0.0132, 0.0126,\n",
            "        0.0136, 0.0130, 0.0147, 0.0133, 0.0133, 0.0147, 0.0157, 0.0147, 0.0119,\n",
            "        0.0148, 0.0159, 0.0142, 0.0136, 0.0138, 0.0148, 0.0143, 0.0127, 0.0166,\n",
            "        0.0126, 0.0142, 0.0138, 0.0152, 0.0148, 0.0147, 0.0153, 0.0143, 0.0149,\n",
            "        0.0148, 0.0133, 0.0141, 0.0161, 0.0150, 0.0141],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [12]\n",
            "DEBUGGING: logits looks like: tensor([25.1291, 24.9432, 24.9667, 24.9566, 24.9581, 24.9826, 24.9644, 24.9323,\n",
            "        24.9748, 24.9663, 24.9824, 24.9761, 24.9642, 24.9734, 24.9756, 24.9656,\n",
            "        24.9594, 24.9533, 24.9598, 24.9731, 24.9542, 24.9597, 24.9524, 24.9675,\n",
            "        24.9892, 24.9387, 24.9580, 24.9788, 24.9548, 24.9835, 24.9605, 24.9587,\n",
            "        24.9566, 24.9518, 24.9421, 24.9316, 24.9497, 24.9387, 24.9692, 24.9437,\n",
            "        24.9452, 24.9688, 24.9867, 24.9692, 24.9176, 24.9707, 24.9890, 24.9601,\n",
            "        24.9500, 24.9539, 24.9711, 24.9622, 24.9327, 25.0001, 24.9306, 24.9608,\n",
            "        24.9537, 24.9781, 24.9707, 24.9700, 24.9791, 24.9629, 24.9723, 24.9712,\n",
            "        24.9451, 24.9594, 24.9925, 24.9746, 24.9588], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0246, 0.0129, 0.0126, 0.0128, 0.0128, 0.0124, 0.0128, 0.0132, 0.0129,\n",
            "        0.0120, 0.0130, 0.0125, 0.0125, 0.0137, 0.0130, 0.0128, 0.0131, 0.0130,\n",
            "        0.0133, 0.0137, 0.0133, 0.0133, 0.0129, 0.0131, 0.0130, 0.0127, 0.0122,\n",
            "        0.0132, 0.0129, 0.0132, 0.0128, 0.0126, 0.0127, 0.0128, 0.0134, 0.0132,\n",
            "        0.0131, 0.0131, 0.0126, 0.0130, 0.0118, 0.0126, 0.0126, 0.0128, 0.0125,\n",
            "        0.0131, 0.0124, 0.0126, 0.0128, 0.0131, 0.0123, 0.0119, 0.0128, 0.0132,\n",
            "        0.0127, 0.0124, 0.0131, 0.0130, 0.0129, 0.0132, 0.0130, 0.0129, 0.0130,\n",
            "        0.0127, 0.0125, 0.0131, 0.0132, 0.0123, 0.0126, 0.0130, 0.0126, 0.0125,\n",
            "        0.0125, 0.0128, 0.0126, 0.0133, 0.0128], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [34]\n",
            "DEBUGGING: logits looks like: tensor([25.1444, 24.9835, 24.9768, 24.9805, 24.9816, 24.9736, 24.9820, 24.9885,\n",
            "        24.9827, 24.9645, 24.9843, 24.9750, 24.9745, 24.9973, 24.9856, 24.9818,\n",
            "        24.9872, 24.9851, 24.9906, 24.9972, 24.9903, 24.9909, 24.9835, 24.9874,\n",
            "        24.9855, 24.9789, 24.9687, 24.9894, 24.9839, 24.9893, 24.9810, 24.9777,\n",
            "        24.9790, 24.9820, 24.9918, 24.9890, 24.9873, 24.9859, 24.9779, 24.9850,\n",
            "        24.9611, 24.9764, 24.9767, 24.9804, 24.9746, 24.9865, 24.9724, 24.9775,\n",
            "        24.9805, 24.9872, 24.9701, 24.9635, 24.9814, 24.9894, 24.9784, 24.9736,\n",
            "        24.9866, 24.9849, 24.9836, 24.9888, 24.9854, 24.9830, 24.9845, 24.9787,\n",
            "        24.9753, 24.9873, 24.9888, 24.9718, 24.9778, 24.9842, 24.9780, 24.9746,\n",
            "        24.9744, 24.9813, 24.9778, 24.9904, 24.9818], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0222, 0.0112, 0.0114, 0.0113, 0.0113, 0.0112, 0.0113, 0.0114, 0.0113,\n",
            "        0.0113, 0.0112, 0.0112, 0.0112, 0.0114, 0.0113, 0.0113, 0.0111, 0.0112,\n",
            "        0.0112, 0.0114, 0.0114, 0.0111, 0.0113, 0.0112, 0.0112, 0.0113, 0.0112,\n",
            "        0.0112, 0.0111, 0.0114, 0.0114, 0.0113, 0.0111, 0.0112, 0.0114, 0.0111,\n",
            "        0.0113, 0.0112, 0.0112, 0.0113, 0.0113, 0.0111, 0.0113, 0.0112, 0.0113,\n",
            "        0.0113, 0.0112, 0.0113, 0.0112, 0.0109, 0.0112, 0.0113, 0.0113, 0.0111,\n",
            "        0.0112, 0.0111, 0.0112, 0.0112, 0.0112, 0.0112, 0.0111, 0.0113, 0.0110,\n",
            "        0.0113, 0.0111, 0.0114, 0.0112, 0.0112, 0.0112, 0.0113, 0.0111, 0.0112,\n",
            "        0.0112, 0.0113, 0.0114, 0.0112, 0.0112, 0.0113, 0.0112, 0.0113, 0.0113,\n",
            "        0.0112, 0.0111, 0.0115, 0.0113, 0.0116, 0.0112, 0.0113],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [14]\n",
            "DEBUGGING: logits looks like: tensor([25.1321, 24.9601, 24.9645, 24.9637, 24.9634, 24.9606, 24.9632, 24.9643,\n",
            "        24.9621, 24.9630, 24.9605, 24.9605, 24.9614, 24.9641, 24.9627, 24.9619,\n",
            "        24.9584, 24.9616, 24.9615, 24.9645, 24.9660, 24.9591, 24.9635, 24.9609,\n",
            "        24.9612, 24.9633, 24.9617, 24.9610, 24.9580, 24.9641, 24.9643, 24.9621,\n",
            "        24.9595, 24.9609, 24.9645, 24.9596, 24.9624, 24.9615, 24.9610, 24.9634,\n",
            "        24.9623, 24.9583, 24.9632, 24.9616, 24.9630, 24.9628, 24.9599, 24.9620,\n",
            "        24.9606, 24.9541, 24.9598, 24.9634, 24.9632, 24.9590, 24.9606, 24.9574,\n",
            "        24.9609, 24.9611, 24.9600, 24.9602, 24.9593, 24.9633, 24.9557, 24.9621,\n",
            "        24.9577, 24.9652, 24.9600, 24.9605, 24.9614, 24.9633, 24.9588, 24.9605,\n",
            "        24.9606, 24.9627, 24.9641, 24.9598, 24.9597, 24.9635, 24.9599, 24.9622,\n",
            "        24.9625, 24.9616, 24.9577, 24.9670, 24.9633, 24.9686, 24.9608, 24.9639],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0203, 0.0103, 0.0103, 0.0104, 0.0103, 0.0102, 0.0103, 0.0103, 0.0103,\n",
            "        0.0104, 0.0103, 0.0104, 0.0102, 0.0103, 0.0103, 0.0103, 0.0104, 0.0103,\n",
            "        0.0104, 0.0103, 0.0103, 0.0105, 0.0103, 0.0104, 0.0104, 0.0103, 0.0103,\n",
            "        0.0104, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0104, 0.0104, 0.0103, 0.0104, 0.0103, 0.0104, 0.0104, 0.0102,\n",
            "        0.0103, 0.0103, 0.0103, 0.0106, 0.0102, 0.0104, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0102, 0.0103, 0.0103, 0.0103, 0.0102, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0102, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0102, 0.0103, 0.0102, 0.0104, 0.0103, 0.0104, 0.0104, 0.0103, 0.0102,\n",
            "        0.0103, 0.0103, 0.0103, 0.0102, 0.0103, 0.0102, 0.0104, 0.0103, 0.0103,\n",
            "        0.0104, 0.0103, 0.0103, 0.0103, 0.0104, 0.0103],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [81]\n",
            "DEBUGGING: logits looks like: tensor([25.1248, 24.9562, 24.9557, 24.9568, 24.9548, 24.9528, 24.9545, 24.9546,\n",
            "        24.9557, 24.9569, 24.9546, 24.9567, 24.9539, 24.9549, 24.9558, 24.9560,\n",
            "        24.9569, 24.9558, 24.9568, 24.9547, 24.9565, 24.9593, 24.9555, 24.9567,\n",
            "        24.9568, 24.9551, 24.9558, 24.9589, 24.9554, 24.9566, 24.9551, 24.9560,\n",
            "        24.9552, 24.9546, 24.9552, 24.9554, 24.9557, 24.9583, 24.9573, 24.9566,\n",
            "        24.9570, 24.9565, 24.9569, 24.9570, 24.9518, 24.9561, 24.9564, 24.9557,\n",
            "        24.9631, 24.9528, 24.9584, 24.9560, 24.9563, 24.9551, 24.9563, 24.9533,\n",
            "        24.9547, 24.9549, 24.9561, 24.9518, 24.9550, 24.9547, 24.9551, 24.9563,\n",
            "        24.9536, 24.9560, 24.9545, 24.9546, 24.9553, 24.9561, 24.9554, 24.9554,\n",
            "        24.9539, 24.9557, 24.9541, 24.9573, 24.9560, 24.9572, 24.9567, 24.9556,\n",
            "        24.9538, 24.9562, 24.9565, 24.9552, 24.9538, 24.9556, 24.9525, 24.9570,\n",
            "        24.9542, 24.9566, 24.9572, 24.9563, 24.9551, 24.9557, 24.9577, 24.9559],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0092, 0.0086, 0.0119, 0.0082, 0.0086, 0.0127, 0.0122, 0.0082, 0.0135,\n",
            "        0.0075, 0.0084, 0.0073, 0.0108, 0.0077, 0.0116, 0.0076, 0.0103, 0.0091,\n",
            "        0.0124, 0.0127, 0.0081, 0.0108, 0.0081, 0.0071, 0.0072, 0.0120, 0.0123,\n",
            "        0.0084, 0.0064, 0.0117, 0.0102, 0.0086, 0.0095, 0.0127, 0.0074, 0.0085,\n",
            "        0.0089, 0.0103, 0.0104, 0.0081, 0.0090, 0.0090, 0.0069, 0.0106, 0.0078,\n",
            "        0.0096, 0.0100, 0.0111, 0.0090, 0.0079, 0.0075, 0.0102, 0.0081, 0.0082,\n",
            "        0.0061, 0.0051, 0.0099, 0.0065, 0.0077, 0.0069, 0.0088, 0.0071, 0.0064,\n",
            "        0.0085, 0.0085, 0.0108, 0.0075, 0.0080, 0.0083, 0.0082, 0.0104, 0.0086,\n",
            "        0.0109, 0.0149, 0.0123, 0.0107, 0.0063, 0.0102, 0.0102, 0.0108, 0.0086,\n",
            "        0.0121, 0.0111, 0.0102, 0.0106, 0.0129, 0.0106, 0.0081, 0.0091, 0.0083,\n",
            "        0.0094, 0.0089, 0.0082, 0.0104, 0.0109, 0.0077, 0.0124, 0.0103, 0.0087,\n",
            "        0.0081, 0.0128, 0.0084, 0.0063, 0.0142, 0.0055, 0.0092, 0.0075],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [98]\n",
            "DEBUGGING: logits looks like: tensor([25.0067, 24.9913, 25.0719, 24.9795, 24.9897, 25.0882, 25.0773, 24.9790,\n",
            "        25.1041, 24.9554, 24.9862, 24.9500, 25.0469, 24.9642, 25.0658, 24.9591,\n",
            "        25.0360, 25.0049, 25.0813, 25.0876, 24.9769, 25.0488, 24.9760, 24.9431,\n",
            "        24.9460, 25.0749, 25.0800, 24.9862, 24.9163, 25.0668, 25.0333, 24.9922,\n",
            "        25.0160, 25.0887, 24.9526, 24.9887, 25.0000, 25.0368, 25.0388, 24.9744,\n",
            "        25.0033, 25.0031, 24.9367, 25.0429, 24.9673, 25.0171, 25.0296, 25.0535,\n",
            "        25.0016, 24.9703, 24.9570, 25.0337, 24.9751, 24.9774, 24.9046, 24.8578,\n",
            "        25.0254, 24.9220, 24.9640, 24.9349, 24.9960, 24.9413, 24.9178, 24.9883,\n",
            "        24.9883, 25.0485, 24.9559, 24.9713, 24.9807, 24.9791, 25.0391, 24.9900,\n",
            "        25.0500, 25.1275, 25.0796, 25.0451, 24.9140, 25.0327, 25.0326, 25.0473,\n",
            "        24.9918, 25.0769, 25.0551, 25.0335, 25.0420, 25.0913, 25.0442, 24.9759,\n",
            "        25.0058, 24.9820, 25.0134, 25.0004, 24.9780, 25.0390, 25.0494, 24.9637,\n",
            "        25.0827, 25.0350, 24.9933, 24.9762, 25.0904, 24.9840, 24.9113, 25.1164,\n",
            "        24.8774, 25.0072, 24.9563], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.7082653196202955 and immediate abs rewards look like: [0.03063765095475901, 0.007546072431068751, 0.034447410116627, 0.005731749686674448, 0.0006782379236938141, 0.0001712795556159108, 0.0022325919553622953, 0.006470602395438618, 0.00013915488443672075, 0.0005946724759269273, 0.01960544338908221, 3.9450991152989445e-05, 0.0017169446432490076, 0.00024042588756856276, 0.004063966024205001, 0.0002185361322517565, 0.0008863790826580953, 0.0005109324938530335, 0.00044975213131692726, 0.0005431571025837911, 0.00014139848917693598, 0.0015588582105010573, 0.0028781120267922233, 0.0033362541130372847, 0.0008464903153253545, 0.0006514097240142291, 0.0017613568784327072, 0.0001562429720252112, 0.0008367668337996292, 0.000489889180244063, 0.0015279217218449048, 0.00011296952425254858, 0.0013568520466833434, 0.001358439757495944, 4.4856783006252954e-05, 0.00037770804192405194, 1.9606502519309288e-05, 5.185811005503638e-06, 0.00034925385489259497, 1.2463693565223366e-05, 1.9430239262874238e-05, 6.54690579722228e-05, 0.0002990136049447756, 6.385785491147544e-05, 0.00041456096505498863, 2.8254556582396617e-05, 2.4883646347007016e-05, 2.35508891819336e-05, 0.5725798520679746, 0.0]\n",
            "DEBUGGING: the total relative reward of the trajectory = 81.65694470624308 and immediate relative rewards look like: [0.08261989991023588, 0.04103771839310667, 0.28158019826023417, 0.06306189669533913, 0.009342371256136659, 0.0028316722183165987, 0.04306397298844501, 0.14272782905235293, 0.003459317621776586, 0.016426460960627255, 0.5958087360715081, 0.0013150292197886126, 0.062001278189476317, 0.009354432376095213, 0.16942540132515482, 0.009729068130280785, 0.04192980459409988, 0.02559750149672985, 0.023787573556738325, 0.03024357649262113, 0.008268132565679768, 0.09549700741267099, 0.18441003716380197, 0.22323780235170057, 0.059055931923937356, 0.04727501908624363, 0.132768248235056, 0.012219550597862866, 0.06778258398213256, 0.04106163971358499, 0.1323547193560213, 0.010105862212810963, 0.12517633851247634, 0.12916947056254452, 0.004392398873994709, 0.0380425645196361, 0.0020298259621453884, 0.0005513909244642754, 0.038112354333504676, 0.0013951122731351582, 0.0022292867006335777, 0.007694699492663198, 0.03598103423113705, 0.007863530618280823, 0.052210669461297196, 0.0036379360507136182, 0.003273588194705114, 0.0031641986339632703, 78.53264003348721, 0.0]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 3\n",
            "DEBUGGING: the action_prob is: tensor([0.0178, 0.0149, 0.0148, 0.0146, 0.0127, 0.0135, 0.0146, 0.0139, 0.0126,\n",
            "        0.0127, 0.0146, 0.0144, 0.0139, 0.0160, 0.0150, 0.0133, 0.0147, 0.0165,\n",
            "        0.0139, 0.0147, 0.0144, 0.0147, 0.0155, 0.0141, 0.0168, 0.0137, 0.0139,\n",
            "        0.0157, 0.0140, 0.0141, 0.0139, 0.0138, 0.0133, 0.0138, 0.0143, 0.0131,\n",
            "        0.0139, 0.0137, 0.0137, 0.0143, 0.0143, 0.0130, 0.0144, 0.0133, 0.0149,\n",
            "        0.0147, 0.0137, 0.0144, 0.0149, 0.0133, 0.0141, 0.0137, 0.0146, 0.0125,\n",
            "        0.0129, 0.0144, 0.0137, 0.0145, 0.0141, 0.0193, 0.0143, 0.0139, 0.0144,\n",
            "        0.0145, 0.0145, 0.0142, 0.0137, 0.0151, 0.0132, 0.0141],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [45]\n",
            "DEBUGGING: logits looks like: tensor([25.1422, 25.0978, 25.0965, 25.0925, 25.0573, 25.0725, 25.0930, 25.0811,\n",
            "        25.0564, 25.0580, 25.0927, 25.0903, 25.0811, 25.1154, 25.0993, 25.0696,\n",
            "        25.0953, 25.1236, 25.0799, 25.0949, 25.0890, 25.0941, 25.1073, 25.0834,\n",
            "        25.1277, 25.0770, 25.0812, 25.1104, 25.0824, 25.0849, 25.0809, 25.0782,\n",
            "        25.0694, 25.0787, 25.0876, 25.0667, 25.0804, 25.0770, 25.0765, 25.0872,\n",
            "        25.0873, 25.0647, 25.0894, 25.0704, 25.0974, 25.0941, 25.0774, 25.0896,\n",
            "        25.0988, 25.0689, 25.0843, 25.0775, 25.0930, 25.0541, 25.0629, 25.0888,\n",
            "        25.0766, 25.0907, 25.0842, 25.1630, 25.0881, 25.0800, 25.0902, 25.0908,\n",
            "        25.0907, 25.0856, 25.0778, 25.1009, 25.0686, 25.0841],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0141, 0.0120, 0.0127, 0.0103, 0.0134, 0.0134, 0.0135, 0.0134, 0.0127,\n",
            "        0.0155, 0.0110, 0.0111, 0.0146, 0.0132, 0.0142, 0.0113, 0.0120, 0.0115,\n",
            "        0.0136, 0.0131, 0.0122, 0.0139, 0.0114, 0.0140, 0.0126, 0.0111, 0.0129,\n",
            "        0.0127, 0.0127, 0.0141, 0.0226, 0.0124, 0.0124, 0.0117, 0.0125, 0.0152,\n",
            "        0.0142, 0.0150, 0.0123, 0.0147, 0.0136, 0.0117, 0.0106, 0.0138, 0.0130,\n",
            "        0.0097, 0.0113, 0.0130, 0.0150, 0.0147, 0.0120, 0.0117, 0.0141, 0.0127,\n",
            "        0.0125, 0.0136, 0.0123, 0.0101, 0.0121, 0.0137, 0.0130, 0.0134, 0.0150,\n",
            "        0.0139, 0.0150, 0.0125, 0.0113, 0.0121, 0.0100, 0.0118, 0.0098, 0.0122,\n",
            "        0.0099, 0.0105, 0.0133, 0.0136, 0.0123, 0.0122],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [67]\n",
            "DEBUGGING: logits looks like: tensor([25.2095, 25.1693, 25.1838, 25.1322, 25.1976, 25.1967, 25.1987, 25.1977,\n",
            "        25.1841, 25.2338, 25.1484, 25.1514, 25.2189, 25.1941, 25.2123, 25.1550,\n",
            "        25.1693, 25.1599, 25.2011, 25.1921, 25.1737, 25.2058, 25.1570, 25.2086,\n",
            "        25.1815, 25.1514, 25.1873, 25.1832, 25.1843, 25.2093, 25.3284, 25.1784,\n",
            "        25.1776, 25.1632, 25.1793, 25.2286, 25.2127, 25.2251, 25.1753, 25.2211,\n",
            "        25.2011, 25.1627, 25.1388, 25.2039, 25.1894, 25.1168, 25.1550, 25.1899,\n",
            "        25.2264, 25.2201, 25.1702, 25.1634, 25.2104, 25.1849, 25.1801, 25.2008,\n",
            "        25.1754, 25.1257, 25.1715, 25.2033, 25.1901, 25.1976, 25.2258, 25.2058,\n",
            "        25.2265, 25.1805, 25.1549, 25.1728, 25.1242, 25.1667, 25.1180, 25.1740,\n",
            "        25.1209, 25.1372, 25.1949, 25.2008, 25.1759, 25.1736],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0112, 0.0109, 0.0108, 0.0116, 0.0106, 0.0115, 0.0128, 0.0108, 0.0113,\n",
            "        0.0114, 0.0106, 0.0111, 0.0111, 0.0116, 0.0111, 0.0112, 0.0107, 0.0114,\n",
            "        0.0126, 0.0114, 0.0114, 0.0105, 0.0110, 0.0113, 0.0118, 0.0115, 0.0114,\n",
            "        0.0112, 0.0110, 0.0107, 0.0110, 0.0100, 0.0108, 0.0110, 0.0111, 0.0100,\n",
            "        0.0108, 0.0115, 0.0106, 0.0109, 0.0123, 0.0111, 0.0117, 0.0108, 0.0112,\n",
            "        0.0112, 0.0114, 0.0121, 0.0110, 0.0117, 0.0123, 0.0126, 0.0111, 0.0114,\n",
            "        0.0109, 0.0110, 0.0113, 0.0110, 0.0106, 0.0117, 0.0109, 0.0117, 0.0118,\n",
            "        0.0104, 0.0102, 0.0112, 0.0111, 0.0102, 0.0117, 0.0114, 0.0107, 0.0114,\n",
            "        0.0111, 0.0119, 0.0107, 0.0113, 0.0113, 0.0107, 0.0116, 0.0116, 0.0116,\n",
            "        0.0109, 0.0118, 0.0121, 0.0111, 0.0112, 0.0113, 0.0121, 0.0109],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [67]\n",
            "DEBUGGING: logits looks like: tensor([25.1201, 25.1139, 25.1117, 25.1300, 25.1083, 25.1268, 25.1542, 25.1109,\n",
            "        25.1234, 25.1264, 25.1080, 25.1192, 25.1197, 25.1296, 25.1184, 25.1215,\n",
            "        25.1097, 25.1265, 25.1509, 25.1265, 25.1257, 25.1044, 25.1156, 25.1239,\n",
            "        25.1345, 25.1277, 25.1254, 25.1215, 25.1164, 25.1106, 25.1158, 25.0922,\n",
            "        25.1127, 25.1171, 25.1190, 25.0937, 25.1119, 25.1275, 25.1084, 25.1137,\n",
            "        25.1449, 25.1194, 25.1330, 25.1117, 25.1221, 25.1214, 25.1249, 25.1396,\n",
            "        25.1164, 25.1322, 25.1449, 25.1496, 25.1196, 25.1251, 25.1138, 25.1172,\n",
            "        25.1235, 25.1163, 25.1082, 25.1317, 25.1152, 25.1314, 25.1343, 25.1026,\n",
            "        25.0970, 25.1213, 25.1191, 25.0983, 25.1328, 25.1264, 25.1088, 25.1251,\n",
            "        25.1185, 25.1351, 25.1098, 25.1241, 25.1222, 25.1096, 25.1303, 25.1294,\n",
            "        25.1308, 25.1151, 25.1345, 25.1409, 25.1179, 25.1201, 25.1240, 25.1398,\n",
            "        25.1133], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0091, 0.0093, 0.0081, 0.0090, 0.0089, 0.0167, 0.0119, 0.0121, 0.0112,\n",
            "        0.0141, 0.0097, 0.0113, 0.0121, 0.0098, 0.0088, 0.0095, 0.0084, 0.0109,\n",
            "        0.0108, 0.0098, 0.0112, 0.0086, 0.0072, 0.0090, 0.0130, 0.0110, 0.0091,\n",
            "        0.0130, 0.0087, 0.0155, 0.0094, 0.0092, 0.0105, 0.0114, 0.0072, 0.0111,\n",
            "        0.0115, 0.0187, 0.0088, 0.0100, 0.0108, 0.0124, 0.0097, 0.0091, 0.0105,\n",
            "        0.0099, 0.0106, 0.0089, 0.0092, 0.0099, 0.0085, 0.0138, 0.0083, 0.0086,\n",
            "        0.0098, 0.0124, 0.0094, 0.0092, 0.0128, 0.0092, 0.0087, 0.0107, 0.0119,\n",
            "        0.0114, 0.0070, 0.0137, 0.0071, 0.0093, 0.0095, 0.0080, 0.0074, 0.0108,\n",
            "        0.0080, 0.0154, 0.0086, 0.0094, 0.0093, 0.0093, 0.0065, 0.0111, 0.0110,\n",
            "        0.0084, 0.0087, 0.0141, 0.0106, 0.0092, 0.0079, 0.0065, 0.0082, 0.0174,\n",
            "        0.0109, 0.0088, 0.0087, 0.0102, 0.0119, 0.0094, 0.0084, 0.0078],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [55]\n",
            "DEBUGGING: logits looks like: tensor([25.1002, 25.1062, 25.0714, 25.0980, 25.0967, 25.2539, 25.1680, 25.1736,\n",
            "        25.1530, 25.2111, 25.1181, 25.1557, 25.1727, 25.1208, 25.0939, 25.1125,\n",
            "        25.0820, 25.1476, 25.1452, 25.1197, 25.1540, 25.0885, 25.0431, 25.0985,\n",
            "        25.1908, 25.1498, 25.1019, 25.1905, 25.0894, 25.2346, 25.1098, 25.1032,\n",
            "        25.1364, 25.1577, 25.0435, 25.1520, 25.1599, 25.2813, 25.0940, 25.1239,\n",
            "        25.1451, 25.1783, 25.1178, 25.1010, 25.1371, 25.1227, 25.1391, 25.0950,\n",
            "        25.1054, 25.1236, 25.0834, 25.2059, 25.0787, 25.0884, 25.1187, 25.1781,\n",
            "        25.1100, 25.1037, 25.1876, 25.1036, 25.0888, 25.1428, 25.1675, 25.1588,\n",
            "        25.0371, 25.2044, 25.0388, 25.1057, 25.1125, 25.0683, 25.0480, 25.1450,\n",
            "        25.0686, 25.2337, 25.0860, 25.1100, 25.1068, 25.1062, 25.0169, 25.1503,\n",
            "        25.1486, 25.0804, 25.0901, 25.2115, 25.1393, 25.1032, 25.0656, 25.0157,\n",
            "        25.0755, 25.2638, 25.1460, 25.0940, 25.0916, 25.1291, 25.1683, 25.1102,\n",
            "        25.0820, 25.0618], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0090, 0.0089, 0.0094, 0.0084, 0.0089, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0086, 0.0090, 0.0095, 0.0091, 0.0095, 0.0088, 0.0093, 0.0090, 0.0091,\n",
            "        0.0092, 0.0087, 0.0095, 0.0090, 0.0091, 0.0091, 0.0094, 0.0094, 0.0087,\n",
            "        0.0094, 0.0092, 0.0090, 0.0092, 0.0091, 0.0096, 0.0095, 0.0091, 0.0081,\n",
            "        0.0095, 0.0088, 0.0095, 0.0093, 0.0089, 0.0098, 0.0092, 0.0084, 0.0092,\n",
            "        0.0094, 0.0092, 0.0091, 0.0090, 0.0090, 0.0090, 0.0090, 0.0091, 0.0093,\n",
            "        0.0094, 0.0089, 0.0092, 0.0097, 0.0092, 0.0091, 0.0096, 0.0094, 0.0090,\n",
            "        0.0092, 0.0092, 0.0095, 0.0091, 0.0091, 0.0092, 0.0092, 0.0091, 0.0092,\n",
            "        0.0087, 0.0087, 0.0089, 0.0092, 0.0087, 0.0092, 0.0092, 0.0091, 0.0095,\n",
            "        0.0096, 0.0096, 0.0090, 0.0088, 0.0092, 0.0088, 0.0092, 0.0090, 0.0091,\n",
            "        0.0095, 0.0092, 0.0095, 0.0090, 0.0097, 0.0098, 0.0088, 0.0094, 0.0091,\n",
            "        0.0092, 0.0097, 0.0093, 0.0095, 0.0093, 0.0096, 0.0089, 0.0096, 0.0092,\n",
            "        0.0087], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [32]\n",
            "DEBUGGING: logits looks like: tensor([25.0859, 25.0829, 25.0969, 25.0685, 25.0813, 25.0972, 25.0962, 25.0969,\n",
            "        25.0948, 25.0728, 25.0839, 25.0989, 25.0870, 25.0979, 25.0795, 25.0939,\n",
            "        25.0863, 25.0875, 25.0909, 25.0766, 25.1000, 25.0862, 25.0882, 25.0881,\n",
            "        25.0960, 25.0973, 25.0763, 25.0964, 25.0911, 25.0844, 25.0901, 25.0883,\n",
            "        25.1024, 25.0982, 25.0885, 25.0589, 25.0991, 25.0793, 25.0979, 25.0930,\n",
            "        25.0833, 25.1054, 25.0911, 25.0688, 25.0920, 25.0963, 25.0919, 25.0893,\n",
            "        25.0845, 25.0844, 25.0848, 25.0861, 25.0876, 25.0935, 25.0958, 25.0821,\n",
            "        25.0913, 25.1040, 25.0897, 25.0881, 25.1006, 25.0967, 25.0858, 25.0919,\n",
            "        25.0918, 25.0982, 25.0893, 25.0876, 25.0907, 25.0912, 25.0890, 25.0906,\n",
            "        25.0763, 25.0754, 25.0824, 25.0896, 25.0773, 25.0915, 25.0897, 25.0875,\n",
            "        25.0985, 25.1016, 25.1018, 25.0842, 25.0809, 25.0914, 25.0783, 25.0896,\n",
            "        25.0848, 25.0872, 25.0975, 25.0901, 25.0977, 25.0840, 25.1045, 25.1068,\n",
            "        25.0806, 25.0965, 25.0876, 25.0916, 25.1040, 25.0935, 25.0988, 25.0940,\n",
            "        25.1022, 25.0835, 25.1005, 25.0908, 25.0760], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.7058461509313929 and immediate abs rewards look like: [0.004276747504718514, 0.020243600725279975, 0.008884555647455272, 7.451402370861615e-05, 0.013131415354564524, 8.374180742976023e-05, 0.0018871892370952992, 0.0006900128246343229, 0.007894316266629176, 0.0015416329870276968, 0.0002148813546227757, 0.0018953121821141394, 0.001258164352748281, 0.6437700666533601, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 9.094947017729282e-13, 9.094947017729282e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0]\n",
            "DEBUGGING: the total relative reward of the trajectory = 35.15720115237646 and immediate relative rewards look like: [0.015805582676096372, 0.14986548707338726, 0.0994048053228448, 0.0011152952199198136, 0.2456888718211928, 0.0018894561201135986, 0.04967872000829253, 0.020773629544983702, 0.26744535029947947, 0.05820382052732352, 0.008929245554126008, 0.08592521020828128, 0.061837285492813636, 34.09063839109106, 3.410605131648481e-11, 3.637978807090886e-11, 0.0, 4.092726157978177e-11, 4.320099833420427e-11, 4.547473508864641e-11, 4.774847184308959e-11, 5.002220859751105e-11, 5.2295945351955264e-11, 1.0913936421280102e-10, 5.6843418860808015e-11, 0.0, 6.139089236968661e-11, 6.366462912410498e-11, 6.593836587855229e-11, 0.0, 0.0, 0.0, 1.5006662579253316e-10, 1.546140993014681e-10, 0.0, 0.0, 0.0, 0.0, 0.0, 9.094947017729282e-11, 9.322320693174634e-11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0913936421275139e-10, 1.1141310096715838e-10, 0.0]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 200 number of matrices\n",
            "DEBUGGING: ACT_MAT has 200 number of matrices\n",
            "DEBUGGING: VAL looks like: [[26.709601074344835, 26.88368318346565, 26.91990896947122, 26.96724550175606, 26.98211251677357, 27.246574854356155, 27.47071464831013, 27.637023903990148, 27.797831452609003, 28.070001724821562, 28.346480549156002, 28.625336738016422, 28.894173181374853, 29.070821384973993, 29.248690934073284, 29.531807621249932, 29.74654702079117, 29.97127125260597, 30.124292493756386, 30.400196005011995, 30.58239196913403, 30.81627698180978, 31.015477240037985, 30.818441750316705, 1.1926377392577065e-09, 1.1664061885695269e-09, 1.1582833030617324e-09, 1.1286424661433845e-09, 1.075735188908373e-09, 1.0866012009175486e-09, 1.0516428947766652e-09, 9.91067732716433e-10, 1.0010785178953868e-09, 9.101354552528785e-10, 8.412408137395872e-10, 8.229433181190654e-10, 8.312558768879448e-10, 8.113263874578581e-10, 7.904300221229471e-10, 7.386998651592997e-10, 6.849160454960744e-10, 6.918343893899741e-10, 6.023610562664815e-10, 4.4384840626148373e-10, 2.799067787236582e-10, 2.1383300615222637e-10, 1.4556068587505774e-10, 1.1104930308648015e-10, 1.1217101321866682e-10, 7.579122514774975e-11], [4.479520142835267, 4.4768213476796275, 4.499642230335055, 4.503547714879415, 4.517901580707901, 4.551296082117702, 4.3740759258060855, 4.359895317088133, 3.325443855870084, 3.2673176797659162, 2.9288902517428115, 2.944512085734338, 2.681332138655671, 2.1706412518995615, 1.7741440919823217, 1.7834080235121033, 1.7419675306111282, 1.4273253007601256, 1.3914306186548273, 1.3656796495256598, 1.1477801957352947, 0.5530208735991909, 0.47643406603683935, 0.4753871120805691, 0.39164643801770777, 0.38767278836051555, 0.3367965703036533, 0.3384561270428379, 0.3389735214262479, 0.33431379146075546, 0.3244516796783366, 0.298448383876887, 0.2931901856484991, 0.2937668565972991, 0.2948953505757632, 0.2937466793395156, 0.29651782993950965, 0.29929113363755033, 0.2860591688662197, 0.24589375016427872, 0.2463006672596938, 0.24869498780348182, 0.20898824082339335, 0.20997366812491672, 0.030172192436226836, 0.029886871703769084, 0.023252272268439976, 0.021267358908411926, 0.014944353301833569, 0.011849133964817031], [51.11035187175915, 51.5431636079282, 52.022349383368784, 52.263403217281365, 52.727617495541445, 53.25078295382354, 53.78580937535882, 54.285601416535734, 54.68977130048826, 55.23869897259241, 55.780073244072504, 55.74168132121312, 56.30340029494276, 56.80949395631645, 57.373878307010465, 57.78227566230839, 58.35610767088698, 58.903209965952406, 59.472335822682496, 60.049038635480564, 60.62504551412923, 61.229068062185405, 61.751081873507815, 62.1885574104485, 62.59123192737051, 63.16381413681472, 63.75407991689745, 64.26395118046707, 64.9007390200699, 65.48783478392704, 66.10785166082167, 66.64191610249055, 67.30485882856338, 67.85826514146557, 68.41322795040709, 69.09983389043748, 69.75938517769478, 70.46197510276025, 71.17315526448058, 71.85357869711827, 72.5779632170153, 73.30882215183298, 74.04154288115184, 74.75309277466738, 75.50023155964556, 76.21012211129722, 76.97624664166315, 77.75047783178631, 78.53264003348721, 0.0], [30.921644807680988, 31.21801941919686, 31.38197366881159, 31.59855440756439, 31.916605163984315, 31.990824537538504, 32.312055637796355, 32.58825951291723, 32.8964503872447, 32.95859094640931, 33.23271426856766, 33.55937881112478, 33.81156929385505, 34.090638392285086, 1.2060839166858355e-09, 1.1838160256256068e-09, 1.159026502580503e-09, 1.170733840990407e-09, 1.1412187670814396e-09, 1.1091088573204396e-09, 1.0743779012442355e-09, 1.0369994236375213e-09, 9.969466818585963e-10, 9.541926631380214e-10, 8.53589190833556e-10, 8.047937090633818e-10, 8.129229384478604e-10, 7.591232788668422e-10, 7.024834845886234e-10, 6.429748673839102e-10, 6.494695630140507e-10, 6.560298616303542e-10, 6.626564258892467e-10, 5.177674748451652e-10, 3.668215914582799e-10, 3.7052686005886857e-10, 3.7426955561501874e-10, 3.7805005617678663e-10, 3.818687436129158e-10, 3.857260036494099e-10, 2.977540742142597e-10, 2.0659683563890237e-10, 2.0868367236252764e-10, 2.107915882449774e-10, 2.129207962070479e-10, 2.150715113202504e-10, 2.1724395082853578e-10, 2.1943833417023817e-10, 1.1141310096715838e-10, 0.0]]\n",
            "DEBUGGING: traj_returns = [26.709601074344835, 4.479520142835267, 51.11035187175915, 30.921644807680988]\n",
            "DEBUGGING: actions = [[45], [42], [23], [34], [14], [55], [59], [9], [6], [11], [30], [20], [42], [58], [13], [20], [74], [30], [36], [46], [18], [75], [31], [0], [51], [46], [18], [76], [72], [82], [87], [50], [86], [29], [63], [12], [48], [69], [51], [94], [57], [99], [12], [88], [61], [1], [51], [82], [42], [12], [12], [55], [32], [9], [43], [1], [2], [51], [42], [52], [23], [0], [16], [63], [56], [67], [44], [52], [27], [8], [24], [25], [35], [1], [7], [41], [35], [84], [78], [18], [8], [51], [54], [63], [22], [64], [93], [60], [33], [57], [66], [34], [59], [32], [24], [44], [75], [10], [41], [71], [12], [57], [59], [35], [41], [12], [54], [19], [42], [12], [27], [21], [3], [46], [61], [2], [30], [27], [15], [34], [23], [17], [34], [24], [63], [74], [73], [13], [17], [14], [37], [1], [37], [55], [16], [80], [4], [49], [8], [81], [89], [60], [25], [13], [85], [92], [105], [10], [0], [98], [23], [2], [20], [35], [20], [62], [15], [36], [32], [45], [19], [43], [43], [0], [4], [13], [34], [44], [24], [67], [69], [16], [57], [0], [26], [39], [64], [35], [17], [67], [63], [54], [4], [4], [20], [34], [74], [44], [83], [55], [89], [7], [91], [89], [21], [15], [82], [60], [22], [32]]\n",
            "DEBUGGING: actions length = 200\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[2.8112, 1.3459, 0.3565,  ..., 2.2904, 0.3186, 2.1211],\n",
            "        [2.7441, 1.2712, 0.3429,  ..., 2.2154, 0.2517, 2.0391],\n",
            "        [2.8128, 1.3797, 0.3531,  ..., 2.3269, 0.3380, 2.1179],\n",
            "        ...,\n",
            "        [2.8013, 1.3396, 0.3495,  ..., 2.2894, 0.3037, 2.0990],\n",
            "        [2.8007, 1.3394, 0.3495,  ..., 2.2886, 0.3034, 2.0990],\n",
            "        [2.8016, 1.3405, 0.3496,  ..., 2.2896, 0.3045, 2.1003]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[2.8000, 1.3385, 0.3492,  ..., 2.2881, 0.3026, 2.0979],\n",
            "        [2.7998, 1.3382, 0.3491,  ..., 2.2878, 0.3023, 2.0975],\n",
            "        [2.8011, 1.3396, 0.3494,  ..., 2.2890, 0.3038, 2.0993],\n",
            "        ...,\n",
            "        [2.8012, 1.3401, 0.3494,  ..., 2.2894, 0.3042, 2.0996],\n",
            "        [2.8006, 1.3390, 0.3494,  ..., 2.2885, 0.3032, 2.0986],\n",
            "        [2.7994, 1.3372, 0.3490,  ..., 2.2871, 0.3014, 2.0966]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([25.0859, 25.0829, 25.0969, 25.0685, 25.0813, 25.0972, 25.0962, 25.0969,\n",
            "        25.0948, 25.0728, 25.0839, 25.0989, 25.0870, 25.0979, 25.0795, 25.0939,\n",
            "        25.0863, 25.0875, 25.0909, 25.0766, 25.1000, 25.0862, 25.0882, 25.0881,\n",
            "        25.0960, 25.0973, 25.0763, 25.0964, 25.0911, 25.0844, 25.0901, 25.0883,\n",
            "        25.1024, 25.0982, 25.0885, 25.0589, 25.0991, 25.0793, 25.0979, 25.0930,\n",
            "        25.0833, 25.1054, 25.0911, 25.0688, 25.0920, 25.0963, 25.0919, 25.0893,\n",
            "        25.0845, 25.0844, 25.0848, 25.0861, 25.0876, 25.0935, 25.0958, 25.0821,\n",
            "        25.0913, 25.1040, 25.0897, 25.0881, 25.1006, 25.0967, 25.0858, 25.0919,\n",
            "        25.0918, 25.0982, 25.0893, 25.0876, 25.0907, 25.0912, 25.0890, 25.0906,\n",
            "        25.0763, 25.0754, 25.0824, 25.0896, 25.0773, 25.0915, 25.0897, 25.0875,\n",
            "        25.0985, 25.1016, 25.1018, 25.0842, 25.0809, 25.0914, 25.0783, 25.0896,\n",
            "        25.0848, 25.0872, 25.0975, 25.0901, 25.0977, 25.0840, 25.1045, 25.1068,\n",
            "        25.0806, 25.0965, 25.0876, 25.0916, 25.1040, 25.0935, 25.0988, 25.0940,\n",
            "        25.1022, 25.0835, 25.1005, 25.0908, 25.0760], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[2.83052795e+01 2.85304219e+01 2.87059686e+01 2.88331877e+01\n",
            "  2.90360592e+01 2.92598696e+01 2.94856639e+01 2.97176950e+01\n",
            "  2.96773742e+01 2.98836523e+01 3.00720396e+01 3.02177272e+01\n",
            "  3.04226187e+01 3.05353987e+01 2.20991783e+01 2.22743728e+01\n",
            "  2.24611556e+01 2.25754516e+01 2.27470147e+01 2.29537286e+01\n",
            "  2.30888044e+01 2.31495915e+01 2.33107483e+01 2.33705966e+01\n",
            "  1.57457196e+01 1.58878717e+01 1.60227191e+01 1.61506018e+01\n",
            "  1.63099281e+01 1.64555371e+01 1.66080758e+01 1.67350911e+01\n",
            "  1.68995123e+01 1.70380080e+01 1.71770308e+01 1.73483951e+01\n",
            "  1.75139758e+01 1.76903166e+01 1.78648036e+01 1.80248681e+01\n",
            "  1.82060660e+01 1.83893793e+01 1.85626328e+01 1.87407666e+01\n",
            "  1.88826009e+01 1.90600022e+01 1.92498747e+01 1.94429363e+01\n",
            "  1.96368961e+01 2.96228351e-03]]\n",
            "DEBUGGING: baseline2 looks like: 28.30527947415506\n",
            "DEBUGGING: ADS looks like: [-1.59567840e+00 -1.64673871e+00 -1.78605959e+00 -1.86594221e+00\n",
            " -2.05394667e+00 -2.01329475e+00 -2.01494925e+00 -2.08067113e+00\n",
            " -1.87954280e+00 -1.81365061e+00 -1.72555903e+00 -1.59239050e+00\n",
            " -1.52844555e+00 -1.46457736e+00  7.14951260e+00  7.25743479e+00\n",
            "  7.28539146e+00  7.39581962e+00  7.37727776e+00  7.44646743e+00\n",
            "  7.49358755e+00  7.66668550e+00  7.70472894e+00  7.44784518e+00\n",
            " -1.57457196e+01 -1.58878717e+01 -1.60227191e+01 -1.61506018e+01\n",
            " -1.63099281e+01 -1.64555371e+01 -1.66080758e+01 -1.67350911e+01\n",
            " -1.68995123e+01 -1.70380080e+01 -1.71770308e+01 -1.73483951e+01\n",
            " -1.75139758e+01 -1.76903166e+01 -1.78648036e+01 -1.80248681e+01\n",
            " -1.82060660e+01 -1.83893793e+01 -1.85626328e+01 -1.87407666e+01\n",
            " -1.88826009e+01 -1.90600022e+01 -1.92498747e+01 -1.94429363e+01\n",
            " -1.96368961e+01 -2.96228343e-03 -2.38257593e+01 -2.40536005e+01\n",
            " -2.42063263e+01 -2.43296400e+01 -2.45181576e+01 -2.47085735e+01\n",
            " -2.51115880e+01 -2.53577997e+01 -2.63519304e+01 -2.66163347e+01\n",
            " -2.71431493e+01 -2.72732152e+01 -2.77412866e+01 -2.83647575e+01\n",
            " -2.03250342e+01 -2.04909648e+01 -2.07191880e+01 -2.11481263e+01\n",
            " -2.13555841e+01 -2.15880489e+01 -2.19410242e+01 -2.25965706e+01\n",
            " -2.28343142e+01 -2.28952095e+01 -1.53540732e+01 -1.55001989e+01\n",
            " -1.56859226e+01 -1.58121457e+01 -1.59709546e+01 -1.61212234e+01\n",
            " -1.62836242e+01 -1.64366427e+01 -1.66063221e+01 -1.67442411e+01\n",
            " -1.68821355e+01 -1.70546485e+01 -1.72174579e+01 -1.73910254e+01\n",
            " -1.75787444e+01 -1.77789744e+01 -1.79597653e+01 -1.81406843e+01\n",
            " -1.83536445e+01 -1.85307929e+01 -1.88524287e+01 -1.90301154e+01\n",
            " -1.92266225e+01 -1.94216689e+01 -1.96219517e+01  8.88685045e-03\n",
            "  2.28050724e+01  2.30127417e+01  2.33163808e+01  2.34302155e+01\n",
            "  2.36915583e+01  2.39909133e+01  2.43001455e+01  2.45679064e+01\n",
            "  2.50123971e+01  2.53550466e+01  2.57080337e+01  2.55239541e+01\n",
            "  2.58807816e+01  2.62740952e+01  3.52747000e+01  3.55079028e+01\n",
            "  3.58949521e+01  3.63277583e+01  3.67253211e+01  3.70953101e+01\n",
            "  3.75362411e+01  3.80794766e+01  3.84403336e+01  3.88179608e+01\n",
            "  4.68455123e+01  4.72759424e+01  4.77313608e+01  4.81133494e+01\n",
            "  4.85908109e+01  4.90322976e+01  4.94997758e+01  4.99068250e+01\n",
            "  5.04053466e+01  5.08202571e+01  5.12361971e+01  5.17514387e+01\n",
            "  5.22454094e+01  5.27716585e+01  5.33083517e+01  5.38287106e+01\n",
            "  5.43718972e+01  5.49194429e+01  5.54789101e+01  5.60123262e+01\n",
            "  5.66176306e+01  5.71501199e+01  5.77263719e+01  5.83075415e+01\n",
            "  5.88957439e+01 -2.96228351e-03  2.61636533e+00  2.68759753e+00\n",
            "  2.67600511e+00  2.76536670e+00  2.88054597e+00  2.73095493e+00\n",
            "  2.82639174e+00  2.87056448e+00  3.21907614e+00  3.07493862e+00\n",
            "  3.16067469e+00  3.34165157e+00  3.38895057e+00  3.55523965e+00\n",
            " -2.20991783e+01 -2.22743728e+01 -2.24611556e+01 -2.25754516e+01\n",
            " -2.27470147e+01 -2.29537286e+01 -2.30888044e+01 -2.31495915e+01\n",
            " -2.33107483e+01 -2.33705966e+01 -1.57457196e+01 -1.58878717e+01\n",
            " -1.60227191e+01 -1.61506018e+01 -1.63099281e+01 -1.64555371e+01\n",
            " -1.66080758e+01 -1.67350911e+01 -1.68995123e+01 -1.70380080e+01\n",
            " -1.71770308e+01 -1.73483951e+01 -1.75139758e+01 -1.76903166e+01\n",
            " -1.78648036e+01 -1.80248681e+01 -1.82060660e+01 -1.83893793e+01\n",
            " -1.85626328e+01 -1.87407666e+01 -1.88826009e+01 -1.90600022e+01\n",
            " -1.92498747e+01 -1.94429363e+01 -1.96368961e+01 -2.96228351e-03]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(0.0739, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-2.1296e-02, -2.8833e-03, -1.7090e-02,  ..., -1.5477e-02,\n",
            "         -2.5327e-02, -6.0155e-01],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [-2.8940e-02, -4.0907e-03, -2.3298e-02,  ..., -2.1089e-02,\n",
            "         -3.4353e-02, -8.8975e-01],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 2.2561e-03,  5.5118e-04,  1.9096e-03,  ...,  1.7094e-03,\n",
            "          2.6085e-03,  1.4805e-01]])\n",
            "   Last layer:\n",
            "tensor([[ 0.0000,  0.0578,  0.0000, -0.1587,  0.0000,  0.0000,  0.0000, -0.2813,\n",
            "         -0.0827,  0.0000, -0.0786,  0.0000,  0.0000, -0.0223,  0.0000, -0.0828,\n",
            "          0.0000, -0.1778, -0.1816,  0.0000],\n",
            "        [ 0.0000,  0.0127,  0.0000, -0.0962,  0.0000,  0.0000,  0.0000, -0.2152,\n",
            "         -0.0814,  0.0000, -0.0901,  0.0000,  0.0000, -0.0259,  0.0000, -0.0888,\n",
            "          0.0000, -0.1474, -0.1244,  0.0000],\n",
            "        [ 0.0000,  0.0062,  0.0000, -0.0191,  0.0000,  0.0000,  0.0000, -0.0353,\n",
            "         -0.0110,  0.0000, -0.0109,  0.0000,  0.0000, -0.0031,  0.0000, -0.0113,\n",
            "          0.0000, -0.0227, -0.0223,  0.0000],\n",
            "        [ 0.0000,  0.0106,  0.0000, -0.0453,  0.0000,  0.0000,  0.0000, -0.0921,\n",
            "         -0.0319,  0.0000, -0.0337,  0.0000,  0.0000, -0.0097,  0.0000, -0.0339,\n",
            "          0.0000, -0.0613, -0.0556,  0.0000],\n",
            "        [ 0.0000,  0.0098,  0.0000, -0.0210,  0.0000,  0.0000,  0.0000, -0.0330,\n",
            "         -0.0080,  0.0000, -0.0064,  0.0000,  0.0000, -0.0018,  0.0000, -0.0073,\n",
            "          0.0000, -0.0198, -0.0227,  0.0000],\n",
            "        [ 0.0000,  0.0029,  0.0000,  0.0263,  0.0000,  0.0000,  0.0000,  0.0716,\n",
            "          0.0312,  0.0000,  0.0368,  0.0000,  0.0000,  0.0107,  0.0000,  0.0353,\n",
            "          0.0000,  0.0516,  0.0381,  0.0000],\n",
            "        [ 0.0000,  0.0438,  0.0000, -0.1180,  0.0000,  0.0000,  0.0000, -0.2073,\n",
            "         -0.0603,  0.0000, -0.0568,  0.0000,  0.0000, -0.0161,  0.0000, -0.0600,\n",
            "          0.0000, -0.1306, -0.1344,  0.0000],\n",
            "        [ 0.0000,  0.0383,  0.0000, -0.1375,  0.0000,  0.0000,  0.0000, -0.2673,\n",
            "         -0.0882,  0.0000, -0.0905,  0.0000,  0.0000, -0.0259,  0.0000, -0.0921,\n",
            "          0.0000, -0.1750, -0.1649,  0.0000],\n",
            "        [ 0.0000, -0.0082,  0.0000, -0.0526,  0.0000,  0.0000,  0.0000, -0.1481,\n",
            "         -0.0657,  0.0000, -0.0781,  0.0000,  0.0000, -0.0226,  0.0000, -0.0748,\n",
            "          0.0000, -0.1075, -0.0778,  0.0000],\n",
            "        [ 0.0000,  0.0382,  0.0000, -0.1423,  0.0000,  0.0000,  0.0000, -0.2795,\n",
            "         -0.0932,  0.0000, -0.0964,  0.0000,  0.0000, -0.0276,  0.0000, -0.0978,\n",
            "          0.0000, -0.1836, -0.1716,  0.0000]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0141,  0.0088, -0.0456,  ...,  0.0174, -0.0246, -0.0653],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [ 0.0189,  0.0118, -0.0614,  ...,  0.0233, -0.0332, -0.1025],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0008, -0.0005,  0.0028,  ..., -0.0009,  0.0015,  0.0240]])\n",
            "   Last layer:\n",
            "tensor([[ 1.6997e-05,  9.9248e-03,  0.0000e+00, -1.6729e-02,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -3.2938e-02, -8.1571e-03,  0.0000e+00,\n",
            "         -1.1647e-04,  0.0000e+00,  0.0000e+00,  1.7169e-02,  0.0000e+00,\n",
            "         -1.5672e-02,  0.0000e+00, -2.5262e-02, -2.9187e-02,  0.0000e+00],\n",
            "        [ 2.3069e-05,  3.9920e-03,  0.0000e+00, -1.1160e-02,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -2.5796e-02, -8.6909e-03,  0.0000e+00,\n",
            "         -7.0088e-03,  0.0000e+00,  0.0000e+00,  6.8218e-03,  0.0000e+00,\n",
            "         -1.3908e-02,  0.0000e+00, -1.9787e-02, -1.9008e-02,  0.0000e+00],\n",
            "        [-4.6493e-07,  7.8654e-04,  0.0000e+00, -4.1547e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -1.0544e-02, -4.0761e-03,  0.0000e+00,\n",
            "         -4.4722e-03,  0.0000e+00,  0.0000e+00,  1.2236e-03,  0.0000e+00,\n",
            "         -6.0529e-03,  0.0000e+00, -8.0898e-03, -6.8843e-03,  0.0000e+00],\n",
            "        [ 1.2313e-06,  2.4359e-03,  0.0000e+00, -5.5847e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -1.2320e-02, -3.7999e-03,  0.0000e+00,\n",
            "         -2.3707e-03,  0.0000e+00,  0.0000e+00,  4.2388e-03,  0.0000e+00,\n",
            "         -6.4321e-03,  0.0000e+00, -9.4614e-03, -9.6304e-03,  0.0000e+00],\n",
            "        [ 1.1477e-05,  2.0104e-03,  0.0000e+00,  2.3375e-04,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  3.5462e-03,  2.7272e-03,  0.0000e+00,\n",
            "          5.7020e-03,  0.0000e+00,  0.0000e+00,  3.5906e-03,  0.0000e+00,\n",
            "          3.0381e-03,  0.0000e+00,  2.7310e-03, -5.9138e-06,  0.0000e+00],\n",
            "        [ 1.3184e-07, -1.3041e-04,  0.0000e+00,  5.8740e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  1.6412e-02,  6.9658e-03,  0.0000e+00,\n",
            "          8.9171e-03,  0.0000e+00,  0.0000e+00, -8.8425e-05,  0.0000e+00,\n",
            "          9.9195e-03,  0.0000e+00,  1.2601e-02,  9.6332e-03,  0.0000e+00],\n",
            "        [-2.8517e-06,  7.3119e-03,  0.0000e+00, -1.4644e-02,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -3.0825e-02, -8.7888e-03,  0.0000e+00,\n",
            "         -3.7653e-03,  0.0000e+00,  0.0000e+00,  1.2613e-02,  0.0000e+00,\n",
            "         -1.5546e-02,  0.0000e+00, -2.3661e-02, -2.5309e-02,  0.0000e+00],\n",
            "        [ 1.6925e-05,  7.6823e-03,  0.0000e+00, -1.5452e-02,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -3.2606e-02, -9.3623e-03,  0.0000e+00,\n",
            "         -4.0288e-03,  0.0000e+00,  0.0000e+00,  1.3265e-02,  0.0000e+00,\n",
            "         -1.6449e-02,  0.0000e+00, -2.4995e-02, -2.6716e-02,  0.0000e+00],\n",
            "        [ 1.7093e-05, -4.3508e-04,  0.0000e+00, -8.3300e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -2.4101e-02, -1.0643e-02,  0.0000e+00,\n",
            "         -1.4209e-02,  0.0000e+00,  0.0000e+00, -1.0101e-03,  0.0000e+00,\n",
            "         -1.4790e-02,  0.0000e+00, -1.8484e-02, -1.3510e-02,  0.0000e+00],\n",
            "        [ 2.9326e-05,  6.9870e-03,  0.0000e+00, -1.4544e-02,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -3.0932e-02, -9.1062e-03,  0.0000e+00,\n",
            "         -4.3738e-03,  0.0000e+00,  0.0000e+00,  1.1964e-02,  0.0000e+00,\n",
            "         -1.5693e-02,  0.0000e+00, -2.3694e-02, -2.5024e-02,  0.0000e+00]])\n",
            "DEBUGGING: training for one iteration takes 0.005311 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 20\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0287, 0.0141, 0.0131, 0.0124, 0.0150, 0.0158, 0.0164, 0.0129, 0.0144,\n",
            "        0.0129, 0.0153, 0.0157, 0.0145, 0.0148, 0.0108, 0.0114, 0.0135, 0.0150,\n",
            "        0.0171, 0.0149, 0.0144, 0.0151, 0.0143, 0.0146, 0.0147, 0.0143, 0.0129,\n",
            "        0.0161, 0.0145, 0.0132, 0.0142, 0.0155, 0.0141, 0.0136, 0.0168, 0.0117,\n",
            "        0.0180, 0.0145, 0.0167, 0.0119, 0.0135, 0.0140, 0.0147, 0.0153, 0.0143,\n",
            "        0.0154, 0.0142, 0.0140, 0.0143, 0.0161, 0.0147, 0.0143, 0.0142, 0.0146,\n",
            "        0.0124, 0.0151, 0.0134, 0.0092, 0.0141, 0.0144, 0.0127, 0.0151, 0.0144,\n",
            "        0.0144, 0.0156, 0.0138, 0.0134, 0.0147, 0.0142],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [10]\n",
            "DEBUGGING: logits looks like: tensor([30.1022, 29.9243, 29.9052, 29.8922, 29.9397, 29.9523, 29.9614, 29.9011,\n",
            "        29.9297, 29.9025, 29.9442, 29.9508, 29.9303, 29.9356, 29.8581, 29.8702,\n",
            "        29.9135, 29.9400, 29.9720, 29.9371, 29.9292, 29.9416, 29.9276, 29.9333,\n",
            "        29.9340, 29.9285, 29.9028, 29.9566, 29.9309, 29.9068, 29.9255, 29.9475,\n",
            "        29.9238, 29.9160, 29.9675, 29.8773, 29.9854, 29.9316, 29.9658, 29.8807,\n",
            "        29.9126, 29.9218, 29.9352, 29.9442, 29.9274, 29.9464, 29.9258, 29.9230,\n",
            "        29.9276, 29.9578, 29.9342, 29.9281, 29.9257, 29.9324, 29.8927, 29.9418,\n",
            "        29.9115, 29.8183, 29.9249, 29.9294, 29.8985, 29.9405, 29.9297, 29.9286,\n",
            "        29.9495, 29.9191, 29.9111, 29.9345, 29.9257], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0251, 0.0128, 0.0129, 0.0129, 0.0128, 0.0125, 0.0129, 0.0128, 0.0128,\n",
            "        0.0129, 0.0128, 0.0129, 0.0129, 0.0126, 0.0129, 0.0129, 0.0129, 0.0129,\n",
            "        0.0128, 0.0129, 0.0129, 0.0129, 0.0128, 0.0127, 0.0129, 0.0128, 0.0130,\n",
            "        0.0129, 0.0129, 0.0128, 0.0126, 0.0130, 0.0129, 0.0126, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0129, 0.0130, 0.0128, 0.0128, 0.0127, 0.0129, 0.0129,\n",
            "        0.0128, 0.0127, 0.0128, 0.0128, 0.0129, 0.0129, 0.0128, 0.0129, 0.0129,\n",
            "        0.0129, 0.0126, 0.0128, 0.0128, 0.0128, 0.0129, 0.0130, 0.0128, 0.0129,\n",
            "        0.0128, 0.0128, 0.0127, 0.0128, 0.0130, 0.0127, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0127, 0.0129, 0.0128, 0.0127], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [12]\n",
            "DEBUGGING: logits looks like: tensor([30.1027, 29.9345, 29.9375, 29.9362, 29.9356, 29.9285, 29.9367, 29.9355,\n",
            "        29.9343, 29.9368, 29.9351, 29.9363, 29.9368, 29.9298, 29.9365, 29.9371,\n",
            "        29.9361, 29.9363, 29.9353, 29.9364, 29.9369, 29.9359, 29.9350, 29.9331,\n",
            "        29.9362, 29.9352, 29.9384, 29.9364, 29.9364, 29.9342, 29.9310, 29.9382,\n",
            "        29.9362, 29.9316, 29.9340, 29.9354, 29.9343, 29.9353, 29.9357, 29.9379,\n",
            "        29.9349, 29.9347, 29.9328, 29.9373, 29.9366, 29.9346, 29.9335, 29.9346,\n",
            "        29.9343, 29.9367, 29.9374, 29.9353, 29.9365, 29.9370, 29.9363, 29.9315,\n",
            "        29.9353, 29.9348, 29.9341, 29.9367, 29.9378, 29.9350, 29.9357, 29.9346,\n",
            "        29.9355, 29.9336, 29.9352, 29.9382, 29.9323, 29.9348, 29.9344, 29.9355,\n",
            "        29.9357, 29.9324, 29.9363, 29.9356, 29.9328], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0233, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0120, 0.0119, 0.0119,\n",
            "        0.0119, 0.0120, 0.0120, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0120, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0119, 0.0120, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0119, 0.0119, 0.0119, 0.0120, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0120, 0.0119, 0.0119, 0.0119,\n",
            "        0.0121, 0.0117], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [14]\n",
            "DEBUGGING: logits looks like: tensor([30.1033, 29.9363, 29.9353, 29.9353, 29.9364, 29.9361, 29.9362, 29.9350,\n",
            "        29.9364, 29.9356, 29.9361, 29.9354, 29.9364, 29.9351, 29.9355, 29.9369,\n",
            "        29.9360, 29.9358, 29.9359, 29.9371, 29.9368, 29.9361, 29.9357, 29.9358,\n",
            "        29.9361, 29.9354, 29.9358, 29.9359, 29.9357, 29.9359, 29.9359, 29.9360,\n",
            "        29.9362, 29.9359, 29.9352, 29.9359, 29.9366, 29.9368, 29.9360, 29.9358,\n",
            "        29.9359, 29.9354, 29.9360, 29.9359, 29.9364, 29.9357, 29.9350, 29.9376,\n",
            "        29.9355, 29.9352, 29.9361, 29.9362, 29.9359, 29.9355, 29.9351, 29.9359,\n",
            "        29.9353, 29.9359, 29.9374, 29.9359, 29.9365, 29.9359, 29.9360, 29.9358,\n",
            "        29.9362, 29.9362, 29.9353, 29.9359, 29.9357, 29.9358, 29.9358, 29.9357,\n",
            "        29.9361, 29.9353, 29.9358, 29.9355, 29.9358, 29.9369, 29.9347, 29.9358,\n",
            "        29.9359, 29.9390, 29.9319], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0128, 0.0117, 0.0094, 0.0093, 0.0087, 0.0111, 0.0072, 0.0084, 0.0088,\n",
            "        0.0086, 0.0099, 0.0117, 0.0126, 0.0111, 0.0105, 0.0103, 0.0115, 0.0079,\n",
            "        0.0138, 0.0110, 0.0087, 0.0102, 0.0096, 0.0095, 0.0098, 0.0097, 0.0107,\n",
            "        0.0081, 0.0121, 0.0093, 0.0131, 0.0092, 0.0143, 0.0103, 0.0088, 0.0101,\n",
            "        0.0115, 0.0105, 0.0087, 0.0111, 0.0087, 0.0102, 0.0112, 0.0103, 0.0118,\n",
            "        0.0113, 0.0126, 0.0096, 0.0129, 0.0100, 0.0101, 0.0071, 0.0102, 0.0093,\n",
            "        0.0100, 0.0109, 0.0133, 0.0137, 0.0076, 0.0097, 0.0106, 0.0110, 0.0095,\n",
            "        0.0099, 0.0112, 0.0088, 0.0119, 0.0121, 0.0093, 0.0100, 0.0105, 0.0142,\n",
            "        0.0122, 0.0110, 0.0103, 0.0119, 0.0084, 0.0094, 0.0103, 0.0111, 0.0086,\n",
            "        0.0109, 0.0089, 0.0106, 0.0088, 0.0087, 0.0141, 0.0113, 0.0109, 0.0106,\n",
            "        0.0109, 0.0095, 0.0096, 0.0101, 0.0109, 0.0096],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [40]\n",
            "DEBUGGING: logits looks like: tensor([30.1408, 30.1190, 30.0648, 30.0613, 30.0456, 30.1052, 29.9991, 30.0359,\n",
            "        30.0492, 30.0421, 30.0774, 30.1196, 30.1380, 30.1063, 30.0922, 30.0878,\n",
            "        30.1141, 30.0216, 30.1602, 30.1032, 30.0438, 30.0849, 30.0686, 30.0667,\n",
            "        30.0745, 30.0721, 30.0969, 30.0262, 30.1270, 30.0629, 30.1479, 30.0584,\n",
            "        30.1697, 30.0881, 30.0489, 30.0834, 30.1146, 30.0924, 30.0461, 30.1069,\n",
            "        30.0441, 30.0835, 30.1083, 30.0873, 30.1203, 30.1109, 30.1367, 30.0692,\n",
            "        30.1429, 30.0804, 30.0819, 29.9936, 30.0849, 30.0624, 30.0797, 30.1007,\n",
            "        30.1516, 30.1584, 30.0107, 30.0719, 30.0949, 30.1030, 30.0679, 30.0783,\n",
            "        30.1084, 30.0481, 30.1242, 30.1269, 30.0628, 30.0796, 30.0927, 30.1670,\n",
            "        30.1291, 30.1045, 30.0868, 30.1240, 30.0366, 30.0640, 30.0862, 30.1047,\n",
            "        30.0413, 30.1011, 30.0494, 30.0950, 30.0472, 30.0437, 30.1648, 30.1111,\n",
            "        30.1009, 30.0934, 30.1004, 30.0681, 30.0692, 30.0823, 30.1011, 30.0688],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0118, 0.0066, 0.0120, 0.0146, 0.0105, 0.0098, 0.0108, 0.0107, 0.0061,\n",
            "        0.0070, 0.0094, 0.0088, 0.0106, 0.0121, 0.0109, 0.0098, 0.0123, 0.0073,\n",
            "        0.0074, 0.0077, 0.0069, 0.0086, 0.0100, 0.0141, 0.0095, 0.0174, 0.0102,\n",
            "        0.0092, 0.0076, 0.0063, 0.0130, 0.0156, 0.0064, 0.0110, 0.0133, 0.0113,\n",
            "        0.0075, 0.0095, 0.0081, 0.0063, 0.0095, 0.0144, 0.0164, 0.0071, 0.0085,\n",
            "        0.0119, 0.0055, 0.0060, 0.0076, 0.0056, 0.0100, 0.0096, 0.0090, 0.0063,\n",
            "        0.0162, 0.0067, 0.0087, 0.0069, 0.0090, 0.0060, 0.0071, 0.0088, 0.0130,\n",
            "        0.0058, 0.0089, 0.0040, 0.0115, 0.0054, 0.0132, 0.0067, 0.0111, 0.0078,\n",
            "        0.0059, 0.0067, 0.0105, 0.0129, 0.0103, 0.0070, 0.0077, 0.0079, 0.0084,\n",
            "        0.0161, 0.0138, 0.0075, 0.0078, 0.0120, 0.0103, 0.0176, 0.0120, 0.0090,\n",
            "        0.0062, 0.0134, 0.0111, 0.0113, 0.0052, 0.0092, 0.0063, 0.0059, 0.0090,\n",
            "        0.0106, 0.0088, 0.0088, 0.0067, 0.0086, 0.0087, 0.0052],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [2]\n",
            "DEBUGGING: logits looks like: tensor([30.0883, 29.9433, 30.0939, 30.1431, 30.0604, 30.0429, 30.0680, 30.0658,\n",
            "        29.9229, 29.9601, 30.0317, 30.0152, 30.0635, 30.0950, 30.0697, 30.0419,\n",
            "        30.0999, 29.9690, 29.9709, 29.9818, 29.9559, 30.0112, 30.0477, 30.1339,\n",
            "        30.0339, 30.1861, 30.0531, 30.0271, 29.9777, 29.9315, 30.1129, 30.1590,\n",
            "        29.9343, 30.0711, 30.1182, 30.0780, 29.9753, 30.0340, 29.9957, 29.9303,\n",
            "        30.0337, 30.1382, 30.1711, 29.9621, 30.0057, 30.0920, 29.8997, 29.9218,\n",
            "        29.9779, 29.9049, 30.0485, 30.0378, 30.0223, 29.9307, 30.1677, 29.9470,\n",
            "        30.0121, 29.9538, 30.0203, 29.9201, 29.9623, 30.0167, 30.1128, 29.9127,\n",
            "        30.0176, 29.8181, 30.0826, 29.8926, 30.1175, 29.9459, 30.0727, 29.9860,\n",
            "        29.9166, 29.9478, 30.0588, 30.1114, 30.0558, 29.9576, 29.9824, 29.9902,\n",
            "        30.0054, 30.1662, 30.1280, 29.9756, 29.9846, 30.0937, 30.0554, 30.1885,\n",
            "        30.0928, 30.0215, 29.9265, 30.1205, 30.0746, 30.0773, 29.8835, 30.0256,\n",
            "        29.9305, 29.9161, 30.0213, 30.0616, 30.0151, 30.0162, 29.9461, 30.0100,\n",
            "        30.0137, 29.8850], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.8867413289822252 and immediate abs rewards look like: [0.007054489052279678, 0.009010713994939579, 0.023855028729940386, 0.02393996595947101, 0.032964888662718295, 0.0004545420156318869, 0.0023564006951346528, 0.0033158536916744197, 0.0009329037629868253, 0.0008580111275477975, 0.000780528621362464, 0.0013923420260653074, 0.0011785502188104147, 0.003362161838595057, 0.0022131632590571826, 0.0012339697450443055, 0.0007078517796799133, 7.031701079540653e-05, 3.446405071372283e-05, 4.899367286270717e-05, 9.478050378675107e-05, 0.00015216157680697506, 2.0269021661079023e-06, 2.234218754892936e-06, 0.0003526424079609569, 0.0003295536703262769, 0.0005904269673919771, 1.6257967672572704e-05, 3.838043312498485e-05, 8.68657998580602e-05, 0.00017562918901603553, 0.0003203899477739469, 2.7827480607811594e-05, 1.0642325378285022e-05, 8.163591746779275e-05, 0.7686947337501806, 2.2737367544323206e-12, 1.8189894035458565e-12, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0]\n",
            "DEBUGGING: the total relative reward of the trajectory = 102.21808865785694 and immediate relative rewards look like: [0.024437551717840296, 0.06258120758150677, 0.24929697064572134, 0.3363747161078011, 0.5838872968817514, 0.009775390573287586, 0.05913257336583893, 0.0951769907462896, 0.030160807881610248, 0.030832028689678465, 0.030862039423995236, 0.06007477406233483, 0.055115516996296104, 0.1693999413020667, 0.11961816354352059, 0.0711972766996213, 0.043413363462168564, 0.004567473236027704, 0.0023630561843784757, 0.0035361405712260497, 0.007182994164229419, 0.012081190769910653, 0.0001682544401310555, 0.00019352771261252723, 0.03181860431485851, 0.030928678048450828, 0.05754975402695315, 0.0016437296446945187, 0.004018986243005814, 0.009409894334688166, 0.01966014339838618, 0.037024115759459866, 0.0033166062495990486, 0.001306850477463598, 0.010319566226710672, 99.94966248060386, 4.206412995701706e-10, 3.45607986673477e-10, 8.867573342288067e-11, 9.094947017729282e-11, 1.864464138634079e-10, 0.0, 9.777068044061202e-11, 0.0, 1.0231815394945443e-10, 0.0, 1.0686562745829477e-10, 2.1827872842540352e-10, 1.1141310096718371e-10, 0.0]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0234, 0.0133, 0.0142, 0.0139, 0.0140, 0.0144, 0.0150, 0.0135, 0.0142,\n",
            "        0.0138, 0.0134, 0.0142, 0.0136, 0.0158, 0.0156, 0.0159, 0.0140, 0.0140,\n",
            "        0.0131, 0.0138, 0.0155, 0.0147, 0.0142, 0.0148, 0.0141, 0.0140, 0.0145,\n",
            "        0.0135, 0.0138, 0.0139, 0.0154, 0.0145, 0.0139, 0.0143, 0.0147, 0.0140,\n",
            "        0.0138, 0.0135, 0.0131, 0.0143, 0.0144, 0.0136, 0.0141, 0.0144, 0.0146,\n",
            "        0.0142, 0.0132, 0.0141, 0.0144, 0.0136, 0.0143, 0.0141, 0.0136, 0.0136,\n",
            "        0.0138, 0.0138, 0.0141, 0.0146, 0.0142, 0.0139, 0.0146, 0.0143, 0.0145,\n",
            "        0.0132, 0.0145, 0.0136, 0.0136, 0.0153, 0.0144, 0.0140],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [62]\n",
            "DEBUGGING: logits looks like: tensor([30.0280, 29.8858, 29.9030, 29.8970, 29.9004, 29.9061, 29.9163, 29.8913,\n",
            "        29.9036, 29.8952, 29.8894, 29.9038, 29.8916, 29.9303, 29.9259, 29.9313,\n",
            "        29.8993, 29.8997, 29.8831, 29.8950, 29.9248, 29.9120, 29.9033, 29.9140,\n",
            "        29.9011, 29.9004, 29.9075, 29.8910, 29.8953, 29.8972, 29.9234, 29.9078,\n",
            "        29.8971, 29.9041, 29.9116, 29.9004, 29.8966, 29.8896, 29.8835, 29.9046,\n",
            "        29.9060, 29.8919, 29.9021, 29.9063, 29.9096, 29.9030, 29.8857, 29.9016,\n",
            "        29.9059, 29.8922, 29.9051, 29.9016, 29.8923, 29.8917, 29.8962, 29.8956,\n",
            "        29.9020, 29.9106, 29.9034, 29.8983, 29.9101, 29.9055, 29.9075, 29.8839,\n",
            "        29.9077, 29.8920, 29.8928, 29.9217, 29.9059, 29.8992],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0199, 0.0130, 0.0126, 0.0123, 0.0129, 0.0126, 0.0131, 0.0129, 0.0128,\n",
            "        0.0116, 0.0129, 0.0127, 0.0128, 0.0129, 0.0128, 0.0122, 0.0127, 0.0124,\n",
            "        0.0127, 0.0118, 0.0126, 0.0120, 0.0128, 0.0130, 0.0127, 0.0129, 0.0122,\n",
            "        0.0131, 0.0122, 0.0129, 0.0125, 0.0125, 0.0118, 0.0130, 0.0126, 0.0127,\n",
            "        0.0130, 0.0127, 0.0132, 0.0130, 0.0120, 0.0126, 0.0126, 0.0128, 0.0130,\n",
            "        0.0127, 0.0133, 0.0126, 0.0128, 0.0135, 0.0122, 0.0127, 0.0126, 0.0136,\n",
            "        0.0130, 0.0127, 0.0128, 0.0130, 0.0124, 0.0126, 0.0125, 0.0124, 0.0128,\n",
            "        0.0131, 0.0128, 0.0129, 0.0123, 0.0134, 0.0134, 0.0129, 0.0126, 0.0133,\n",
            "        0.0135, 0.0126, 0.0128, 0.0127, 0.0131, 0.0124],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [20]\n",
            "DEBUGGING: logits looks like: tensor([30.0176, 29.9107, 29.9037, 29.8975, 29.9094, 29.9046, 29.9135, 29.9090,\n",
            "        29.9077, 29.8834, 29.9100, 29.9047, 29.9081, 29.9091, 29.9069, 29.8959,\n",
            "        29.9053, 29.8995, 29.9052, 29.8865, 29.9033, 29.8917, 29.9078, 29.9117,\n",
            "        29.9061, 29.9097, 29.8964, 29.9129, 29.8961, 29.9104, 29.9015, 29.9025,\n",
            "        29.8863, 29.9124, 29.9036, 29.9065, 29.9115, 29.9057, 29.9155, 29.9122,\n",
            "        29.8922, 29.9041, 29.9040, 29.9076, 29.9119, 29.9064, 29.9178, 29.9029,\n",
            "        29.9077, 29.9204, 29.8961, 29.9051, 29.9032, 29.9235, 29.9111, 29.9055,\n",
            "        29.9077, 29.9109, 29.8987, 29.9044, 29.9012, 29.8998, 29.9070, 29.9127,\n",
            "        29.9077, 29.9099, 29.8969, 29.9200, 29.9190, 29.9096, 29.9037, 29.9174,\n",
            "        29.9207, 29.9046, 29.9085, 29.9049, 29.9129, 29.9001],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0206, 0.0113, 0.0111, 0.0112, 0.0112, 0.0111, 0.0112, 0.0111, 0.0112,\n",
            "        0.0111, 0.0112, 0.0109, 0.0112, 0.0109, 0.0111, 0.0112, 0.0112, 0.0112,\n",
            "        0.0110, 0.0112, 0.0112, 0.0111, 0.0111, 0.0111, 0.0110, 0.0111, 0.0112,\n",
            "        0.0112, 0.0111, 0.0111, 0.0111, 0.0112, 0.0111, 0.0110, 0.0112, 0.0111,\n",
            "        0.0112, 0.0111, 0.0111, 0.0111, 0.0111, 0.0112, 0.0112, 0.0111, 0.0112,\n",
            "        0.0110, 0.0111, 0.0110, 0.0111, 0.0111, 0.0111, 0.0113, 0.0117, 0.0111,\n",
            "        0.0111, 0.0111, 0.0110, 0.0113, 0.0112, 0.0111, 0.0112, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0110, 0.0110, 0.0111, 0.0114, 0.0110,\n",
            "        0.0109, 0.0111, 0.0110, 0.0111, 0.0110, 0.0112, 0.0111, 0.0112, 0.0111,\n",
            "        0.0110, 0.0111, 0.0111, 0.0112, 0.0112, 0.0111, 0.0111, 0.0112],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [41]\n",
            "DEBUGGING: logits looks like: tensor([30.0382, 29.8868, 29.8840, 29.8848, 29.8865, 29.8840, 29.8850, 29.8839,\n",
            "        29.8855, 29.8844, 29.8852, 29.8782, 29.8855, 29.8799, 29.8827, 29.8851,\n",
            "        29.8860, 29.8857, 29.8820, 29.8867, 29.8851, 29.8824, 29.8840, 29.8834,\n",
            "        29.8811, 29.8834, 29.8853, 29.8850, 29.8844, 29.8826, 29.8830, 29.8866,\n",
            "        29.8838, 29.8822, 29.8854, 29.8841, 29.8848, 29.8838, 29.8834, 29.8829,\n",
            "        29.8832, 29.8854, 29.8854, 29.8844, 29.8857, 29.8822, 29.8835, 29.8822,\n",
            "        29.8824, 29.8833, 29.8831, 29.8879, 29.8962, 29.8840, 29.8825, 29.8839,\n",
            "        29.8816, 29.8871, 29.8859, 29.8845, 29.8848, 29.8837, 29.8826, 29.8837,\n",
            "        29.8835, 29.8832, 29.8837, 29.8820, 29.8823, 29.8835, 29.8904, 29.8818,\n",
            "        29.8792, 29.8841, 29.8823, 29.8836, 29.8823, 29.8848, 29.8826, 29.8846,\n",
            "        29.8845, 29.8823, 29.8844, 29.8843, 29.8847, 29.8847, 29.8844, 29.8826,\n",
            "        29.8852], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0191, 0.0100, 0.0101, 0.0098, 0.0100, 0.0099, 0.0099, 0.0100, 0.0100,\n",
            "        0.0099, 0.0099, 0.0098, 0.0098, 0.0099, 0.0100, 0.0100, 0.0099, 0.0099,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0099, 0.0100, 0.0099, 0.0098, 0.0099,\n",
            "        0.0100, 0.0099, 0.0098, 0.0100, 0.0099, 0.0099, 0.0099, 0.0100, 0.0101,\n",
            "        0.0102, 0.0098, 0.0099, 0.0100, 0.0100, 0.0100, 0.0099, 0.0100, 0.0100,\n",
            "        0.0100, 0.0083, 0.0099, 0.0099, 0.0099, 0.0100, 0.0100, 0.0098, 0.0096,\n",
            "        0.0100, 0.0101, 0.0097, 0.0097, 0.0099, 0.0099, 0.0099, 0.0100, 0.0099,\n",
            "        0.0099, 0.0099, 0.0100, 0.0099, 0.0100, 0.0099, 0.0098, 0.0098, 0.0098,\n",
            "        0.0098, 0.0102, 0.0098, 0.0100, 0.0099, 0.0098, 0.0100, 0.0099, 0.0099,\n",
            "        0.0100, 0.0100, 0.0099, 0.0096, 0.0101, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0099, 0.0100, 0.0099, 0.0099, 0.0099, 0.0100, 0.0100,\n",
            "        0.0100], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [11]\n",
            "DEBUGGING: logits looks like: tensor([30.0329, 29.8700, 29.8724, 29.8670, 29.8704, 29.8681, 29.8689, 29.8698,\n",
            "        29.8703, 29.8684, 29.8688, 29.8657, 29.8663, 29.8679, 29.8706, 29.8719,\n",
            "        29.8673, 29.8673, 29.8704, 29.8698, 29.8702, 29.8700, 29.8694, 29.8711,\n",
            "        29.8691, 29.8659, 29.8682, 29.8710, 29.8682, 29.8664, 29.8710, 29.8675,\n",
            "        29.8674, 29.8675, 29.8702, 29.8737, 29.8751, 29.8665, 29.8682, 29.8706,\n",
            "        29.8720, 29.8699, 29.8680, 29.8715, 29.8711, 29.8706, 29.8247, 29.8685,\n",
            "        29.8680, 29.8687, 29.8699, 29.8701, 29.8663, 29.8604, 29.8708, 29.8724,\n",
            "        29.8640, 29.8636, 29.8678, 29.8687, 29.8688, 29.8704, 29.8692, 29.8693,\n",
            "        29.8689, 29.8703, 29.8690, 29.8703, 29.8678, 29.8665, 29.8667, 29.8657,\n",
            "        29.8662, 29.8754, 29.8672, 29.8710, 29.8677, 29.8667, 29.8711, 29.8686,\n",
            "        29.8682, 29.8707, 29.8704, 29.8674, 29.8611, 29.8727, 29.8704, 29.8705,\n",
            "        29.8705, 29.8704, 29.8711, 29.8719, 29.8685, 29.8705, 29.8680, 29.8686,\n",
            "        29.8691, 29.8719, 29.8712, 29.8709], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0180, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0091, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0091, 0.0093, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0091, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0091, 0.0091, 0.0092,\n",
            "        0.0092, 0.0091, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0091, 0.0092,\n",
            "        0.0092, 0.0092, 0.0091, 0.0092, 0.0092, 0.0091, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0091, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0091, 0.0092, 0.0092, 0.0092, 0.0092, 0.0091,\n",
            "        0.0092, 0.0091, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [55]\n",
            "DEBUGGING: logits looks like: tensor([30.0279, 29.8598, 29.8598, 29.8606, 29.8599, 29.8602, 29.8604, 29.8594,\n",
            "        29.8607, 29.8590, 29.8610, 29.8604, 29.8601, 29.8600, 29.8596, 29.8602,\n",
            "        29.8596, 29.8602, 29.8607, 29.8599, 29.8593, 29.8599, 29.8601, 29.8596,\n",
            "        29.8591, 29.8621, 29.8603, 29.8598, 29.8597, 29.8598, 29.8591, 29.8612,\n",
            "        29.8610, 29.8594, 29.8608, 29.8598, 29.8599, 29.8607, 29.8596, 29.8605,\n",
            "        29.8601, 29.8606, 29.8586, 29.8589, 29.8597, 29.8598, 29.8589, 29.8598,\n",
            "        29.8601, 29.8600, 29.8597, 29.8596, 29.8584, 29.8601, 29.8600, 29.8600,\n",
            "        29.8586, 29.8601, 29.8602, 29.8589, 29.8607, 29.8604, 29.8600, 29.8601,\n",
            "        29.8599, 29.8609, 29.8597, 29.8595, 29.8573, 29.8604, 29.8598, 29.8598,\n",
            "        29.8594, 29.8608, 29.8592, 29.8595, 29.8610, 29.8607, 29.8596, 29.8596,\n",
            "        29.8610, 29.8602, 29.8595, 29.8593, 29.8591, 29.8593, 29.8596, 29.8595,\n",
            "        29.8612, 29.8571, 29.8617, 29.8590, 29.8599, 29.8599, 29.8599, 29.8604,\n",
            "        29.8597, 29.8606, 29.8604, 29.8607, 29.8601, 29.8607, 29.8602, 29.8601,\n",
            "        29.8600, 29.8601, 29.8596, 29.8597], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.074249504126783 and immediate abs rewards look like: [0.003554526105290279, 0.017798721729832323, 0.0019209970378142316, 0.0024921018016357266, 0.0018703172627283493, 0.002566985761404794, 0.00025548160238031414, 0.0010523284913688258, 0.0013873343054910947, 5.5853408866823884e-05, 0.0023189153139355767, 0.0015601897684973665, 0.004150381165345607, 0.0031014238434181607, 0.00011583131845327443, 0.0009354505455121398, 0.00011507482668093871, 0.0005643308359140065, 0.0037282684920683096, 0.008538319209947076, 0.0006487683676823508, 0.00012655850514420308, 0.0027641966753435554, 0.00011626489003901952, 0.003507688446006796, 5.583791062235832e-05, 5.4683520829712506e-05, 0.00021894134715694236, 8.614829766884213e-06, 0.0003470897545412299, 9.393375830768491e-05, 0.0007034177538116637, 0.002065024331841414, 0.00014005968296260107, 0.00032235917660727864, 0.00045164337007008726, 0.00013839035636920016, 0.0014477243967121467, 0.0002728359067987185, 0.001009439516110433, 0.0013594049105449812, 3.120086148555856e-06, 6.506372392323101e-05, 3.6795518099097535e-07, 8.697739303897833e-06, 0.00010325801940780366, 7.209719115053304e-07, 4.451063705346314e-05, 8.437114138359902e-05, 3.683618615468731e-06]\n",
            "DEBUGGING: the total relative reward of the trajectory = 2.7694724613539434 and immediate relative rewards look like: [0.009585414739565284, 0.0960869699248356, 0.01563094264586845, 0.027051374085695894, 0.025394674600987963, 0.04184588017226262, 0.004862265446192329, 0.02289035229123652, 0.033959327236569595, 0.0015196690854905365, 0.06940384291003655, 0.05097282668089713, 0.14695890190084884, 0.11839815821924726, 0.004741770026698719, 0.040848656058577654, 0.005340440730300704, 0.027731143521528286, 0.19341457016452873, 0.4667379481887474, 0.03732453314491241, 0.0076291584843162456, 0.17421055187026047, 0.007651852372334738, 0.2404814729307425, 0.003985116937991397, 0.004052895933044253, 0.016828195766609363, 0.0006858395699310928, 0.02858524485058425, 0.007994727099212512, 0.061800893641475285, 0.18713475237321284, 0.013084397732738536, 0.03100176905981685, 0.044680189328362575, 0.014072728877016884, 0.1512017358683068, 0.02925676206250957, 0.11102812388001615, 0.1533013442931353, 0.000360571638579269, 0.007698097947270348, 4.454833064475353e-05, 0.0010769679515858497, 0.013069727752516165, 9.324240390519087e-05, 0.00587898554115431, 0.011376080803954768, 0.0005068242776860694]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0175, 0.0149, 0.0142, 0.0143, 0.0135, 0.0127, 0.0157, 0.0151, 0.0157,\n",
            "        0.0143, 0.0150, 0.0159, 0.0136, 0.0142, 0.0140, 0.0150, 0.0144, 0.0147,\n",
            "        0.0150, 0.0142, 0.0151, 0.0158, 0.0147, 0.0152, 0.0145, 0.0146, 0.0144,\n",
            "        0.0148, 0.0143, 0.0134, 0.0165, 0.0166, 0.0149, 0.0151, 0.0157, 0.0167,\n",
            "        0.0147, 0.0142, 0.0135, 0.0166, 0.0137, 0.0123, 0.0146, 0.0140, 0.0144,\n",
            "        0.0146, 0.0145, 0.0129, 0.0157, 0.0151, 0.0142, 0.0123, 0.0141, 0.0171,\n",
            "        0.0135, 0.0161, 0.0148, 0.0149, 0.0138, 0.0163, 0.0140, 0.0134, 0.0154,\n",
            "        0.0138, 0.0152, 0.0149, 0.0155, 0.0138], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [1]\n",
            "DEBUGGING: logits looks like: tensor([30.2614, 30.2210, 30.2094, 30.2109, 30.1959, 30.1815, 30.2349, 30.2236,\n",
            "        30.2341, 30.2103, 30.2226, 30.2378, 30.1986, 30.2088, 30.2046, 30.2222,\n",
            "        30.2118, 30.2179, 30.2228, 30.2088, 30.2241, 30.2352, 30.2169, 30.2264,\n",
            "        30.2137, 30.2159, 30.2124, 30.2200, 30.2110, 30.1944, 30.2461, 30.2479,\n",
            "        30.2217, 30.2251, 30.2335, 30.2501, 30.2175, 30.2084, 30.1970, 30.2488,\n",
            "        30.1995, 30.1738, 30.2156, 30.2047, 30.2121, 30.2165, 30.2141, 30.1851,\n",
            "        30.2348, 30.2239, 30.2082, 30.1740, 30.2071, 30.2552, 30.1963, 30.2402,\n",
            "        30.2195, 30.2215, 30.2023, 30.2428, 30.2063, 30.1937, 30.2297, 30.2019,\n",
            "        30.2267, 30.2202, 30.2307, 30.2014], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0156, 0.0127, 0.0130, 0.0132, 0.0138, 0.0140, 0.0137, 0.0132, 0.0122,\n",
            "        0.0127, 0.0122, 0.0135, 0.0131, 0.0134, 0.0127, 0.0134, 0.0121, 0.0123,\n",
            "        0.0127, 0.0133, 0.0128, 0.0127, 0.0133, 0.0127, 0.0127, 0.0126, 0.0126,\n",
            "        0.0136, 0.0117, 0.0118, 0.0121, 0.0136, 0.0134, 0.0113, 0.0124, 0.0131,\n",
            "        0.0125, 0.0161, 0.0132, 0.0126, 0.0131, 0.0130, 0.0126, 0.0126, 0.0129,\n",
            "        0.0128, 0.0142, 0.0125, 0.0110, 0.0138, 0.0130, 0.0139, 0.0128, 0.0128,\n",
            "        0.0129, 0.0133, 0.0129, 0.0120, 0.0127, 0.0125, 0.0125, 0.0120, 0.0127,\n",
            "        0.0137, 0.0131, 0.0132, 0.0129, 0.0129, 0.0118, 0.0126, 0.0129, 0.0151,\n",
            "        0.0118, 0.0146, 0.0129, 0.0136, 0.0146], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [74]\n",
            "DEBUGGING: logits looks like: tensor([30.2690, 30.2171, 30.2241, 30.2276, 30.2384, 30.2422, 30.2355, 30.2269,\n",
            "        30.2066, 30.2176, 30.2064, 30.2320, 30.2253, 30.2306, 30.2165, 30.2306,\n",
            "        30.2054, 30.2091, 30.2177, 30.2294, 30.2192, 30.2173, 30.2296, 30.2168,\n",
            "        30.2171, 30.2150, 30.2162, 30.2338, 30.1966, 30.1991, 30.2054, 30.2349,\n",
            "        30.2305, 30.1874, 30.2123, 30.2248, 30.2139, 30.2770, 30.2278, 30.2152,\n",
            "        30.2259, 30.2225, 30.2146, 30.2160, 30.2221, 30.2183, 30.2444, 30.2142,\n",
            "        30.1821, 30.2378, 30.2225, 30.2403, 30.2192, 30.2191, 30.2219, 30.2291,\n",
            "        30.2216, 30.2036, 30.2167, 30.2140, 30.2132, 30.2022, 30.2172, 30.2354,\n",
            "        30.2249, 30.2276, 30.2212, 30.2221, 30.1986, 30.2159, 30.2213, 30.2606,\n",
            "        30.1998, 30.2530, 30.2203, 30.2336, 30.2528], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0133, 0.0113, 0.0112, 0.0111, 0.0109, 0.0110, 0.0114, 0.0112, 0.0110,\n",
            "        0.0107, 0.0113, 0.0113, 0.0114, 0.0111, 0.0117, 0.0110, 0.0113, 0.0111,\n",
            "        0.0109, 0.0117, 0.0111, 0.0110, 0.0113, 0.0110, 0.0112, 0.0112, 0.0113,\n",
            "        0.0113, 0.0111, 0.0113, 0.0117, 0.0111, 0.0114, 0.0113, 0.0112, 0.0112,\n",
            "        0.0113, 0.0112, 0.0111, 0.0112, 0.0111, 0.0115, 0.0111, 0.0109, 0.0113,\n",
            "        0.0111, 0.0111, 0.0113, 0.0111, 0.0113, 0.0112, 0.0112, 0.0112, 0.0111,\n",
            "        0.0113, 0.0113, 0.0114, 0.0111, 0.0112, 0.0111, 0.0111, 0.0113, 0.0111,\n",
            "        0.0113, 0.0112, 0.0111, 0.0111, 0.0111, 0.0112, 0.0111, 0.0111, 0.0113,\n",
            "        0.0113, 0.0113, 0.0114, 0.0111, 0.0112, 0.0113, 0.0114, 0.0112, 0.0113,\n",
            "        0.0114, 0.0112, 0.0114, 0.0112, 0.0113, 0.0113, 0.0114, 0.0113],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [33]\n",
            "DEBUGGING: logits looks like: tensor([30.2847, 30.2435, 30.2413, 30.2402, 30.2362, 30.2371, 30.2454, 30.2423,\n",
            "        30.2373, 30.2307, 30.2452, 30.2441, 30.2462, 30.2387, 30.2527, 30.2365,\n",
            "        30.2432, 30.2406, 30.2350, 30.2523, 30.2405, 30.2374, 30.2441, 30.2385,\n",
            "        30.2428, 30.2411, 30.2441, 30.2445, 30.2397, 30.2438, 30.2524, 30.2401,\n",
            "        30.2456, 30.2448, 30.2426, 30.2426, 30.2439, 30.2415, 30.2408, 30.2418,\n",
            "        30.2407, 30.2480, 30.2388, 30.2345, 30.2437, 30.2388, 30.2395, 30.2433,\n",
            "        30.2394, 30.2437, 30.2430, 30.2412, 30.2413, 30.2404, 30.2447, 30.2444,\n",
            "        30.2458, 30.2408, 30.2420, 30.2397, 30.2397, 30.2448, 30.2396, 30.2432,\n",
            "        30.2410, 30.2401, 30.2391, 30.2394, 30.2422, 30.2391, 30.2407, 30.2438,\n",
            "        30.2446, 30.2435, 30.2465, 30.2405, 30.2411, 30.2439, 30.2463, 30.2421,\n",
            "        30.2430, 30.2468, 30.2412, 30.2458, 30.2412, 30.2444, 30.2432, 30.2457,\n",
            "        30.2436], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0122, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0103, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [59]\n",
            "DEBUGGING: logits looks like: tensor([30.2975, 30.2566, 30.2562, 30.2568, 30.2569, 30.2570, 30.2569, 30.2573,\n",
            "        30.2571, 30.2565, 30.2560, 30.2575, 30.2575, 30.2571, 30.2567, 30.2562,\n",
            "        30.2573, 30.2573, 30.2578, 30.2574, 30.2564, 30.2570, 30.2574, 30.2569,\n",
            "        30.2570, 30.2572, 30.2563, 30.2578, 30.2571, 30.2569, 30.2559, 30.2569,\n",
            "        30.2572, 30.2574, 30.2567, 30.2575, 30.2578, 30.2568, 30.2566, 30.2574,\n",
            "        30.2570, 30.2566, 30.2572, 30.2571, 30.2564, 30.2569, 30.2568, 30.2572,\n",
            "        30.2570, 30.2571, 30.2564, 30.2573, 30.2579, 30.2572, 30.2561, 30.2568,\n",
            "        30.2562, 30.2547, 30.2568, 30.2568, 30.2567, 30.2570, 30.2571, 30.2573,\n",
            "        30.2572, 30.2565, 30.2572, 30.2567, 30.2571, 30.2568, 30.2567, 30.2566,\n",
            "        30.2569, 30.2573, 30.2575, 30.2575, 30.2561, 30.2574, 30.2568, 30.2569,\n",
            "        30.2569, 30.2567, 30.2568, 30.2576, 30.2572, 30.2568, 30.2571, 30.2575,\n",
            "        30.2571, 30.2579, 30.2573, 30.2569, 30.2570, 30.2566, 30.2569, 30.2571],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0107, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0091, 0.0092, 0.0092, 0.0092,\n",
            "        0.0091, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0091, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0091, 0.0092, 0.0091, 0.0092, 0.0092, 0.0091, 0.0092, 0.0091,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0091, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0091, 0.0092, 0.0091, 0.0092, 0.0091, 0.0092,\n",
            "        0.0092, 0.0092, 0.0091, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0091, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0091, 0.0091, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0091, 0.0092, 0.0091, 0.0092, 0.0092, 0.0091, 0.0091,\n",
            "        0.0092, 0.0092, 0.0092, 0.0091, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [11]\n",
            "DEBUGGING: logits looks like: tensor([30.3080, 30.2686, 30.2680, 30.2679, 30.2686, 30.2679, 30.2680, 30.2684,\n",
            "        30.2680, 30.2680, 30.2681, 30.2684, 30.2681, 30.2680, 30.2676, 30.2685,\n",
            "        30.2679, 30.2687, 30.2677, 30.2687, 30.2678, 30.2683, 30.2678, 30.2678,\n",
            "        30.2676, 30.2679, 30.2679, 30.2679, 30.2680, 30.2679, 30.2683, 30.2681,\n",
            "        30.2684, 30.2680, 30.2679, 30.2679, 30.2678, 30.2677, 30.2678, 30.2677,\n",
            "        30.2678, 30.2683, 30.2676, 30.2687, 30.2678, 30.2692, 30.2686, 30.2688,\n",
            "        30.2678, 30.2682, 30.2678, 30.2679, 30.2675, 30.2681, 30.2680, 30.2685,\n",
            "        30.2684, 30.2676, 30.2682, 30.2676, 30.2678, 30.2677, 30.2679, 30.2678,\n",
            "        30.2681, 30.2677, 30.2678, 30.2678, 30.2678, 30.2681, 30.2679, 30.2681,\n",
            "        30.2682, 30.2686, 30.2680, 30.2680, 30.2679, 30.2683, 30.2677, 30.2678,\n",
            "        30.2680, 30.2679, 30.2684, 30.2681, 30.2687, 30.2677, 30.2677, 30.2679,\n",
            "        30.2685, 30.2680, 30.2680, 30.2683, 30.2677, 30.2681, 30.2678, 30.2679,\n",
            "        30.2681, 30.2674, 30.2676, 30.2680, 30.2681, 30.2683, 30.2678, 30.2680,\n",
            "        30.2680, 30.2684, 30.2681, 30.2682, 30.2684], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.12084076502242169 and immediate abs rewards look like: [0.007805409814864106, 0.0014461469313573616, 0.009591424099653523, 0.024366288354030985, 0.03327962593311895, 0.000789035103025526, 0.00038691294048476266, 0.002686401462142385, 0.002051237790965388, 0.004593840244069725, 0.0029772392877021048, 0.0006677411370219488, 0.0046325482985594135, 0.003288140108452353, 0.011270753355347551, 0.0006147880171738507, 0.0001865145236479293, 0.002039487927504524, 0.00029948608516860986, 6.988873747104662e-05, 0.0019512418725753378, 0.0003469396028776828, 6.804984150221571e-05, 0.0001238734175785794, 4.2335185753472615e-05, 2.160054782507359e-05, 0.0013395627984209568, 2.391192674622289e-05, 0.0006205172576301266, 7.916149115771987e-05, 0.00017537847361381864, 1.4780142009840347e-05, 0.0006422224764719431, 0.0005763153117186448, 4.599329031407251e-05, 0.0010569125847723626, 0.00016865165343915578, 3.860622655338375e-06, 5.7975003073806874e-06, 1.676651299931109e-05, 3.2733298212406226e-05, 2.427262597848312e-05, 6.709393665005337e-05, 7.583197384519735e-05, 5.3144734010857064e-05, 9.007531571114669e-05, 2.3702573798800586e-05, 1.0425295840832405e-05, 4.768983990288689e-05, 4.901276633972884e-05]\n",
            "DEBUGGING: the total relative reward of the trajectory = 3.2993472537571735 and immediate relative rewards look like: [0.02516809693482651, 0.00934956700048062, 0.0930585973058889, 0.31619192786950806, 0.5441222673201483, 0.01565120298818208, 0.008956219855843838, 0.07107713647640013, 0.061110180410023224, 0.15216890739643632, 0.10864702158909934, 0.026609085061201132, 0.2000323254810712, 0.15313836076547266, 0.5630217336225479, 0.032882097540158375, 0.010601442830862192, 0.12275074228035104, 0.019039564059945956, 0.004677431793678889, 0.1371231658346658, 0.025558811809907468, 0.005241668051540597, 0.00995666325524893, 0.003544736470908233, 0.0018809911267874849, 0.12113750300233414, 0.0022434648587839365, 0.0602978754537417, 0.007959314740550779, 0.018221744791706084, 0.0015852804127866275, 0.07103609850264497, 0.06569196183796386, 0.005397834932575636, 0.12758668198611678, 0.020931967188952814, 0.0004921338519323508, 0.0007584871678020789, 0.002249813074588357, 0.0045021479375437256, 0.0034199279838834887, 0.009678458069117539, 0.011193586383887273, 0.008023207390104127, 0.013901018929308258, 0.003737571416926731, 0.0016789169488169147, 0.00784012616824009, 0.00822218559568023]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 3\n",
            "DEBUGGING: the action_prob is: tensor([0.0183, 0.0143, 0.0152, 0.0147, 0.0154, 0.0142, 0.0150, 0.0150, 0.0149,\n",
            "        0.0149, 0.0149, 0.0154, 0.0149, 0.0151, 0.0143, 0.0148, 0.0161, 0.0140,\n",
            "        0.0148, 0.0144, 0.0149, 0.0131, 0.0141, 0.0144, 0.0155, 0.0145, 0.0153,\n",
            "        0.0155, 0.0143, 0.0151, 0.0136, 0.0148, 0.0151, 0.0142, 0.0148, 0.0150,\n",
            "        0.0152, 0.0147, 0.0153, 0.0155, 0.0146, 0.0150, 0.0148, 0.0148, 0.0145,\n",
            "        0.0145, 0.0152, 0.0150, 0.0149, 0.0153, 0.0144, 0.0141, 0.0156, 0.0148,\n",
            "        0.0156, 0.0151, 0.0143, 0.0158, 0.0150, 0.0152, 0.0152, 0.0157, 0.0149,\n",
            "        0.0141, 0.0153, 0.0153, 0.0154], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [2]\n",
            "DEBUGGING: logits looks like: tensor([30.0333, 29.9727, 29.9867, 29.9796, 29.9911, 29.9711, 29.9845, 29.9841,\n",
            "        29.9815, 29.9827, 29.9827, 29.9907, 29.9818, 29.9855, 29.9712, 29.9799,\n",
            "        30.0022, 29.9661, 29.9809, 29.9736, 29.9824, 29.9505, 29.9677, 29.9741,\n",
            "        29.9922, 29.9747, 29.9891, 29.9921, 29.9723, 29.9857, 29.9604, 29.9805,\n",
            "        29.9864, 29.9707, 29.9809, 29.9836, 29.9871, 29.9790, 29.9891, 29.9918,\n",
            "        29.9779, 29.9844, 29.9801, 29.9804, 29.9761, 29.9762, 29.9872, 29.9835,\n",
            "        29.9828, 29.9883, 29.9741, 29.9690, 29.9943, 29.9806, 29.9942, 29.9857,\n",
            "        29.9726, 29.9972, 29.9846, 29.9872, 29.9867, 29.9946, 29.9822, 29.9687,\n",
            "        29.9885, 29.9893, 29.9898], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0155, 0.0125, 0.0121, 0.0144, 0.0131, 0.0135, 0.0136, 0.0137, 0.0127,\n",
            "        0.0131, 0.0143, 0.0148, 0.0124, 0.0130, 0.0123, 0.0103, 0.0142, 0.0148,\n",
            "        0.0117, 0.0124, 0.0127, 0.0117, 0.0141, 0.0117, 0.0139, 0.0117, 0.0134,\n",
            "        0.0129, 0.0125, 0.0118, 0.0127, 0.0124, 0.0125, 0.0129, 0.0136, 0.0124,\n",
            "        0.0133, 0.0127, 0.0127, 0.0135, 0.0117, 0.0106, 0.0123, 0.0127, 0.0122,\n",
            "        0.0100, 0.0127, 0.0135, 0.0127, 0.0115, 0.0128, 0.0138, 0.0115, 0.0141,\n",
            "        0.0118, 0.0112, 0.0142, 0.0145, 0.0129, 0.0146, 0.0130, 0.0128, 0.0120,\n",
            "        0.0127, 0.0148, 0.0124, 0.0129, 0.0120, 0.0138, 0.0110, 0.0134, 0.0133,\n",
            "        0.0133, 0.0128, 0.0125, 0.0126, 0.0111, 0.0126],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [6]\n",
            "DEBUGGING: logits looks like: tensor([30.0499, 29.9957, 29.9889, 30.0313, 30.0076, 30.0161, 30.0167, 30.0186,\n",
            "        29.9992, 30.0072, 30.0296, 30.0387, 29.9940, 30.0061, 29.9915, 29.9481,\n",
            "        30.0279, 30.0391, 29.9804, 29.9950, 30.0011, 29.9801, 30.0271, 29.9805,\n",
            "        30.0225, 29.9803, 30.0140, 30.0048, 29.9969, 29.9824, 30.0003, 29.9932,\n",
            "        29.9958, 30.0047, 30.0172, 29.9947, 30.0120, 30.0005, 29.9993, 30.0158,\n",
            "        29.9795, 29.9561, 29.9913, 29.9997, 29.9896, 29.9399, 30.0001, 30.0162,\n",
            "        29.9997, 29.9751, 30.0024, 30.0208, 29.9763, 30.0266, 29.9822, 29.9686,\n",
            "        30.0275, 30.0337, 30.0038, 30.0355, 30.0060, 30.0024, 29.9853, 29.9992,\n",
            "        30.0391, 29.9937, 30.0038, 29.9867, 30.0213, 29.9632, 30.0134, 30.0119,\n",
            "        30.0110, 30.0024, 29.9960, 29.9983, 29.9670, 29.9980],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0144, 0.0114, 0.0115, 0.0112, 0.0115, 0.0116, 0.0110, 0.0114, 0.0113,\n",
            "        0.0112, 0.0114, 0.0114, 0.0112, 0.0117, 0.0112, 0.0109, 0.0112, 0.0110,\n",
            "        0.0119, 0.0111, 0.0113, 0.0111, 0.0114, 0.0113, 0.0112, 0.0109, 0.0113,\n",
            "        0.0116, 0.0116, 0.0113, 0.0116, 0.0109, 0.0112, 0.0111, 0.0114, 0.0112,\n",
            "        0.0109, 0.0114, 0.0110, 0.0111, 0.0114, 0.0113, 0.0112, 0.0115, 0.0116,\n",
            "        0.0108, 0.0116, 0.0114, 0.0116, 0.0115, 0.0116, 0.0116, 0.0115, 0.0115,\n",
            "        0.0114, 0.0117, 0.0114, 0.0110, 0.0112, 0.0115, 0.0110, 0.0111, 0.0114,\n",
            "        0.0120, 0.0115, 0.0113, 0.0116, 0.0116, 0.0108, 0.0113, 0.0116, 0.0114,\n",
            "        0.0118, 0.0111, 0.0114, 0.0114, 0.0109, 0.0109, 0.0112, 0.0114, 0.0111,\n",
            "        0.0112, 0.0120, 0.0116, 0.0113, 0.0108, 0.0112, 0.0113],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [61]\n",
            "DEBUGGING: logits looks like: tensor([30.0438, 29.9858, 29.9869, 29.9811, 29.9874, 29.9895, 29.9754, 29.9862,\n",
            "        29.9836, 29.9811, 29.9846, 29.9842, 29.9801, 29.9910, 29.9813, 29.9749,\n",
            "        29.9812, 29.9764, 29.9960, 29.9782, 29.9833, 29.9791, 29.9858, 29.9831,\n",
            "        29.9805, 29.9733, 29.9827, 29.9899, 29.9903, 29.9836, 29.9905, 29.9730,\n",
            "        29.9817, 29.9778, 29.9856, 29.9811, 29.9742, 29.9846, 29.9771, 29.9795,\n",
            "        29.9855, 29.9828, 29.9812, 29.9863, 29.9901, 29.9724, 29.9895, 29.9842,\n",
            "        29.9886, 29.9878, 29.9900, 29.9890, 29.9874, 29.9863, 29.9849, 29.9925,\n",
            "        29.9847, 29.9771, 29.9814, 29.9867, 29.9756, 29.9774, 29.9842, 29.9982,\n",
            "        29.9879, 29.9823, 29.9892, 29.9890, 29.9718, 29.9834, 29.9900, 29.9859,\n",
            "        29.9946, 29.9790, 29.9847, 29.9852, 29.9734, 29.9746, 29.9810, 29.9859,\n",
            "        29.9780, 29.9802, 29.9971, 29.9901, 29.9831, 29.9724, 29.9807, 29.9827],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0105, 0.0109, 0.0122, 0.0103, 0.0106, 0.0057, 0.0084, 0.0095, 0.0083,\n",
            "        0.0098, 0.0100, 0.0108, 0.0137, 0.0133, 0.0076, 0.0120, 0.0104, 0.0080,\n",
            "        0.0116, 0.0125, 0.0120, 0.0107, 0.0076, 0.0099, 0.0068, 0.0089, 0.0084,\n",
            "        0.0092, 0.0091, 0.0086, 0.0163, 0.0106, 0.0111, 0.0078, 0.0114, 0.0098,\n",
            "        0.0057, 0.0109, 0.0095, 0.0092, 0.0082, 0.0128, 0.0085, 0.0071, 0.0107,\n",
            "        0.0058, 0.0062, 0.0109, 0.0119, 0.0154, 0.0103, 0.0096, 0.0081, 0.0138,\n",
            "        0.0110, 0.0119, 0.0100, 0.0100, 0.0088, 0.0122, 0.0074, 0.0069, 0.0096,\n",
            "        0.0087, 0.0086, 0.0115, 0.0096, 0.0068, 0.0119, 0.0082, 0.0088, 0.0127,\n",
            "        0.0073, 0.0111, 0.0127, 0.0135, 0.0109, 0.0125, 0.0090, 0.0128, 0.0107,\n",
            "        0.0115, 0.0070, 0.0079, 0.0086, 0.0121, 0.0094, 0.0070, 0.0124, 0.0108,\n",
            "        0.0113, 0.0136, 0.0123, 0.0105, 0.0093, 0.0103, 0.0119, 0.0093, 0.0105],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [77]\n",
            "DEBUGGING: logits looks like: tensor([30.0056, 30.0155, 30.0427, 29.9993, 30.0070, 29.8545, 29.9510, 29.9815,\n",
            "        29.9453, 29.9888, 29.9931, 30.0114, 30.0727, 30.0642, 29.9231, 30.0397,\n",
            "        30.0029, 29.9379, 30.0305, 30.0496, 30.0380, 30.0107, 29.9239, 29.9916,\n",
            "        29.8976, 29.9635, 29.9493, 29.9714, 29.9706, 29.9560, 30.1152, 30.0089,\n",
            "        30.0181, 29.9325, 30.0265, 29.9870, 29.8533, 30.0144, 29.9806, 29.9733,\n",
            "        29.9421, 30.0539, 29.9537, 29.9062, 30.0097, 29.8558, 29.8749, 30.0150,\n",
            "        30.0369, 30.1007, 29.9997, 29.9831, 29.9407, 30.0736, 30.0173, 30.0366,\n",
            "        29.9926, 29.9940, 29.9623, 30.0427, 29.9162, 29.9000, 29.9818, 29.9595,\n",
            "        29.9567, 30.0290, 29.9825, 29.8983, 30.0364, 29.9431, 29.9623, 30.0527,\n",
            "        29.9131, 30.0192, 30.0534, 30.0691, 30.0148, 30.0489, 29.9676, 30.0550,\n",
            "        30.0094, 30.0286, 29.9024, 29.9344, 29.9557, 30.0403, 29.9770, 29.9044,\n",
            "        30.0466, 30.0132, 30.0234, 30.0693, 30.0449, 30.0050, 29.9745, 30.0017,\n",
            "        30.0370, 29.9759, 30.0055], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0094, 0.0091, 0.0100, 0.0093, 0.0092, 0.0095, 0.0107, 0.0072, 0.0102,\n",
            "        0.0077, 0.0096, 0.0099, 0.0076, 0.0095, 0.0078, 0.0091, 0.0098, 0.0104,\n",
            "        0.0097, 0.0087, 0.0096, 0.0102, 0.0092, 0.0104, 0.0091, 0.0098, 0.0103,\n",
            "        0.0085, 0.0085, 0.0091, 0.0090, 0.0107, 0.0078, 0.0091, 0.0098, 0.0097,\n",
            "        0.0072, 0.0083, 0.0088, 0.0099, 0.0110, 0.0086, 0.0096, 0.0095, 0.0097,\n",
            "        0.0091, 0.0094, 0.0089, 0.0095, 0.0107, 0.0100, 0.0092, 0.0094, 0.0109,\n",
            "        0.0103, 0.0078, 0.0092, 0.0099, 0.0090, 0.0083, 0.0094, 0.0093, 0.0096,\n",
            "        0.0108, 0.0081, 0.0079, 0.0094, 0.0093, 0.0088, 0.0085, 0.0107, 0.0089,\n",
            "        0.0097, 0.0098, 0.0089, 0.0091, 0.0096, 0.0099, 0.0095, 0.0094, 0.0094,\n",
            "        0.0098, 0.0095, 0.0093, 0.0101, 0.0100, 0.0096, 0.0101, 0.0091, 0.0089,\n",
            "        0.0093, 0.0097, 0.0096, 0.0075, 0.0094, 0.0102, 0.0110, 0.0080, 0.0083,\n",
            "        0.0082, 0.0095, 0.0084, 0.0095, 0.0101, 0.0100, 0.0109, 0.0087],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [1]\n",
            "DEBUGGING: logits looks like: tensor([29.9810, 29.9744, 29.9987, 29.9805, 29.9761, 29.9849, 30.0149, 29.9165,\n",
            "        30.0028, 29.9328, 29.9882, 29.9944, 29.9287, 29.9859, 29.9343, 29.9753,\n",
            "        29.9935, 30.0077, 29.9901, 29.9617, 29.9879, 30.0035, 29.9756, 30.0065,\n",
            "        29.9734, 29.9937, 30.0042, 29.9567, 29.9563, 29.9751, 29.9713, 30.0156,\n",
            "        29.9343, 29.9750, 29.9930, 29.9912, 29.9138, 29.9506, 29.9653, 29.9952,\n",
            "        30.0209, 29.9592, 29.9871, 29.9852, 29.9907, 29.9747, 29.9831, 29.9695,\n",
            "        29.9858, 30.0136, 29.9970, 29.9775, 29.9818, 30.0180, 30.0048, 29.9349,\n",
            "        29.9776, 29.9939, 29.9724, 29.9504, 29.9834, 29.9804, 29.9873, 30.0157,\n",
            "        29.9438, 29.9400, 29.9827, 29.9785, 29.9646, 29.9576, 30.0150, 29.9682,\n",
            "        29.9893, 29.9922, 29.9698, 29.9740, 29.9881, 29.9948, 29.9842, 29.9810,\n",
            "        29.9818, 29.9915, 29.9852, 29.9802, 30.0000, 29.9970, 29.9874, 30.0012,\n",
            "        29.9727, 29.9687, 29.9805, 29.9895, 29.9884, 29.9241, 29.9824, 30.0030,\n",
            "        30.0207, 29.9418, 29.9514, 29.9468, 29.9849, 29.9555, 29.9843, 29.9990,\n",
            "        29.9967, 30.0184, 29.9628], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.7058461509268454 and immediate abs rewards look like: [0.0037891482907070895, 0.0033748833438949077, 0.0009859615356617724, 0.00231018750127987, 0.0009342687926618964, 0.00013536435835703742, 0.0017497078133601462, 0.00010406447245259187, 0.0006315550781437196, 0.00029092852764733834, 0.0028062414407941105, 0.0008175348898475931, 0.003007237834026455, 0.0006554965407303825, 3.922361929653562e-05, 0.00011786934692281648, 2.802029621307156e-05, 0.0024139429547176405, 0.0005649490462928952, 0.0003254513603678788, 0.0005330309591045079, 6.224529170140158e-05, 0.00011791498536695144, 0.0001394599594277679, 0.00018094979986926774, 5.230528631727793e-05, 0.0010759962792690203, 0.00109537513799296, 0.0007814747154952784, 0.0002532550643081777, 0.00022806429751653923, 0.0003379417457836098, 0.0002656622696122213, 0.0002818167536133842, 0.00016269460502371658, 6.269110235734843e-05, 0.675133235625708, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 94.72374485625178 and immediate relative rewards look like: [0.014003561471581884, 0.024980104717325896, 0.010960478026834199, 0.0342542282924213, 0.017330890695689414, 0.0030142909954666737, 0.04545848802115539, 0.0030919040983645486, 0.021110769663340592, 0.010807830305662212, 0.1146877063591409, 0.03648713209569064, 0.1454438583947612, 0.03417974814444584, 0.0021918729881321938, 0.007025927541723251, 0.0017746941647178473, 0.16188474279678983, 0.04002764644881294, 0.024277544815176084, 0.04175544608063949, 0.005109247580023929, 0.010118932155934238, 0.01248871429266393, 0.016880203169860424, 0.005074903755103209, 0.1084156278836339, 0.11450189854328342, 0.08464130303287973, 0.028384129498782062, 0.02641534431130111, 0.040407883940895076, 0.03276219182742409, 0.03581112576437373, 0.02128429112428173, 0.008436315494937353, 93.3782638766876, 8.640199666844783e-11, 8.86757334228605e-11, 1.818989403545443e-10, 9.322320693174634e-11, 9.549694368615746e-11, 9.777068044061202e-11, 1.000444171950221e-10, 1.0231815394947769e-10, 0.0, 0.0, 0.0, 1.1141310096718371e-10, 1.1368683772159018e-10]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 200 number of matrices\n",
            "DEBUGGING: ACT_MAT has 200 number of matrices\n",
            "DEBUGGING: VAL looks like: [[72.40005765492708, 73.10668697293863, 73.78192501551226, 74.27538186350156, 74.68586580544824, 74.85048334198635, 75.59667469839704, 76.30054760104161, 76.97512182858112, 77.72218284919143, 78.47611193990076, 79.23762616209774, 79.9773246343792, 80.72950415897263, 81.37384264411169, 82.07497422279613, 82.83209792535001, 83.62493390089682, 84.46501659359677, 85.31581165395191, 86.1740156700815, 87.03720472314876, 87.90416518422107, 88.79191609068782, 89.68860864946991, 90.5624141870253, 91.44594495856246, 92.31151030761163, 93.24228947269387, 94.18007119843521, 95.12188010515204, 96.06284844621582, 96.99578215197612, 97.97218741992577, 98.96048542368517, 99.94966248228127, 1.6943503831993734e-09, 1.2865748319486897e-09, 9.504715608840533e-10, 8.705008358193663e-10, 7.874256218606803e-10, 6.0704970504775e-10, 6.131815202502525e-10, 5.206170099087278e-10, 5.258757675845735e-10, 4.278359733688072e-10, 4.3215754885738097e-10, 3.2857769838291537e-10, 1.1141310096718371e-10, 0.0], [2.224815394868452, 2.2376060405342293, 2.1631505763731247, 2.169211751239653, 2.163798360761573, 2.1600037233945306, 2.13955337699219, 2.1562536480262606, 2.1549124199343677, 2.142376861310907, 2.1624820123489057, 2.114220373170575, 2.084088430797654, 1.9566964938351568, 1.856867005672636, 1.8708335713595325, 1.848469611415106, 1.861746637055359, 1.8525409025594248, 1.6758851842372688, 1.2213608444934558, 1.1959962740894379, 1.2003708238435573, 1.0365255272457543, 1.0392663382559792, 0.8068533993184208, 0.8109780630105348, 0.8150759263408995, 0.8063108389639295, 0.8137626256505035, 0.7931084654544639, 0.7930441801568195, 0.7386295823387317, 0.5570654848136554, 0.5494758455362797, 0.5237111883600635, 0.4838696959916171, 0.47454239102484874, 0.3266067223803454, 0.3003534952705412, 0.19123774887931827, 0.038319600592104014, 0.03834245348840883, 0.03095389448599847, 0.031221561773084563, 0.03044908466818052, 0.01755490597541854, 0.017638044011629647, 0.011877836838863976, 0.0005068242776860694], [2.889131121842826, 2.8928919443515144, 2.9126690680313474, 2.84809138457117, 2.5574741986885474, 2.0336888195640395, 2.0384218349251086, 2.049965267746732, 1.9988769002730629, 1.9573401210737775, 1.8234052663407487, 1.7320790351026762, 1.7226969192338133, 1.538045044194689, 1.3988956398274912, 0.8443170769746902, 0.8196312923581129, 0.8172018682093443, 0.7014657837666598, 0.6893194138451655, 0.6915575576277643, 0.560034739184948, 0.5398746741162025, 0.5400333394592545, 0.5354309860646521, 0.5372588379734786, 0.5407857038855466, 0.4238870715992045, 0.4259026330711319, 0.3692977349670608, 0.36498830325910103, 0.3502692509771666, 0.3522060308731111, 0.2840100326974405, 0.22052330389846125, 0.21729845350089455, 0.09061795102502807, 0.07038988266270228, 0.07060378667754538, 0.0705508075855993, 0.0689909035464757, 0.06514015718073937, 0.062343665855409985, 0.053197179582113584, 0.042427871917400314, 0.03475218639120827, 0.02106178531505052, 0.017499205957700797, 0.01598008990796352, 0.00822218559568023], [66.15082660727329, 66.80487176343607, 67.45443601890781, 68.12472276856666, 68.77825105078205, 69.45547490917814, 70.15400062442694, 70.81670922869272, 71.52890638847914, 72.23009658466243, 72.9487765195523, 73.5697866799931, 74.27606014939133, 74.87941039494604, 75.60124307757737, 76.36267798443356, 77.12692126958773, 77.90418846002324, 78.52757951234994, 79.28035542010217, 80.05664431847171, 80.8231200731223, 81.63435436923461, 82.44872266371584, 83.26892318123554, 84.09297270511685, 84.937270506426, 85.68571199852765, 86.43556575755997, 87.22315601467383, 88.07552715674248, 88.93849678023352, 89.79604939019457, 90.66998706905771, 91.54967266999327, 92.45291755441312, 93.37826387769513, 1.017717179001126e-09, 9.407224063966447e-10, 8.606532050240245e-10, 6.856103683530103e-10, 5.983708701224888e-10, 5.079534610467993e-10, 4.1432604101635076e-10, 3.1745618567810976e-10, 2.173111431602344e-10, 2.1950620521235797e-10, 2.217234396084424e-10, 2.23963070311558e-10, 1.1368683772159018e-10]]\n",
            "DEBUGGING: traj_returns = [72.40005765492708, 2.224815394868452, 2.889131121842826, 66.15082660727329]\n",
            "DEBUGGING: actions = [[15], [23], [23], [29], [15], [21], [31], [51], [19], [10], [51], [11], [34], [60], [21], [19], [29], [32], [13], [12], [7], [68], [62], [67], [73], [33], [47], [47], [19], [14], [57], [66], [53], [20], [23], [0], [35], [15], [76], [40], [91], [95], [14], [78], [72], [29], [48], [3], [98], [2], [6], [8], [5], [20], [15], [51], [58], [2], [58], [62], [59], [29], [24], [7], [40], [62], [21], [8], [13], [20], [72], [44], [9], [32], [77], [81], [26], [80], [41], [41], [46], [33], [77], [28], [7], [71], [29], [39], [47], [11], [98], [39], [100], [59], [15], [58], [65], [16], [77], [55], [37], [37], [29], [59], [40], [33], [53], [23], [25], [1], [53], [56], [23], [36], [36], [51], [44], [36], [67], [74], [37], [3], [3], [44], [66], [27], [62], [53], [82], [33], [50], [15], [17], [54], [84], [19], [83], [36], [34], [59], [51], [42], [80], [70], [7], [98], [56], [57], [93], [11], [39], [27], [24], [17], [3], [31], [34], [1], [34], [2], [60], [46], [51], [27], [70], [68], [47], [7], [39], [6], [19], [10], [24], [65], [38], [59], [10], [51], [47], [61], [65], [35], [44], [53], [9], [49], [0], [29], [50], [77], [13], [91], [13], [0], [89], [19], [14], [55], [62], [1]]\n",
            "DEBUGGING: actions length = 200\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[3.0765, 1.5040, 0.3785,  ..., 2.5484, 0.2970, 2.1222],\n",
            "        [3.0159, 1.4209, 0.3571,  ..., 2.4689, 0.2314, 2.0299],\n",
            "        [3.0847, 1.5271, 0.3640,  ..., 2.5819, 0.3210, 2.1183],\n",
            "        ...,\n",
            "        [3.0674, 1.4924, 0.3659,  ..., 2.5437, 0.2803, 2.0903],\n",
            "        [3.0726, 1.4988, 0.3667,  ..., 2.5500, 0.2852, 2.0967],\n",
            "        [3.0714, 1.4977, 0.3669,  ..., 2.5488, 0.2841, 2.0954]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[3.0695, 1.4955, 0.3667,  ..., 2.5466, 0.2821, 2.0930],\n",
            "        [3.0692, 1.4950, 0.3672,  ..., 2.5460, 0.2814, 2.0928],\n",
            "        [3.0710, 1.4969, 0.3668,  ..., 2.5481, 0.2832, 2.0946],\n",
            "        ...,\n",
            "        [3.0709, 1.4967, 0.3667,  ..., 2.5478, 0.2834, 2.0950],\n",
            "        [3.0720, 1.4989, 0.3669,  ..., 2.5499, 0.2856, 2.0969],\n",
            "        [3.0683, 1.4934, 0.3662,  ..., 2.5449, 0.2806, 2.0909]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([29.9810, 29.9744, 29.9987, 29.9805, 29.9761, 29.9849, 30.0149, 29.9165,\n",
            "        30.0028, 29.9328, 29.9882, 29.9944, 29.9287, 29.9859, 29.9343, 29.9753,\n",
            "        29.9935, 30.0077, 29.9901, 29.9617, 29.9879, 30.0035, 29.9756, 30.0065,\n",
            "        29.9734, 29.9937, 30.0042, 29.9567, 29.9563, 29.9751, 29.9713, 30.0156,\n",
            "        29.9343, 29.9750, 29.9930, 29.9912, 29.9138, 29.9506, 29.9653, 29.9952,\n",
            "        30.0209, 29.9592, 29.9871, 29.9852, 29.9907, 29.9747, 29.9831, 29.9695,\n",
            "        29.9858, 30.0136, 29.9970, 29.9775, 29.9818, 30.0180, 30.0048, 29.9349,\n",
            "        29.9776, 29.9939, 29.9724, 29.9504, 29.9834, 29.9804, 29.9873, 30.0157,\n",
            "        29.9438, 29.9400, 29.9827, 29.9785, 29.9646, 29.9576, 30.0150, 29.9682,\n",
            "        29.9893, 29.9922, 29.9698, 29.9740, 29.9881, 29.9948, 29.9842, 29.9810,\n",
            "        29.9818, 29.9915, 29.9852, 29.9802, 30.0000, 29.9970, 29.9874, 30.0012,\n",
            "        29.9727, 29.9687, 29.9805, 29.9895, 29.9884, 29.9241, 29.9824, 30.0030,\n",
            "        30.0207, 29.9418, 29.9514, 29.9468, 29.9849, 29.9555, 29.9843, 29.9990,\n",
            "        29.9967, 30.0184, 29.9628], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[3.59162077e+01 3.62605142e+01 3.65780452e+01 3.68543519e+01\n",
            "  3.70463474e+01 3.71249127e+01 3.74821626e+01 3.78308689e+01\n",
            "  3.81644544e+01 3.85129991e+01 3.88526939e+01 3.91634281e+01\n",
            "  3.95150425e+01 3.97759140e+01 4.00577121e+01 4.02882007e+01\n",
            "  4.06567800e+01 4.10520177e+01 4.13866507e+01 4.17403429e+01\n",
            "  4.20358946e+01 4.24040890e+01 4.28196913e+01 4.32042994e+01\n",
            "  4.36330573e+01 4.39998748e+01 4.44337448e+01 4.48090463e+01\n",
            "  4.52275172e+01 4.56465719e+01 4.60888760e+01 4.65361647e+01\n",
            "  4.69706668e+01 4.73708125e+01 4.78200393e+01 4.82858974e+01\n",
            "  2.34881879e+01 1.36233069e-01 9.93026277e-02 9.27260761e-02\n",
            "  6.50571635e-02 2.58649397e-02 2.51715301e-02 2.10377688e-02\n",
            "  1.84123586e-02 1.63003179e-02 9.65417299e-03 8.78431263e-03\n",
            "  6.96448177e-03 2.18225250e-03]]\n",
            "DEBUGGING: baseline2 looks like: 35.916207694727916\n",
            "DEBUGGING: ADS looks like: [ 3.64838500e+01  3.68461728e+01  3.72038798e+01  3.74210299e+01\n",
            "  3.76395185e+01  3.77255706e+01  3.81145121e+01  3.84696787e+01\n",
            "  3.88106674e+01  3.92091837e+01  3.96234180e+01  4.00741981e+01\n",
            "  4.04622821e+01  4.09535901e+01  4.13161306e+01  4.17867735e+01\n",
            "  4.21753179e+01  4.25729162e+01  4.30783659e+01  4.35754687e+01\n",
            "  4.41381211e+01  4.46331158e+01  4.50844739e+01  4.55876167e+01\n",
            "  4.60555514e+01  4.65625394e+01  4.70122002e+01  4.75024640e+01\n",
            "  4.80147723e+01  4.85334993e+01  4.90330041e+01  4.95266838e+01\n",
            "  5.00251154e+01  5.06013749e+01  5.11404461e+01  5.16637651e+01\n",
            " -2.34881879e+01 -1.36233068e-01 -9.93026268e-02 -9.27260753e-02\n",
            " -6.50571627e-02 -2.58649391e-02 -2.51715295e-02 -2.10377682e-02\n",
            " -1.84123581e-02 -1.63003175e-02 -9.65417255e-03 -8.78431230e-03\n",
            " -6.96448166e-03 -2.18225250e-03 -3.36913923e+01 -3.40229081e+01\n",
            " -3.44148946e+01 -3.46851402e+01 -3.48825490e+01 -3.49649090e+01\n",
            " -3.53426093e+01 -3.56746153e+01 -3.60095420e+01 -3.63706222e+01\n",
            " -3.66902119e+01 -3.70492077e+01 -3.74309541e+01 -3.78192175e+01\n",
            " -3.82008451e+01 -3.84173671e+01 -3.88083104e+01 -3.91902711e+01\n",
            " -3.95341098e+01 -4.00644577e+01 -4.08145338e+01 -4.12080927e+01\n",
            " -4.16193204e+01 -4.21677739e+01 -4.25937910e+01 -4.31930214e+01\n",
            " -4.36227667e+01 -4.39939704e+01 -4.44212063e+01 -4.48328093e+01\n",
            " -4.52957675e+01 -4.57431205e+01 -4.62320372e+01 -4.68137470e+01\n",
            " -4.72705635e+01 -4.77621862e+01 -2.30043182e+01  3.38309322e-01\n",
            "  2.27304095e-01  2.07627419e-01  1.26180585e-01  1.24546608e-02\n",
            "  1.31709234e-02  9.91612574e-03  1.28092031e-02  1.41487667e-02\n",
            "  7.90073299e-03  8.85373138e-03  4.91335507e-03 -1.67542822e-03\n",
            " -3.30270766e+01 -3.33676222e+01 -3.36653761e+01 -3.40062606e+01\n",
            " -3.44888732e+01 -3.50912239e+01 -3.54437408e+01 -3.57809037e+01\n",
            " -3.61655775e+01 -3.65556590e+01 -3.70292887e+01 -3.74313490e+01\n",
            " -3.77923456e+01 -3.82378690e+01 -3.86588165e+01 -3.94438836e+01\n",
            " -3.98371487e+01 -4.02348158e+01 -4.06851849e+01 -4.10510235e+01\n",
            " -4.13443370e+01 -4.18440542e+01 -4.22798166e+01 -4.26642661e+01\n",
            " -4.30976263e+01 -4.34626159e+01 -4.38929591e+01 -4.43851593e+01\n",
            " -4.48016145e+01 -4.52772742e+01 -4.57238877e+01 -4.61858954e+01\n",
            " -4.66184608e+01 -4.70868025e+01 -4.75995160e+01 -4.80685990e+01\n",
            " -2.33975699e+01 -6.58431863e-02 -2.86988411e-02 -2.21752686e-02\n",
            "  3.93374007e-03  3.92752174e-02  3.71721357e-02  3.21594108e-02\n",
            "  2.40155133e-02  1.84518685e-02  1.14076123e-02  8.71489333e-03\n",
            "  9.01560814e-03  6.03993310e-03  3.02346189e+01  3.05443576e+01\n",
            "  3.08763908e+01  3.12703708e+01  3.17319037e+01  3.23305622e+01\n",
            "  3.26718380e+01  3.29858403e+01  3.33644520e+01  3.37170975e+01\n",
            "  3.40960826e+01  3.44063586e+01  3.47610176e+01  3.51034964e+01\n",
            "  3.55435310e+01  3.60744773e+01  3.64701412e+01  3.68521707e+01\n",
            "  3.71409288e+01  3.75400125e+01  3.80207497e+01  3.84190311e+01\n",
            "  3.88146631e+01  3.92444233e+01  3.96358659e+01  4.00930979e+01\n",
            "  4.05035257e+01  4.08766657e+01  4.12080486e+01  4.15765841e+01\n",
            "  4.19866511e+01  4.24023321e+01  4.28253826e+01  4.32991746e+01\n",
            "  4.37296334e+01  4.41670201e+01  6.98900760e+01 -1.36233068e-01\n",
            " -9.93026268e-02 -9.27260753e-02 -6.50571628e-02 -2.58649391e-02\n",
            " -2.51715296e-02 -2.10377683e-02 -1.84123583e-02 -1.63003177e-02\n",
            " -9.65417277e-03 -8.78431241e-03 -6.96448155e-03 -2.18225238e-03]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(-0.0732, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0141,  0.0088, -0.0456,  ...,  0.0174, -0.0246, -0.0653],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [ 0.0189,  0.0118, -0.0614,  ...,  0.0233, -0.0332, -0.1025],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0008, -0.0005,  0.0028,  ..., -0.0009,  0.0015,  0.0240]])\n",
            "   Last layer:\n",
            "tensor([[ 1.6997e-05,  9.9248e-03,  0.0000e+00, -1.6729e-02,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -3.2938e-02, -8.1571e-03,  0.0000e+00,\n",
            "         -1.1647e-04,  0.0000e+00,  0.0000e+00,  1.7169e-02,  0.0000e+00,\n",
            "         -1.5672e-02,  0.0000e+00, -2.5262e-02, -2.9187e-02,  0.0000e+00],\n",
            "        [ 2.3069e-05,  3.9920e-03,  0.0000e+00, -1.1160e-02,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -2.5796e-02, -8.6909e-03,  0.0000e+00,\n",
            "         -7.0088e-03,  0.0000e+00,  0.0000e+00,  6.8218e-03,  0.0000e+00,\n",
            "         -1.3908e-02,  0.0000e+00, -1.9787e-02, -1.9008e-02,  0.0000e+00],\n",
            "        [-4.6493e-07,  7.8654e-04,  0.0000e+00, -4.1547e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -1.0544e-02, -4.0761e-03,  0.0000e+00,\n",
            "         -4.4722e-03,  0.0000e+00,  0.0000e+00,  1.2236e-03,  0.0000e+00,\n",
            "         -6.0529e-03,  0.0000e+00, -8.0898e-03, -6.8843e-03,  0.0000e+00],\n",
            "        [ 1.2313e-06,  2.4359e-03,  0.0000e+00, -5.5847e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -1.2320e-02, -3.7999e-03,  0.0000e+00,\n",
            "         -2.3707e-03,  0.0000e+00,  0.0000e+00,  4.2388e-03,  0.0000e+00,\n",
            "         -6.4321e-03,  0.0000e+00, -9.4614e-03, -9.6304e-03,  0.0000e+00],\n",
            "        [ 1.1477e-05,  2.0104e-03,  0.0000e+00,  2.3375e-04,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  3.5462e-03,  2.7272e-03,  0.0000e+00,\n",
            "          5.7020e-03,  0.0000e+00,  0.0000e+00,  3.5906e-03,  0.0000e+00,\n",
            "          3.0381e-03,  0.0000e+00,  2.7310e-03, -5.9138e-06,  0.0000e+00],\n",
            "        [ 1.3184e-07, -1.3041e-04,  0.0000e+00,  5.8740e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  1.6412e-02,  6.9658e-03,  0.0000e+00,\n",
            "          8.9171e-03,  0.0000e+00,  0.0000e+00, -8.8425e-05,  0.0000e+00,\n",
            "          9.9195e-03,  0.0000e+00,  1.2601e-02,  9.6332e-03,  0.0000e+00],\n",
            "        [-2.8517e-06,  7.3119e-03,  0.0000e+00, -1.4644e-02,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -3.0825e-02, -8.7888e-03,  0.0000e+00,\n",
            "         -3.7653e-03,  0.0000e+00,  0.0000e+00,  1.2613e-02,  0.0000e+00,\n",
            "         -1.5546e-02,  0.0000e+00, -2.3661e-02, -2.5309e-02,  0.0000e+00],\n",
            "        [ 1.6925e-05,  7.6823e-03,  0.0000e+00, -1.5452e-02,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -3.2606e-02, -9.3623e-03,  0.0000e+00,\n",
            "         -4.0288e-03,  0.0000e+00,  0.0000e+00,  1.3265e-02,  0.0000e+00,\n",
            "         -1.6449e-02,  0.0000e+00, -2.4995e-02, -2.6716e-02,  0.0000e+00],\n",
            "        [ 1.7093e-05, -4.3508e-04,  0.0000e+00, -8.3300e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -2.4101e-02, -1.0643e-02,  0.0000e+00,\n",
            "         -1.4209e-02,  0.0000e+00,  0.0000e+00, -1.0101e-03,  0.0000e+00,\n",
            "         -1.4790e-02,  0.0000e+00, -1.8484e-02, -1.3510e-02,  0.0000e+00],\n",
            "        [ 2.9326e-05,  6.9870e-03,  0.0000e+00, -1.4544e-02,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -3.0932e-02, -9.1062e-03,  0.0000e+00,\n",
            "         -4.3738e-03,  0.0000e+00,  0.0000e+00,  1.1964e-02,  0.0000e+00,\n",
            "         -1.5693e-02,  0.0000e+00, -2.3694e-02, -2.5024e-02,  0.0000e+00]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 7.1455e-03,  3.8722e-02, -2.8939e-02,  ...,  4.4360e-02,\n",
            "          9.6732e-02, -3.9177e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 6.5188e-02,  9.9952e-02,  3.2588e-02,  ...,  1.0159e-01,\n",
            "          1.6137e-01,  1.0573e+01],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-1.0183e-02, -1.3380e-02, -7.6979e-03,  ..., -1.3203e-02,\n",
            "         -1.8854e-02, -2.0622e+00]])\n",
            "   Last layer:\n",
            "tensor([[-0.0026,  0.0356,  0.0000, -0.0107,  0.0000,  0.0000,  0.0000,  0.0050,\n",
            "         -0.0543,  0.0000,  0.0236,  0.0000,  0.0000, -0.0267,  0.0000,  0.0379,\n",
            "          0.0000,  0.0009,  0.0235,  0.0000],\n",
            "        [-0.0012,  0.0174,  0.0000, -0.0051,  0.0000,  0.0000,  0.0000,  0.0029,\n",
            "         -0.0261,  0.0000,  0.0119,  0.0000,  0.0000, -0.0129,  0.0000,  0.0187,\n",
            "          0.0000,  0.0008,  0.0117,  0.0000],\n",
            "        [-0.0012,  0.0042,  0.0000, -0.0053,  0.0000,  0.0000,  0.0000, -0.0111,\n",
            "         -0.0117,  0.0000, -0.0061,  0.0000,  0.0000, -0.0045,  0.0000, -0.0034,\n",
            "          0.0000, -0.0090, -0.0033,  0.0000],\n",
            "        [ 0.0013,  0.0114,  0.0000,  0.0055,  0.0000,  0.0000,  0.0000,  0.0277,\n",
            "         -0.0053,  0.0000,  0.0274,  0.0000,  0.0000, -0.0056,  0.0000,  0.0297,\n",
            "          0.0000,  0.0206,  0.0211,  0.0000],\n",
            "        [-0.0017,  0.0054,  0.0000, -0.0069,  0.0000,  0.0000,  0.0000, -0.0146,\n",
            "         -0.0154,  0.0000, -0.0081,  0.0000,  0.0000, -0.0058,  0.0000, -0.0046,\n",
            "          0.0000, -0.0118, -0.0044,  0.0000],\n",
            "        [-0.0005, -0.0051,  0.0000, -0.0019,  0.0000,  0.0000,  0.0000, -0.0107,\n",
            "          0.0030,  0.0000, -0.0110,  0.0000,  0.0000,  0.0026,  0.0000, -0.0121,\n",
            "          0.0000, -0.0079, -0.0085,  0.0000],\n",
            "        [ 0.0011,  0.0283,  0.0000,  0.0050,  0.0000,  0.0000,  0.0000,  0.0433,\n",
            "         -0.0248,  0.0000,  0.0487,  0.0000,  0.0000, -0.0167,  0.0000,  0.0566,\n",
            "          0.0000,  0.0314,  0.0391,  0.0000],\n",
            "        [-0.0009,  0.0297,  0.0000, -0.0036,  0.0000,  0.0000,  0.0000,  0.0198,\n",
            "         -0.0378,  0.0000,  0.0316,  0.0000,  0.0000, -0.0204,  0.0000,  0.0421,\n",
            "          0.0000,  0.0130,  0.0277,  0.0000],\n",
            "        [-0.0007,  0.0034,  0.0000, -0.0030,  0.0000,  0.0000,  0.0000, -0.0054,\n",
            "         -0.0076,  0.0000, -0.0023,  0.0000,  0.0000, -0.0031,  0.0000, -0.0005,\n",
            "          0.0000, -0.0044, -0.0009,  0.0000],\n",
            "        [-0.0035,  0.0243,  0.0000, -0.0146,  0.0000,  0.0000,  0.0000, -0.0180,\n",
            "         -0.0468,  0.0000, -0.0003,  0.0000,  0.0000, -0.0206,  0.0000,  0.0112,\n",
            "          0.0000, -0.0161,  0.0048,  0.0000]])\n",
            "DEBUGGING: training for one iteration takes 0.004450 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 21\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0148, 0.0139, 0.0185, 0.0129, 0.0152, 0.0134, 0.0149, 0.0148, 0.0161,\n",
            "        0.0122, 0.0169, 0.0132, 0.0147, 0.0137, 0.0143, 0.0170, 0.0146, 0.0112,\n",
            "        0.0171, 0.0154, 0.0161, 0.0155, 0.0146, 0.0128, 0.0138, 0.0145, 0.0175,\n",
            "        0.0154, 0.0138, 0.0128, 0.0139, 0.0177, 0.0153, 0.0149, 0.0163, 0.0145,\n",
            "        0.0128, 0.0137, 0.0119, 0.0155, 0.0129, 0.0143, 0.0140, 0.0132, 0.0129,\n",
            "        0.0172, 0.0129, 0.0144, 0.0174, 0.0143, 0.0131, 0.0127, 0.0131, 0.0152,\n",
            "        0.0136, 0.0130, 0.0155, 0.0133, 0.0169, 0.0114, 0.0124, 0.0155, 0.0149,\n",
            "        0.0143, 0.0131, 0.0176, 0.0140, 0.0168, 0.0117],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [24]\n",
            "DEBUGGING: logits looks like: tensor([25.6127, 25.5975, 25.6680, 25.5775, 25.6195, 25.5880, 25.6145, 25.6132,\n",
            "        25.6341, 25.5648, 25.6455, 25.5830, 25.6115, 25.5935, 25.6045, 25.6471,\n",
            "        25.6085, 25.5431, 25.6484, 25.6217, 25.6341, 25.6239, 25.6098, 25.5760,\n",
            "        25.5957, 25.6079, 25.6542, 25.6218, 25.5950, 25.5764, 25.5959, 25.6570,\n",
            "        25.6206, 25.6149, 25.6364, 25.6070, 25.5753, 25.5927, 25.5571, 25.6247,\n",
            "        25.5786, 25.6036, 25.5976, 25.5834, 25.5786, 25.6495, 25.5789, 25.6052,\n",
            "        25.6528, 25.6046, 25.5823, 25.5735, 25.5815, 25.6185, 25.5920, 25.5801,\n",
            "        25.6239, 25.5854, 25.6456, 25.5465, 25.5681, 25.6247, 25.6144, 25.6034,\n",
            "        25.5817, 25.6562, 25.5986, 25.6437, 25.5533], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0108, 0.0131, 0.0095, 0.0109, 0.0120, 0.0130, 0.0127, 0.0133, 0.0120,\n",
            "        0.0117, 0.0139, 0.0127, 0.0131, 0.0131, 0.0126, 0.0123, 0.0125, 0.0129,\n",
            "        0.0106, 0.0126, 0.0135, 0.0145, 0.0125, 0.0131, 0.0140, 0.0132, 0.0141,\n",
            "        0.0130, 0.0128, 0.0122, 0.0129, 0.0113, 0.0119, 0.0132, 0.0131, 0.0132,\n",
            "        0.0138, 0.0149, 0.0135, 0.0124, 0.0133, 0.0130, 0.0120, 0.0110, 0.0138,\n",
            "        0.0122, 0.0120, 0.0114, 0.0129, 0.0127, 0.0131, 0.0129, 0.0111, 0.0123,\n",
            "        0.0123, 0.0129, 0.0121, 0.0125, 0.0126, 0.0127, 0.0124, 0.0133, 0.0135,\n",
            "        0.0127, 0.0126, 0.0129, 0.0136, 0.0142, 0.0129, 0.0125, 0.0120, 0.0135,\n",
            "        0.0109, 0.0130, 0.0117, 0.0129, 0.0117, 0.0133, 0.0129],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [28]\n",
            "DEBUGGING: logits looks like: tensor([25.6391, 25.6870, 25.6078, 25.6403, 25.6644, 25.6851, 25.6791, 25.6914,\n",
            "        25.6659, 25.6584, 25.7011, 25.6788, 25.6874, 25.6865, 25.6778, 25.6718,\n",
            "        25.6753, 25.6836, 25.6342, 25.6769, 25.6938, 25.7118, 25.6743, 25.6860,\n",
            "        25.7040, 25.6896, 25.7060, 25.6856, 25.6809, 25.6701, 25.6837, 25.6504,\n",
            "        25.6637, 25.6881, 25.6877, 25.6881, 25.6996, 25.7187, 25.6943, 25.6733,\n",
            "        25.6898, 25.6860, 25.6660, 25.6435, 25.6991, 25.6691, 25.6655, 25.6522,\n",
            "        25.6821, 25.6795, 25.6865, 25.6835, 25.6460, 25.6715, 25.6718, 25.6835,\n",
            "        25.6672, 25.6752, 25.6781, 25.6794, 25.6735, 25.6899, 25.6943, 25.6800,\n",
            "        25.6774, 25.6837, 25.6954, 25.7063, 25.6834, 25.6749, 25.6648, 25.6953,\n",
            "        25.6400, 25.6858, 25.6592, 25.6826, 25.6589, 25.6909, 25.6834],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0116, 0.0116, 0.0117, 0.0124, 0.0118, 0.0112, 0.0129, 0.0124, 0.0144,\n",
            "        0.0109, 0.0112, 0.0120, 0.0117, 0.0122, 0.0115, 0.0121, 0.0120, 0.0118,\n",
            "        0.0108, 0.0120, 0.0134, 0.0130, 0.0114, 0.0106, 0.0114, 0.0117, 0.0122,\n",
            "        0.0112, 0.0115, 0.0108, 0.0105, 0.0116, 0.0113, 0.0116, 0.0122, 0.0124,\n",
            "        0.0111, 0.0110, 0.0110, 0.0112, 0.0117, 0.0127, 0.0110, 0.0116, 0.0123,\n",
            "        0.0115, 0.0113, 0.0114, 0.0115, 0.0115, 0.0105, 0.0118, 0.0110, 0.0112,\n",
            "        0.0129, 0.0125, 0.0112, 0.0121, 0.0117, 0.0125, 0.0115, 0.0114, 0.0124,\n",
            "        0.0097, 0.0119, 0.0116, 0.0109, 0.0105, 0.0118, 0.0105, 0.0112, 0.0109,\n",
            "        0.0117, 0.0116, 0.0120, 0.0112, 0.0124, 0.0116, 0.0134, 0.0115, 0.0114,\n",
            "        0.0102, 0.0113, 0.0116, 0.0112, 0.0112], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [23]\n",
            "DEBUGGING: logits looks like: tensor([25.6401, 25.6418, 25.6432, 25.6582, 25.6452, 25.6326, 25.6668, 25.6581,\n",
            "        25.6944, 25.6259, 25.6326, 25.6495, 25.6440, 25.6525, 25.6383, 25.6509,\n",
            "        25.6501, 25.6451, 25.6220, 25.6502, 25.6766, 25.6691, 25.6368, 25.6192,\n",
            "        25.6370, 25.6426, 25.6535, 25.6329, 25.6391, 25.6236, 25.6148, 25.6405,\n",
            "        25.6343, 25.6407, 25.6527, 25.6574, 25.6300, 25.6282, 25.6277, 25.6311,\n",
            "        25.6429, 25.6629, 25.6268, 25.6403, 25.6557, 25.6392, 25.6351, 25.6368,\n",
            "        25.6389, 25.6385, 25.6163, 25.6446, 25.6277, 25.6328, 25.6664, 25.6592,\n",
            "        25.6311, 25.6511, 25.6437, 25.6602, 25.6381, 25.6366, 25.6580, 25.5966,\n",
            "        25.6480, 25.6406, 25.6246, 25.6152, 25.6454, 25.6169, 25.6317, 25.6248,\n",
            "        25.6424, 25.6399, 25.6499, 25.6311, 25.6566, 25.6409, 25.6760, 25.6394,\n",
            "        25.6356, 25.6080, 25.6347, 25.6414, 25.6318, 25.6321],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0106, 0.0105, 0.0103, 0.0107, 0.0103, 0.0112, 0.0106, 0.0109, 0.0114,\n",
            "        0.0101, 0.0105, 0.0105, 0.0107, 0.0103, 0.0104, 0.0101, 0.0105, 0.0106,\n",
            "        0.0105, 0.0101, 0.0103, 0.0105, 0.0105, 0.0108, 0.0102, 0.0099, 0.0109,\n",
            "        0.0102, 0.0100, 0.0106, 0.0100, 0.0107, 0.0106, 0.0109, 0.0107, 0.0099,\n",
            "        0.0105, 0.0107, 0.0106, 0.0104, 0.0106, 0.0105, 0.0106, 0.0103, 0.0102,\n",
            "        0.0102, 0.0108, 0.0103, 0.0109, 0.0108, 0.0105, 0.0107, 0.0106, 0.0106,\n",
            "        0.0108, 0.0100, 0.0105, 0.0103, 0.0107, 0.0104, 0.0106, 0.0109, 0.0106,\n",
            "        0.0107, 0.0106, 0.0106, 0.0103, 0.0106, 0.0101, 0.0102, 0.0103, 0.0114,\n",
            "        0.0101, 0.0104, 0.0104, 0.0105, 0.0110, 0.0106, 0.0108, 0.0106, 0.0105,\n",
            "        0.0104, 0.0110, 0.0110, 0.0109, 0.0105, 0.0106, 0.0106, 0.0105, 0.0109,\n",
            "        0.0100, 0.0105, 0.0100, 0.0107, 0.0104], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [40]\n",
            "DEBUGGING: logits looks like: tensor([25.6648, 25.6609, 25.6575, 25.6667, 25.6557, 25.6779, 25.6639, 25.6702,\n",
            "        25.6810, 25.6519, 25.6610, 25.6624, 25.6674, 25.6569, 25.6600, 25.6525,\n",
            "        25.6618, 25.6648, 25.6622, 25.6527, 25.6578, 25.6605, 25.6610, 25.6680,\n",
            "        25.6545, 25.6475, 25.6715, 25.6549, 25.6494, 25.6637, 25.6485, 25.6669,\n",
            "        25.6631, 25.6708, 25.6651, 25.6477, 25.6617, 25.6661, 25.6636, 25.6588,\n",
            "        25.6644, 25.6615, 25.6642, 25.6574, 25.6544, 25.6532, 25.6679, 25.6559,\n",
            "        25.6709, 25.6686, 25.6622, 25.6670, 25.6646, 25.6639, 25.6693, 25.6499,\n",
            "        25.6624, 25.6562, 25.6667, 25.6591, 25.6631, 25.6698, 25.6644, 25.6660,\n",
            "        25.6651, 25.6645, 25.6566, 25.6641, 25.6519, 25.6541, 25.6571, 25.6818,\n",
            "        25.6519, 25.6591, 25.6598, 25.6624, 25.6731, 25.6646, 25.6679, 25.6631,\n",
            "        25.6618, 25.6585, 25.6725, 25.6725, 25.6699, 25.6613, 25.6641, 25.6637,\n",
            "        25.6619, 25.6706, 25.6497, 25.6613, 25.6504, 25.6668, 25.6585],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0094, 0.0095, 0.0095, 0.0095, 0.0086, 0.0094, 0.0095, 0.0093, 0.0082,\n",
            "        0.0093, 0.0095, 0.0089, 0.0093, 0.0091, 0.0092, 0.0087, 0.0094, 0.0096,\n",
            "        0.0091, 0.0097, 0.0090, 0.0097, 0.0093, 0.0092, 0.0092, 0.0092, 0.0091,\n",
            "        0.0094, 0.0090, 0.0096, 0.0089, 0.0092, 0.0097, 0.0099, 0.0091, 0.0091,\n",
            "        0.0093, 0.0092, 0.0093, 0.0093, 0.0089, 0.0091, 0.0088, 0.0093, 0.0092,\n",
            "        0.0092, 0.0086, 0.0094, 0.0092, 0.0090, 0.0088, 0.0093, 0.0095, 0.0098,\n",
            "        0.0095, 0.0088, 0.0095, 0.0093, 0.0101, 0.0097, 0.0089, 0.0096, 0.0096,\n",
            "        0.0093, 0.0090, 0.0092, 0.0093, 0.0096, 0.0094, 0.0093, 0.0092, 0.0093,\n",
            "        0.0091, 0.0089, 0.0095, 0.0091, 0.0093, 0.0093, 0.0088, 0.0093, 0.0092,\n",
            "        0.0087, 0.0089, 0.0089, 0.0097, 0.0093, 0.0090, 0.0094, 0.0094, 0.0093,\n",
            "        0.0092, 0.0093, 0.0094, 0.0093, 0.0094, 0.0091, 0.0097, 0.0096, 0.0091,\n",
            "        0.0091, 0.0091, 0.0094, 0.0098, 0.0104, 0.0093, 0.0094, 0.0089, 0.0085],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [39]\n",
            "DEBUGGING: logits looks like: tensor([25.6872, 25.6885, 25.6879, 25.6875, 25.6636, 25.6858, 25.6889, 25.6831,\n",
            "        25.6533, 25.6833, 25.6880, 25.6723, 25.6837, 25.6767, 25.6804, 25.6664,\n",
            "        25.6851, 25.6915, 25.6776, 25.6943, 25.6758, 25.6937, 25.6842, 25.6818,\n",
            "        25.6808, 25.6820, 25.6779, 25.6854, 25.6739, 25.6906, 25.6731, 25.6804,\n",
            "        25.6936, 25.7000, 25.6771, 25.6789, 25.6847, 25.6811, 25.6834, 25.6840,\n",
            "        25.6722, 25.6789, 25.6708, 25.6826, 25.6802, 25.6803, 25.6651, 25.6864,\n",
            "        25.6807, 25.6749, 25.6697, 25.6825, 25.6887, 25.6955, 25.6893, 25.6702,\n",
            "        25.6888, 25.6823, 25.7033, 25.6939, 25.6730, 25.6902, 25.6905, 25.6830,\n",
            "        25.6748, 25.6819, 25.6830, 25.6902, 25.6863, 25.6836, 25.6808, 25.6828,\n",
            "        25.6777, 25.6735, 25.6887, 25.6769, 25.6832, 25.6822, 25.6710, 25.6828,\n",
            "        25.6809, 25.6664, 25.6730, 25.6711, 25.6943, 25.6835, 25.6754, 25.6872,\n",
            "        25.6872, 25.6839, 25.6809, 25.6839, 25.6855, 25.6835, 25.6869, 25.6786,\n",
            "        25.6930, 25.6901, 25.6788, 25.6769, 25.6792, 25.6853, 25.6960, 25.7122,\n",
            "        25.6837, 25.6852, 25.6715, 25.6609], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.6072478073172078 and immediate abs rewards look like: [0.05008092628236227, 0.014605022179239313, 0.008086458492471138, 0.01740465791885981, 0.03299096620276032, 0.004245865249231429, 0.4798339109781864, 4.547473508864641e-13, 9.094947017729282e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 4.547473508864641e-13, 2.2737367544323206e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 4.547473508864641e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 4.547473508864641e-13, 2.2737367544323206e-13, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 2.2737367544323206e-13, 1.1368683772161603e-12, 1.8189894035458565e-12, 0.0, 2.2737367544323206e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 6.821210263296962e-13, 0.0, 2.2737367544323206e-13, 4.547473508864641e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 10.680823204553846 and immediate relative rewards look like: [0.13883417208257812, 0.08211603597888173, 0.0684797512197761, 0.19697019724218265, 0.4690119792694966, 0.07311885241322821, 9.652292215013018, 1.2126596023639042e-11, 2.7284841053191986e-11, 7.579122514773254e-12, 8.33703476625121e-12, 9.094947017727904e-12, 1.9705718538411953e-11, 1.0610771520684967e-11, 0.0, 2.4253192047278084e-11, 2.5769016550229062e-11, 1.3642420526593922e-11, 1.4400332778070272e-11, 1.5158245029548802e-11, 3.18323145620549e-11, 1.667406953250242e-11, 1.7431981783981122e-11, 1.818989403545994e-11, 1.894780628693888e-11, 3.941143707682987e-11, 2.0463630789889334e-11, 0.0, 4.395891058568487e-11, 0.0, 4.699055959160129e-11, 2.4253192047281764e-11, 1.250555214937871e-10, 2.0615213240195748e-10, 0.0, 2.728484105318371e-11, 0.0, 0.0, 0.0, 6.063298011819062e-11, 6.214880462113597e-11, 0.0, 6.518045362705492e-11, 3.3348139065009893e-11, 3.410605131648481e-11, 3.486396356796489e-11, 1.0686562745833528e-10, 0.0, 3.7137700322391754e-11, 7.579122514773254e-11]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0144, 0.0140, 0.0144, 0.0156, 0.0142, 0.0143, 0.0138, 0.0137, 0.0149,\n",
            "        0.0149, 0.0150, 0.0159, 0.0146, 0.0154, 0.0144, 0.0151, 0.0148, 0.0158,\n",
            "        0.0145, 0.0145, 0.0184, 0.0149, 0.0151, 0.0150, 0.0145, 0.0144, 0.0144,\n",
            "        0.0155, 0.0137, 0.0149, 0.0151, 0.0153, 0.0143, 0.0145, 0.0149, 0.0149,\n",
            "        0.0151, 0.0148, 0.0145, 0.0149, 0.0148, 0.0150, 0.0147, 0.0150, 0.0154,\n",
            "        0.0164, 0.0138, 0.0153, 0.0153, 0.0145, 0.0141, 0.0167, 0.0149, 0.0153,\n",
            "        0.0136, 0.0185, 0.0152, 0.0153, 0.0138, 0.0160, 0.0145, 0.0150, 0.0144,\n",
            "        0.0151, 0.0147, 0.0145, 0.0148], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [40]\n",
            "DEBUGGING: logits looks like: tensor([25.7231, 25.7156, 25.7226, 25.7432, 25.7195, 25.7216, 25.7134, 25.7115,\n",
            "        25.7316, 25.7327, 25.7342, 25.7484, 25.7266, 25.7396, 25.7235, 25.7359,\n",
            "        25.7296, 25.7464, 25.7255, 25.7256, 25.7843, 25.7326, 25.7352, 25.7328,\n",
            "        25.7257, 25.7241, 25.7231, 25.7415, 25.7113, 25.7321, 25.7356, 25.7383,\n",
            "        25.7218, 25.7250, 25.7314, 25.7325, 25.7360, 25.7310, 25.7252, 25.7324,\n",
            "        25.7298, 25.7330, 25.7287, 25.7328, 25.7405, 25.7565, 25.7124, 25.7389,\n",
            "        25.7380, 25.7246, 25.7181, 25.7607, 25.7326, 25.7384, 25.7089, 25.7859,\n",
            "        25.7370, 25.7381, 25.7134, 25.7490, 25.7255, 25.7344, 25.7240, 25.7347,\n",
            "        25.7293, 25.7251, 25.7303], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0119, 0.0091, 0.0100, 0.0122, 0.0145, 0.0145, 0.0138, 0.0093, 0.0147,\n",
            "        0.0123, 0.0120, 0.0114, 0.0167, 0.0141, 0.0155, 0.0102, 0.0135, 0.0129,\n",
            "        0.0113, 0.0122, 0.0150, 0.0147, 0.0134, 0.0120, 0.0146, 0.0131, 0.0177,\n",
            "        0.0166, 0.0103, 0.0101, 0.0127, 0.0149, 0.0156, 0.0159, 0.0153, 0.0117,\n",
            "        0.0116, 0.0112, 0.0094, 0.0095, 0.0163, 0.0170, 0.0131, 0.0112, 0.0108,\n",
            "        0.0104, 0.0115, 0.0101, 0.0084, 0.0142, 0.0131, 0.0139, 0.0157, 0.0104,\n",
            "        0.0135, 0.0149, 0.0151, 0.0161, 0.0115, 0.0127, 0.0125, 0.0118, 0.0105,\n",
            "        0.0118, 0.0146, 0.0121, 0.0143, 0.0144, 0.0179, 0.0142, 0.0116, 0.0128,\n",
            "        0.0141, 0.0109, 0.0114, 0.0147, 0.0132], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [5]\n",
            "DEBUGGING: logits looks like: tensor([25.7543, 25.6884, 25.7119, 25.7603, 25.8046, 25.8036, 25.7926, 25.6944,\n",
            "        25.8079, 25.7626, 25.7570, 25.7433, 25.8397, 25.7978, 25.8208, 25.7176,\n",
            "        25.7868, 25.7755, 25.7422, 25.7603, 25.8132, 25.8082, 25.7853, 25.7577,\n",
            "        25.8057, 25.7783, 25.8537, 25.8377, 25.7181, 25.7128, 25.7717, 25.8109,\n",
            "        25.8223, 25.8275, 25.8181, 25.7502, 25.7490, 25.7403, 25.6955, 25.6976,\n",
            "        25.8343, 25.8437, 25.7788, 25.7409, 25.7315, 25.7201, 25.7464, 25.7136,\n",
            "        25.6687, 25.7992, 25.7783, 25.7933, 25.8237, 25.7220, 25.7860, 25.8106,\n",
            "        25.8153, 25.8301, 25.7454, 25.7721, 25.7672, 25.7539, 25.7227, 25.7536,\n",
            "        25.8065, 25.7582, 25.8002, 25.8018, 25.8575, 25.7992, 25.7492, 25.7740,\n",
            "        25.7982, 25.7325, 25.7453, 25.8086, 25.7818], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0121, 0.0111, 0.0121, 0.0108, 0.0116, 0.0113, 0.0121, 0.0113, 0.0114,\n",
            "        0.0114, 0.0116, 0.0113, 0.0111, 0.0115, 0.0112, 0.0111, 0.0110, 0.0117,\n",
            "        0.0113, 0.0109, 0.0118, 0.0116, 0.0116, 0.0118, 0.0120, 0.0115, 0.0123,\n",
            "        0.0115, 0.0111, 0.0116, 0.0120, 0.0113, 0.0114, 0.0116, 0.0118, 0.0114,\n",
            "        0.0115, 0.0114, 0.0117, 0.0114, 0.0122, 0.0113, 0.0104, 0.0113, 0.0118,\n",
            "        0.0118, 0.0112, 0.0111, 0.0115, 0.0119, 0.0122, 0.0112, 0.0114, 0.0118,\n",
            "        0.0114, 0.0114, 0.0113, 0.0116, 0.0118, 0.0114, 0.0116, 0.0115, 0.0118,\n",
            "        0.0116, 0.0112, 0.0117, 0.0116, 0.0117, 0.0116, 0.0110, 0.0119, 0.0118,\n",
            "        0.0113, 0.0116, 0.0106, 0.0117, 0.0116, 0.0121, 0.0110, 0.0106, 0.0116,\n",
            "        0.0115, 0.0111, 0.0118, 0.0119, 0.0114, 0.0111],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [4]\n",
            "DEBUGGING: logits looks like: tensor([25.7374, 25.7164, 25.7371, 25.7084, 25.7266, 25.7206, 25.7373, 25.7204,\n",
            "        25.7231, 25.7217, 25.7261, 25.7201, 25.7168, 25.7253, 25.7174, 25.7163,\n",
            "        25.7146, 25.7294, 25.7215, 25.7123, 25.7321, 25.7272, 25.7280, 25.7319,\n",
            "        25.7350, 25.7246, 25.7409, 25.7246, 25.7164, 25.7275, 25.7356, 25.7207,\n",
            "        25.7233, 25.7279, 25.7322, 25.7222, 25.7243, 25.7225, 25.7295, 25.7229,\n",
            "        25.7388, 25.7201, 25.7003, 25.7202, 25.7304, 25.7320, 25.7173, 25.7163,\n",
            "        25.7246, 25.7343, 25.7395, 25.7189, 25.7221, 25.7310, 25.7219, 25.7223,\n",
            "        25.7196, 25.7268, 25.7303, 25.7226, 25.7278, 25.7251, 25.7320, 25.7278,\n",
            "        25.7182, 25.7294, 25.7274, 25.7291, 25.7272, 25.7138, 25.7341, 25.7313,\n",
            "        25.7216, 25.7279, 25.7039, 25.7292, 25.7280, 25.7377, 25.7129, 25.7036,\n",
            "        25.7281, 25.7244, 25.7153, 25.7308, 25.7339, 25.7221, 25.7158],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0056, 0.0071, 0.0128, 0.0079, 0.0061, 0.0079, 0.0118, 0.0128, 0.0101,\n",
            "        0.0104, 0.0104, 0.0082, 0.0128, 0.0094, 0.0071, 0.0090, 0.0140, 0.0114,\n",
            "        0.0176, 0.0075, 0.0085, 0.0117, 0.0082, 0.0060, 0.0155, 0.0126, 0.0059,\n",
            "        0.0109, 0.0170, 0.0108, 0.0085, 0.0168, 0.0111, 0.0107, 0.0120, 0.0106,\n",
            "        0.0156, 0.0144, 0.0087, 0.0096, 0.0115, 0.0071, 0.0071, 0.0152, 0.0103,\n",
            "        0.0092, 0.0121, 0.0078, 0.0073, 0.0113, 0.0155, 0.0096, 0.0125, 0.0158,\n",
            "        0.0078, 0.0112, 0.0094, 0.0177, 0.0099, 0.0103, 0.0070, 0.0146, 0.0059,\n",
            "        0.0056, 0.0075, 0.0096, 0.0100, 0.0115, 0.0103, 0.0056, 0.0115, 0.0125,\n",
            "        0.0095, 0.0092, 0.0120, 0.0148, 0.0090, 0.0101, 0.0115, 0.0108, 0.0062,\n",
            "        0.0110, 0.0106, 0.0072, 0.0127, 0.0130, 0.0162, 0.0089, 0.0090, 0.0138,\n",
            "        0.0098, 0.0093, 0.0086, 0.0111, 0.0104], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [23]\n",
            "DEBUGGING: logits looks like: tensor([25.6636, 25.7234, 25.8697, 25.7485, 25.6849, 25.7508, 25.8506, 25.8708,\n",
            "        25.8106, 25.8175, 25.8185, 25.7586, 25.8692, 25.7934, 25.7227, 25.7815,\n",
            "        25.8918, 25.8421, 25.9503, 25.7380, 25.7664, 25.8482, 25.7585, 25.6826,\n",
            "        25.9180, 25.8664, 25.6754, 25.8288, 25.9404, 25.8284, 25.7683, 25.9383,\n",
            "        25.8341, 25.8258, 25.8536, 25.8231, 25.9189, 25.8995, 25.7725, 25.7994,\n",
            "        25.8426, 25.7244, 25.7236, 25.9132, 25.8167, 25.7869, 25.8556, 25.7465,\n",
            "        25.7294, 25.8382, 25.9183, 25.7990, 25.8642, 25.9222, 25.7450, 25.8375,\n",
            "        25.7921, 25.9517, 25.8065, 25.8159, 25.7200, 25.9022, 25.6752, 25.6626,\n",
            "        25.7377, 25.7984, 25.8085, 25.8424, 25.8147, 25.6614, 25.8438, 25.8647,\n",
            "        25.7953, 25.7872, 25.8535, 25.9066, 25.7811, 25.8112, 25.8441, 25.8273,\n",
            "        25.6894, 25.8333, 25.8237, 25.7277, 25.8686, 25.8743, 25.9289, 25.7788,\n",
            "        25.7815, 25.8884, 25.8046, 25.7890, 25.7718, 25.8341, 25.8189],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0083, 0.0090, 0.0089, 0.0091, 0.0089, 0.0092, 0.0093, 0.0091, 0.0087,\n",
            "        0.0086, 0.0091, 0.0091, 0.0097, 0.0094, 0.0096, 0.0097, 0.0096, 0.0093,\n",
            "        0.0092, 0.0089, 0.0106, 0.0099, 0.0102, 0.0094, 0.0092, 0.0090, 0.0094,\n",
            "        0.0092, 0.0104, 0.0092, 0.0088, 0.0102, 0.0098, 0.0107, 0.0093, 0.0101,\n",
            "        0.0084, 0.0091, 0.0092, 0.0082, 0.0095, 0.0099, 0.0086, 0.0092, 0.0095,\n",
            "        0.0097, 0.0096, 0.0096, 0.0098, 0.0095, 0.0088, 0.0091, 0.0089, 0.0091,\n",
            "        0.0090, 0.0087, 0.0095, 0.0091, 0.0091, 0.0089, 0.0090, 0.0095, 0.0092,\n",
            "        0.0095, 0.0091, 0.0090, 0.0086, 0.0087, 0.0088, 0.0096, 0.0098, 0.0091,\n",
            "        0.0087, 0.0086, 0.0090, 0.0089, 0.0105, 0.0091, 0.0084, 0.0089, 0.0090,\n",
            "        0.0099, 0.0089, 0.0089, 0.0094, 0.0096, 0.0086, 0.0089, 0.0094, 0.0098,\n",
            "        0.0089, 0.0085, 0.0091, 0.0098, 0.0093, 0.0095, 0.0093, 0.0085, 0.0083,\n",
            "        0.0089, 0.0091, 0.0100, 0.0096, 0.0096, 0.0089, 0.0118, 0.0095, 0.0091],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [71]\n",
            "DEBUGGING: logits looks like: tensor([25.7821, 25.8024, 25.7996, 25.8029, 25.7977, 25.8055, 25.8091, 25.8043,\n",
            "        25.7931, 25.7894, 25.8042, 25.8051, 25.8209, 25.8119, 25.8166, 25.8201,\n",
            "        25.8171, 25.8092, 25.8059, 25.7972, 25.8415, 25.8246, 25.8322, 25.8115,\n",
            "        25.8054, 25.8009, 25.8123, 25.8057, 25.8367, 25.8056, 25.7942, 25.8333,\n",
            "        25.8221, 25.8435, 25.8101, 25.8304, 25.7846, 25.8048, 25.8071, 25.7792,\n",
            "        25.8157, 25.8239, 25.7887, 25.8069, 25.8152, 25.8189, 25.8160, 25.8169,\n",
            "        25.8224, 25.8143, 25.7956, 25.8031, 25.7987, 25.8049, 25.8015, 25.7914,\n",
            "        25.8157, 25.8051, 25.8049, 25.7988, 25.8009, 25.8145, 25.8060, 25.8138,\n",
            "        25.8036, 25.8000, 25.7889, 25.7938, 25.7947, 25.8180, 25.8215, 25.8027,\n",
            "        25.7932, 25.7899, 25.8012, 25.7978, 25.8407, 25.8029, 25.7848, 25.7973,\n",
            "        25.8019, 25.8244, 25.7991, 25.7983, 25.8110, 25.8159, 25.7888, 25.7977,\n",
            "        25.8115, 25.8233, 25.7981, 25.7865, 25.8036, 25.8219, 25.8101, 25.8136,\n",
            "        25.8085, 25.7860, 25.7798, 25.7979, 25.8045, 25.8267, 25.8185, 25.8161,\n",
            "        25.7987, 25.8689, 25.8152, 25.8027], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 1.1013110904273162 and immediate abs rewards look like: [0.01125381209749321, 0.035039800018239475, 0.003596434338760446, 0.034581632553454256, 0.00029723530724368175, 0.00039019444693622063, 0.00019646346754598198, 0.009252310146621312, 0.006985653126321267, 0.0010803151117215748, 0.0006026680580362154, 0.014525678302561573, 0.0012602625897670805, 0.0028936517837792053, 0.0021229268645583943, 4.678604591390467e-05, 0.0032971909076877637, 0.973888075249306, 4.547473508864641e-13, 9.094947017729282e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 0.0, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 61.29779010052713 and immediate relative rewards look like: [0.03628727260632164, 0.22679061818108054, 0.03531666542963417, 0.45331839893925946, 0.004926269958548407, 0.00776109381186761, 0.004559598724424352, 0.2454229869327897, 0.20910237488282965, 0.036013894373225, 0.02210787137038613, 0.5814080373641792, 0.05491323891472076, 0.13584087038827977, 0.10688186937141345, 0.002514337886657472, 0.18827261463323464, 58.946352084948394, 4.320099833421409e-11, 9.09494701773135e-11, 0.0, 5.002220859749968e-11, 5.2295945351943374e-11, 1.091393642127762e-10, 1.1368683772159018e-10, 5.911715561525378e-11, 0.0, 6.366462912410498e-11, 0.0, 6.821210263298513e-11, 7.048583938740194e-11, 0.0, 7.503331289624952e-11, 7.73070496506989e-11, 7.958078640514932e-11, 8.185452315956354e-11, 1.6825651982795347e-10, 0.0, 8.867573342288067e-11, 0.0, 0.0, 9.549694368615746e-11, 9.777068044061202e-11, 0.0, 0.0, 1.0459189070388675e-10, 1.0686562745834337e-10, 0.0, 0.0, 1.1368683772161603e-10]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0353, 0.0151, 0.0142, 0.0140, 0.0146, 0.0138, 0.0136, 0.0145, 0.0147,\n",
            "        0.0150, 0.0149, 0.0137, 0.0141, 0.0136, 0.0141, 0.0152, 0.0139, 0.0149,\n",
            "        0.0143, 0.0140, 0.0144, 0.0136, 0.0143, 0.0127, 0.0137, 0.0136, 0.0144,\n",
            "        0.0149, 0.0146, 0.0142, 0.0138, 0.0142, 0.0140, 0.0150, 0.0139, 0.0147,\n",
            "        0.0146, 0.0147, 0.0163, 0.0143, 0.0140, 0.0146, 0.0147, 0.0158, 0.0147,\n",
            "        0.0145, 0.0140, 0.0147, 0.0148, 0.0161, 0.0142, 0.0145, 0.0139, 0.0135,\n",
            "        0.0168, 0.0148, 0.0142, 0.0142, 0.0152, 0.0148, 0.0144, 0.0137, 0.0140,\n",
            "        0.0148, 0.0139, 0.0135, 0.0144, 0.0141], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [29]\n",
            "DEBUGGING: logits looks like: tensor([25.6080, 25.3957, 25.3805, 25.3776, 25.3871, 25.3732, 25.3693, 25.3854,\n",
            "        25.3900, 25.3940, 25.3931, 25.3722, 25.3789, 25.3693, 25.3782, 25.3973,\n",
            "        25.3753, 25.3928, 25.3820, 25.3770, 25.3845, 25.3688, 25.3826, 25.3530,\n",
            "        25.3716, 25.3691, 25.3845, 25.3924, 25.3878, 25.3797, 25.3725, 25.3813,\n",
            "        25.3771, 25.3948, 25.3746, 25.3895, 25.3881, 25.3898, 25.4156, 25.3819,\n",
            "        25.3772, 25.3872, 25.3885, 25.4072, 25.3885, 25.3862, 25.3762, 25.3886,\n",
            "        25.3905, 25.4113, 25.3801, 25.3849, 25.3755, 25.3672, 25.4220, 25.3908,\n",
            "        25.3801, 25.3802, 25.3983, 25.3916, 25.3847, 25.3709, 25.3766, 25.3906,\n",
            "        25.3749, 25.3680, 25.3841, 25.3789], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0293, 0.0134, 0.0125, 0.0122, 0.0126, 0.0125, 0.0119, 0.0123, 0.0123,\n",
            "        0.0133, 0.0123, 0.0125, 0.0124, 0.0129, 0.0123, 0.0128, 0.0120, 0.0131,\n",
            "        0.0124, 0.0124, 0.0124, 0.0123, 0.0125, 0.0123, 0.0124, 0.0126, 0.0125,\n",
            "        0.0128, 0.0123, 0.0120, 0.0122, 0.0124, 0.0121, 0.0119, 0.0115, 0.0120,\n",
            "        0.0127, 0.0129, 0.0118, 0.0127, 0.0124, 0.0124, 0.0124, 0.0125, 0.0131,\n",
            "        0.0121, 0.0126, 0.0128, 0.0119, 0.0126, 0.0124, 0.0127, 0.0126, 0.0130,\n",
            "        0.0134, 0.0126, 0.0121, 0.0122, 0.0123, 0.0123, 0.0129, 0.0124, 0.0126,\n",
            "        0.0122, 0.0121, 0.0124, 0.0122, 0.0126, 0.0126, 0.0122, 0.0123, 0.0119,\n",
            "        0.0131, 0.0127, 0.0122, 0.0121, 0.0121, 0.0123, 0.0127],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [38]\n",
            "DEBUGGING: logits looks like: tensor([25.5883, 25.3930, 25.3759, 25.3699, 25.3770, 25.3759, 25.3623, 25.3719,\n",
            "        25.3706, 25.3903, 25.3723, 25.3749, 25.3734, 25.3832, 25.3706, 25.3818,\n",
            "        25.3658, 25.3867, 25.3731, 25.3745, 25.3733, 25.3720, 25.3764, 25.3705,\n",
            "        25.3738, 25.3781, 25.3745, 25.3817, 25.3707, 25.3652, 25.3696, 25.3739,\n",
            "        25.3684, 25.3637, 25.3554, 25.3655, 25.3803, 25.3826, 25.3620, 25.3795,\n",
            "        25.3737, 25.3743, 25.3738, 25.3754, 25.3867, 25.3679, 25.3766, 25.3810,\n",
            "        25.3623, 25.3765, 25.3741, 25.3803, 25.3780, 25.3845, 25.3932, 25.3771,\n",
            "        25.3683, 25.3700, 25.3716, 25.3709, 25.3838, 25.3730, 25.3780, 25.3698,\n",
            "        25.3682, 25.3728, 25.3700, 25.3784, 25.3779, 25.3703, 25.3722, 25.3624,\n",
            "        25.3866, 25.3787, 25.3701, 25.3677, 25.3679, 25.3723, 25.3799],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0121, 0.0111, 0.0129, 0.0104, 0.0108, 0.0102, 0.0122, 0.0116, 0.0107,\n",
            "        0.0111, 0.0114, 0.0114, 0.0117, 0.0124, 0.0117, 0.0117, 0.0090, 0.0119,\n",
            "        0.0128, 0.0121, 0.0112, 0.0104, 0.0124, 0.0118, 0.0130, 0.0110, 0.0112,\n",
            "        0.0107, 0.0103, 0.0127, 0.0113, 0.0110, 0.0117, 0.0107, 0.0121, 0.0125,\n",
            "        0.0116, 0.0131, 0.0126, 0.0116, 0.0109, 0.0115, 0.0114, 0.0104, 0.0127,\n",
            "        0.0120, 0.0123, 0.0128, 0.0109, 0.0091, 0.0098, 0.0112, 0.0090, 0.0118,\n",
            "        0.0114, 0.0111, 0.0109, 0.0115, 0.0126, 0.0107, 0.0128, 0.0118, 0.0123,\n",
            "        0.0120, 0.0109, 0.0107, 0.0117, 0.0125, 0.0109, 0.0120, 0.0113, 0.0116,\n",
            "        0.0116, 0.0113, 0.0092, 0.0117, 0.0143, 0.0124, 0.0105, 0.0133, 0.0117,\n",
            "        0.0121, 0.0116, 0.0104, 0.0110, 0.0112, 0.0113],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [38]\n",
            "DEBUGGING: logits looks like: tensor([25.5526, 25.5321, 25.5690, 25.5149, 25.5253, 25.5101, 25.5554, 25.5426,\n",
            "        25.5240, 25.5311, 25.5385, 25.5392, 25.5444, 25.5604, 25.5453, 25.5460,\n",
            "        25.4805, 25.5501, 25.5671, 25.5536, 25.5346, 25.5167, 25.5591, 25.5476,\n",
            "        25.5711, 25.5299, 25.5333, 25.5236, 25.5133, 25.5666, 25.5355, 25.5292,\n",
            "        25.5452, 25.5231, 25.5537, 25.5620, 25.5424, 25.5730, 25.5629, 25.5437,\n",
            "        25.5276, 25.5415, 25.5392, 25.5146, 25.5656, 25.5519, 25.5575, 25.5674,\n",
            "        25.5279, 25.4832, 25.5004, 25.5354, 25.4790, 25.5471, 25.5395, 25.5320,\n",
            "        25.5273, 25.5399, 25.5645, 25.5224, 25.5672, 25.5474, 25.5583, 25.5513,\n",
            "        25.5278, 25.5224, 25.5457, 25.5612, 25.5284, 25.5514, 25.5360, 25.5423,\n",
            "        25.5427, 25.5360, 25.4862, 25.5444, 25.5948, 25.5598, 25.5193, 25.5771,\n",
            "        25.5444, 25.5544, 25.5421, 25.5165, 25.5303, 25.5341, 25.5370],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0102, 0.0105, 0.0103, 0.0101, 0.0102, 0.0104, 0.0104, 0.0103, 0.0103,\n",
            "        0.0102, 0.0102, 0.0103, 0.0104, 0.0102, 0.0105, 0.0102, 0.0103, 0.0103,\n",
            "        0.0102, 0.0105, 0.0106, 0.0102, 0.0102, 0.0103, 0.0102, 0.0103, 0.0103,\n",
            "        0.0104, 0.0103, 0.0104, 0.0105, 0.0105, 0.0104, 0.0103, 0.0102, 0.0104,\n",
            "        0.0103, 0.0103, 0.0101, 0.0106, 0.0103, 0.0106, 0.0103, 0.0102, 0.0104,\n",
            "        0.0102, 0.0103, 0.0104, 0.0104, 0.0101, 0.0104, 0.0104, 0.0102, 0.0106,\n",
            "        0.0103, 0.0102, 0.0103, 0.0104, 0.0104, 0.0102, 0.0103, 0.0104, 0.0103,\n",
            "        0.0103, 0.0103, 0.0102, 0.0103, 0.0103, 0.0102, 0.0103, 0.0102, 0.0104,\n",
            "        0.0102, 0.0103, 0.0103, 0.0104, 0.0103, 0.0103, 0.0101, 0.0103, 0.0104,\n",
            "        0.0102, 0.0103, 0.0103, 0.0100, 0.0102, 0.0104, 0.0103, 0.0105, 0.0103,\n",
            "        0.0103, 0.0102, 0.0104, 0.0102, 0.0103, 0.0103, 0.0103],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [80]\n",
            "DEBUGGING: logits looks like: tensor([25.5434, 25.5510, 25.5449, 25.5422, 25.5429, 25.5489, 25.5477, 25.5471,\n",
            "        25.5458, 25.5444, 25.5443, 25.5453, 25.5480, 25.5440, 25.5519, 25.5434,\n",
            "        25.5450, 25.5458, 25.5429, 25.5504, 25.5535, 25.5429, 25.5439, 25.5449,\n",
            "        25.5427, 25.5453, 25.5470, 25.5490, 25.5451, 25.5472, 25.5503, 25.5499,\n",
            "        25.5490, 25.5456, 25.5442, 25.5485, 25.5458, 25.5456, 25.5411, 25.5534,\n",
            "        25.5454, 25.5541, 25.5469, 25.5439, 25.5487, 25.5436, 25.5471, 25.5476,\n",
            "        25.5478, 25.5402, 25.5480, 25.5482, 25.5434, 25.5539, 25.5463, 25.5427,\n",
            "        25.5456, 25.5488, 25.5485, 25.5446, 25.5453, 25.5480, 25.5461, 25.5448,\n",
            "        25.5465, 25.5435, 25.5468, 25.5464, 25.5445, 25.5453, 25.5447, 25.5473,\n",
            "        25.5426, 25.5461, 25.5471, 25.5478, 25.5469, 25.5467, 25.5420, 25.5458,\n",
            "        25.5488, 25.5434, 25.5459, 25.5456, 25.5390, 25.5428, 25.5493, 25.5454,\n",
            "        25.5510, 25.5448, 25.5461, 25.5446, 25.5474, 25.5429, 25.5453, 25.5469,\n",
            "        25.5452], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0091, 0.0093, 0.0093, 0.0091, 0.0092, 0.0088, 0.0095, 0.0095, 0.0082,\n",
            "        0.0091, 0.0092, 0.0092, 0.0091, 0.0083, 0.0096, 0.0094, 0.0094, 0.0085,\n",
            "        0.0094, 0.0091, 0.0093, 0.0094, 0.0094, 0.0090, 0.0091, 0.0093, 0.0093,\n",
            "        0.0100, 0.0090, 0.0093, 0.0095, 0.0097, 0.0087, 0.0092, 0.0094, 0.0093,\n",
            "        0.0092, 0.0090, 0.0090, 0.0094, 0.0084, 0.0097, 0.0089, 0.0091, 0.0093,\n",
            "        0.0096, 0.0092, 0.0093, 0.0089, 0.0091, 0.0091, 0.0093, 0.0093, 0.0096,\n",
            "        0.0103, 0.0090, 0.0096, 0.0092, 0.0091, 0.0093, 0.0096, 0.0092, 0.0092,\n",
            "        0.0094, 0.0092, 0.0093, 0.0093, 0.0091, 0.0094, 0.0093, 0.0092, 0.0095,\n",
            "        0.0095, 0.0091, 0.0094, 0.0092, 0.0093, 0.0097, 0.0092, 0.0093, 0.0094,\n",
            "        0.0086, 0.0094, 0.0096, 0.0095, 0.0095, 0.0090, 0.0091, 0.0093, 0.0090,\n",
            "        0.0095, 0.0091, 0.0093, 0.0088, 0.0104, 0.0095, 0.0091, 0.0097, 0.0097,\n",
            "        0.0092, 0.0093, 0.0093, 0.0086, 0.0097, 0.0101, 0.0089, 0.0096, 0.0077],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [96]\n",
            "DEBUGGING: logits looks like: tensor([25.5316, 25.5375, 25.5372, 25.5320, 25.5330, 25.5232, 25.5425, 25.5405,\n",
            "        25.5047, 25.5310, 25.5341, 25.5342, 25.5305, 25.5085, 25.5432, 25.5392,\n",
            "        25.5387, 25.5143, 25.5402, 25.5316, 25.5356, 25.5404, 25.5398, 25.5280,\n",
            "        25.5310, 25.5362, 25.5378, 25.5537, 25.5281, 25.5374, 25.5414, 25.5478,\n",
            "        25.5210, 25.5340, 25.5399, 25.5369, 25.5327, 25.5283, 25.5294, 25.5404,\n",
            "        25.5116, 25.5475, 25.5254, 25.5301, 25.5366, 25.5442, 25.5345, 25.5364,\n",
            "        25.5246, 25.5315, 25.5300, 25.5374, 25.5375, 25.5447, 25.5626, 25.5286,\n",
            "        25.5431, 25.5341, 25.5298, 25.5373, 25.5432, 25.5338, 25.5351, 25.5389,\n",
            "        25.5349, 25.5376, 25.5354, 25.5315, 25.5400, 25.5378, 25.5344, 25.5415,\n",
            "        25.5421, 25.5317, 25.5385, 25.5340, 25.5360, 25.5477, 25.5332, 25.5371,\n",
            "        25.5382, 25.5162, 25.5403, 25.5445, 25.5415, 25.5414, 25.5282, 25.5298,\n",
            "        25.5367, 25.5280, 25.5423, 25.5320, 25.5360, 25.5226, 25.5639, 25.5426,\n",
            "        25.5315, 25.5469, 25.5479, 25.5350, 25.5365, 25.5367, 25.5162, 25.5466,\n",
            "        25.5569, 25.5243, 25.5449, 25.4896], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.7082653196293904 and immediate abs rewards look like: [0.02564727795197541, 0.0035795935341411678, 0.018703506914789614, 0.0002602765662231832, 0.006128183797500242, 0.004655124701002933, 0.009048959329447825, 0.010940207369458221, 0.009252126690626028, 0.0007029556300039985, 0.009038022158165404, 0.0004817693293261982, 0.0019551718796719797, 0.0027837775023726863, 0.002143600308954774, 0.00940616070147371, 0.003442960475695145, 0.00011483314574434189, 0.0020408808341016993, 0.0004790937509824289, 0.00013247856668385793, 0.0008303098029500688, 0.0005182345189496118, 2.788022993627237e-05, 0.5859519339296639, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 1.8189894035458565e-12, 1.3642420526593924e-12, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0]\n",
            "DEBUGGING: the total relative reward of the trajectory = 43.29065176495943 and immediate relative rewards look like: [0.06916246746498896, 0.0194404822527832, 0.15251409175359987, 0.002844292343765765, 0.08371665005475673, 0.07643994883986469, 0.17357538525750452, 0.24042810729309524, 0.22943567625084305, 0.01941839526570367, 0.2746855739363607, 0.016013121912391605, 0.07041121974328808, 0.10802180199357149, 0.08919061439715302, 0.41770990398083085, 0.16287658076423775, 0.005757497370311391, 0.10801376910505196, 0.02670578439008131, 0.00775492758452, 0.05092038932458856, 0.03323407338875533, 0.0018659489265065797, 40.850515060274994, 3.9411437076826886e-11, 0.0, 4.244308608273022e-11, 1.758356423427128e-10, 1.3642420526598057e-10, 9.398111918318834e-11, 4.850638409456353e-11, 5.0022208597511047e-11, 0.0, 5.305385760342886e-11, 5.456968210637569e-11, 0.0, 5.760133111229419e-11, 0.0, 6.063298011819521e-11, 6.214880462114068e-11, 0.0, 0.0, 0.0, 0.0, 6.972792713592449e-11, 7.124375163889018e-11, 0.0, 7.427540064478913e-11, 0.0]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 3\n",
            "DEBUGGING: the action_prob is: tensor([0.0187, 0.0134, 0.0128, 0.0133, 0.0156, 0.0135, 0.0158, 0.0151, 0.0150,\n",
            "        0.0144, 0.0142, 0.0156, 0.0097, 0.0134, 0.0140, 0.0129, 0.0115, 0.0152,\n",
            "        0.0128, 0.0148, 0.0135, 0.0143, 0.0140, 0.0151, 0.0150, 0.0135, 0.0133,\n",
            "        0.0137, 0.0146, 0.0144, 0.0143, 0.0135, 0.0146, 0.0129, 0.0150, 0.0150,\n",
            "        0.0148, 0.0147, 0.0142, 0.0146, 0.0149, 0.0144, 0.0116, 0.0132, 0.0135,\n",
            "        0.0154, 0.0138, 0.0164, 0.0151, 0.0125, 0.0156, 0.0139, 0.0144, 0.0172,\n",
            "        0.0151, 0.0142, 0.0142, 0.0137, 0.0146, 0.0150, 0.0143, 0.0136, 0.0124,\n",
            "        0.0097, 0.0149, 0.0148, 0.0160, 0.0138, 0.0140, 0.0212],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [63]\n",
            "DEBUGGING: logits looks like: tensor([25.6070, 25.5233, 25.5126, 25.5208, 25.5613, 25.5243, 25.5644, 25.5523,\n",
            "        25.5515, 25.5405, 25.5385, 25.5608, 25.4428, 25.5232, 25.5335, 25.5143,\n",
            "        25.4842, 25.5545, 25.5108, 25.5476, 25.5254, 25.5389, 25.5342, 25.5533,\n",
            "        25.5509, 25.5242, 25.5215, 25.5293, 25.5452, 25.5406, 25.5389, 25.5255,\n",
            "        25.5452, 25.5134, 25.5506, 25.5506, 25.5487, 25.5456, 25.5381, 25.5452,\n",
            "        25.5493, 25.5412, 25.4860, 25.5199, 25.5254, 25.5581, 25.5301, 25.5732,\n",
            "        25.5524, 25.5063, 25.5615, 25.5325, 25.5411, 25.5851, 25.5524, 25.5378,\n",
            "        25.5384, 25.5286, 25.5453, 25.5509, 25.5385, 25.5262, 25.5036, 25.4428,\n",
            "        25.5496, 25.5481, 25.5676, 25.5302, 25.5346, 25.6376],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0164, 0.0126, 0.0126, 0.0110, 0.0119, 0.0135, 0.0119, 0.0129, 0.0129,\n",
            "        0.0135, 0.0129, 0.0124, 0.0124, 0.0127, 0.0125, 0.0118, 0.0125, 0.0129,\n",
            "        0.0121, 0.0131, 0.0131, 0.0115, 0.0123, 0.0135, 0.0114, 0.0129, 0.0119,\n",
            "        0.0127, 0.0130, 0.0137, 0.0121, 0.0142, 0.0121, 0.0133, 0.0114, 0.0124,\n",
            "        0.0122, 0.0123, 0.0128, 0.0118, 0.0134, 0.0127, 0.0127, 0.0120, 0.0127,\n",
            "        0.0124, 0.0135, 0.0128, 0.0124, 0.0125, 0.0128, 0.0131, 0.0124, 0.0125,\n",
            "        0.0124, 0.0134, 0.0126, 0.0117, 0.0130, 0.0127, 0.0121, 0.0124, 0.0134,\n",
            "        0.0130, 0.0128, 0.0126, 0.0123, 0.0139, 0.0113, 0.0126, 0.0129, 0.0118,\n",
            "        0.0134, 0.0133, 0.0130, 0.0127, 0.0129, 0.0119, 0.0123],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [25]\n",
            "DEBUGGING: logits looks like: tensor([25.5698, 25.5036, 25.5044, 25.4700, 25.4895, 25.5209, 25.4889, 25.5103,\n",
            "        25.5098, 25.5212, 25.5100, 25.4990, 25.4995, 25.5053, 25.5028, 25.4885,\n",
            "        25.5026, 25.5099, 25.4947, 25.5131, 25.5145, 25.4805, 25.4984, 25.5221,\n",
            "        25.4800, 25.5107, 25.4900, 25.5059, 25.5126, 25.5246, 25.4934, 25.5346,\n",
            "        25.4938, 25.5165, 25.4798, 25.5003, 25.4962, 25.4987, 25.5075, 25.4880,\n",
            "        25.5193, 25.5054, 25.5054, 25.4923, 25.5054, 25.5002, 25.5220, 25.5079,\n",
            "        25.4995, 25.5018, 25.5086, 25.5141, 25.5004, 25.5020, 25.5005, 25.5197,\n",
            "        25.5045, 25.4860, 25.5118, 25.5056, 25.4935, 25.5002, 25.5194, 25.5119,\n",
            "        25.5082, 25.5033, 25.4985, 25.5278, 25.4776, 25.5044, 25.5096, 25.4879,\n",
            "        25.5200, 25.5178, 25.5124, 25.5050, 25.5094, 25.4903, 25.4984],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0155, 0.0114, 0.0113, 0.0114, 0.0116, 0.0115, 0.0114, 0.0115, 0.0115,\n",
            "        0.0115, 0.0114, 0.0114, 0.0116, 0.0114, 0.0115, 0.0113, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0115, 0.0116, 0.0113, 0.0116, 0.0113, 0.0116, 0.0115,\n",
            "        0.0115, 0.0115, 0.0116, 0.0112, 0.0115, 0.0114, 0.0112, 0.0113, 0.0115,\n",
            "        0.0117, 0.0112, 0.0113, 0.0112, 0.0114, 0.0114, 0.0116, 0.0114, 0.0115,\n",
            "        0.0115, 0.0113, 0.0114, 0.0114, 0.0117, 0.0115, 0.0113, 0.0114, 0.0115,\n",
            "        0.0115, 0.0113, 0.0114, 0.0116, 0.0114, 0.0113, 0.0114, 0.0116, 0.0116,\n",
            "        0.0114, 0.0115, 0.0115, 0.0110, 0.0116, 0.0115, 0.0115, 0.0115, 0.0113,\n",
            "        0.0115, 0.0113, 0.0117, 0.0114, 0.0114, 0.0116, 0.0111, 0.0117, 0.0116,\n",
            "        0.0116, 0.0115, 0.0115, 0.0115, 0.0113, 0.0114],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [6]\n",
            "DEBUGGING: logits looks like: tensor([25.5595, 25.4822, 25.4804, 25.4815, 25.4864, 25.4848, 25.4817, 25.4853,\n",
            "        25.4852, 25.4847, 25.4825, 25.4820, 25.4877, 25.4833, 25.4849, 25.4801,\n",
            "        25.4830, 25.4833, 25.4832, 25.4813, 25.4854, 25.4872, 25.4809, 25.4862,\n",
            "        25.4799, 25.4865, 25.4847, 25.4847, 25.4852, 25.4873, 25.4786, 25.4843,\n",
            "        25.4832, 25.4789, 25.4805, 25.4848, 25.4891, 25.4791, 25.4804, 25.4774,\n",
            "        25.4830, 25.4822, 25.4859, 25.4827, 25.4841, 25.4846, 25.4798, 25.4818,\n",
            "        25.4816, 25.4899, 25.4847, 25.4810, 25.4822, 25.4841, 25.4851, 25.4812,\n",
            "        25.4814, 25.4863, 25.4823, 25.4812, 25.4832, 25.4868, 25.4860, 25.4826,\n",
            "        25.4837, 25.4840, 25.4745, 25.4867, 25.4838, 25.4837, 25.4852, 25.4811,\n",
            "        25.4842, 25.4812, 25.4893, 25.4829, 25.4829, 25.4857, 25.4766, 25.4889,\n",
            "        25.4864, 25.4857, 25.4849, 25.4840, 25.4837, 25.4804, 25.4819],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0140, 0.0101, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0101,\n",
            "        0.0103, 0.0102, 0.0102, 0.0102, 0.0101, 0.0102, 0.0101, 0.0101, 0.0103,\n",
            "        0.0102, 0.0101, 0.0103, 0.0100, 0.0103, 0.0102, 0.0101, 0.0102, 0.0102,\n",
            "        0.0101, 0.0101, 0.0101, 0.0102, 0.0102, 0.0101, 0.0102, 0.0102, 0.0100,\n",
            "        0.0102, 0.0102, 0.0102, 0.0100, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0103, 0.0102, 0.0101, 0.0101, 0.0101, 0.0102, 0.0102,\n",
            "        0.0101, 0.0101, 0.0102, 0.0102, 0.0100, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0102, 0.0101, 0.0101, 0.0101, 0.0102, 0.0101, 0.0101, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0101, 0.0101, 0.0102, 0.0102, 0.0101, 0.0102,\n",
            "        0.0102, 0.0102, 0.0103, 0.0101, 0.0101, 0.0102, 0.0101, 0.0101, 0.0101,\n",
            "        0.0104, 0.0101, 0.0103, 0.0102, 0.0102, 0.0101, 0.0102, 0.0102],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [81]\n",
            "DEBUGGING: logits looks like: tensor([25.5487, 25.4674, 25.4698, 25.4707, 25.4691, 25.4697, 25.4707, 25.4707,\n",
            "        25.4684, 25.4733, 25.4698, 25.4693, 25.4689, 25.4687, 25.4690, 25.4663,\n",
            "        25.4682, 25.4719, 25.4695, 25.4686, 25.4712, 25.4650, 25.4720, 25.4704,\n",
            "        25.4668, 25.4700, 25.4696, 25.4678, 25.4680, 25.4665, 25.4692, 25.4692,\n",
            "        25.4684, 25.4700, 25.4705, 25.4652, 25.4711, 25.4688, 25.4687, 25.4650,\n",
            "        25.4693, 25.4709, 25.4693, 25.4691, 25.4699, 25.4710, 25.4711, 25.4715,\n",
            "        25.4703, 25.4673, 25.4679, 25.4684, 25.4688, 25.4688, 25.4684, 25.4681,\n",
            "        25.4701, 25.4698, 25.4653, 25.4671, 25.4686, 25.4678, 25.4673, 25.4703,\n",
            "        25.4677, 25.4667, 25.4687, 25.4702, 25.4682, 25.4671, 25.4696, 25.4696,\n",
            "        25.4702, 25.4689, 25.4692, 25.4666, 25.4676, 25.4694, 25.4698, 25.4682,\n",
            "        25.4691, 25.4688, 25.4694, 25.4716, 25.4681, 25.4686, 25.4691, 25.4682,\n",
            "        25.4681, 25.4681, 25.4737, 25.4687, 25.4718, 25.4710, 25.4696, 25.4684,\n",
            "        25.4691, 25.4693], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0123, 0.0098, 0.0099, 0.0092, 0.0101, 0.0091, 0.0091, 0.0091, 0.0089,\n",
            "        0.0089, 0.0091, 0.0095, 0.0089, 0.0100, 0.0090, 0.0090, 0.0101, 0.0090,\n",
            "        0.0089, 0.0089, 0.0089, 0.0092, 0.0092, 0.0092, 0.0089, 0.0101, 0.0133,\n",
            "        0.0091, 0.0089, 0.0091, 0.0097, 0.0088, 0.0092, 0.0098, 0.0087, 0.0090,\n",
            "        0.0089, 0.0106, 0.0089, 0.0091, 0.0092, 0.0092, 0.0088, 0.0090, 0.0090,\n",
            "        0.0091, 0.0092, 0.0091, 0.0087, 0.0089, 0.0091, 0.0091, 0.0094, 0.0088,\n",
            "        0.0092, 0.0093, 0.0092, 0.0089, 0.0089, 0.0094, 0.0090, 0.0092, 0.0091,\n",
            "        0.0090, 0.0090, 0.0092, 0.0091, 0.0092, 0.0073, 0.0091, 0.0084, 0.0093,\n",
            "        0.0089, 0.0092, 0.0092, 0.0089, 0.0089, 0.0093, 0.0091, 0.0092, 0.0089,\n",
            "        0.0089, 0.0091, 0.0095, 0.0089, 0.0097, 0.0097, 0.0090, 0.0096, 0.0090,\n",
            "        0.0090, 0.0091, 0.0092, 0.0095, 0.0084, 0.0091, 0.0116, 0.0090, 0.0098,\n",
            "        0.0103, 0.0091, 0.0089, 0.0094, 0.0095, 0.0084, 0.0113, 0.0089, 0.0090],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [83]\n",
            "DEBUGGING: logits looks like: tensor([25.5398, 25.4837, 25.4860, 25.4664, 25.4908, 25.4638, 25.4647, 25.4630,\n",
            "        25.4585, 25.4595, 25.4646, 25.4759, 25.4583, 25.4890, 25.4605, 25.4618,\n",
            "        25.4895, 25.4601, 25.4593, 25.4589, 25.4583, 25.4674, 25.4657, 25.4678,\n",
            "        25.4593, 25.4913, 25.5598, 25.4643, 25.4581, 25.4646, 25.4796, 25.4555,\n",
            "        25.4662, 25.4837, 25.4532, 25.4612, 25.4584, 25.5024, 25.4590, 25.4646,\n",
            "        25.4660, 25.4683, 25.4551, 25.4604, 25.4615, 25.4647, 25.4670, 25.4632,\n",
            "        25.4520, 25.4584, 25.4637, 25.4634, 25.4711, 25.4566, 25.4663, 25.4685,\n",
            "        25.4673, 25.4593, 25.4574, 25.4722, 25.4612, 25.4663, 25.4644, 25.4628,\n",
            "        25.4609, 25.4659, 25.4635, 25.4661, 25.4101, 25.4643, 25.4441, 25.4699,\n",
            "        25.4597, 25.4670, 25.4681, 25.4593, 25.4599, 25.4706, 25.4635, 25.4665,\n",
            "        25.4595, 25.4598, 25.4652, 25.4754, 25.4574, 25.4796, 25.4804, 25.4611,\n",
            "        25.4780, 25.4605, 25.4613, 25.4630, 25.4677, 25.4737, 25.4430, 25.4653,\n",
            "        25.5255, 25.4624, 25.4821, 25.4948, 25.4634, 25.4590, 25.4715, 25.4739,\n",
            "        25.4433, 25.5184, 25.4586, 25.4623], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.0756266977755331 and immediate abs rewards look like: [0.0016034652480811928, 0.006290657292083779, 0.007538296900293062, 9.625970506021986e-06, 7.0841683736944105e-06, 0.0008420726685471891, 0.004782425190569484, 0.006862231088234694, 0.00019730294297914952, 0.004673017261666246, 0.0003967985617236991, 0.0015140207419790386, 0.0036519841396511765, 0.0008261990492428595, 0.004225962333748612, 0.0003254409020883031, 0.00800911922033265, 0.005010641953049344, 0.0004647404448405723, 0.0019015927091459162, 0.0030900304882379714, 0.0017465456448917394, 2.2694190192851238e-05, 0.0011524578908392868, 0.0032461384002999694, 0.00034922816303151194, 0.0007888389750405622, 0.00010669683751984849, 8.059733454501838e-05, 9.672219403000781e-05, 0.0002593300023363554, 0.0008419242662967008, 1.4159407328406814e-05, 2.235149804619141e-05, 0.0034298400541956653, 3.0684491321153473e-06, 5.8361274568596855e-06, 2.1765716610389063e-05, 9.61031582846772e-06, 6.659005521214567e-06, 3.5708471841644496e-06, 6.31986904409132e-06, 4.891035587206716e-06, 2.105583007505629e-06, 1.0149773515877314e-05, 2.0360334929137025e-05, 3.2296707104251254e-06, 1.7750409369909903e-05, 0.0011055747877435351, 2.5571715923433658e-05]\n",
            "DEBUGGING: the total relative reward of the trajectory = 4.029555099854212 and immediate relative rewards look like: [0.005925929112914215, 0.04652435467726906, 0.08382243443538233, 0.0001431150962900178, 0.00013165621400860894, 0.01877951830604848, 0.12447031933851113, 0.20447858515275255, 0.006631015579033519, 0.17451524043370598, 0.016328914735175355, 0.06797858046678035, 0.17773679630640177, 0.043362270631297016, 0.23771178053129685, 0.019557562819652125, 0.5114573040805835, 0.33982095541858803, 0.03333258754358327, 0.14359147423020113, 0.24517440567781226, 0.14534598075622343, 0.001975740713829827, 0.10469536646338504, 0.3073177809434393, 0.0344268628439334, 0.08076523764446462, 0.011332138132304112, 0.008866220656449516, 0.011007293085286052, 0.030497450071423522, 0.1022150402835366, 0.001773329992413649, 0.0028841538989967893, 0.4555945015477294, 0.00041978187158513836, 0.0008205957231741569, 0.0031431151159102347, 0.001424326583029465, 0.0010122278098476554, 0.0005563718123977199, 0.0010087137865566749, 0.0007992468040501087, 0.00035207685877928504, 0.0017357279223054415, 0.0035592394750488397, 0.0005768646456445317, 0.0032379337911266066, 0.20587588976797205, 0.004861090066081861]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 200 number of matrices\n",
            "DEBUGGING: ACT_MAT has 200 number of matrices\n",
            "DEBUGGING: VAL looks like: [[10.085873891937238, 10.04751486854006, 10.066059426829474, 10.098565328898685, 10.001611244097479, 9.628888146290892, 9.652292216038044, 1.0353795455735097e-09, 1.0335888379291624e-09, 1.0164686837131015e-09, 1.0190803648467963e-09, 1.0209528586672172e-09, 1.022078698635848e-09, 1.01249795969438e-09, 1.01200726078151e-09, 1.0222295563449596e-09, 1.0080569336340217e-09, 9.922100172563562e-10, 9.884521179088508e-10, 9.838906920512935e-10, 9.785176232542875e-10, 9.562477865578107e-10, 9.490643606316246e-10, 9.410428069168114e-10, 9.321746594761126e-10, 9.224513668577511e-10, 8.919595250312337e-10, 8.80298883072065e-10, 8.891907909818838e-10, 8.537695761577767e-10, 8.623935112704815e-10, 8.23639345130182e-10, 8.074607606897982e-10, 6.892982214101123e-10, 4.880263525334897e-10, 4.929559116499896e-10, 4.703748187846524e-10, 4.75126079580457e-10, 4.799253329095525e-10, 4.847730635450025e-10, 4.2842432669374943e-10, 3.699752748208217e-10, 3.737123988089108e-10, 3.1164842947662214e-10, 2.8111140445617396e-10, 2.495003567067567e-10, 2.1680443751393115e-10, 1.110493030864605e-10, 1.1217101321864697e-10, 7.579122514773254e-11], [51.85845797415085, 52.3456269712571, 52.64528924553133, 53.141386444547166, 53.2202707531393, 53.7528732153341, 54.28799204194165, 54.83174994264366, 55.13770399566755, 55.48343598059063, 56.007497056785255, 56.55089816708573, 56.53483851487025, 57.050429571672254, 57.48948353665048, 57.962223906342494, 58.54516118025842, 58.94635208649009, 1.5572657918557561e-09, 1.529358377294487e-09, 1.452938290017347e-09, 1.4676144343609565e-09, 1.431911339155007e-09, 1.3935509028313773e-09, 1.2973853925440415e-09, 1.195655105881264e-09, 1.1480181315818285e-09, 1.1596142743250794e-09, 1.107019843637348e-09, 1.1182018622599475e-09, 1.0605957167949115e-09, 1.0001109872803127e-09, 1.0102131184649623e-09, 9.44626066231023e-10, 8.760798147276001e-10, 8.045444730529806e-10, 7.299898483771889e-10, 5.674074025749853e-10, 5.731387904797832e-10, 4.893566232898006e-10, 4.94299619484647e-10, 4.99292544933987e-10, 4.0787434469477726e-10, 3.132360244991568e-10, 3.16400024746623e-10, 3.1959598459254853e-10, 2.1717585241278968e-10, 1.1142446965095587e-10, 1.1254996934439987e-10, 1.1368683772161603e-10], [34.29421230455027, 34.57075741119725, 34.90032013024694, 35.0987939782761, 35.450454228214475, 35.72397735167649, 36.00761353821881, 36.19599813430435, 36.31875760304167, 36.453860532111946, 36.8024668048952, 36.89674871814024, 37.25326827901802, 37.558441474014884, 37.828706739415466, 38.12072335860436, 38.0838519743672, 38.3040155490939, 38.68510914315514, 38.966763004091, 39.33339113101103, 39.722864851945964, 40.07267117436503, 40.44387585957199, 40.850515061258065, 9.930041515394452e-10, 9.632249641036549e-10, 9.72954509195611e-10, 9.399105283968491e-10, 7.717928141960974e-10, 6.417864736667846e-10, 5.533387419026226e-10, 5.09931674553595e-10, 4.645550161172565e-10, 4.692474910275318e-10, 4.203976095192959e-10, 3.6952315900294973e-10, 3.732557161645957e-10, 3.1884281318414296e-10, 3.220634476607505e-10, 2.640711793359144e-10, 2.039619946613876e-10, 2.0602221682968445e-10, 2.081032493229136e-10, 2.1020530234637736e-10, 2.1232858822866402e-10, 1.4404107181084801e-10, 7.353264663834124e-11, 7.427540064478913e-11, 0.0], [3.312180311070503, 3.339650890866251, 3.3263904405949316, 3.275321218342979, 3.3082607103501913, 3.3415444991274574, 3.3563282634559686, 3.2645029738560174, 3.090933725962894, 3.1154572832160206, 2.970648528062944, 2.984161225583605, 2.94563903547154, 2.7958608476415536, 2.780301592939653, 2.5682725377862186, 2.5744595706732993, 2.083840673325976, 1.7616360786943313, 1.7457611021724728, 1.6183531595376481, 1.3870492463230666, 1.2542457227947912, 1.2649191738191528, 1.1719432397533007, 0.8733590493028903, 0.8474062489484414, 0.7743848599030069, 0.7707603250209119, 0.7695900044085479, 0.7662451629527898, 0.743179507960976, 0.6474388562398378, 0.6521874002499234, 0.655861865000936, 0.20229026611435005, 0.20390958004319687, 0.20514038820204314, 0.20403764958195245, 0.20465992222113433, 0.20570474182958248, 0.20722057577493408, 0.20829481008927012, 0.2095914780658788, 0.21135293051222173, 0.2117345480706225, 0.21027808949047844, 0.21181941903518578, 0.2106883689333931, 0.004861090066081861]]\n",
            "DEBUGGING: traj_returns = [10.085873891937238, 51.85845797415085, 34.29421230455027, 3.312180311070503]\n",
            "DEBUGGING: actions = [[9], [2], [25], [53], [26], [62], [0], [3], [20], [24], [59], [44], [30], [68], [26], [66], [5], [67], [52], [28], [33], [31], [32], [26], [50], [1], [24], [75], [70], [23], [10], [77], [76], [56], [10], [41], [53], [94], [76], [40], [65], [63], [88], [74], [32], [82], [45], [44], [76], [39], [41], [19], [49], [11], [64], [45], [44], [45], [17], [40], [21], [1], [12], [63], [57], [10], [68], [0], [41], [5], [37], [27], [72], [57], [50], [46], [13], [25], [73], [4], [38], [63], [68], [41], [67], [6], [83], [41], [90], [23], [82], [71], [87], [16], [28], [101], [82], [0], [43], [71], [31], [60], [26], [40], [32], [3], [13], [37], [56], [29], [51], [68], [2], [6], [11], [71], [7], [33], [13], [38], [42], [10], [19], [5], [0], [47], [33], [23], [71], [38], [42], [76], [28], [55], [72], [59], [84], [85], [22], [80], [28], [17], [96], [100], [96], [43], [28], [83], [95], [96], [1], [43], [12], [59], [4], [35], [32], [32], [31], [63], [10], [28], [31], [16], [70], [52], [71], [2], [11], [25], [74], [72], [46], [33], [5], [26], [58], [79], [9], [6], [28], [45], [14], [88], [22], [87], [28], [64], [25], [81], [94], [13], [28], [3], [43], [17], [70], [85], [55], [83]]\n",
            "DEBUGGING: actions length = 200\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[ 2.9069,  1.1545,  0.1364,  ...,  2.2865,  0.0132,  1.7506],\n",
            "        [ 2.8429,  1.1045,  0.1105,  ...,  2.2197, -0.0583,  1.6498],\n",
            "        [ 2.9044,  1.1902,  0.1222,  ...,  2.3119,  0.0327,  1.7406],\n",
            "        ...,\n",
            "        [ 2.8806,  1.1440,  0.1148,  ...,  2.2663, -0.0110,  1.7049],\n",
            "        [ 2.8807,  1.1441,  0.1149,  ...,  2.2664, -0.0109,  1.7052],\n",
            "        [ 2.8807,  1.1441,  0.1147,  ...,  2.2664, -0.0110,  1.7051]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[ 2.8883,  1.1509,  0.1180,  ...,  2.2749, -0.0041,  1.7145],\n",
            "        [ 2.8833,  1.1460,  0.1160,  ...,  2.2689, -0.0088,  1.7086],\n",
            "        [ 2.8835,  1.1462,  0.1155,  ...,  2.2691, -0.0081,  1.7087],\n",
            "        ...,\n",
            "        [ 2.8870,  1.1484,  0.1172,  ...,  2.2724, -0.0043,  1.7146],\n",
            "        [ 2.8807,  1.1439,  0.1147,  ...,  2.2663, -0.0112,  1.7049],\n",
            "        [ 2.8810,  1.1443,  0.1148,  ...,  2.2667, -0.0107,  1.7054]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([25.5398, 25.4837, 25.4860, 25.4664, 25.4908, 25.4638, 25.4647, 25.4630,\n",
            "        25.4585, 25.4595, 25.4646, 25.4759, 25.4583, 25.4890, 25.4605, 25.4618,\n",
            "        25.4895, 25.4601, 25.4593, 25.4589, 25.4583, 25.4674, 25.4657, 25.4678,\n",
            "        25.4593, 25.4913, 25.5598, 25.4643, 25.4581, 25.4646, 25.4796, 25.4555,\n",
            "        25.4662, 25.4837, 25.4532, 25.4612, 25.4584, 25.5024, 25.4590, 25.4646,\n",
            "        25.4660, 25.4683, 25.4551, 25.4604, 25.4615, 25.4647, 25.4670, 25.4632,\n",
            "        25.4520, 25.4584, 25.4637, 25.4634, 25.4711, 25.4566, 25.4663, 25.4685,\n",
            "        25.4673, 25.4593, 25.4574, 25.4722, 25.4612, 25.4663, 25.4644, 25.4628,\n",
            "        25.4609, 25.4659, 25.4635, 25.4661, 25.4101, 25.4643, 25.4441, 25.4699,\n",
            "        25.4597, 25.4670, 25.4681, 25.4593, 25.4599, 25.4706, 25.4635, 25.4665,\n",
            "        25.4595, 25.4598, 25.4652, 25.4754, 25.4574, 25.4796, 25.4804, 25.4611,\n",
            "        25.4780, 25.4605, 25.4613, 25.4630, 25.4677, 25.4737, 25.4430, 25.4653,\n",
            "        25.5255, 25.4624, 25.4821, 25.4948, 25.4634, 25.4590, 25.4715, 25.4739,\n",
            "        25.4433, 25.5184, 25.4586, 25.4623], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[2.48876811e+01 2.50758875e+01 2.52345148e+01 2.54035167e+01\n",
            "  2.54951492e+01 2.56118208e+01 2.58260565e+01 2.35730628e+01\n",
            "  2.36368488e+01 2.37631884e+01 2.39451531e+01 2.41079520e+01\n",
            "  2.41834365e+01 2.43511830e+01 2.45246230e+01 2.46628050e+01\n",
            "  2.48008682e+01 2.48335521e+01 1.01116863e+01 1.01781310e+01\n",
            "  1.02379361e+01 1.02774785e+01 1.03317292e+01 1.04271988e+01\n",
            "  1.05056146e+01 2.18339763e-01 2.11851563e-01 1.93596216e-01\n",
            "  1.92690082e-01 1.92397502e-01 1.91561291e-01 1.85794878e-01\n",
            "  1.61859715e-01 1.63046851e-01 1.63965467e-01 5.05725670e-02\n",
            "  5.09773954e-02 5.12850974e-02 5.10094127e-02 5.11649809e-02\n",
            "  5.14261858e-02 5.18051442e-02 5.20737028e-02 5.23978697e-02\n",
            "  5.28382328e-02 5.29336372e-02 5.25695225e-02 5.29548548e-02\n",
            "  5.26720923e-02 1.21527256e-03]]\n",
            "DEBUGGING: baseline2 looks like: 24.887681120427217\n",
            "DEBUGGING: ADS looks like: [-1.48018072e+01 -1.50283727e+01 -1.51684554e+01 -1.53049514e+01\n",
            " -1.54935380e+01 -1.59829327e+01 -1.61737643e+01 -2.35730628e+01\n",
            " -2.36368488e+01 -2.37631884e+01 -2.39451531e+01 -2.41079520e+01\n",
            " -2.41834365e+01 -2.43511830e+01 -2.45246230e+01 -2.46628049e+01\n",
            " -2.48008682e+01 -2.48335521e+01 -1.01116863e+01 -1.01781310e+01\n",
            " -1.02379361e+01 -1.02774785e+01 -1.03317292e+01 -1.04271988e+01\n",
            " -1.05056146e+01 -2.18339762e-01 -2.11851562e-01 -1.93596215e-01\n",
            " -1.92690081e-01 -1.92397501e-01 -1.91561291e-01 -1.85794877e-01\n",
            " -1.61859714e-01 -1.63046850e-01 -1.63965466e-01 -5.05725665e-02\n",
            " -5.09773949e-02 -5.12850969e-02 -5.10094123e-02 -5.11649804e-02\n",
            " -5.14261853e-02 -5.18051438e-02 -5.20737024e-02 -5.23978694e-02\n",
            " -5.28382325e-02 -5.29336370e-02 -5.25695223e-02 -5.29548547e-02\n",
            " -5.26720922e-02 -1.21527249e-03  2.69707769e+01  2.72697394e+01\n",
            "  2.74107744e+01  2.77378697e+01  2.77251215e+01  2.81410524e+01\n",
            "  2.84619355e+01  3.12586872e+01  3.15008552e+01  3.17202475e+01\n",
            "  3.20623440e+01  3.24429461e+01  3.23514021e+01  3.26992466e+01\n",
            "  3.29648606e+01  3.32994190e+01  3.37442930e+01  3.41128000e+01\n",
            " -1.01116863e+01 -1.01781310e+01 -1.02379361e+01 -1.02774785e+01\n",
            " -1.03317292e+01 -1.04271988e+01 -1.05056146e+01 -2.18339762e-01\n",
            " -2.11851562e-01 -1.93596215e-01 -1.92690081e-01 -1.92397501e-01\n",
            " -1.91561290e-01 -1.85794877e-01 -1.61859714e-01 -1.63046850e-01\n",
            " -1.63965466e-01 -5.05725662e-02 -5.09773947e-02 -5.12850968e-02\n",
            " -5.10094122e-02 -5.11649804e-02 -5.14261853e-02 -5.18051437e-02\n",
            " -5.20737024e-02 -5.23978694e-02 -5.28382325e-02 -5.29336369e-02\n",
            " -5.25695223e-02 -5.29548547e-02 -5.26720922e-02 -1.21527245e-03\n",
            "  9.40653118e+00  9.49486988e+00  9.66580532e+00  9.69527724e+00\n",
            "  9.95530499e+00  1.01121565e+01  1.01815570e+01  1.26229354e+01\n",
            "  1.26819088e+01  1.26906721e+01  1.28573137e+01  1.27887967e+01\n",
            "  1.30698318e+01  1.32072585e+01  1.33040838e+01  1.34579184e+01\n",
            "  1.32829838e+01  1.34704635e+01  2.85734228e+01  2.87886320e+01\n",
            "  2.90954551e+01  2.94453863e+01  2.97409419e+01  3.00166771e+01\n",
            "  3.03449005e+01 -2.18339762e-01 -2.11851562e-01 -1.93596215e-01\n",
            " -1.92690081e-01 -1.92397501e-01 -1.91561291e-01 -1.85794877e-01\n",
            " -1.61859714e-01 -1.63046850e-01 -1.63965466e-01 -5.05725665e-02\n",
            " -5.09773950e-02 -5.12850970e-02 -5.10094124e-02 -5.11649806e-02\n",
            " -5.14261855e-02 -5.18051440e-02 -5.20737026e-02 -5.23978695e-02\n",
            " -5.28382326e-02 -5.29336370e-02 -5.25695224e-02 -5.29548548e-02\n",
            " -5.26720922e-02 -1.21527256e-03 -2.15755008e+01 -2.17362366e+01\n",
            " -2.19081244e+01 -2.21281955e+01 -2.21868885e+01 -2.22702763e+01\n",
            " -2.24697283e+01 -2.03085598e+01 -2.05459151e+01 -2.06477312e+01\n",
            " -2.09745046e+01 -2.11237908e+01 -2.12377974e+01 -2.15553221e+01\n",
            " -2.17443214e+01 -2.20945324e+01 -2.22264086e+01 -2.27497114e+01\n",
            " -8.35005023e+00 -8.43236993e+00 -8.61958291e+00 -8.89042928e+00\n",
            " -9.07748350e+00 -9.16227959e+00 -9.33367134e+00  6.55019286e-01\n",
            "  6.35554686e-01  5.80788644e-01  5.78070243e-01  5.77192503e-01\n",
            "  5.74683872e-01  5.57384630e-01  4.85579142e-01  4.89140550e-01\n",
            "  4.91896398e-01  1.51717699e-01  1.52932185e-01  1.53855291e-01\n",
            "  1.53028237e-01  1.53494941e-01  1.54278556e-01  1.55415432e-01\n",
            "  1.56221107e-01  1.57193608e-01  1.58514698e-01  1.58800911e-01\n",
            "  1.57708567e-01  1.58864564e-01  1.58016277e-01  3.64581750e-03]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(-0.1578, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 7.1455e-03,  3.8722e-02, -2.8939e-02,  ...,  4.4360e-02,\n",
            "          9.6732e-02, -3.9177e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 6.5188e-02,  9.9952e-02,  3.2588e-02,  ...,  1.0159e-01,\n",
            "          1.6137e-01,  1.0573e+01],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-1.0183e-02, -1.3380e-02, -7.6979e-03,  ..., -1.3203e-02,\n",
            "         -1.8854e-02, -2.0622e+00]])\n",
            "   Last layer:\n",
            "tensor([[-0.0026,  0.0356,  0.0000, -0.0107,  0.0000,  0.0000,  0.0000,  0.0050,\n",
            "         -0.0543,  0.0000,  0.0236,  0.0000,  0.0000, -0.0267,  0.0000,  0.0379,\n",
            "          0.0000,  0.0009,  0.0235,  0.0000],\n",
            "        [-0.0012,  0.0174,  0.0000, -0.0051,  0.0000,  0.0000,  0.0000,  0.0029,\n",
            "         -0.0261,  0.0000,  0.0119,  0.0000,  0.0000, -0.0129,  0.0000,  0.0187,\n",
            "          0.0000,  0.0008,  0.0117,  0.0000],\n",
            "        [-0.0012,  0.0042,  0.0000, -0.0053,  0.0000,  0.0000,  0.0000, -0.0111,\n",
            "         -0.0117,  0.0000, -0.0061,  0.0000,  0.0000, -0.0045,  0.0000, -0.0034,\n",
            "          0.0000, -0.0090, -0.0033,  0.0000],\n",
            "        [ 0.0013,  0.0114,  0.0000,  0.0055,  0.0000,  0.0000,  0.0000,  0.0277,\n",
            "         -0.0053,  0.0000,  0.0274,  0.0000,  0.0000, -0.0056,  0.0000,  0.0297,\n",
            "          0.0000,  0.0206,  0.0211,  0.0000],\n",
            "        [-0.0017,  0.0054,  0.0000, -0.0069,  0.0000,  0.0000,  0.0000, -0.0146,\n",
            "         -0.0154,  0.0000, -0.0081,  0.0000,  0.0000, -0.0058,  0.0000, -0.0046,\n",
            "          0.0000, -0.0118, -0.0044,  0.0000],\n",
            "        [-0.0005, -0.0051,  0.0000, -0.0019,  0.0000,  0.0000,  0.0000, -0.0107,\n",
            "          0.0030,  0.0000, -0.0110,  0.0000,  0.0000,  0.0026,  0.0000, -0.0121,\n",
            "          0.0000, -0.0079, -0.0085,  0.0000],\n",
            "        [ 0.0011,  0.0283,  0.0000,  0.0050,  0.0000,  0.0000,  0.0000,  0.0433,\n",
            "         -0.0248,  0.0000,  0.0487,  0.0000,  0.0000, -0.0167,  0.0000,  0.0566,\n",
            "          0.0000,  0.0314,  0.0391,  0.0000],\n",
            "        [-0.0009,  0.0297,  0.0000, -0.0036,  0.0000,  0.0000,  0.0000,  0.0198,\n",
            "         -0.0378,  0.0000,  0.0316,  0.0000,  0.0000, -0.0204,  0.0000,  0.0421,\n",
            "          0.0000,  0.0130,  0.0277,  0.0000],\n",
            "        [-0.0007,  0.0034,  0.0000, -0.0030,  0.0000,  0.0000,  0.0000, -0.0054,\n",
            "         -0.0076,  0.0000, -0.0023,  0.0000,  0.0000, -0.0031,  0.0000, -0.0005,\n",
            "          0.0000, -0.0044, -0.0009,  0.0000],\n",
            "        [-0.0035,  0.0243,  0.0000, -0.0146,  0.0000,  0.0000,  0.0000, -0.0180,\n",
            "         -0.0468,  0.0000, -0.0003,  0.0000,  0.0000, -0.0206,  0.0000,  0.0112,\n",
            "          0.0000, -0.0161,  0.0048,  0.0000]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0301,  0.0411,  0.0139,  ...,  0.0242,  0.0112, -0.3360],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [ 0.0366,  0.0500,  0.0168,  ...,  0.0293,  0.0133, -0.4984],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0044, -0.0061, -0.0020,  ..., -0.0035, -0.0016,  0.0720]])\n",
            "   Last layer:\n",
            "tensor([[ 0.0114,  0.0343,  0.0000, -0.0801,  0.0000,  0.0000,  0.0000, -0.1575,\n",
            "         -0.0281,  0.0000, -0.0663,  0.0000,  0.0000, -0.0118,  0.0000, -0.0592,\n",
            "          0.0000, -0.0906, -0.1115,  0.0287],\n",
            "        [-0.0008, -0.0006,  0.0000, -0.0407,  0.0000,  0.0000,  0.0000, -0.1036,\n",
            "         -0.0311,  0.0000, -0.0671,  0.0000,  0.0000, -0.0090,  0.0000, -0.0568,\n",
            "          0.0000, -0.0709, -0.0672,  0.0083],\n",
            "        [-0.0019, -0.0050,  0.0000, -0.0073,  0.0000,  0.0000,  0.0000, -0.0250,\n",
            "         -0.0101,  0.0000, -0.0211,  0.0000,  0.0000, -0.0024,  0.0000, -0.0174,\n",
            "          0.0000, -0.0195, -0.0150, -0.0002],\n",
            "        [ 0.0019,  0.0062,  0.0000, -0.0284,  0.0000,  0.0000,  0.0000, -0.0636,\n",
            "         -0.0155,  0.0000, -0.0345,  0.0000,  0.0000, -0.0052,  0.0000, -0.0298,\n",
            "          0.0000, -0.0403, -0.0430,  0.0081],\n",
            "        [ 0.0038,  0.0106,  0.0000, -0.0100,  0.0000,  0.0000,  0.0000, -0.0114,\n",
            "          0.0024,  0.0000,  0.0034,  0.0000,  0.0000, -0.0004,  0.0000,  0.0020,\n",
            "          0.0000, -0.0026, -0.0102,  0.0058],\n",
            "        [ 0.0006,  0.0010,  0.0000,  0.0120,  0.0000,  0.0000,  0.0000,  0.0316,\n",
            "          0.0099,  0.0000,  0.0213,  0.0000,  0.0000,  0.0028,  0.0000,  0.0180,\n",
            "          0.0000,  0.0220,  0.0203, -0.0021],\n",
            "        [ 0.0109,  0.0324,  0.0000, -0.0696,  0.0000,  0.0000,  0.0000, -0.1336,\n",
            "         -0.0220,  0.0000, -0.0529,  0.0000,  0.0000, -0.0098,  0.0000, -0.0477,\n",
            "          0.0000, -0.0752, -0.0955,  0.0259],\n",
            "        [ 0.0051,  0.0167,  0.0000, -0.0695,  0.0000,  0.0000,  0.0000, -0.1538,\n",
            "         -0.0366,  0.0000, -0.0818,  0.0000,  0.0000, -0.0124,  0.0000, -0.0708,\n",
            "          0.0000, -0.0967, -0.1044,  0.0203],\n",
            "        [-0.0080, -0.0212,  0.0000, -0.0131,  0.0000,  0.0000,  0.0000, -0.0609,\n",
            "         -0.0296,  0.0000, -0.0605,  0.0000,  0.0000, -0.0064,  0.0000, -0.0494,\n",
            "          0.0000, -0.0519, -0.0340, -0.0047],\n",
            "        [ 0.0014,  0.0061,  0.0000, -0.0565,  0.0000,  0.0000,  0.0000, -0.1349,\n",
            "         -0.0367,  0.0000, -0.0803,  0.0000,  0.0000, -0.0113,  0.0000, -0.0686,\n",
            "          0.0000, -0.0890, -0.0893,  0.0140]])\n",
            "DEBUGGING: training for one iteration takes 0.005500 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 22\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0185, 0.0145, 0.0147, 0.0147, 0.0146, 0.0150, 0.0145, 0.0145, 0.0145,\n",
            "        0.0143, 0.0141, 0.0157, 0.0147, 0.0150, 0.0147, 0.0147, 0.0150, 0.0146,\n",
            "        0.0150, 0.0162, 0.0145, 0.0146, 0.0145, 0.0144, 0.0142, 0.0145, 0.0148,\n",
            "        0.0149, 0.0146, 0.0143, 0.0145, 0.0142, 0.0146, 0.0142, 0.0148, 0.0150,\n",
            "        0.0140, 0.0146, 0.0146, 0.0151, 0.0149, 0.0143, 0.0145, 0.0145, 0.0148,\n",
            "        0.0147, 0.0144, 0.0149, 0.0149, 0.0144, 0.0145, 0.0147, 0.0146, 0.0148,\n",
            "        0.0143, 0.0142, 0.0139, 0.0133, 0.0140, 0.0144, 0.0149, 0.0143, 0.0145,\n",
            "        0.0160, 0.0145, 0.0158, 0.0157, 0.0150], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [29]\n",
            "DEBUGGING: logits looks like: tensor([27.4318, 27.3706, 27.3748, 27.3737, 27.3729, 27.3787, 27.3711, 27.3708,\n",
            "        27.3702, 27.3675, 27.3641, 27.3912, 27.3735, 27.3796, 27.3745, 27.3746,\n",
            "        27.3789, 27.3726, 27.3792, 27.3982, 27.3704, 27.3729, 27.3700, 27.3697,\n",
            "        27.3650, 27.3708, 27.3762, 27.3775, 27.3720, 27.3671, 27.3712, 27.3665,\n",
            "        27.3721, 27.3652, 27.3759, 27.3785, 27.3628, 27.3732, 27.3730, 27.3812,\n",
            "        27.3778, 27.3678, 27.3714, 27.3702, 27.3757, 27.3743, 27.3694, 27.3784,\n",
            "        27.3772, 27.3691, 27.3709, 27.3736, 27.3721, 27.3752, 27.3681, 27.3651,\n",
            "        27.3612, 27.3484, 27.3620, 27.3690, 27.3772, 27.3674, 27.3711, 27.3958,\n",
            "        27.3709, 27.3923, 27.3912, 27.3789], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0158, 0.0127, 0.0128, 0.0130, 0.0126, 0.0128, 0.0127, 0.0129, 0.0129,\n",
            "        0.0127, 0.0127, 0.0128, 0.0128, 0.0129, 0.0130, 0.0129, 0.0127, 0.0128,\n",
            "        0.0129, 0.0128, 0.0129, 0.0128, 0.0129, 0.0128, 0.0128, 0.0127, 0.0128,\n",
            "        0.0128, 0.0128, 0.0127, 0.0127, 0.0127, 0.0129, 0.0127, 0.0128, 0.0127,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0127, 0.0127, 0.0127, 0.0128,\n",
            "        0.0127, 0.0128, 0.0127, 0.0127, 0.0128, 0.0128, 0.0128, 0.0129, 0.0128,\n",
            "        0.0128, 0.0127, 0.0128, 0.0128, 0.0127, 0.0127, 0.0128, 0.0127, 0.0127,\n",
            "        0.0128, 0.0127, 0.0128, 0.0128, 0.0128, 0.0127, 0.0127, 0.0129, 0.0127,\n",
            "        0.0128, 0.0126, 0.0127, 0.0128, 0.0128, 0.0129],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [67]\n",
            "DEBUGGING: logits looks like: tensor([27.4253, 27.3700, 27.3717, 27.3768, 27.3695, 27.3716, 27.3710, 27.3751,\n",
            "        27.3742, 27.3703, 27.3707, 27.3728, 27.3719, 27.3747, 27.3757, 27.3735,\n",
            "        27.3710, 27.3721, 27.3735, 27.3719, 27.3743, 27.3725, 27.3749, 27.3715,\n",
            "        27.3728, 27.3714, 27.3718, 27.3724, 27.3734, 27.3713, 27.3712, 27.3701,\n",
            "        27.3739, 27.3714, 27.3730, 27.3707, 27.3730, 27.3717, 27.3725, 27.3718,\n",
            "        27.3722, 27.3711, 27.3713, 27.3713, 27.3723, 27.3705, 27.3725, 27.3710,\n",
            "        27.3713, 27.3717, 27.3721, 27.3723, 27.3743, 27.3731, 27.3725, 27.3704,\n",
            "        27.3716, 27.3732, 27.3697, 27.3713, 27.3721, 27.3704, 27.3696, 27.3732,\n",
            "        27.3708, 27.3732, 27.3720, 27.3719, 27.3714, 27.3714, 27.3736, 27.3705,\n",
            "        27.3724, 27.3680, 27.3706, 27.3726, 27.3719, 27.3742],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0139, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [88]\n",
            "DEBUGGING: logits looks like: tensor([27.4332, 27.3797, 27.3794, 27.3801, 27.3797, 27.3799, 27.3794, 27.3795,\n",
            "        27.3798, 27.3797, 27.3804, 27.3797, 27.3794, 27.3798, 27.3798, 27.3796,\n",
            "        27.3795, 27.3787, 27.3799, 27.3797, 27.3797, 27.3801, 27.3800, 27.3795,\n",
            "        27.3796, 27.3794, 27.3799, 27.3800, 27.3799, 27.3797, 27.3799, 27.3792,\n",
            "        27.3798, 27.3796, 27.3797, 27.3796, 27.3797, 27.3798, 27.3794, 27.3796,\n",
            "        27.3795, 27.3798, 27.3799, 27.3799, 27.3796, 27.3797, 27.3795, 27.3798,\n",
            "        27.3797, 27.3799, 27.3795, 27.3798, 27.3794, 27.3790, 27.3794, 27.3797,\n",
            "        27.3800, 27.3798, 27.3794, 27.3795, 27.3796, 27.3800, 27.3795, 27.3797,\n",
            "        27.3796, 27.3795, 27.3794, 27.3797, 27.3797, 27.3795, 27.3801, 27.3799,\n",
            "        27.3798, 27.3796, 27.3792, 27.3799, 27.3798, 27.3799, 27.3796, 27.3797,\n",
            "        27.3797, 27.3797, 27.3795, 27.3796, 27.3799, 27.3795, 27.3798, 27.3797,\n",
            "        27.3798], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0125, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0100, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [93]\n",
            "DEBUGGING: logits looks like: tensor([27.4395, 27.3862, 27.3862, 27.3861, 27.3862, 27.3864, 27.3863, 27.3862,\n",
            "        27.3861, 27.3862, 27.3861, 27.3861, 27.3862, 27.3863, 27.3862, 27.3862,\n",
            "        27.3862, 27.3861, 27.3862, 27.3862, 27.3863, 27.3862, 27.3858, 27.3864,\n",
            "        27.3861, 27.3861, 27.3863, 27.3862, 27.3863, 27.3862, 27.3860, 27.3863,\n",
            "        27.3860, 27.3862, 27.3862, 27.3861, 27.3860, 27.3863, 27.3863, 27.3863,\n",
            "        27.3862, 27.3862, 27.3860, 27.3863, 27.3860, 27.3861, 27.3863, 27.3862,\n",
            "        27.3862, 27.3862, 27.3862, 27.3859, 27.3862, 27.3864, 27.3861, 27.3862,\n",
            "        27.3859, 27.3863, 27.3862, 27.3861, 27.3862, 27.3863, 27.3861, 27.3862,\n",
            "        27.3863, 27.3863, 27.3862, 27.3858, 27.3863, 27.3863, 27.3862, 27.3864,\n",
            "        27.3864, 27.3860, 27.3862, 27.3863, 27.3861, 27.3862, 27.3852, 27.3863,\n",
            "        27.3861, 27.3862, 27.3863, 27.3861, 27.3862, 27.3862, 27.3862, 27.3863,\n",
            "        27.3862, 27.3862, 27.3861, 27.3862, 27.3863, 27.3861, 27.3858, 27.3863,\n",
            "        27.3859, 27.3862, 27.3860], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0114, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0093, 0.0092, 0.0093, 0.0092,\n",
            "        0.0093, 0.0092, 0.0093, 0.0093, 0.0092, 0.0092, 0.0092, 0.0092, 0.0093,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0093, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0093, 0.0092, 0.0092, 0.0093,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0093, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0093, 0.0092,\n",
            "        0.0092, 0.0092, 0.0093, 0.0092, 0.0092, 0.0093, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0093, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0093, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0093, 0.0092, 0.0092],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [61]\n",
            "DEBUGGING: logits looks like: tensor([27.4446, 27.3918, 27.3919, 27.3919, 27.3920, 27.3921, 27.3923, 27.3922,\n",
            "        27.3917, 27.3918, 27.3920, 27.3917, 27.3919, 27.3917, 27.3927, 27.3920,\n",
            "        27.3926, 27.3918, 27.3925, 27.3923, 27.3924, 27.3937, 27.3919, 27.3919,\n",
            "        27.3920, 27.3916, 27.3924, 27.3919, 27.3916, 27.3919, 27.3920, 27.3920,\n",
            "        27.3923, 27.3919, 27.3919, 27.3917, 27.3920, 27.3921, 27.3924, 27.3922,\n",
            "        27.3919, 27.3922, 27.3919, 27.3922, 27.3920, 27.3919, 27.3923, 27.3923,\n",
            "        27.3911, 27.3922, 27.3923, 27.3919, 27.3919, 27.3925, 27.3918, 27.3919,\n",
            "        27.3921, 27.3923, 27.3917, 27.3919, 27.3920, 27.3925, 27.3915, 27.3920,\n",
            "        27.3921, 27.3920, 27.3918, 27.3918, 27.3917, 27.3918, 27.3925, 27.3917,\n",
            "        27.3920, 27.3922, 27.3925, 27.3920, 27.3922, 27.3924, 27.3917, 27.3917,\n",
            "        27.3914, 27.3921, 27.3925, 27.3920, 27.3923, 27.3920, 27.3923, 27.3922,\n",
            "        27.3915, 27.3920, 27.3918, 27.3919, 27.3920, 27.3922, 27.3918, 27.3920,\n",
            "        27.3921, 27.3932, 27.3916, 27.3922, 27.3919, 27.3921, 27.3917, 27.3919,\n",
            "        27.3919, 27.3925, 27.3918, 27.3919], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.17918999384619383 and immediate abs rewards look like: [0.07269872199117344, 0.00878223351583074, 0.015873520786044537, 0.00013896278983338561, 0.001970898986201064, 0.02691131704204963, 0.0021100555534303567, 0.009413543461050722, 7.239717183438188e-05, 0.011589370305955526, 0.0033591415694900206, 0.004907128662580362, 0.0004041173326641001, 0.0035543347703423933, 7.688345931455842e-06, 0.004959691809290234, 0.002135387335329142, 0.000367061875977015, 0.0029878587986331695, 0.00037563680439234304, 0.0014137219959593494, 0.00047730703954584897, 0.0001332560761966306, 0.0001908528704461787, 0.000354490643076133, 2.797831461975875e-05, 0.0008545720938855084, 5.033379238739144e-06, 1.6072623338914127e-05, 3.879695213981904e-07, 4.5975850753166014e-06, 2.826425361490692e-05, 4.601008754434588e-05, 0.0001470288520977192, 2.8132254556112457e-05, 5.872883411939256e-05, 0.00020876455482721212, 0.0004032498165997822, 9.304974582846626e-06, 0.00032195773405874206, 4.217306991449732e-05, 3.202210791641846e-05, 8.711623354429321e-05, 0.00035192455197829986, 0.0004011696382804075, 1.3388261322688777e-05, 4.2213224105580593e-05, 0.0007983335926837754, 4.13973043578153e-06, 6.873257507322705e-05]\n",
            "DEBUGGING: the total relative reward of the trajectory = 3.071087253166399 and immediate relative rewards look like: [0.20153514777662376, 0.04969365711924001, 0.13506441111917145, 0.0015836696643244767, 0.02807744900963672, 0.4603131968335449, 0.04243301169424241, 0.21648030418974207, 0.0018780890317655593, 0.33405730143140017, 0.1068649636433533, 0.1704689016718663, 0.015230167795071177, 0.14427500783035185, 0.0003347161769623957, 0.23031815346042156, 0.10551268943317209, 0.019215891066168084, 0.16512345959552147, 0.021871110680639527, 0.08643776133142977, 0.03058577694624631, 0.008928412282027528, 0.013344004393885094, 0.025819342039191225, 0.0021195327843945787, 0.06722971223493895, 0.0004107472065236087, 0.0013584458640408413, 3.392176717190757e-05, 0.0004153852991737226, 0.0026360141420899113, 0.004425178699237901, 0.014569717165846265, 0.0028698607934553428, 0.00616234047079482, 0.022514262811429087, 0.04466665662985369, 0.001057928996425649, 0.03754367261397421, 0.0050412442266208005, 0.003921238086561056, 0.01092183486532085, 0.045148292883107975, 0.052641011614204515, 0.0017960419451887762, 0.005786054762174075, 0.11175504722471098, 0.0005917125394190779, 0.010024801323741108]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0232, 0.0147, 0.0141, 0.0134, 0.0145, 0.0157, 0.0141, 0.0146, 0.0140,\n",
            "        0.0139, 0.0140, 0.0146, 0.0138, 0.0136, 0.0133, 0.0141, 0.0148, 0.0141,\n",
            "        0.0143, 0.0139, 0.0149, 0.0147, 0.0138, 0.0147, 0.0141, 0.0140, 0.0141,\n",
            "        0.0141, 0.0133, 0.0144, 0.0143, 0.0141, 0.0150, 0.0137, 0.0133, 0.0146,\n",
            "        0.0139, 0.0137, 0.0135, 0.0133, 0.0142, 0.0169, 0.0136, 0.0157, 0.0143,\n",
            "        0.0139, 0.0139, 0.0137, 0.0148, 0.0131, 0.0150, 0.0146, 0.0150, 0.0140,\n",
            "        0.0148, 0.0141, 0.0129, 0.0139, 0.0152, 0.0148, 0.0135, 0.0137, 0.0131,\n",
            "        0.0141, 0.0129, 0.0142, 0.0135, 0.0137, 0.0144, 0.0137],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [27]\n",
            "DEBUGGING: logits looks like: tensor([27.3593, 27.2455, 27.2350, 27.2219, 27.2427, 27.2620, 27.2355, 27.2433,\n",
            "        27.2330, 27.2316, 27.2341, 27.2441, 27.2293, 27.2266, 27.2195, 27.2345,\n",
            "        27.2468, 27.2359, 27.2384, 27.2315, 27.2493, 27.2461, 27.2305, 27.2461,\n",
            "        27.2352, 27.2338, 27.2346, 27.2344, 27.2206, 27.2402, 27.2392, 27.2353,\n",
            "        27.2506, 27.2279, 27.2198, 27.2440, 27.2323, 27.2285, 27.2245, 27.2212,\n",
            "        27.2367, 27.2808, 27.2269, 27.2612, 27.2389, 27.2321, 27.2321, 27.2279,\n",
            "        27.2470, 27.2175, 27.2503, 27.2445, 27.2501, 27.2334, 27.2469, 27.2352,\n",
            "        27.2137, 27.2318, 27.2546, 27.2470, 27.2245, 27.2285, 27.2168, 27.2359,\n",
            "        27.2120, 27.2361, 27.2236, 27.2282, 27.2401, 27.2282],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0142, 0.0124, 0.0159, 0.0112, 0.0137, 0.0139, 0.0094, 0.0136, 0.0097,\n",
            "        0.0128, 0.0120, 0.0142, 0.0138, 0.0116, 0.0102, 0.0125, 0.0122, 0.0133,\n",
            "        0.0129, 0.0125, 0.0128, 0.0137, 0.0142, 0.0110, 0.0127, 0.0137, 0.0134,\n",
            "        0.0136, 0.0130, 0.0126, 0.0121, 0.0133, 0.0121, 0.0136, 0.0111, 0.0131,\n",
            "        0.0108, 0.0136, 0.0135, 0.0133, 0.0126, 0.0100, 0.0134, 0.0130, 0.0119,\n",
            "        0.0121, 0.0137, 0.0135, 0.0123, 0.0143, 0.0131, 0.0133, 0.0128, 0.0159,\n",
            "        0.0130, 0.0137, 0.0140, 0.0093, 0.0136, 0.0122, 0.0125, 0.0130, 0.0132,\n",
            "        0.0132, 0.0127, 0.0134, 0.0124, 0.0141, 0.0119, 0.0123, 0.0143, 0.0143,\n",
            "        0.0127, 0.0136, 0.0134, 0.0128, 0.0117, 0.0115],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [29]\n",
            "DEBUGGING: logits looks like: tensor([27.3446, 27.3110, 27.3737, 27.2857, 27.3367, 27.3406, 27.2430, 27.3338,\n",
            "        27.2499, 27.3186, 27.3025, 27.3445, 27.3388, 27.2941, 27.2631, 27.3128,\n",
            "        27.3069, 27.3288, 27.3204, 27.3142, 27.3188, 27.3353, 27.3449, 27.2813,\n",
            "        27.3175, 27.3361, 27.3298, 27.3336, 27.3233, 27.3162, 27.3053, 27.3291,\n",
            "        27.3056, 27.3339, 27.2838, 27.3252, 27.2771, 27.3348, 27.3327, 27.3284,\n",
            "        27.3155, 27.2568, 27.3313, 27.3239, 27.3012, 27.3043, 27.3365, 27.3316,\n",
            "        27.3092, 27.3468, 27.3249, 27.3278, 27.3196, 27.3737, 27.3232, 27.3353,\n",
            "        27.3411, 27.2394, 27.3352, 27.3080, 27.3138, 27.3223, 27.3269, 27.3267,\n",
            "        27.3171, 27.3313, 27.3104, 27.3439, 27.3003, 27.3100, 27.3477, 27.3466,\n",
            "        27.3174, 27.3349, 27.3303, 27.3184, 27.2957, 27.2931],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0113, 0.0114, 0.0134, 0.0115, 0.0110, 0.0125, 0.0102, 0.0111, 0.0114,\n",
            "        0.0115, 0.0114, 0.0118, 0.0112, 0.0120, 0.0117, 0.0117, 0.0122, 0.0107,\n",
            "        0.0118, 0.0116, 0.0117, 0.0123, 0.0113, 0.0107, 0.0118, 0.0124, 0.0118,\n",
            "        0.0113, 0.0115, 0.0109, 0.0108, 0.0120, 0.0113, 0.0107, 0.0119, 0.0119,\n",
            "        0.0105, 0.0107, 0.0069, 0.0119, 0.0116, 0.0125, 0.0107, 0.0115, 0.0111,\n",
            "        0.0122, 0.0126, 0.0112, 0.0121, 0.0111, 0.0107, 0.0106, 0.0108, 0.0125,\n",
            "        0.0114, 0.0117, 0.0102, 0.0111, 0.0113, 0.0109, 0.0114, 0.0120, 0.0109,\n",
            "        0.0108, 0.0107, 0.0114, 0.0108, 0.0114, 0.0105, 0.0111, 0.0115, 0.0129,\n",
            "        0.0112, 0.0117, 0.0108, 0.0105, 0.0112, 0.0109, 0.0096, 0.0120, 0.0116,\n",
            "        0.0124, 0.0108, 0.0111, 0.0117, 0.0114, 0.0130, 0.0107],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [75]\n",
            "DEBUGGING: logits looks like: tensor([27.3036, 27.3045, 27.3449, 27.3071, 27.2955, 27.3276, 27.2782, 27.2982,\n",
            "        27.3062, 27.3068, 27.3056, 27.3142, 27.3000, 27.3176, 27.3108, 27.3121,\n",
            "        27.3216, 27.2884, 27.3137, 27.3097, 27.3122, 27.3251, 27.3021, 27.2885,\n",
            "        27.3141, 27.3271, 27.3131, 27.3027, 27.3076, 27.2937, 27.2911, 27.3190,\n",
            "        27.3036, 27.2893, 27.3155, 27.3158, 27.2857, 27.2884, 27.1800, 27.3163,\n",
            "        27.3087, 27.3285, 27.2904, 27.3065, 27.2995, 27.3225, 27.3303, 27.3001,\n",
            "        27.3206, 27.2989, 27.2903, 27.2876, 27.2924, 27.3285, 27.3051, 27.3127,\n",
            "        27.2772, 27.2994, 27.3039, 27.2950, 27.3048, 27.3179, 27.2931, 27.2919,\n",
            "        27.2882, 27.3041, 27.2913, 27.3056, 27.2846, 27.2990, 27.3078, 27.3359,\n",
            "        27.3008, 27.3120, 27.2923, 27.2848, 27.3006, 27.2944, 27.2621, 27.3189,\n",
            "        27.3089, 27.3262, 27.2918, 27.2983, 27.3124, 27.3056, 27.3388, 27.2903],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0158, 0.0099, 0.0106, 0.0088, 0.0100, 0.0101, 0.0098, 0.0092, 0.0115,\n",
            "        0.0093, 0.0081, 0.0091, 0.0081, 0.0125, 0.0093, 0.0166, 0.0110, 0.0100,\n",
            "        0.0072, 0.0106, 0.0110, 0.0083, 0.0104, 0.0109, 0.0100, 0.0092, 0.0129,\n",
            "        0.0127, 0.0112, 0.0114, 0.0129, 0.0082, 0.0099, 0.0091, 0.0134, 0.0103,\n",
            "        0.0116, 0.0113, 0.0121, 0.0100, 0.0145, 0.0100, 0.0083, 0.0108, 0.0116,\n",
            "        0.0080, 0.0105, 0.0156, 0.0095, 0.0093, 0.0100, 0.0103, 0.0112, 0.0090,\n",
            "        0.0107, 0.0095, 0.0115, 0.0094, 0.0120, 0.0098, 0.0096, 0.0091, 0.0103,\n",
            "        0.0098, 0.0145, 0.0107, 0.0115, 0.0103, 0.0110, 0.0103, 0.0113, 0.0124,\n",
            "        0.0117, 0.0120, 0.0110, 0.0098, 0.0095, 0.0104, 0.0101, 0.0094, 0.0109,\n",
            "        0.0094, 0.0086, 0.0103, 0.0112, 0.0097, 0.0103, 0.0098, 0.0100, 0.0114,\n",
            "        0.0107, 0.0104, 0.0102, 0.0105, 0.0062], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [12]\n",
            "DEBUGGING: logits looks like: tensor([27.3808, 27.2619, 27.2798, 27.2324, 27.2656, 27.2680, 27.2605, 27.2439,\n",
            "        27.3010, 27.2477, 27.2142, 27.2423, 27.2137, 27.3209, 27.2467, 27.3930,\n",
            "        27.2897, 27.2662, 27.1837, 27.2807, 27.2889, 27.2193, 27.2755, 27.2872,\n",
            "        27.2652, 27.2460, 27.3293, 27.3260, 27.2930, 27.2978, 27.3299, 27.2175,\n",
            "        27.2627, 27.2419, 27.3385, 27.2729, 27.3023, 27.2963, 27.3129, 27.2652,\n",
            "        27.3592, 27.2665, 27.2194, 27.2838, 27.3022, 27.2088, 27.2783, 27.3776,\n",
            "        27.2542, 27.2469, 27.2664, 27.2732, 27.2948, 27.2395, 27.2827, 27.2535,\n",
            "        27.3003, 27.2495, 27.3105, 27.2606, 27.2545, 27.2413, 27.2727, 27.2615,\n",
            "        27.3585, 27.2820, 27.3011, 27.2732, 27.2886, 27.2728, 27.2958, 27.3201,\n",
            "        27.3055, 27.3105, 27.2903, 27.2609, 27.2538, 27.2748, 27.2679, 27.2495,\n",
            "        27.2870, 27.2498, 27.2291, 27.2733, 27.2947, 27.2577, 27.2724, 27.2612,\n",
            "        27.2656, 27.2984, 27.2827, 27.2758, 27.2718, 27.2785, 27.1449],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0083, 0.0087, 0.0087, 0.0097, 0.0091, 0.0087, 0.0092, 0.0088, 0.0085,\n",
            "        0.0094, 0.0085, 0.0091, 0.0092, 0.0098, 0.0084, 0.0095, 0.0098, 0.0092,\n",
            "        0.0089, 0.0096, 0.0088, 0.0093, 0.0090, 0.0093, 0.0093, 0.0098, 0.0088,\n",
            "        0.0087, 0.0095, 0.0094, 0.0071, 0.0096, 0.0089, 0.0095, 0.0091, 0.0092,\n",
            "        0.0092, 0.0093, 0.0098, 0.0093, 0.0088, 0.0089, 0.0095, 0.0096, 0.0097,\n",
            "        0.0095, 0.0094, 0.0088, 0.0089, 0.0092, 0.0093, 0.0093, 0.0090, 0.0094,\n",
            "        0.0093, 0.0089, 0.0094, 0.0090, 0.0089, 0.0100, 0.0089, 0.0091, 0.0090,\n",
            "        0.0094, 0.0090, 0.0087, 0.0085, 0.0093, 0.0093, 0.0085, 0.0088, 0.0083,\n",
            "        0.0088, 0.0093, 0.0093, 0.0098, 0.0088, 0.0091, 0.0096, 0.0089, 0.0092,\n",
            "        0.0087, 0.0097, 0.0090, 0.0097, 0.0092, 0.0096, 0.0095, 0.0095, 0.0095,\n",
            "        0.0100, 0.0090, 0.0093, 0.0095, 0.0099, 0.0096, 0.0091, 0.0095, 0.0088,\n",
            "        0.0096, 0.0093, 0.0090, 0.0089, 0.0092, 0.0093, 0.0095, 0.0092, 0.0090,\n",
            "        0.0097], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [31]\n",
            "DEBUGGING: logits looks like: tensor([27.2436, 27.2544, 27.2565, 27.2829, 27.2681, 27.2570, 27.2708, 27.2580,\n",
            "        27.2512, 27.2746, 27.2505, 27.2662, 27.2698, 27.2850, 27.2479, 27.2774,\n",
            "        27.2843, 27.2699, 27.2620, 27.2812, 27.2585, 27.2724, 27.2655, 27.2725,\n",
            "        27.2719, 27.2847, 27.2584, 27.2547, 27.2771, 27.2748, 27.2065, 27.2798,\n",
            "        27.2610, 27.2768, 27.2682, 27.2690, 27.2701, 27.2715, 27.2858, 27.2714,\n",
            "        27.2580, 27.2622, 27.2783, 27.2799, 27.2827, 27.2767, 27.2751, 27.2576,\n",
            "        27.2604, 27.2689, 27.2723, 27.2733, 27.2648, 27.2764, 27.2712, 27.2612,\n",
            "        27.2755, 27.2648, 27.2628, 27.2894, 27.2615, 27.2678, 27.2639, 27.2760,\n",
            "        27.2641, 27.2560, 27.2511, 27.2736, 27.2722, 27.2487, 27.2579, 27.2434,\n",
            "        27.2597, 27.2731, 27.2730, 27.2863, 27.2591, 27.2670, 27.2798, 27.2618,\n",
            "        27.2693, 27.2561, 27.2819, 27.2651, 27.2835, 27.2687, 27.2796, 27.2787,\n",
            "        27.2767, 27.2781, 27.2897, 27.2656, 27.2727, 27.2778, 27.2876, 27.2793,\n",
            "        27.2663, 27.2777, 27.2587, 27.2793, 27.2736, 27.2633, 27.2614, 27.2684,\n",
            "        27.2716, 27.2773, 27.2684, 27.2639, 27.2835], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.8867413289895012 and immediate abs rewards look like: [0.005821298033424682, 0.0287016390793724, 0.0002810360815601598, 0.003429773736570496, 0.003662815896404936, 0.014679519545552466, 0.003963641677728447, 0.006232936551896273, 0.0004744589923575404, 0.00013286492594488664, 0.006271391192058218, 0.8130899532593503, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 0.0, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 1.3642420526593924e-12, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 1.3642420526593924e-12, 9.094947017729282e-13, 9.094947017729282e-13, 1.8189894035458565e-12, 1.3642420526593924e-12, 0.0, 4.547473508864641e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 35.86794481306603 and immediate relative rewards look like: [0.020165637894186895, 0.1992532855555519, 0.002955973662769576, 0.04810447508072254, 0.06429359569721611, 0.3096025425410328, 0.09803488251236582, 0.17643289257329986, 0.015142476507321753, 0.0047123673991941485, 0.24468414894172577, 34.68456253169477, 2.955857780761345e-11, 0.0, 0.0, 3.637978807091713e-11, 7.730704965071648e-11, 4.0927261579772465e-11, 0.0, 0.0, 4.774847184307873e-11, 0.0, 1.0459189070386297e-10, 5.45696821063881e-11, 5.6843418860808015e-11, 5.91171556152269e-11, 1.2278178473928948e-10, 6.366462912410498e-11, 6.593836587855229e-11, 6.821210263296962e-11, 0.0, 7.27595761418508e-11, 0.0, 0.0, 0.0, 0.0, 8.412825991399586e-11, 2.592059900053435e-10, 8.867573342282018e-11, 9.094947017727214e-11, 0.0, 0.0, 0.0, 3.001332515850663e-10, 2.0463630789904844e-10, 2.0918378140782106e-10, 4.274625098331791e-10, 3.274180926384775e-10, 0.0, 1.1368683772161603e-10]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0187, 0.0149, 0.0144, 0.0159, 0.0141, 0.0170, 0.0158, 0.0159, 0.0141,\n",
            "        0.0165, 0.0147, 0.0147, 0.0140, 0.0141, 0.0147, 0.0145, 0.0152, 0.0152,\n",
            "        0.0138, 0.0151, 0.0135, 0.0148, 0.0167, 0.0141, 0.0153, 0.0144, 0.0143,\n",
            "        0.0145, 0.0143, 0.0154, 0.0165, 0.0147, 0.0159, 0.0141, 0.0139, 0.0135,\n",
            "        0.0162, 0.0154, 0.0161, 0.0145, 0.0144, 0.0150, 0.0122, 0.0138, 0.0146,\n",
            "        0.0149, 0.0155, 0.0140, 0.0154, 0.0168, 0.0145, 0.0147, 0.0142, 0.0148,\n",
            "        0.0146, 0.0142, 0.0159, 0.0148, 0.0149, 0.0132, 0.0147, 0.0156, 0.0130,\n",
            "        0.0147, 0.0156, 0.0171, 0.0155], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [2]\n",
            "DEBUGGING: logits looks like: tensor([27.4537, 27.3959, 27.3883, 27.4130, 27.3830, 27.4294, 27.4107, 27.4134,\n",
            "        27.3832, 27.4222, 27.3928, 27.3937, 27.3803, 27.3827, 27.3935, 27.3900,\n",
            "        27.4020, 27.4013, 27.3763, 27.4002, 27.3712, 27.3946, 27.4247, 27.3818,\n",
            "        27.4035, 27.3877, 27.3855, 27.3898, 27.3866, 27.4049, 27.4222, 27.3927,\n",
            "        27.4120, 27.3833, 27.3782, 27.3712, 27.4169, 27.4046, 27.4157, 27.3887,\n",
            "        27.3871, 27.3978, 27.3463, 27.3767, 27.3913, 27.3956, 27.4065, 27.3811,\n",
            "        27.4043, 27.4269, 27.3890, 27.3923, 27.3836, 27.3948, 27.3918, 27.3837,\n",
            "        27.4126, 27.3951, 27.3970, 27.3661, 27.3926, 27.4073, 27.3632, 27.3929,\n",
            "        27.4076, 27.4301, 27.4070], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0153, 0.0122, 0.0125, 0.0127, 0.0125, 0.0122, 0.0126, 0.0127, 0.0123,\n",
            "        0.0127, 0.0126, 0.0127, 0.0125, 0.0122, 0.0126, 0.0119, 0.0127, 0.0128,\n",
            "        0.0128, 0.0120, 0.0130, 0.0128, 0.0125, 0.0123, 0.0123, 0.0125, 0.0122,\n",
            "        0.0126, 0.0120, 0.0125, 0.0127, 0.0123, 0.0129, 0.0121, 0.0123, 0.0122,\n",
            "        0.0121, 0.0124, 0.0123, 0.0126, 0.0120, 0.0124, 0.0126, 0.0131, 0.0129,\n",
            "        0.0124, 0.0121, 0.0126, 0.0125, 0.0126, 0.0125, 0.0123, 0.0125, 0.0118,\n",
            "        0.0126, 0.0129, 0.0124, 0.0122, 0.0125, 0.0125, 0.0121, 0.0123, 0.0125,\n",
            "        0.0123, 0.0126, 0.0125, 0.0128, 0.0123, 0.0124, 0.0127, 0.0124, 0.0125,\n",
            "        0.0122, 0.0123, 0.0127, 0.0124, 0.0126, 0.0126, 0.0124, 0.0123],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [0]\n",
            "DEBUGGING: logits looks like: tensor([27.4623, 27.4058, 27.4115, 27.4145, 27.4104, 27.4045, 27.4139, 27.4152,\n",
            "        27.4067, 27.4151, 27.4142, 27.4162, 27.4121, 27.4057, 27.4126, 27.3983,\n",
            "        27.4145, 27.4176, 27.4174, 27.4021, 27.4211, 27.4174, 27.4111, 27.4082,\n",
            "        27.4072, 27.4121, 27.4050, 27.4132, 27.4017, 27.4122, 27.4154, 27.4078,\n",
            "        27.4191, 27.4039, 27.4082, 27.4061, 27.4038, 27.4096, 27.4076, 27.4139,\n",
            "        27.4004, 27.4094, 27.4137, 27.4225, 27.4197, 27.4103, 27.4038, 27.4131,\n",
            "        27.4119, 27.4127, 27.4116, 27.4082, 27.4119, 27.3967, 27.4142, 27.4191,\n",
            "        27.4089, 27.4044, 27.4108, 27.4118, 27.4027, 27.4071, 27.4115, 27.4066,\n",
            "        27.4123, 27.4123, 27.4170, 27.4083, 27.4095, 27.4149, 27.4097, 27.4118,\n",
            "        27.4063, 27.4068, 27.4159, 27.4094, 27.4134, 27.4139, 27.4099, 27.4071],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0115, 0.0116, 0.0109, 0.0115, 0.0111, 0.0119, 0.0109, 0.0109, 0.0110,\n",
            "        0.0118, 0.0114, 0.0112, 0.0117, 0.0113, 0.0112, 0.0114, 0.0109, 0.0114,\n",
            "        0.0109, 0.0107, 0.0115, 0.0108, 0.0113, 0.0113, 0.0113, 0.0116, 0.0127,\n",
            "        0.0113, 0.0116, 0.0113, 0.0110, 0.0112, 0.0119, 0.0111, 0.0108, 0.0108,\n",
            "        0.0117, 0.0113, 0.0110, 0.0112, 0.0117, 0.0112, 0.0119, 0.0115, 0.0118,\n",
            "        0.0107, 0.0114, 0.0115, 0.0119, 0.0117, 0.0111, 0.0112, 0.0114, 0.0106,\n",
            "        0.0107, 0.0113, 0.0125, 0.0108, 0.0108, 0.0108, 0.0110, 0.0118, 0.0116,\n",
            "        0.0107, 0.0121, 0.0127, 0.0119, 0.0118, 0.0116, 0.0117, 0.0118, 0.0114,\n",
            "        0.0118, 0.0108, 0.0116, 0.0113, 0.0112, 0.0116, 0.0115, 0.0118, 0.0119,\n",
            "        0.0103, 0.0107, 0.0107, 0.0128, 0.0118, 0.0111, 0.0104],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [32]\n",
            "DEBUGGING: logits looks like: tensor([27.4513, 27.4526, 27.4388, 27.4512, 27.4420, 27.4597, 27.4379, 27.4381,\n",
            "        27.4397, 27.4579, 27.4486, 27.4445, 27.4565, 27.4479, 27.4442, 27.4494,\n",
            "        27.4373, 27.4493, 27.4383, 27.4342, 27.4519, 27.4348, 27.4478, 27.4477,\n",
            "        27.4470, 27.4535, 27.4761, 27.4479, 27.4540, 27.4462, 27.4401, 27.4442,\n",
            "        27.4604, 27.4416, 27.4346, 27.4354, 27.4553, 27.4464, 27.4407, 27.4456,\n",
            "        27.4558, 27.4438, 27.4600, 27.4512, 27.4568, 27.4342, 27.4481, 27.4505,\n",
            "        27.4591, 27.4564, 27.4425, 27.4437, 27.4491, 27.4312, 27.4339, 27.4478,\n",
            "        27.4721, 27.4365, 27.4361, 27.4351, 27.4393, 27.4569, 27.4533, 27.4343,\n",
            "        27.4643, 27.4766, 27.4588, 27.4586, 27.4528, 27.4546, 27.4575, 27.4495,\n",
            "        27.4568, 27.4361, 27.4529, 27.4461, 27.4456, 27.4532, 27.4502, 27.4568,\n",
            "        27.4608, 27.4243, 27.4328, 27.4340, 27.4787, 27.4583, 27.4434, 27.4253],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0103, 0.0106, 0.0105, 0.0105, 0.0103, 0.0104, 0.0103, 0.0105, 0.0105,\n",
            "        0.0102, 0.0103, 0.0106, 0.0103, 0.0106, 0.0103, 0.0104, 0.0105, 0.0104,\n",
            "        0.0103, 0.0107, 0.0104, 0.0104, 0.0103, 0.0103, 0.0106, 0.0106, 0.0104,\n",
            "        0.0106, 0.0104, 0.0106, 0.0104, 0.0103, 0.0105, 0.0103, 0.0107, 0.0103,\n",
            "        0.0104, 0.0104, 0.0101, 0.0106, 0.0104, 0.0104, 0.0104, 0.0105, 0.0102,\n",
            "        0.0106, 0.0105, 0.0106, 0.0103, 0.0105, 0.0105, 0.0105, 0.0105, 0.0107,\n",
            "        0.0104, 0.0104, 0.0104, 0.0106, 0.0105, 0.0105, 0.0105, 0.0105, 0.0102,\n",
            "        0.0103, 0.0104, 0.0104, 0.0104, 0.0103, 0.0106, 0.0105, 0.0104, 0.0103,\n",
            "        0.0102, 0.0104, 0.0104, 0.0103, 0.0104, 0.0104, 0.0102, 0.0103, 0.0106,\n",
            "        0.0101, 0.0104, 0.0106, 0.0104, 0.0103, 0.0102, 0.0105, 0.0102, 0.0103,\n",
            "        0.0105, 0.0102, 0.0104, 0.0105, 0.0104, 0.0103],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [29]\n",
            "DEBUGGING: logits looks like: tensor([27.4603, 27.4680, 27.4670, 27.4656, 27.4621, 27.4644, 27.4620, 27.4655,\n",
            "        27.4651, 27.4587, 27.4607, 27.4691, 27.4621, 27.4673, 27.4615, 27.4632,\n",
            "        27.4659, 27.4646, 27.4612, 27.4707, 27.4638, 27.4633, 27.4612, 27.4615,\n",
            "        27.4676, 27.4683, 27.4633, 27.4680, 27.4633, 27.4673, 27.4630, 27.4619,\n",
            "        27.4662, 27.4620, 27.4699, 27.4616, 27.4632, 27.4630, 27.4569, 27.4675,\n",
            "        27.4633, 27.4627, 27.4633, 27.4665, 27.4582, 27.4674, 27.4665, 27.4677,\n",
            "        27.4611, 27.4649, 27.4670, 27.4662, 27.4647, 27.4713, 27.4646, 27.4634,\n",
            "        27.4640, 27.4677, 27.4649, 27.4655, 27.4653, 27.4657, 27.4594, 27.4619,\n",
            "        27.4647, 27.4628, 27.4637, 27.4618, 27.4678, 27.4655, 27.4623, 27.4599,\n",
            "        27.4596, 27.4643, 27.4640, 27.4612, 27.4647, 27.4645, 27.4580, 27.4617,\n",
            "        27.4675, 27.4553, 27.4642, 27.4675, 27.4646, 27.4609, 27.4580, 27.4665,\n",
            "        27.4578, 27.4616, 27.4671, 27.4589, 27.4644, 27.4668, 27.4640, 27.4616],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0095, 0.0094, 0.0094, 0.0095, 0.0094, 0.0094, 0.0094, 0.0095, 0.0094,\n",
            "        0.0095, 0.0094, 0.0094, 0.0095, 0.0095, 0.0095, 0.0095, 0.0094, 0.0093,\n",
            "        0.0095, 0.0095, 0.0094, 0.0095, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0095, 0.0095, 0.0093, 0.0095, 0.0093, 0.0095, 0.0095, 0.0095,\n",
            "        0.0094, 0.0093, 0.0094, 0.0095, 0.0095, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0095, 0.0094, 0.0093, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0091, 0.0094, 0.0094, 0.0095, 0.0095, 0.0094, 0.0094, 0.0095, 0.0095,\n",
            "        0.0095, 0.0094, 0.0095, 0.0094, 0.0095, 0.0095, 0.0095, 0.0093, 0.0094,\n",
            "        0.0094, 0.0094, 0.0095, 0.0094, 0.0094, 0.0094, 0.0095, 0.0095, 0.0093,\n",
            "        0.0095, 0.0095, 0.0094, 0.0094, 0.0095, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0095, 0.0095, 0.0095, 0.0094, 0.0093, 0.0094, 0.0095,\n",
            "        0.0094, 0.0094, 0.0095, 0.0093, 0.0095, 0.0094, 0.0095],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [13]\n",
            "DEBUGGING: logits looks like: tensor([27.4702, 27.4697, 27.4689, 27.4723, 27.4698, 27.4687, 27.4681, 27.4705,\n",
            "        27.4688, 27.4713, 27.4690, 27.4689, 27.4719, 27.4705, 27.4702, 27.4720,\n",
            "        27.4698, 27.4652, 27.4702, 27.4705, 27.4674, 27.4706, 27.4684, 27.4686,\n",
            "        27.4687, 27.4696, 27.4695, 27.4694, 27.4712, 27.4702, 27.4669, 27.4705,\n",
            "        27.4664, 27.4714, 27.4702, 27.4705, 27.4690, 27.4662, 27.4690, 27.4714,\n",
            "        27.4714, 27.4692, 27.4697, 27.4688, 27.4698, 27.4724, 27.4683, 27.4670,\n",
            "        27.4687, 27.4688, 27.4690, 27.4684, 27.4682, 27.4678, 27.4615, 27.4693,\n",
            "        27.4696, 27.4709, 27.4718, 27.4689, 27.4674, 27.4708, 27.4705, 27.4700,\n",
            "        27.4688, 27.4701, 27.4681, 27.4701, 27.4707, 27.4707, 27.4646, 27.4695,\n",
            "        27.4688, 27.4686, 27.4706, 27.4685, 27.4698, 27.4691, 27.4712, 27.4703,\n",
            "        27.4670, 27.4699, 27.4709, 27.4690, 27.4691, 27.4718, 27.4675, 27.4693,\n",
            "        27.4687, 27.4693, 27.4687, 27.4692, 27.4708, 27.4699, 27.4708, 27.4692,\n",
            "        27.4659, 27.4698, 27.4714, 27.4691, 27.4684, 27.4701, 27.4668, 27.4703,\n",
            "        27.4695, 27.4722], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.11290771943049549 and immediate abs rewards look like: [0.0004934376884193625, 0.008662220323458314, 0.008466904205306491, 0.0019533291433617705, 0.0011144204113406886, 0.008272368726920831, 0.0012675271113948838, 0.0010580647908682295, 0.008123727795918967, 0.011746622660666617, 0.0015005248669694993, 0.0018894693630500115, 0.00015575982433801983, 0.00010943266852336819, 0.002962256010960118, 0.0024664122079229855, 0.003877980809647852, 0.0021022350842940796, 0.0009841541905188933, 0.034104242532066564, 4.547473508864641e-13, 9.094947017729282e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.00011700714230755693, 0.0009969139023269236, 0.0005170110262042726, 0.0008888607535482151, 0.009076836181066028]\n",
            "DEBUGGING: the total relative reward of the trajectory = 6.005314351879174 and immediate relative rewards look like: [0.0015910615672972772, 0.055870556050523566, 0.08214565267235316, 0.02533756733858908, 0.018081054006234464, 0.1611175666463288, 0.02887917838720472, 0.027562016924844508, 0.23815311643366555, 0.38363846470938434, 0.05411458513048594, 0.07437264988860637, 0.006645997953960254, 0.005028733615456127, 0.14585222328580946, 0.12966039491871448, 0.21678456543369867, 0.12458958328467383, 0.06160917632597078, 2.2480600405215725, 3.183231456205249e-11, 6.669627813002485e-11, 6.972792713591393e-11, 3.637978807092265e-11, 3.789561257387201e-11, 3.9411437076820915e-11, 4.0927261579781764e-11, 4.244308608273022e-11, 0.0, 4.5474735088646405e-11, 9.398111918318834e-11, 4.850638409456353e-11, 0.0, 0.0, 5.305385760342081e-11, 5.456968210636742e-11, 0.0, 5.760133111228545e-11, 5.91171556152493e-11, 0.0, 0.0, 6.366462912410498e-11, 6.518045362704997e-11, 0.0, 0.0, 0.017941095153825394, 0.15618926978459025, 0.08275249077300628, 0.14525955923746056, 1.5140777509284558]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 3\n",
            "DEBUGGING: the action_prob is: tensor([0.0244, 0.0135, 0.0134, 0.0141, 0.0140, 0.0136, 0.0150, 0.0139, 0.0193,\n",
            "        0.0216, 0.0135, 0.0139, 0.0134, 0.0170, 0.0133, 0.0151, 0.0138, 0.0171,\n",
            "        0.0137, 0.0139, 0.0143, 0.0138, 0.0154, 0.0149, 0.0143, 0.0136, 0.0130,\n",
            "        0.0127, 0.0135, 0.0146, 0.0132, 0.0159, 0.0132, 0.0139, 0.0148, 0.0139,\n",
            "        0.0123, 0.0152, 0.0151, 0.0159, 0.0131, 0.0110, 0.0139, 0.0120, 0.0141,\n",
            "        0.0140, 0.0132, 0.0147, 0.0155, 0.0146, 0.0151, 0.0138, 0.0164, 0.0142,\n",
            "        0.0141, 0.0145, 0.0142, 0.0143, 0.0141, 0.0134, 0.0143, 0.0138, 0.0146,\n",
            "        0.0140, 0.0148, 0.0150, 0.0159, 0.0128, 0.0134],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [53]\n",
            "DEBUGGING: logits looks like: tensor([27.3457, 27.1982, 27.1958, 27.2083, 27.2070, 27.1996, 27.2240, 27.2057,\n",
            "        27.2879, 27.3152, 27.1985, 27.2051, 27.1958, 27.2559, 27.1946, 27.2261,\n",
            "        27.2043, 27.2576, 27.2020, 27.2051, 27.2125, 27.2039, 27.2318, 27.2222,\n",
            "        27.2121, 27.2007, 27.1892, 27.1834, 27.1988, 27.2171, 27.1934, 27.2397,\n",
            "        27.1931, 27.2058, 27.2213, 27.2051, 27.1743, 27.2272, 27.2261, 27.2397,\n",
            "        27.1916, 27.1473, 27.2063, 27.1684, 27.2093, 27.2081, 27.1921, 27.2196,\n",
            "        27.2331, 27.2175, 27.2269, 27.2032, 27.2469, 27.2113, 27.2088, 27.2163,\n",
            "        27.2116, 27.2122, 27.2082, 27.1961, 27.2134, 27.2046, 27.2184, 27.2079,\n",
            "        27.2205, 27.2245, 27.2395, 27.1850, 27.1971], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0215, 0.0128, 0.0126, 0.0127, 0.0128, 0.0126, 0.0128, 0.0128, 0.0127,\n",
            "        0.0129, 0.0126, 0.0127, 0.0129, 0.0128, 0.0129, 0.0125, 0.0122, 0.0128,\n",
            "        0.0128, 0.0126, 0.0128, 0.0128, 0.0126, 0.0130, 0.0127, 0.0124, 0.0126,\n",
            "        0.0126, 0.0128, 0.0124, 0.0128, 0.0128, 0.0126, 0.0124, 0.0129, 0.0127,\n",
            "        0.0124, 0.0127, 0.0129, 0.0126, 0.0126, 0.0128, 0.0129, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0126, 0.0127, 0.0128, 0.0128, 0.0127, 0.0128,\n",
            "        0.0128, 0.0126, 0.0127, 0.0128, 0.0127, 0.0128, 0.0128, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0125,\n",
            "        0.0127, 0.0127, 0.0126, 0.0127, 0.0127, 0.0127],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [77]\n",
            "DEBUGGING: logits looks like: tensor([27.3368, 27.2070, 27.2045, 27.2058, 27.2076, 27.2033, 27.2076, 27.2072,\n",
            "        27.2050, 27.2086, 27.2032, 27.2051, 27.2098, 27.2085, 27.2095, 27.2020,\n",
            "        27.1950, 27.2075, 27.2067, 27.2040, 27.2077, 27.2074, 27.2037, 27.2107,\n",
            "        27.2053, 27.1992, 27.2037, 27.2045, 27.2079, 27.2002, 27.2067, 27.2072,\n",
            "        27.2043, 27.2000, 27.2089, 27.2059, 27.2004, 27.2060, 27.2104, 27.2042,\n",
            "        27.2039, 27.2074, 27.2091, 27.2052, 27.2052, 27.2064, 27.2058, 27.2061,\n",
            "        27.2040, 27.2048, 27.2079, 27.2073, 27.2048, 27.2080, 27.2071, 27.2044,\n",
            "        27.2060, 27.2072, 27.2058, 27.2068, 27.2073, 27.2066, 27.2047, 27.2054,\n",
            "        27.2061, 27.2080, 27.2069, 27.2071, 27.2072, 27.2074, 27.2082, 27.2018,\n",
            "        27.2050, 27.2061, 27.2033, 27.2064, 27.2053, 27.2059],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0196, 0.0116, 0.0117, 0.0117, 0.0117, 0.0113, 0.0116, 0.0117, 0.0117,\n",
            "        0.0117, 0.0117, 0.0117, 0.0117, 0.0115, 0.0117, 0.0117, 0.0117, 0.0114,\n",
            "        0.0117, 0.0116, 0.0115, 0.0117, 0.0117, 0.0116, 0.0117, 0.0117, 0.0117,\n",
            "        0.0117, 0.0117, 0.0116, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117,\n",
            "        0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0116, 0.0117, 0.0117, 0.0117,\n",
            "        0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0115, 0.0117, 0.0117,\n",
            "        0.0117, 0.0118, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117,\n",
            "        0.0117, 0.0117, 0.0117, 0.0115, 0.0117, 0.0117, 0.0116, 0.0117, 0.0117,\n",
            "        0.0117, 0.0117, 0.0116, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117,\n",
            "        0.0117, 0.0117, 0.0117, 0.0117], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [84]\n",
            "DEBUGGING: logits looks like: tensor([27.3303, 27.1999, 27.2023, 27.2021, 27.2016, 27.1923, 27.2005, 27.2017,\n",
            "        27.2016, 27.2012, 27.2018, 27.2024, 27.2018, 27.1964, 27.2021, 27.2022,\n",
            "        27.2011, 27.1948, 27.2011, 27.2006, 27.1966, 27.2019, 27.2011, 27.2004,\n",
            "        27.2015, 27.2014, 27.2016, 27.2019, 27.2017, 27.2003, 27.2007, 27.2015,\n",
            "        27.2017, 27.2012, 27.2010, 27.2007, 27.2008, 27.2012, 27.2011, 27.2014,\n",
            "        27.2010, 27.2001, 27.2023, 27.2023, 27.2021, 27.2009, 27.2014, 27.2009,\n",
            "        27.2013, 27.2009, 27.2027, 27.1983, 27.2020, 27.2014, 27.2014, 27.2030,\n",
            "        27.2016, 27.2021, 27.2017, 27.2020, 27.2015, 27.2026, 27.2013, 27.2022,\n",
            "        27.2018, 27.2013, 27.1980, 27.2019, 27.2019, 27.2004, 27.2009, 27.2006,\n",
            "        27.2019, 27.2022, 27.2004, 27.2012, 27.2021, 27.2019, 27.2011, 27.2014,\n",
            "        27.2012, 27.2012, 27.2018, 27.2019, 27.2013], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0168, 0.0101, 0.0102, 0.0102, 0.0103, 0.0099, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0107, 0.0103, 0.0103, 0.0100, 0.0102, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0102, 0.0100, 0.0101, 0.0102, 0.0101, 0.0100, 0.0102,\n",
            "        0.0101, 0.0101, 0.0102, 0.0101, 0.0101, 0.0101, 0.0102, 0.0102, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0102, 0.0102, 0.0102,\n",
            "        0.0101, 0.0101, 0.0101, 0.0102, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0102, 0.0102, 0.0101, 0.0101, 0.0100, 0.0102, 0.0101, 0.0101, 0.0102,\n",
            "        0.0101, 0.0101, 0.0100, 0.0101, 0.0101, 0.0102, 0.0102, 0.0101, 0.0102,\n",
            "        0.0102, 0.0101, 0.0102, 0.0101, 0.0101, 0.0101, 0.0101, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0101, 0.0103, 0.0102, 0.0102, 0.0101, 0.0102, 0.0101,\n",
            "        0.0101, 0.0100, 0.0097, 0.0102, 0.0101, 0.0102, 0.0101, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [89]\n",
            "DEBUGGING: logits looks like: tensor([27.3254, 27.1991, 27.1995, 27.1999, 27.2035, 27.1936, 27.1987, 27.1992,\n",
            "        27.1975, 27.1989, 27.2117, 27.2030, 27.2024, 27.1963, 27.1995, 27.1987,\n",
            "        27.1989, 27.1993, 27.1972, 27.1981, 27.1997, 27.1964, 27.1985, 27.1994,\n",
            "        27.1980, 27.1951, 27.1994, 27.1974, 27.1991, 27.1997, 27.1969, 27.1992,\n",
            "        27.1981, 27.2015, 27.1993, 27.1983, 27.1979, 27.1980, 27.1987, 27.1986,\n",
            "        27.1979, 27.1988, 27.1996, 27.1994, 27.1995, 27.1991, 27.1989, 27.1991,\n",
            "        27.1993, 27.1989, 27.1980, 27.1980, 27.1985, 27.1986, 27.1995, 27.2013,\n",
            "        27.1971, 27.1989, 27.1964, 27.2005, 27.1978, 27.1980, 27.2003, 27.1979,\n",
            "        27.1991, 27.1961, 27.1990, 27.1991, 27.1993, 27.1999, 27.1991, 27.2008,\n",
            "        27.2001, 27.1987, 27.1997, 27.1991, 27.1992, 27.1992, 27.1982, 27.2002,\n",
            "        27.1994, 27.1998, 27.1994, 27.1976, 27.2033, 27.1998, 27.1998, 27.1979,\n",
            "        27.2000, 27.1984, 27.1989, 27.1956, 27.1868, 27.2003, 27.1973, 27.2013,\n",
            "        27.1989, 27.1973], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0097, 0.0098, 0.0087, 0.0085, 0.0092, 0.0098, 0.0099, 0.0095, 0.0092,\n",
            "        0.0095, 0.0084, 0.0098, 0.0092, 0.0089, 0.0095, 0.0088, 0.0090, 0.0092,\n",
            "        0.0095, 0.0088, 0.0095, 0.0090, 0.0099, 0.0086, 0.0098, 0.0087, 0.0090,\n",
            "        0.0090, 0.0092, 0.0090, 0.0099, 0.0091, 0.0099, 0.0107, 0.0096, 0.0091,\n",
            "        0.0082, 0.0102, 0.0094, 0.0103, 0.0100, 0.0119, 0.0101, 0.0093, 0.0093,\n",
            "        0.0097, 0.0091, 0.0086, 0.0083, 0.0088, 0.0104, 0.0099, 0.0097, 0.0088,\n",
            "        0.0092, 0.0092, 0.0089, 0.0083, 0.0112, 0.0098, 0.0087, 0.0095, 0.0102,\n",
            "        0.0089, 0.0096, 0.0094, 0.0105, 0.0080, 0.0076, 0.0109, 0.0101, 0.0089,\n",
            "        0.0092, 0.0091, 0.0082, 0.0083, 0.0104, 0.0090, 0.0091, 0.0091, 0.0087,\n",
            "        0.0082, 0.0090, 0.0099, 0.0091, 0.0081, 0.0079, 0.0104, 0.0086, 0.0087,\n",
            "        0.0100, 0.0088, 0.0104, 0.0100, 0.0120, 0.0114, 0.0096, 0.0082, 0.0085,\n",
            "        0.0082, 0.0085, 0.0083, 0.0096, 0.0113, 0.0098, 0.0098, 0.0087],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [51]\n",
            "DEBUGGING: logits looks like: tensor([27.3064, 27.3095, 27.2806, 27.2727, 27.2930, 27.3084, 27.3109, 27.3011,\n",
            "        27.2921, 27.3007, 27.2717, 27.3089, 27.2921, 27.2845, 27.3024, 27.2813,\n",
            "        27.2879, 27.2921, 27.3023, 27.2830, 27.3013, 27.2871, 27.3105, 27.2758,\n",
            "        27.3076, 27.2795, 27.2867, 27.2877, 27.2922, 27.2876, 27.3121, 27.2913,\n",
            "        27.3116, 27.3315, 27.3046, 27.2901, 27.2651, 27.3189, 27.2975, 27.3220,\n",
            "        27.3134, 27.3574, 27.3172, 27.2967, 27.2961, 27.3051, 27.2900, 27.2756,\n",
            "        27.2675, 27.2825, 27.3249, 27.3121, 27.3072, 27.2830, 27.2932, 27.2934,\n",
            "        27.2851, 27.2667, 27.3423, 27.3079, 27.2782, 27.3011, 27.3197, 27.2844,\n",
            "        27.3039, 27.2994, 27.3258, 27.2578, 27.2448, 27.3360, 27.3171, 27.2854,\n",
            "        27.2922, 27.2915, 27.2652, 27.2681, 27.3231, 27.2882, 27.2910, 27.2899,\n",
            "        27.2783, 27.2636, 27.2879, 27.3116, 27.2915, 27.2599, 27.2555, 27.3249,\n",
            "        27.2775, 27.2797, 27.3128, 27.2822, 27.3226, 27.3150, 27.3586, 27.3457,\n",
            "        27.3049, 27.2654, 27.2731, 27.2638, 27.2727, 27.2666, 27.3032, 27.3439,\n",
            "        27.3093, 27.3084, 27.2782], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.7082653196216597 and immediate abs rewards look like: [0.0026625325203895045, 0.04640755680247821, 0.0017011765617098717, 0.0004058756171616551, 0.012228838774717588, 0.0001647415306251787, 0.0023793800801286125, 0.0003624991459219018, 0.010654279797108757, 0.011191235663773114, 2.7614491955318954e-06, 2.2718437776347855e-05, 0.00028170160430818214, 1.554533446324058e-05, 1.1363945304765366e-05, 0.000964783906511002, 0.004392373589780618, 0.0006788108362343337, 0.0019694110337695747, 0.001927973810779804, 6.343755558191333e-06, 0.00018821776120603317, 0.0010895462073676754, 0.0014135271603663568, 0.00031451914492208743, 0.0002135195531991485, 0.00011406270459701773, 9.157115300695295e-05, 0.0006008713094161067, 0.0007614220335199207, 4.285758677724516e-06, 0.0022805210792284925, 0.0008080873208200501, 0.001083548980204796, 1.3041299553151475e-05, 2.6068961687997216e-05, 6.950253509785398e-05, 0.0001866604898168589, 0.001378810787173279, 7.384062882920261e-05, 0.0001646683654143999, 0.0004288019927116693, 0.598528320195328, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 74.07226779608163 and immediate relative rewards look like: [0.0071799946630098695, 0.2504723763919855, 0.013947136908339878, 0.004438838299651089, 0.16719365488488735, 0.002711899395078275, 0.045698370916717204, 0.007961950007117953, 0.26328875079737224, 0.3081882652284406, 8.39089547531144e-05, 0.0007530756576456985, 0.010116127579379346, 0.0006012339941008499, 0.0004709097480458648, 0.04264505543590291, 0.2063396130689489, 0.03380517130518365, 0.10354601939855843, 0.10676067528764047, 0.00036904371276680814, 0.01147086380452405, 0.06942389458580363, 0.09401171860859978, 0.021798361007449585, 0.015391665449310072, 0.0085390145744172, 0.0071093643918508295, 0.04831751091497145, 0.06334963942497966, 0.00036853486244577425, 0.20242947825167443, 0.0740178962326765, 0.10227968722625584, 0.0012675978873284515, 0.0026062759663711957, 0.007141668355778201, 0.01969888701640368, 0.1493473376718871, 0.008206347906123351, 0.01875847324675334, 0.05004139555555709, 71.52012011121846, 6.669627813001473e-11, 0.0, 6.972792713593507e-11, 0.0, 0.0, 7.427540064478913e-11, 7.579122514773254e-11]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 200 number of matrices\n",
            "DEBUGGING: ACT_MAT has 200 number of matrices\n",
            "DEBUGGING: VAL looks like: [[2.6880006821264057, 2.511581347828063, 2.4867552431402253, 2.3754452848697514, 2.397840015359017, 2.393699561969071, 1.9529155203389152, 1.9297803117622958, 1.7306060682551048, 1.7461898780033729, 1.4263965419918918, 1.3328601801500388, 1.1741326045234066, 1.1706085219478135, 1.0367005193105674, 1.046834144579399, 0.8247636273929064, 0.7265160989492266, 0.714444654427332, 0.5548698937695056, 0.5383826091806728, 0.45650994732246775, 0.430226434723456, 0.42555355802164496, 0.41637328649268673, 0.3944989337914096, 0.3963428293000152, 0.3324374919849255, 0.33538055028121405, 0.33739606506785175, 0.3407698417178584, 0.3437923802208936, 0.34460238997858955, 0.3436133447266178, 0.33236730056643593, 0.3328256967403844, 0.3299629861308986, 0.3105542659792621, 0.2685733427771802, 0.27021758967752985, 0.23502415865005621, 0.23230597416508628, 0.23069165260457092, 0.22198971488813138, 0.17862769899497313, 0.12725928018259458, 0.12673054367414727, 0.12216615041613453, 0.010516265849922775, 0.010024801323741108], [32.17391435057893, 32.47853405321691, 32.60533410874885, 32.931695085945535, 33.21574809178264, 33.48631767281356, 33.51183346492174, 33.75131169940341, 33.914018996798085, 34.241289414435116, 34.582401057612046, 34.684562534010425, 2.339040236807048e-09, 2.3328097565650855e-09, 2.3563734914798843e-09, 2.380175243919075e-09, 2.3674701574223817e-09, 2.3132960684562274e-09, 2.2953220271479342e-09, 2.3185070981292264e-09, 2.3419263617466935e-09, 2.317351403943045e-09, 2.340758993881864e-09, 2.2587546496747486e-09, 2.226449462190263e-09, 2.1915212558883384e-09, 2.1539435356294053e-09, 2.051678536252642e-09, 2.008094855685391e-09, 1.96177423212812e-09, 1.9126890196920714e-09, 1.932009110800072e-09, 1.8780298329881025e-09, 1.8969998313011136e-09, 1.9161614457587007e-09, 1.9355166118774754e-09, 1.9550672847247227e-09, 1.889837398798714e-09, 1.6471024331246168e-09, 1.5741683835371682e-09, 1.4982009225857536e-09, 1.513334265238135e-09, 1.52862046993751e-09, 1.5440610807449597e-09, 1.256492756727165e-09, 1.0624812614425418e-09, 8.619166464997181e-10, 4.3884256228943336e-10, 1.1254996934439987e-10, 1.1368683772161603e-10], [4.694364378073297, 4.740175067177778, 4.731620718310358, 4.696439460240409, 4.718284740304869, 4.747680491210741, 4.632891842994356, 4.650517843037527, 4.6696523496087705, 4.4762618516920245, 4.133963017154183, 4.1210590222461585, 4.087561992280356, 4.12213736800646, 4.158695590293944, 4.053377138392055, 3.9633502459326677, 3.784409778281787, 3.69678807575466, 3.671897878210797, 1.4382200380699237, 1.4527475131697893, 1.4674217304071648, 1.482244172058017, 1.4972163353753911, 1.5123397326641368, 1.5276158915401268, 1.5430463550496964, 1.5586326818255083, 1.5743764462883922, 1.5902792386292097, 1.6063426651870998, 1.622568348624842, 1.6389579279038806, 1.6555130584887683, 1.672235412561328, 1.6891266792997557, 1.706188564949248, 1.7234227928198453, 1.7408311037987154, 1.7584152563623388, 1.776177026628625, 1.794118208651475, 1.812240614733631, 1.8305460754885161, 1.84903643988739, 1.8495912573066309, 1.7105070581030715, 1.6441965326566317, 1.5140777509284558], [49.044982740210294, 49.533134086411394, 49.78046637375698, 50.26921135035216, 50.77249748692173, 51.11646851720893, 51.63005718971096, 52.105412948277014, 52.623687877040304, 52.88929204671003, 53.1122260419006, 53.64862841711702, 54.18977307218118, 54.72692620666849, 55.27911613401453, 55.83701537804696, 56.35794982081924, 56.71879818964676, 57.257568705395535, 57.73133604646159, 58.2066417890646, 58.79421489429478, 59.37650912170733, 59.906146694062144, 60.416297955003586, 61.00454504444054, 61.60520543332448, 62.21885496843441, 62.84014707479046, 63.4260904685611, 64.00276851427891, 64.64888886809744, 65.09743372711694, 65.68021801099421, 66.24034174117975, 66.90815570029537, 67.58136305487777, 68.25678927931516, 68.9263539316149, 69.47172383226567, 70.16516917612076, 70.85496030593335, 71.52012011149272, 2.7702772045684725e-10, 2.124560023503359e-10, 2.1460202257609687e-10, 1.4633747014157755e-10, 1.478156264056339e-10, 1.4930871354104434e-10, 7.579122514773254e-11]]\n",
            "DEBUGGING: traj_returns = [2.6880006821264057, 32.17391435057893, 4.694364378073297, 49.044982740210294]\n",
            "DEBUGGING: actions = [[52], [40], [26], [2], [26], [27], [12], [8], [65], [29], [6], [21], [19], [41], [43], [42], [12], [69], [23], [67], [44], [54], [74], [33], [27], [75], [64], [77], [49], [88], [49], [86], [22], [91], [72], [33], [5], [33], [8], [93], [68], [87], [96], [50], [78], [57], [104], [1], [103], [61], [28], [24], [16], [13], [46], [26], [19], [64], [33], [27], [40], [0], [60], [59], [28], [32], [19], [57], [2], [29], [48], [48], [31], [19], [62], [10], [49], [58], [61], [75], [87], [58], [59], [26], [81], [64], [80], [83], [79], [12], [27], [3], [63], [22], [67], [65], [78], [28], [92], [31], [34], [2], [52], [20], [43], [61], [43], [52], [48], [2], [35], [32], [33], [53], [37], [8], [19], [37], [48], [0], [53], [31], [72], [36], [74], [15], [11], [40], [65], [32], [56], [36], [74], [52], [34], [86], [84], [68], [11], [29], [6], [41], [41], [94], [35], [30], [3], [81], [40], [13], [58], [39], [50], [16], [29], [41], [60], [1], [16], [53], [4], [56], [45], [7], [17], [2], [37], [20], [26], [77], [77], [45], [26], [18], [73], [52], [38], [80], [61], [84], [53], [9], [48], [31], [40], [67], [50], [29], [83], [89], [2], [74], [0], [50], [16], [23], [5], [5], [59], [51]]\n",
            "DEBUGGING: actions length = 200\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[ 3.0775,  1.0513,  0.0300,  ...,  2.3756, -0.0639,  1.7009],\n",
            "        [ 3.0041,  0.9926,  0.0397,  ...,  2.2966, -0.1557,  1.6008],\n",
            "        [ 3.0556,  1.0336,  0.0466,  ...,  2.3509, -0.0960,  1.6751],\n",
            "        ...,\n",
            "        [ 3.0139,  0.9839,  0.0526,  ...,  2.2956, -0.1649,  1.6115],\n",
            "        [ 3.0169,  0.9873,  0.0513,  ...,  2.2995, -0.1599,  1.6156],\n",
            "        [ 3.0214,  0.9928,  0.0506,  ...,  2.3049, -0.1549,  1.6206]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[ 3.0212,  0.9913,  0.0499,  ...,  2.3043, -0.1546,  1.6207],\n",
            "        [ 3.0206,  0.9919,  0.0493,  ...,  2.3046, -0.1523,  1.6213],\n",
            "        [ 3.0183,  0.9900,  0.0505,  ...,  2.3021, -0.1564,  1.6179],\n",
            "        ...,\n",
            "        [ 3.0210,  0.9927,  0.0497,  ...,  2.3048, -0.1520,  1.6222],\n",
            "        [ 3.0201,  0.9918,  0.0482,  ...,  2.3043, -0.1519,  1.6205],\n",
            "        [ 3.0190,  0.9889,  0.0500,  ...,  2.3016, -0.1552,  1.6195]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([27.3064, 27.3095, 27.2806, 27.2727, 27.2930, 27.3084, 27.3109, 27.3011,\n",
            "        27.2921, 27.3007, 27.2717, 27.3089, 27.2921, 27.2845, 27.3024, 27.2813,\n",
            "        27.2879, 27.2921, 27.3023, 27.2830, 27.3013, 27.2871, 27.3105, 27.2758,\n",
            "        27.3076, 27.2795, 27.2867, 27.2877, 27.2922, 27.2876, 27.3121, 27.2913,\n",
            "        27.3116, 27.3315, 27.3046, 27.2901, 27.2651, 27.3189, 27.2975, 27.3220,\n",
            "        27.3134, 27.3574, 27.3172, 27.2967, 27.2961, 27.3051, 27.2900, 27.2756,\n",
            "        27.2675, 27.2825, 27.3249, 27.3121, 27.3072, 27.2830, 27.2932, 27.2934,\n",
            "        27.2851, 27.2667, 27.3423, 27.3079, 27.2782, 27.3011, 27.3197, 27.2844,\n",
            "        27.3039, 27.2994, 27.3258, 27.2578, 27.2448, 27.3360, 27.3171, 27.2854,\n",
            "        27.2922, 27.2915, 27.2652, 27.2681, 27.3231, 27.2882, 27.2910, 27.2899,\n",
            "        27.2783, 27.2636, 27.2879, 27.3116, 27.2915, 27.2599, 27.2555, 27.3249,\n",
            "        27.2775, 27.2797, 27.3128, 27.2822, 27.3226, 27.3150, 27.3586, 27.3457,\n",
            "        27.3049, 27.2654, 27.2731, 27.2638, 27.2727, 27.2666, 27.3032, 27.3439,\n",
            "        27.3093, 27.3084, 27.2782], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[22.15031554 22.31585614 22.40104411 22.5681978  22.77609258 22.93604156\n",
            "  22.9319245  23.1092557  23.23449132 23.3382583  23.31374666 23.44677754\n",
            "  14.86286692 15.00491802 15.11862806 15.23430667 15.28651592 15.30743102\n",
            "  15.41720036 15.48952596 15.04581111 15.17586809 15.31853932 15.45348611\n",
            "  15.58247189 15.72784593 15.88229104 16.0235847  16.18354008 16.33446575\n",
            "  16.4834544  16.64975598 16.76615112 16.91569732 17.05705553 17.2283042\n",
            "  17.40011318 17.56838303 17.72958752 17.87069313 18.03965215 18.21586083\n",
            "  18.38623249  0.50855758  0.50229344  0.49407393  0.49408045  0.4581683\n",
            "   0.4136782   0.38102564]]\n",
            "DEBUGGING: baseline2 looks like: 22.150315537747232\n",
            "DEBUGGING: ADS looks like: [-19.46231486 -19.80427479 -19.91428887 -20.19275251 -20.37825257\n",
            " -20.542342   -20.97900898 -21.17947539 -21.50388525 -21.59206842\n",
            " -21.88735012 -22.11391736 -13.68873431 -13.8343095  -14.08192754\n",
            " -14.18747252 -14.4617523  -14.58091492 -14.70275571 -14.93465606\n",
            " -14.5074285  -14.71935814 -14.88831289 -15.02793255 -15.16609861\n",
            " -15.33334699 -15.48594821 -15.69114721 -15.84815953 -15.99706968\n",
            " -16.14268456 -16.3059636  -16.42154873 -16.57208398 -16.72468822\n",
            " -16.89547851 -17.07015019 -17.25782876 -17.46101417 -17.60047554\n",
            " -17.80462799 -17.98355485 -18.15554084  -0.28656787  -0.32366574\n",
            "  -0.36681465  -0.36734991  -0.33600215  -0.40316193  -0.37100084\n",
            "  10.02359881  10.16267791  10.20429     10.36349729  10.43965551\n",
            "  10.55027611  10.57990896  10.642056    10.67952767  10.90303112\n",
            "  11.26865439  11.237785   -14.86286692 -15.00491802 -15.11862806\n",
            " -15.23430666 -15.28651592 -15.30743101 -15.41720036 -15.48952595\n",
            " -15.04581111 -15.17586809 -15.31853932 -15.4534861  -15.58247189\n",
            " -15.72784593 -15.88229104 -16.0235847  -16.18354008 -16.33446574\n",
            " -16.4834544  -16.64975598 -16.76615112 -16.91569732 -17.05705552\n",
            " -17.2283042  -17.40011318 -17.56838303 -17.72958752 -17.87069313\n",
            " -18.03965215 -18.21586083 -18.38623249  -0.50855758  -0.50229344\n",
            "  -0.49407393  -0.49408045  -0.4581683   -0.4136782   -0.38102564\n",
            " -17.45595116 -17.57568107 -17.66942339 -17.87175834 -18.05780784\n",
            " -18.18836107 -18.29903266 -18.45873786 -18.56483897 -18.86199645\n",
            " -19.17978365 -19.32571852 -10.77530493 -10.88278066 -10.95993247\n",
            " -11.18092953 -11.32316568 -11.52302124 -11.72041228 -11.81762808\n",
            " -13.60759107 -13.72312058 -13.85111759 -13.97124193 -14.08525556\n",
            " -14.2155062  -14.35467515 -14.48053835 -14.6249074  -14.7600893\n",
            " -14.89317516 -15.04341331 -15.14358277 -15.27673939 -15.40154247\n",
            " -15.55606879 -15.7109865  -15.86219446 -16.00616472 -16.12986203\n",
            " -16.28123689 -16.4396838  -16.59211428   1.30368303   1.32825263\n",
            "   1.35496251   1.35551081   1.25233876   1.23051833   1.13305211\n",
            "  26.8946672   27.21727795  27.37942226  27.70101356  27.9964049\n",
            "  28.18042696  28.69813269  28.99615725  29.38919655  29.55103375\n",
            "  29.79847938  30.20185088  39.32690615  39.72200818  40.16048807\n",
            "  40.60270871  41.0714339   41.41136717  41.84036835  42.24181009\n",
            "  43.16083068  43.61834681  44.0579698   44.45266059  44.83382606\n",
            "  45.27669912  45.72291439  46.19527026  46.656607    47.09162472\n",
            "  47.51931412  47.99913289  48.33128261  48.76452069  49.18328622\n",
            "  49.6798515   50.18124987  50.68840625  51.19676641  51.6010307\n",
            "  52.12551703  52.63909948  53.13388762  -0.50855758  -0.50229344\n",
            "  -0.49407393  -0.49408045  -0.4581683   -0.4136782   -0.38102564]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(0.1369, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0301,  0.0411,  0.0139,  ...,  0.0242,  0.0112, -0.3360],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [ 0.0366,  0.0500,  0.0168,  ...,  0.0293,  0.0133, -0.4984],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0044, -0.0061, -0.0020,  ..., -0.0035, -0.0016,  0.0720]])\n",
            "   Last layer:\n",
            "tensor([[ 0.0114,  0.0343,  0.0000, -0.0801,  0.0000,  0.0000,  0.0000, -0.1575,\n",
            "         -0.0281,  0.0000, -0.0663,  0.0000,  0.0000, -0.0118,  0.0000, -0.0592,\n",
            "          0.0000, -0.0906, -0.1115,  0.0287],\n",
            "        [-0.0008, -0.0006,  0.0000, -0.0407,  0.0000,  0.0000,  0.0000, -0.1036,\n",
            "         -0.0311,  0.0000, -0.0671,  0.0000,  0.0000, -0.0090,  0.0000, -0.0568,\n",
            "          0.0000, -0.0709, -0.0672,  0.0083],\n",
            "        [-0.0019, -0.0050,  0.0000, -0.0073,  0.0000,  0.0000,  0.0000, -0.0250,\n",
            "         -0.0101,  0.0000, -0.0211,  0.0000,  0.0000, -0.0024,  0.0000, -0.0174,\n",
            "          0.0000, -0.0195, -0.0150, -0.0002],\n",
            "        [ 0.0019,  0.0062,  0.0000, -0.0284,  0.0000,  0.0000,  0.0000, -0.0636,\n",
            "         -0.0155,  0.0000, -0.0345,  0.0000,  0.0000, -0.0052,  0.0000, -0.0298,\n",
            "          0.0000, -0.0403, -0.0430,  0.0081],\n",
            "        [ 0.0038,  0.0106,  0.0000, -0.0100,  0.0000,  0.0000,  0.0000, -0.0114,\n",
            "          0.0024,  0.0000,  0.0034,  0.0000,  0.0000, -0.0004,  0.0000,  0.0020,\n",
            "          0.0000, -0.0026, -0.0102,  0.0058],\n",
            "        [ 0.0006,  0.0010,  0.0000,  0.0120,  0.0000,  0.0000,  0.0000,  0.0316,\n",
            "          0.0099,  0.0000,  0.0213,  0.0000,  0.0000,  0.0028,  0.0000,  0.0180,\n",
            "          0.0000,  0.0220,  0.0203, -0.0021],\n",
            "        [ 0.0109,  0.0324,  0.0000, -0.0696,  0.0000,  0.0000,  0.0000, -0.1336,\n",
            "         -0.0220,  0.0000, -0.0529,  0.0000,  0.0000, -0.0098,  0.0000, -0.0477,\n",
            "          0.0000, -0.0752, -0.0955,  0.0259],\n",
            "        [ 0.0051,  0.0167,  0.0000, -0.0695,  0.0000,  0.0000,  0.0000, -0.1538,\n",
            "         -0.0366,  0.0000, -0.0818,  0.0000,  0.0000, -0.0124,  0.0000, -0.0708,\n",
            "          0.0000, -0.0967, -0.1044,  0.0203],\n",
            "        [-0.0080, -0.0212,  0.0000, -0.0131,  0.0000,  0.0000,  0.0000, -0.0609,\n",
            "         -0.0296,  0.0000, -0.0605,  0.0000,  0.0000, -0.0064,  0.0000, -0.0494,\n",
            "          0.0000, -0.0519, -0.0340, -0.0047],\n",
            "        [ 0.0014,  0.0061,  0.0000, -0.0565,  0.0000,  0.0000,  0.0000, -0.1349,\n",
            "         -0.0367,  0.0000, -0.0803,  0.0000,  0.0000, -0.0113,  0.0000, -0.0686,\n",
            "          0.0000, -0.0890, -0.0893,  0.0140]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0385, -0.0350, -0.0313,  ...,  0.0613,  0.0027,  0.1948],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [-0.0306, -0.0279, -0.0250,  ...,  0.0486,  0.0021,  0.1315],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0101,  0.0092,  0.0082,  ..., -0.0164, -0.0008, -0.0855]])\n",
            "   Last layer:\n",
            "tensor([[ 0.0350,  0.0291,  0.0000,  0.0015,  0.0000,  0.0000,  0.0000,  0.1003,\n",
            "          0.0041,  0.0000,  0.0752,  0.0000,  0.0000, -0.0324,  0.0000,  0.0418,\n",
            "          0.0000,  0.0642,  0.0852,  0.0252],\n",
            "        [ 0.0149,  0.0233,  0.0000,  0.0036,  0.0000,  0.0000,  0.0000,  0.0540,\n",
            "          0.0120,  0.0000,  0.0501,  0.0000,  0.0000, -0.0082,  0.0000,  0.0339,\n",
            "          0.0000,  0.0410,  0.0411,  0.0097],\n",
            "        [-0.0024, -0.0112,  0.0000, -0.0026,  0.0000,  0.0000,  0.0000, -0.0163,\n",
            "         -0.0089,  0.0000, -0.0203,  0.0000,  0.0000, -0.0026,  0.0000, -0.0165,\n",
            "          0.0000, -0.0158, -0.0098, -0.0008],\n",
            "        [ 0.0137,  0.0167,  0.0000,  0.0020,  0.0000,  0.0000,  0.0000,  0.0448,\n",
            "          0.0065,  0.0000,  0.0382,  0.0000,  0.0000, -0.0100,  0.0000,  0.0242,\n",
            "          0.0000,  0.0318,  0.0358,  0.0094],\n",
            "        [ 0.0049,  0.0040,  0.0000,  0.0002,  0.0000,  0.0000,  0.0000,  0.0139,\n",
            "          0.0005,  0.0000,  0.0104,  0.0000,  0.0000, -0.0045,  0.0000,  0.0058,\n",
            "          0.0000,  0.0089,  0.0118,  0.0035],\n",
            "        [-0.0063, -0.0155,  0.0000, -0.0031,  0.0000,  0.0000,  0.0000, -0.0285,\n",
            "         -0.0104,  0.0000, -0.0304,  0.0000,  0.0000,  0.0005,  0.0000, -0.0227,\n",
            "          0.0000, -0.0243, -0.0197, -0.0035],\n",
            "        [ 0.0380,  0.0483,  0.0000,  0.0062,  0.0000,  0.0000,  0.0000,  0.1261,\n",
            "          0.0201,  0.0000,  0.1093,  0.0000,  0.0000, -0.0265,  0.0000,  0.0701,\n",
            "          0.0000,  0.0905,  0.0998,  0.0258],\n",
            "        [ 0.0289,  0.0310,  0.0000,  0.0031,  0.0000,  0.0000,  0.0000,  0.0900,\n",
            "          0.0099,  0.0000,  0.0737,  0.0000,  0.0000, -0.0232,  0.0000,  0.0448,\n",
            "          0.0000,  0.0618,  0.0734,  0.0202],\n",
            "        [ 0.0033,  0.0189,  0.0000,  0.0045,  0.0000,  0.0000,  0.0000,  0.0262,\n",
            "          0.0156,  0.0000,  0.0338,  0.0000,  0.0000,  0.0053,  0.0000,  0.0279,\n",
            "          0.0000,  0.0263,  0.0152,  0.0008],\n",
            "        [ 0.0189,  0.0162,  0.0000,  0.0009,  0.0000,  0.0000,  0.0000,  0.0547,\n",
            "          0.0026,  0.0000,  0.0414,  0.0000,  0.0000, -0.0173,  0.0000,  0.0233,\n",
            "          0.0000,  0.0353,  0.0463,  0.0136]])\n",
            "DEBUGGING: training for one iteration takes 0.004270 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 23\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0147, 0.0141, 0.0142, 0.0147, 0.0150, 0.0139, 0.0145, 0.0141, 0.0150,\n",
            "        0.0133, 0.0140, 0.0136, 0.0144, 0.0141, 0.0143, 0.0139, 0.0146, 0.0141,\n",
            "        0.0147, 0.0141, 0.0145, 0.0141, 0.0143, 0.0140, 0.0141, 0.0145, 0.0137,\n",
            "        0.0142, 0.0145, 0.0141, 0.0147, 0.0145, 0.0152, 0.0136, 0.0149, 0.0143,\n",
            "        0.0131, 0.0150, 0.0145, 0.0140, 0.0141, 0.0139, 0.0148, 0.0142, 0.0139,\n",
            "        0.0147, 0.0146, 0.0144, 0.0142, 0.0146, 0.0147, 0.0155, 0.0136, 0.0143,\n",
            "        0.0143, 0.0146, 0.0136, 0.0138, 0.0142, 0.0150, 0.0139, 0.0141, 0.0150,\n",
            "        0.0138, 0.0142, 0.0146, 0.0144, 0.0141, 0.0142, 0.0135],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [21]\n",
            "DEBUGGING: logits looks like: tensor([26.3599, 26.3503, 26.3524, 26.3598, 26.3657, 26.3458, 26.3569, 26.3501,\n",
            "        26.3650, 26.3361, 26.3486, 26.3404, 26.3558, 26.3501, 26.3529, 26.3470,\n",
            "        26.3587, 26.3498, 26.3602, 26.3494, 26.3573, 26.3492, 26.3529, 26.3489,\n",
            "        26.3495, 26.3561, 26.3431, 26.3513, 26.3562, 26.3502, 26.3604, 26.3577,\n",
            "        26.3687, 26.3404, 26.3644, 26.3538, 26.3310, 26.3656, 26.3562, 26.3477,\n",
            "        26.3507, 26.3466, 26.3622, 26.3520, 26.3460, 26.3608, 26.3583, 26.3547,\n",
            "        26.3524, 26.3589, 26.3604, 26.3740, 26.3400, 26.3538, 26.3542, 26.3593,\n",
            "        26.3408, 26.3448, 26.3519, 26.3648, 26.3463, 26.3501, 26.3650, 26.3436,\n",
            "        26.3521, 26.3591, 26.3544, 26.3503, 26.3513, 26.3388],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0129, 0.0125, 0.0124, 0.0125, 0.0125, 0.0124, 0.0126, 0.0124, 0.0126,\n",
            "        0.0125, 0.0125, 0.0124, 0.0125, 0.0125, 0.0125, 0.0124, 0.0125, 0.0126,\n",
            "        0.0125, 0.0127, 0.0125, 0.0125, 0.0125, 0.0124, 0.0126, 0.0125, 0.0124,\n",
            "        0.0124, 0.0125, 0.0125, 0.0124, 0.0126, 0.0125, 0.0126, 0.0125, 0.0125,\n",
            "        0.0125, 0.0124, 0.0125, 0.0127, 0.0125, 0.0125, 0.0126, 0.0125, 0.0126,\n",
            "        0.0125, 0.0125, 0.0126, 0.0126, 0.0124, 0.0125, 0.0124, 0.0126, 0.0125,\n",
            "        0.0125, 0.0126, 0.0121, 0.0125, 0.0125, 0.0126, 0.0124, 0.0126, 0.0124,\n",
            "        0.0125, 0.0126, 0.0126, 0.0125, 0.0124, 0.0125, 0.0123, 0.0125, 0.0124,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0124, 0.0125, 0.0124],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [1]\n",
            "DEBUGGING: logits looks like: tensor([26.3746, 26.3663, 26.3654, 26.3663, 26.3667, 26.3651, 26.3679, 26.3653,\n",
            "        26.3685, 26.3671, 26.3673, 26.3656, 26.3674, 26.3673, 26.3659, 26.3649,\n",
            "        26.3670, 26.3678, 26.3676, 26.3700, 26.3666, 26.3658, 26.3667, 26.3643,\n",
            "        26.3678, 26.3674, 26.3645, 26.3641, 26.3663, 26.3664, 26.3655, 26.3687,\n",
            "        26.3670, 26.3681, 26.3668, 26.3665, 26.3658, 26.3655, 26.3667, 26.3714,\n",
            "        26.3670, 26.3658, 26.3679, 26.3661, 26.3683, 26.3660, 26.3663, 26.3678,\n",
            "        26.3680, 26.3654, 26.3661, 26.3655, 26.3685, 26.3675, 26.3669, 26.3678,\n",
            "        26.3595, 26.3660, 26.3675, 26.3682, 26.3649, 26.3688, 26.3645, 26.3671,\n",
            "        26.3688, 26.3682, 26.3659, 26.3656, 26.3674, 26.3628, 26.3674, 26.3652,\n",
            "        26.3661, 26.3673, 26.3670, 26.3664, 26.3676, 26.3650, 26.3675, 26.3649],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0116, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [6]\n",
            "DEBUGGING: logits looks like: tensor([26.3856, 26.3740, 26.3744, 26.3738, 26.3741, 26.3738, 26.3739, 26.3743,\n",
            "        26.3742, 26.3737, 26.3739, 26.3736, 26.3739, 26.3737, 26.3739, 26.3740,\n",
            "        26.3738, 26.3735, 26.3738, 26.3737, 26.3745, 26.3740, 26.3740, 26.3743,\n",
            "        26.3741, 26.3743, 26.3737, 26.3742, 26.3736, 26.3744, 26.3738, 26.3736,\n",
            "        26.3740, 26.3739, 26.3741, 26.3739, 26.3736, 26.3739, 26.3740, 26.3736,\n",
            "        26.3738, 26.3739, 26.3743, 26.3737, 26.3737, 26.3743, 26.3742, 26.3739,\n",
            "        26.3738, 26.3745, 26.3744, 26.3741, 26.3737, 26.3738, 26.3744, 26.3739,\n",
            "        26.3743, 26.3743, 26.3744, 26.3743, 26.3742, 26.3741, 26.3741, 26.3742,\n",
            "        26.3740, 26.3737, 26.3743, 26.3736, 26.3738, 26.3739, 26.3741, 26.3732,\n",
            "        26.3738, 26.3730, 26.3736, 26.3735, 26.3738, 26.3732, 26.3739, 26.3740,\n",
            "        26.3738, 26.3738, 26.3747, 26.3746, 26.3749, 26.3743, 26.3746, 26.3730,\n",
            "        26.3740, 26.3736], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0106, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [37]\n",
            "DEBUGGING: logits looks like: tensor([26.3943, 26.3830, 26.3829, 26.3828, 26.3831, 26.3827, 26.3827, 26.3830,\n",
            "        26.3828, 26.3831, 26.3827, 26.3828, 26.3828, 26.3827, 26.3825, 26.3829,\n",
            "        26.3827, 26.3827, 26.3829, 26.3828, 26.3830, 26.3828, 26.3827, 26.3827,\n",
            "        26.3828, 26.3827, 26.3827, 26.3828, 26.3828, 26.3831, 26.3828, 26.3826,\n",
            "        26.3827, 26.3829, 26.3824, 26.3827, 26.3828, 26.3825, 26.3828, 26.3830,\n",
            "        26.3826, 26.3827, 26.3826, 26.3824, 26.3828, 26.3827, 26.3828, 26.3827,\n",
            "        26.3827, 26.3829, 26.3827, 26.3828, 26.3829, 26.3827, 26.3829, 26.3829,\n",
            "        26.3828, 26.3830, 26.3826, 26.3827, 26.3830, 26.3828, 26.3826, 26.3838,\n",
            "        26.3829, 26.3830, 26.3828, 26.3831, 26.3832, 26.3829, 26.3827, 26.3828,\n",
            "        26.3826, 26.3828, 26.3826, 26.3832, 26.3827, 26.3830, 26.3830, 26.3832,\n",
            "        26.3824, 26.3827, 26.3827, 26.3825, 26.3827, 26.3831, 26.3828, 26.3829,\n",
            "        26.3829, 26.3827, 26.3827, 26.3827, 26.3827, 26.3826, 26.3826, 26.3829,\n",
            "        26.3829, 26.3827, 26.3828], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0098, 0.0093, 0.0094, 0.0093, 0.0094, 0.0093, 0.0094, 0.0093, 0.0094,\n",
            "        0.0094, 0.0093, 0.0094, 0.0094, 0.0093, 0.0093, 0.0094, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0094, 0.0094, 0.0093,\n",
            "        0.0093, 0.0093, 0.0094, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0094, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0094, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0094, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0094, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0094, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0094, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [96]\n",
            "DEBUGGING: logits looks like: tensor([26.4014, 26.3901, 26.3909, 26.3900, 26.3912, 26.3900, 26.3911, 26.3900,\n",
            "        26.3911, 26.3911, 26.3900, 26.3922, 26.3908, 26.3900, 26.3901, 26.3907,\n",
            "        26.3900, 26.3901, 26.3901, 26.3900, 26.3901, 26.3898, 26.3900, 26.3900,\n",
            "        26.3919, 26.3909, 26.3900, 26.3901, 26.3902, 26.3929, 26.3903, 26.3900,\n",
            "        26.3900, 26.3900, 26.3900, 26.3900, 26.3900, 26.3900, 26.3901, 26.3901,\n",
            "        26.3904, 26.3901, 26.3900, 26.3900, 26.3900, 26.3900, 26.3901, 26.3900,\n",
            "        26.3901, 26.3901, 26.3889, 26.3894, 26.3905, 26.3900, 26.3900, 26.3901,\n",
            "        26.3901, 26.3900, 26.3902, 26.3906, 26.3900, 26.3900, 26.3902, 26.3900,\n",
            "        26.3913, 26.3900, 26.3900, 26.3900, 26.3900, 26.3900, 26.3918, 26.3902,\n",
            "        26.3903, 26.3899, 26.3898, 26.3900, 26.3900, 26.3897, 26.3909, 26.3900,\n",
            "        26.3900, 26.3902, 26.3900, 26.3901, 26.3901, 26.3899, 26.3900, 26.3902,\n",
            "        26.3900, 26.3898, 26.3900, 26.3900, 26.3901, 26.3900, 26.3900, 26.3900,\n",
            "        26.3900, 26.3900, 26.3900, 26.3900, 26.3900, 26.3900, 26.3900, 26.3898,\n",
            "        26.3900, 26.3900, 26.3901], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.21139906491816873 and immediate abs rewards look like: [0.03726173933046084, 0.08988935295451483, 0.01806177905405093, 0.0035331251353909465, 0.0023733111174806254, 0.002426750990025539, 0.009199808498124185, 0.02062700983560717, 0.0009508067394108366, 0.00026206013103546866, 0.0005101514020680042, 0.002804296796739436, 0.002179747259560827, 0.002746332012975472, 0.0026226100146686804, 0.0005609173090306285, 0.0004951093164891063, 1.6695461454219185e-05, 0.004083148408199122, 0.0007073931240029196, 0.0006339372223465034, 0.002873294801247539, 0.001427576094329197, 0.0011739135761672514, 2.967411455756519e-05, 0.0007874100026583619, 0.00023506661545980023, 6.427992548196926e-05, 0.0001485997786403459, 0.0003442642253048689, 0.00012635336838684452, 0.0001743833081491175, 0.00020090726684429683, 0.00010218015404461767, 0.0003145243601920811, 7.362526184806484e-05, 0.0005630800092149002, 0.00014643718304796494, 0.00026155208138334274, 1.865992612692935e-05, 2.5654217552073533e-05, 1.923199783959717e-05, 1.712011680865544e-06, 5.997524795020581e-06, 8.339042778970907e-06, 2.7737588879972463e-05, 1.9071095493927714e-05, 5.764011621067766e-05, 0.00020837138731621963, 1.3445768900055555e-05]\n",
            "DEBUGGING: the total relative reward of the trajectory = 3.1196894419308303 and immediate relative rewards look like: [0.10329686597916006, 0.5035837745191083, 0.15570066466348315, 0.040821368943315596, 0.034311260298457893, 0.04212952713236757, 0.186462712105104, 0.4790709413105195, 0.0249929125002506, 0.0076560309714424285, 0.016395616390423577, 0.09833450537874007, 0.08287169823666424, 0.11251612348649417, 0.11521468513694401, 0.026304823757390794, 0.02467391122752005, 0.0008810934011717164, 0.22745836258000782, 0.04153021684226058, 0.0390867091712753, 0.18562961736945252, 0.09650239774720273, 0.08284010217887286, 0.00218203125298642, 0.06021729669067057, 0.018672492535112252, 0.005295549204373889, 0.012679495849425834, 0.03038910878128549, 0.011526496616656263, 0.01642176557404624, 0.01951177731145079, 0.010224884114379343, 0.03240024671308351, 0.007801813129636007, 0.061326394977536086, 0.01638258968786925, 0.0300323352164895, 0.002197706392224313, 0.0030970254636186055, 0.0023783683592814905, 0.00021676195786216878, 0.0007770211380483318, 0.0011049371799749406, 0.0037569591830361347, 0.0026392892202695916, 0.008146705352264555, 0.0300647404094992, 0.001979728292119731]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0149, 0.0151, 0.0150, 0.0141, 0.0147, 0.0146, 0.0140, 0.0165, 0.0144,\n",
            "        0.0154, 0.0166, 0.0155, 0.0143, 0.0148, 0.0142, 0.0149, 0.0146, 0.0145,\n",
            "        0.0148, 0.0152, 0.0150, 0.0154, 0.0152, 0.0148, 0.0144, 0.0145, 0.0147,\n",
            "        0.0145, 0.0157, 0.0149, 0.0142, 0.0158, 0.0158, 0.0151, 0.0158, 0.0145,\n",
            "        0.0150, 0.0153, 0.0154, 0.0152, 0.0144, 0.0156, 0.0153, 0.0136, 0.0146,\n",
            "        0.0147, 0.0144, 0.0148, 0.0150, 0.0149, 0.0145, 0.0148, 0.0146, 0.0154,\n",
            "        0.0143, 0.0143, 0.0141, 0.0155, 0.0163, 0.0152, 0.0152, 0.0150, 0.0146,\n",
            "        0.0150, 0.0150, 0.0148, 0.0148], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [8]\n",
            "DEBUGGING: logits looks like: tensor([26.2723, 26.2765, 26.2737, 26.2585, 26.2699, 26.2681, 26.2579, 26.2977,\n",
            "        26.2644, 26.2816, 26.2993, 26.2818, 26.2618, 26.2706, 26.2610, 26.2727,\n",
            "        26.2679, 26.2661, 26.2707, 26.2771, 26.2741, 26.2806, 26.2781, 26.2702,\n",
            "        26.2648, 26.2652, 26.2695, 26.2657, 26.2862, 26.2721, 26.2606, 26.2875,\n",
            "        26.2871, 26.2767, 26.2866, 26.2652, 26.2736, 26.2796, 26.2803, 26.2780,\n",
            "        26.2646, 26.2846, 26.2800, 26.2502, 26.2678, 26.2696, 26.2643, 26.2709,\n",
            "        26.2739, 26.2734, 26.2652, 26.2710, 26.2682, 26.2804, 26.2629, 26.2622,\n",
            "        26.2592, 26.2824, 26.2945, 26.2770, 26.2771, 26.2748, 26.2682, 26.2740,\n",
            "        26.2750, 26.2702, 26.2714], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0135, 0.0126, 0.0127, 0.0125, 0.0128, 0.0125, 0.0127, 0.0127, 0.0127,\n",
            "        0.0128, 0.0125, 0.0124, 0.0125, 0.0126, 0.0127, 0.0122, 0.0128, 0.0124,\n",
            "        0.0127, 0.0128, 0.0122, 0.0128, 0.0127, 0.0127, 0.0129, 0.0127, 0.0128,\n",
            "        0.0128, 0.0131, 0.0126, 0.0124, 0.0127, 0.0122, 0.0127, 0.0128, 0.0128,\n",
            "        0.0126, 0.0124, 0.0125, 0.0124, 0.0125, 0.0128, 0.0126, 0.0127, 0.0126,\n",
            "        0.0126, 0.0129, 0.0128, 0.0131, 0.0128, 0.0123, 0.0128, 0.0126, 0.0125,\n",
            "        0.0127, 0.0127, 0.0125, 0.0127, 0.0127, 0.0125, 0.0126, 0.0127, 0.0128,\n",
            "        0.0126, 0.0126, 0.0128, 0.0126, 0.0125, 0.0126, 0.0128, 0.0127, 0.0130,\n",
            "        0.0127, 0.0126, 0.0127, 0.0128, 0.0126, 0.0125, 0.0124],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [48]\n",
            "DEBUGGING: logits looks like: tensor([26.2798, 26.2632, 26.2640, 26.2608, 26.2673, 26.2613, 26.2647, 26.2646,\n",
            "        26.2649, 26.2661, 26.2615, 26.2589, 26.2613, 26.2637, 26.2645, 26.2544,\n",
            "        26.2668, 26.2591, 26.2658, 26.2676, 26.2546, 26.2672, 26.2651, 26.2648,\n",
            "        26.2696, 26.2645, 26.2676, 26.2662, 26.2734, 26.2621, 26.2597, 26.2649,\n",
            "        26.2539, 26.2652, 26.2676, 26.2663, 26.2619, 26.2590, 26.2616, 26.2597,\n",
            "        26.2617, 26.2674, 26.2626, 26.2644, 26.2639, 26.2624, 26.2680, 26.2666,\n",
            "        26.2720, 26.2668, 26.2561, 26.2660, 26.2619, 26.2611, 26.2643, 26.2650,\n",
            "        26.2615, 26.2654, 26.2655, 26.2619, 26.2636, 26.2657, 26.2668, 26.2630,\n",
            "        26.2634, 26.2662, 26.2638, 26.2600, 26.2633, 26.2661, 26.2642, 26.2700,\n",
            "        26.2640, 26.2638, 26.2655, 26.2660, 26.2631, 26.2617, 26.2588],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0124, 0.0114, 0.0113, 0.0113, 0.0113, 0.0113, 0.0111, 0.0108, 0.0113,\n",
            "        0.0114, 0.0115, 0.0114, 0.0114, 0.0112, 0.0114, 0.0115, 0.0114, 0.0115,\n",
            "        0.0112, 0.0113, 0.0115, 0.0116, 0.0113, 0.0116, 0.0112, 0.0113, 0.0112,\n",
            "        0.0111, 0.0115, 0.0112, 0.0114, 0.0113, 0.0113, 0.0113, 0.0112, 0.0113,\n",
            "        0.0114, 0.0113, 0.0110, 0.0113, 0.0113, 0.0115, 0.0118, 0.0113, 0.0113,\n",
            "        0.0115, 0.0113, 0.0113, 0.0111, 0.0116, 0.0112, 0.0116, 0.0113, 0.0117,\n",
            "        0.0113, 0.0115, 0.0115, 0.0114, 0.0114, 0.0111, 0.0112, 0.0112, 0.0113,\n",
            "        0.0114, 0.0112, 0.0113, 0.0116, 0.0115, 0.0115, 0.0115, 0.0113, 0.0114,\n",
            "        0.0113, 0.0113, 0.0112, 0.0114, 0.0113, 0.0120, 0.0114, 0.0113, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0112, 0.0114, 0.0112, 0.0113],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [57]\n",
            "DEBUGGING: logits looks like: tensor([26.2732, 26.2519, 26.2504, 26.2484, 26.2500, 26.2501, 26.2445, 26.2387,\n",
            "        26.2485, 26.2510, 26.2534, 26.2520, 26.2506, 26.2466, 26.2522, 26.2544,\n",
            "        26.2513, 26.2546, 26.2465, 26.2495, 26.2534, 26.2552, 26.2487, 26.2551,\n",
            "        26.2476, 26.2488, 26.2482, 26.2455, 26.2532, 26.2476, 26.2522, 26.2486,\n",
            "        26.2495, 26.2496, 26.2464, 26.2498, 26.2507, 26.2487, 26.2426, 26.2502,\n",
            "        26.2496, 26.2534, 26.2596, 26.2491, 26.2490, 26.2530, 26.2485, 26.2502,\n",
            "        26.2457, 26.2555, 26.2481, 26.2569, 26.2503, 26.2577, 26.2492, 26.2538,\n",
            "        26.2548, 26.2526, 26.2516, 26.2459, 26.2480, 26.2474, 26.2488, 26.2517,\n",
            "        26.2463, 26.2492, 26.2557, 26.2548, 26.2531, 26.2529, 26.2486, 26.2523,\n",
            "        26.2490, 26.2500, 26.2483, 26.2519, 26.2499, 26.2636, 26.2509, 26.2505,\n",
            "        26.2520, 26.2527, 26.2516, 26.2516, 26.2468, 26.2511, 26.2481, 26.2505],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0110, 0.0105, 0.0103, 0.0102, 0.0103, 0.0103, 0.0104, 0.0103, 0.0102,\n",
            "        0.0102, 0.0101, 0.0101, 0.0101, 0.0101, 0.0103, 0.0101, 0.0101, 0.0101,\n",
            "        0.0103, 0.0102, 0.0103, 0.0102, 0.0101, 0.0100, 0.0101, 0.0102, 0.0101,\n",
            "        0.0102, 0.0101, 0.0101, 0.0102, 0.0103, 0.0101, 0.0101, 0.0102, 0.0101,\n",
            "        0.0101, 0.0103, 0.0103, 0.0102, 0.0104, 0.0102, 0.0102, 0.0103, 0.0103,\n",
            "        0.0101, 0.0102, 0.0102, 0.0100, 0.0102, 0.0102, 0.0102, 0.0101, 0.0101,\n",
            "        0.0100, 0.0102, 0.0102, 0.0101, 0.0102, 0.0103, 0.0102, 0.0103, 0.0103,\n",
            "        0.0102, 0.0103, 0.0102, 0.0103, 0.0101, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0101, 0.0102, 0.0102, 0.0103, 0.0101, 0.0103, 0.0102, 0.0102, 0.0101,\n",
            "        0.0101, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0101,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0103, 0.0103, 0.0101, 0.0102],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [12]\n",
            "DEBUGGING: logits looks like: tensor([26.2684, 26.2559, 26.2515, 26.2496, 26.2518, 26.2525, 26.2531, 26.2526,\n",
            "        26.2490, 26.2499, 26.2481, 26.2482, 26.2481, 26.2474, 26.2508, 26.2470,\n",
            "        26.2461, 26.2463, 26.2526, 26.2486, 26.2516, 26.2484, 26.2477, 26.2433,\n",
            "        26.2468, 26.2505, 26.2481, 26.2489, 26.2477, 26.2480, 26.2504, 26.2512,\n",
            "        26.2473, 26.2478, 26.2484, 26.2461, 26.2481, 26.2508, 26.2518, 26.2500,\n",
            "        26.2544, 26.2489, 26.2497, 26.2516, 26.2530, 26.2477, 26.2487, 26.2507,\n",
            "        26.2445, 26.2505, 26.2504, 26.2492, 26.2478, 26.2482, 26.2450, 26.2498,\n",
            "        26.2487, 26.2477, 26.2507, 26.2508, 26.2491, 26.2521, 26.2509, 26.2498,\n",
            "        26.2528, 26.2483, 26.2511, 26.2466, 26.2495, 26.2493, 26.2495, 26.2498,\n",
            "        26.2467, 26.2496, 26.2503, 26.2525, 26.2469, 26.2511, 26.2504, 26.2489,\n",
            "        26.2462, 26.2477, 26.2501, 26.2501, 26.2494, 26.2492, 26.2494, 26.2483,\n",
            "        26.2500, 26.2477, 26.2485, 26.2491, 26.2488, 26.2493, 26.2508, 26.2525,\n",
            "        26.2478, 26.2488], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0100, 0.0093, 0.0092, 0.0092, 0.0093, 0.0093, 0.0091, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0093, 0.0093, 0.0092, 0.0093, 0.0093, 0.0094,\n",
            "        0.0092, 0.0092, 0.0092, 0.0093, 0.0093, 0.0092, 0.0092, 0.0093, 0.0093,\n",
            "        0.0093, 0.0094, 0.0093, 0.0093, 0.0093, 0.0093, 0.0092, 0.0092, 0.0092,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0092, 0.0093, 0.0093, 0.0093, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0093, 0.0092, 0.0093, 0.0092, 0.0093,\n",
            "        0.0092, 0.0093, 0.0092, 0.0092, 0.0093, 0.0092, 0.0093, 0.0092, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0094, 0.0093, 0.0092, 0.0093, 0.0092, 0.0092,\n",
            "        0.0093, 0.0092, 0.0092, 0.0092, 0.0093, 0.0093, 0.0092, 0.0093, 0.0093,\n",
            "        0.0092, 0.0092, 0.0092, 0.0093, 0.0092, 0.0093, 0.0092, 0.0093, 0.0092,\n",
            "        0.0093, 0.0092, 0.0092, 0.0093, 0.0092, 0.0092, 0.0093, 0.0093, 0.0093,\n",
            "        0.0092, 0.0092, 0.0093, 0.0093, 0.0092, 0.0093, 0.0093, 0.0093, 0.0093],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [3]\n",
            "DEBUGGING: logits looks like: tensor([26.2645, 26.2447, 26.2441, 26.2430, 26.2444, 26.2450, 26.2393, 26.2428,\n",
            "        26.2443, 26.2438, 26.2437, 26.2417, 26.2448, 26.2450, 26.2440, 26.2451,\n",
            "        26.2458, 26.2493, 26.2441, 26.2442, 26.2436, 26.2447, 26.2446, 26.2435,\n",
            "        26.2439, 26.2451, 26.2448, 26.2446, 26.2472, 26.2444, 26.2452, 26.2445,\n",
            "        26.2448, 26.2442, 26.2440, 26.2441, 26.2443, 26.2444, 26.2450, 26.2454,\n",
            "        26.2441, 26.2455, 26.2444, 26.2445, 26.2416, 26.2432, 26.2442, 26.2431,\n",
            "        26.2442, 26.2448, 26.2436, 26.2444, 26.2442, 26.2446, 26.2443, 26.2465,\n",
            "        26.2426, 26.2433, 26.2448, 26.2442, 26.2445, 26.2440, 26.2443, 26.2452,\n",
            "        26.2452, 26.2445, 26.2475, 26.2447, 26.2442, 26.2448, 26.2438, 26.2433,\n",
            "        26.2447, 26.2437, 26.2440, 26.2436, 26.2448, 26.2446, 26.2442, 26.2443,\n",
            "        26.2456, 26.2436, 26.2442, 26.2432, 26.2451, 26.2439, 26.2445, 26.2442,\n",
            "        26.2450, 26.2439, 26.2455, 26.2428, 26.2422, 26.2445, 26.2437, 26.2434,\n",
            "        26.2448, 26.2449, 26.2448, 26.2440, 26.2442, 26.2447, 26.2446, 26.2441,\n",
            "        26.2452, 26.2449, 26.2452, 26.2444], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.09178322328079958 and immediate abs rewards look like: [0.002336502514026506, 0.005557620026138466, 0.0004436049025571265, 0.01618262078727639, 0.0023935873969094246, 0.0063707805866215494, 4.555559098662343e-05, 0.0017482875582572888, 0.00011361179167579394, 0.012852266803292878, 2.298764229635708e-05, 3.4599529499246273e-06, 0.009166118664779788, 0.0004364360620456864, 0.0006631144283346657, 0.00016363087752324645, 2.3889163912826916e-05, 0.000737253660645365, 0.0012547528026516375, 0.001615868208318716, 0.0015806368642188318, 0.002907870539729629, 0.008550622188067791, 0.00016787318236310966, 0.0018318925554012822, 0.0004120477028664027, 0.0008852394294081023, 0.0016221830883296207, 2.898877073675976e-05, 7.709094234087388e-05, 1.6779718862380832e-05, 0.000231305660236103, 0.0007714610264883959, 7.246527184179286e-05, 0.0008652248079670244, 1.819984390749596e-06, 4.247033757565077e-06, 2.4767792638158426e-05, 3.141533170492039e-05, 0.00030700257048010826, 0.0002477504222042626, 0.0029204296265561425, 0.00029572754738182994, 0.001994738052417233, 0.00012429253320078715, 0.0003742361632248503, 0.0005174365892344213, 0.00041700860447235755, 0.0008158235586961382, 0.0015528963003816898]\n",
            "DEBUGGING: the total relative reward of the trajectory = 5.327150580469814 and immediate relative rewards look like: [0.008635016123259209, 0.04111411275644946, 0.0049326848426948155, 0.23996397040210204, 0.04463440053623571, 0.14268626619777114, 0.00119319665682904, 0.05233383485332202, 0.0038285103066022608, 0.48124043401588323, 0.0009514030487171507, 0.00015621855575683954, 0.44834318459811645, 0.02306910733221647, 0.037560678671181486, 0.009888893487361215, 0.001534048786353305, 0.05012826361802812, 0.09007949810488493, 0.1221676059877678, 0.1255558340222999, 0.24212686569032704, 0.7451591228650255, 0.015315294616724408, 0.17410069779400644, 0.040755281754953916, 0.09094014656642486, 0.17287625208137264, 0.0032016470833128303, 0.00880795453918172, 0.0019811145165048727, 0.028190463761510187, 0.09696883397579599, 0.009387300479100528, 0.11538274721378174, 0.0002497222428838395, 0.0005989282541721344, 0.0035872278841458578, 0.004669801535481356, 0.046805767418726996, 0.0387209896622559, 0.46761156336661824, 0.04853256435300911, 0.3350121600613121, 0.02136536759133887, 0.06576230391752563, 0.09291600346588143, 0.07649052599194944, 0.15278577758494977, 0.29685099129770665]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0193, 0.0146, 0.0144, 0.0143, 0.0150, 0.0147, 0.0129, 0.0140, 0.0148,\n",
            "        0.0140, 0.0147, 0.0133, 0.0142, 0.0146, 0.0142, 0.0145, 0.0150, 0.0156,\n",
            "        0.0159, 0.0146, 0.0170, 0.0143, 0.0145, 0.0148, 0.0145, 0.0143, 0.0145,\n",
            "        0.0142, 0.0145, 0.0147, 0.0147, 0.0148, 0.0155, 0.0141, 0.0150, 0.0146,\n",
            "        0.0148, 0.0156, 0.0154, 0.0144, 0.0122, 0.0147, 0.0141, 0.0152, 0.0153,\n",
            "        0.0148, 0.0151, 0.0146, 0.0145, 0.0146, 0.0147, 0.0143, 0.0144, 0.0150,\n",
            "        0.0146, 0.0144, 0.0145, 0.0145, 0.0140, 0.0155, 0.0151, 0.0154, 0.0143,\n",
            "        0.0152, 0.0139, 0.0155, 0.0152, 0.0134], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [37]\n",
            "DEBUGGING: logits looks like: tensor([26.3330, 26.2627, 26.2602, 26.2584, 26.2703, 26.2656, 26.2325, 26.2533,\n",
            "        26.2667, 26.2523, 26.2648, 26.2406, 26.2563, 26.2635, 26.2558, 26.2612,\n",
            "        26.2696, 26.2796, 26.2848, 26.2631, 26.3014, 26.2584, 26.2621, 26.2668,\n",
            "        26.2609, 26.2583, 26.2622, 26.2569, 26.2616, 26.2657, 26.2654, 26.2660,\n",
            "        26.2779, 26.2543, 26.2694, 26.2631, 26.2669, 26.2798, 26.2760, 26.2593,\n",
            "        26.2176, 26.2646, 26.2544, 26.2731, 26.2756, 26.2661, 26.2717, 26.2626,\n",
            "        26.2613, 26.2628, 26.2657, 26.2574, 26.2606, 26.2696, 26.2624, 26.2604,\n",
            "        26.2607, 26.2623, 26.2532, 26.2786, 26.2708, 26.2770, 26.2586, 26.2729,\n",
            "        26.2516, 26.2780, 26.2733, 26.2427], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0135, 0.0120, 0.0146, 0.0134, 0.0142, 0.0131, 0.0123, 0.0130, 0.0146,\n",
            "        0.0142, 0.0074, 0.0127, 0.0131, 0.0145, 0.0112, 0.0136, 0.0142, 0.0149,\n",
            "        0.0116, 0.0139, 0.0151, 0.0132, 0.0118, 0.0132, 0.0133, 0.0146, 0.0128,\n",
            "        0.0127, 0.0136, 0.0109, 0.0167, 0.0142, 0.0132, 0.0133, 0.0150, 0.0159,\n",
            "        0.0134, 0.0127, 0.0146, 0.0116, 0.0139, 0.0132, 0.0142, 0.0148, 0.0143,\n",
            "        0.0134, 0.0136, 0.0100, 0.0135, 0.0157, 0.0148, 0.0147, 0.0129, 0.0132,\n",
            "        0.0132, 0.0125, 0.0130, 0.0148, 0.0135, 0.0143, 0.0133, 0.0128, 0.0139,\n",
            "        0.0142, 0.0155, 0.0139, 0.0142, 0.0155, 0.0137, 0.0126, 0.0149, 0.0131,\n",
            "        0.0131, 0.0123], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [23]\n",
            "DEBUGGING: logits looks like: tensor([26.2764, 26.2476, 26.2964, 26.2741, 26.2880, 26.2678, 26.2522, 26.2664,\n",
            "        26.2950, 26.2879, 26.1242, 26.2616, 26.2693, 26.2932, 26.2297, 26.2782,\n",
            "        26.2883, 26.3010, 26.2387, 26.2829, 26.3036, 26.2702, 26.2433, 26.2698,\n",
            "        26.2723, 26.2963, 26.2623, 26.2607, 26.2780, 26.2223, 26.3292, 26.2879,\n",
            "        26.2713, 26.2726, 26.3020, 26.3163, 26.2743, 26.2618, 26.2963, 26.2381,\n",
            "        26.2830, 26.2712, 26.2886, 26.2992, 26.2914, 26.2745, 26.2781, 26.2003,\n",
            "        26.2767, 26.3135, 26.2987, 26.2980, 26.2651, 26.2698, 26.2711, 26.2575,\n",
            "        26.2674, 26.2984, 26.2758, 26.2902, 26.2724, 26.2623, 26.2830, 26.2894,\n",
            "        26.3103, 26.2833, 26.2886, 26.3100, 26.2793, 26.2584, 26.3005, 26.2682,\n",
            "        26.2678, 26.2522], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0116, 0.0119, 0.0116, 0.0116, 0.0115, 0.0120, 0.0111, 0.0118, 0.0119,\n",
            "        0.0117, 0.0117, 0.0117, 0.0125, 0.0113, 0.0119, 0.0116, 0.0113, 0.0118,\n",
            "        0.0120, 0.0114, 0.0117, 0.0117, 0.0116, 0.0116, 0.0119, 0.0118, 0.0115,\n",
            "        0.0116, 0.0117, 0.0120, 0.0125, 0.0118, 0.0114, 0.0113, 0.0115, 0.0118,\n",
            "        0.0113, 0.0116, 0.0117, 0.0116, 0.0128, 0.0114, 0.0123, 0.0113, 0.0120,\n",
            "        0.0114, 0.0114, 0.0118, 0.0115, 0.0114, 0.0116, 0.0117, 0.0116, 0.0108,\n",
            "        0.0113, 0.0116, 0.0115, 0.0111, 0.0121, 0.0114, 0.0115, 0.0118, 0.0117,\n",
            "        0.0114, 0.0110, 0.0113, 0.0119, 0.0117, 0.0114, 0.0121, 0.0118, 0.0116,\n",
            "        0.0122, 0.0109, 0.0114, 0.0112, 0.0117, 0.0116, 0.0111, 0.0117, 0.0118,\n",
            "        0.0118, 0.0115, 0.0109, 0.0119, 0.0115], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [18]\n",
            "DEBUGGING: logits looks like: tensor([26.2684, 26.2753, 26.2705, 26.2689, 26.2672, 26.2770, 26.2584, 26.2735,\n",
            "        26.2753, 26.2725, 26.2705, 26.2705, 26.2885, 26.2624, 26.2751, 26.2701,\n",
            "        26.2618, 26.2734, 26.2772, 26.2657, 26.2726, 26.2726, 26.2700, 26.2698,\n",
            "        26.2766, 26.2733, 26.2677, 26.2694, 26.2711, 26.2783, 26.2886, 26.2728,\n",
            "        26.2648, 26.2622, 26.2675, 26.2747, 26.2636, 26.2691, 26.2712, 26.2704,\n",
            "        26.2931, 26.2650, 26.2846, 26.2635, 26.2780, 26.2645, 26.2652, 26.2744,\n",
            "        26.2668, 26.2641, 26.2703, 26.2707, 26.2687, 26.2519, 26.2639, 26.2703,\n",
            "        26.2668, 26.2591, 26.2805, 26.2651, 26.2662, 26.2742, 26.2720, 26.2655,\n",
            "        26.2571, 26.2636, 26.2748, 26.2723, 26.2654, 26.2795, 26.2732, 26.2693,\n",
            "        26.2827, 26.2530, 26.2661, 26.2605, 26.2715, 26.2695, 26.2585, 26.2717,\n",
            "        26.2730, 26.2743, 26.2678, 26.2532, 26.2763, 26.2681],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0100, 0.0095, 0.0104, 0.0103, 0.0107, 0.0106, 0.0102, 0.0102, 0.0096,\n",
            "        0.0106, 0.0104, 0.0105, 0.0106, 0.0095, 0.0104, 0.0104, 0.0093, 0.0101,\n",
            "        0.0100, 0.0101, 0.0102, 0.0108, 0.0104, 0.0107, 0.0116, 0.0113, 0.0104,\n",
            "        0.0100, 0.0103, 0.0106, 0.0116, 0.0100, 0.0109, 0.0099, 0.0113, 0.0110,\n",
            "        0.0092, 0.0099, 0.0096, 0.0100, 0.0105, 0.0093, 0.0100, 0.0099, 0.0109,\n",
            "        0.0106, 0.0108, 0.0102, 0.0113, 0.0102, 0.0087, 0.0096, 0.0109, 0.0105,\n",
            "        0.0103, 0.0098, 0.0104, 0.0111, 0.0113, 0.0101, 0.0102, 0.0104, 0.0103,\n",
            "        0.0116, 0.0102, 0.0114, 0.0103, 0.0095, 0.0100, 0.0103, 0.0105, 0.0094,\n",
            "        0.0103, 0.0100, 0.0113, 0.0108, 0.0105, 0.0099, 0.0103, 0.0095, 0.0109,\n",
            "        0.0107, 0.0084, 0.0104, 0.0110, 0.0105, 0.0104, 0.0094, 0.0096, 0.0105,\n",
            "        0.0103, 0.0098, 0.0100, 0.0093, 0.0108, 0.0111, 0.0106],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [80]\n",
            "DEBUGGING: logits looks like: tensor([26.2529, 26.2407, 26.2618, 26.2603, 26.2690, 26.2668, 26.2565, 26.2564,\n",
            "        26.2427, 26.2663, 26.2633, 26.2644, 26.2677, 26.2395, 26.2620, 26.2623,\n",
            "        26.2348, 26.2547, 26.2537, 26.2543, 26.2580, 26.2713, 26.2619, 26.2699,\n",
            "        26.2901, 26.2840, 26.2614, 26.2521, 26.2590, 26.2670, 26.2902, 26.2523,\n",
            "        26.2731, 26.2504, 26.2831, 26.2761, 26.2305, 26.2494, 26.2430, 26.2534,\n",
            "        26.2646, 26.2340, 26.2533, 26.2492, 26.2746, 26.2663, 26.2720, 26.2569,\n",
            "        26.2840, 26.2572, 26.2169, 26.2427, 26.2739, 26.2652, 26.2609, 26.2481,\n",
            "        26.2618, 26.2777, 26.2835, 26.2546, 26.2579, 26.2613, 26.2590, 26.2903,\n",
            "        26.2579, 26.2858, 26.2596, 26.2384, 26.2515, 26.2605, 26.2641, 26.2364,\n",
            "        26.2605, 26.2528, 26.2824, 26.2722, 26.2657, 26.2491, 26.2590, 26.2389,\n",
            "        26.2743, 26.2703, 26.2089, 26.2620, 26.2762, 26.2658, 26.2634, 26.2366,\n",
            "        26.2415, 26.2647, 26.2598, 26.2475, 26.2531, 26.2356, 26.2715, 26.2775,\n",
            "        26.2669], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0096, 0.0083, 0.0097, 0.0095, 0.0098, 0.0095, 0.0094, 0.0091, 0.0086,\n",
            "        0.0094, 0.0096, 0.0094, 0.0095, 0.0094, 0.0094, 0.0095, 0.0095, 0.0088,\n",
            "        0.0100, 0.0091, 0.0095, 0.0087, 0.0091, 0.0093, 0.0093, 0.0093, 0.0099,\n",
            "        0.0093, 0.0095, 0.0096, 0.0092, 0.0097, 0.0094, 0.0093, 0.0092, 0.0094,\n",
            "        0.0091, 0.0091, 0.0094, 0.0100, 0.0094, 0.0095, 0.0092, 0.0093, 0.0096,\n",
            "        0.0091, 0.0094, 0.0095, 0.0094, 0.0096, 0.0090, 0.0096, 0.0093, 0.0096,\n",
            "        0.0098, 0.0096, 0.0095, 0.0098, 0.0093, 0.0096, 0.0094, 0.0097, 0.0095,\n",
            "        0.0095, 0.0095, 0.0093, 0.0093, 0.0093, 0.0096, 0.0093, 0.0097, 0.0086,\n",
            "        0.0095, 0.0093, 0.0094, 0.0071, 0.0093, 0.0094, 0.0091, 0.0095, 0.0095,\n",
            "        0.0095, 0.0092, 0.0091, 0.0096, 0.0093, 0.0096, 0.0093, 0.0094, 0.0096,\n",
            "        0.0085, 0.0094, 0.0093, 0.0097, 0.0087, 0.0095, 0.0090, 0.0086, 0.0097,\n",
            "        0.0093, 0.0092, 0.0094, 0.0094, 0.0094, 0.0095, 0.0096, 0.0092],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [34]\n",
            "DEBUGGING: logits looks like: tensor([26.2482, 26.2115, 26.2503, 26.2466, 26.2547, 26.2477, 26.2448, 26.2356,\n",
            "        26.2228, 26.2440, 26.2497, 26.2438, 26.2466, 26.2427, 26.2427, 26.2456,\n",
            "        26.2461, 26.2284, 26.2588, 26.2362, 26.2476, 26.2240, 26.2361, 26.2400,\n",
            "        26.2399, 26.2409, 26.2562, 26.2405, 26.2473, 26.2480, 26.2394, 26.2523,\n",
            "        26.2424, 26.2423, 26.2377, 26.2425, 26.2363, 26.2348, 26.2433, 26.2580,\n",
            "        26.2426, 26.2463, 26.2377, 26.2413, 26.2492, 26.2355, 26.2435, 26.2469,\n",
            "        26.2427, 26.2479, 26.2341, 26.2492, 26.2417, 26.2501, 26.2552, 26.2480,\n",
            "        26.2450, 26.2541, 26.2410, 26.2479, 26.2437, 26.2525, 26.2471, 26.2456,\n",
            "        26.2469, 26.2407, 26.2417, 26.2408, 26.2490, 26.2415, 26.2503, 26.2214,\n",
            "        26.2475, 26.2416, 26.2428, 26.1727, 26.2408, 26.2437, 26.2361, 26.2456,\n",
            "        26.2453, 26.2452, 26.2370, 26.2364, 26.2484, 26.2404, 26.2498, 26.2402,\n",
            "        26.2442, 26.2494, 26.2200, 26.2438, 26.2415, 26.2522, 26.2230, 26.2471,\n",
            "        26.2329, 26.2202, 26.2510, 26.2416, 26.2393, 26.2430, 26.2430, 26.2449,\n",
            "        26.2461, 26.2496, 26.2371], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.8867413289858632 and immediate abs rewards look like: [0.0008041198188948329, 0.01024099322148686, 0.023888370671102166, 0.0019217570074943069, 0.0027652260341710644, 0.0025432918696424167, 0.006283768355842767, 0.006952563296636072, 0.0023959800046213786, 5.038023982706363e-05, 0.0002240266894659726, 0.0031728283206575725, 0.0013331040595403465, 0.004995169052108395, 0.8191697503311843, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 1.3642420526593924e-12, 0.0, 9.094947017729282e-13, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 9.094947017729282e-13, 9.094947017729282e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 1.3642420526593924e-12]\n",
            "DEBUGGING: the total relative reward of the trajectory = 44.918532156795386 and immediate relative rewards look like: [0.0027855624292495943, 0.07097169812984075, 0.2492096057165511, 0.026954929809701773, 0.04851467652634303, 0.05359713182656557, 0.15463237476589198, 0.1959645838426326, 0.07616114845808243, 0.0017808842243325572, 0.008711152906018734, 0.1346001067043706, 0.06133556856255625, 0.24762139862847854, 43.58569133172728, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.4569682106375694e-11, 1.7053025658238527e-10, 0.0, 1.2278178473940115e-10, 0.0, 0.0, 0.0, 0.0, 7.275957614183426e-11, 7.503331289624952e-11, 7.73070496506989e-11, 7.958078640511313e-11, 8.185452315956354e-11, 1.6825651982795347e-10, 8.640199666844783e-11, 0.0, 9.094947017729282e-11, 9.322320693170395e-11, 9.549694368615746e-11, 1.955413608811351e-10, 2.0008883438990773e-10, 2.0463630789886232e-10, 0.0, 1.0686562745834337e-10, 1.0913936421275139e-10, 1.1141310096715838e-10, 3.410605131648481e-10]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 3\n",
            "DEBUGGING: the action_prob is: tensor([0.0193, 0.0149, 0.0147, 0.0149, 0.0150, 0.0149, 0.0149, 0.0148, 0.0149,\n",
            "        0.0147, 0.0151, 0.0149, 0.0148, 0.0147, 0.0150, 0.0147, 0.0148, 0.0149,\n",
            "        0.0149, 0.0146, 0.0151, 0.0148, 0.0148, 0.0148, 0.0148, 0.0149, 0.0150,\n",
            "        0.0149, 0.0147, 0.0146, 0.0147, 0.0147, 0.0145, 0.0148, 0.0150, 0.0147,\n",
            "        0.0147, 0.0150, 0.0151, 0.0148, 0.0149, 0.0149, 0.0148, 0.0148, 0.0152,\n",
            "        0.0149, 0.0150, 0.0151, 0.0148, 0.0149, 0.0146, 0.0149, 0.0149, 0.0152,\n",
            "        0.0146, 0.0150, 0.0148, 0.0148, 0.0150, 0.0146, 0.0157, 0.0145, 0.0148,\n",
            "        0.0148, 0.0148, 0.0147, 0.0151], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [9]\n",
            "DEBUGGING: logits looks like: tensor([26.2867, 26.2221, 26.2196, 26.2223, 26.2241, 26.2215, 26.2227, 26.2204,\n",
            "        26.2230, 26.2187, 26.2252, 26.2220, 26.2207, 26.2194, 26.2238, 26.2190,\n",
            "        26.2202, 26.2219, 26.2228, 26.2181, 26.2254, 26.2202, 26.2199, 26.2203,\n",
            "        26.2211, 26.2227, 26.2246, 26.2218, 26.2185, 26.2175, 26.2194, 26.2187,\n",
            "        26.2154, 26.2213, 26.2232, 26.2188, 26.2193, 26.2247, 26.2263, 26.2198,\n",
            "        26.2223, 26.2226, 26.2204, 26.2200, 26.2266, 26.2216, 26.2242, 26.2254,\n",
            "        26.2202, 26.2228, 26.2181, 26.2220, 26.2224, 26.2273, 26.2179, 26.2247,\n",
            "        26.2213, 26.2203, 26.2245, 26.2177, 26.2359, 26.2158, 26.2208, 26.2215,\n",
            "        26.2214, 26.2195, 26.2251], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0171, 0.0129, 0.0129, 0.0129, 0.0128, 0.0127, 0.0127, 0.0127, 0.0126,\n",
            "        0.0126, 0.0128, 0.0127, 0.0129, 0.0127, 0.0129, 0.0129, 0.0129, 0.0127,\n",
            "        0.0126, 0.0129, 0.0129, 0.0128, 0.0126, 0.0127, 0.0128, 0.0129, 0.0128,\n",
            "        0.0127, 0.0127, 0.0127, 0.0129, 0.0128, 0.0127, 0.0128, 0.0127, 0.0128,\n",
            "        0.0128, 0.0128, 0.0129, 0.0125, 0.0127, 0.0128, 0.0125, 0.0125, 0.0126,\n",
            "        0.0128, 0.0127, 0.0128, 0.0128, 0.0127, 0.0127, 0.0127, 0.0127, 0.0129,\n",
            "        0.0130, 0.0126, 0.0129, 0.0128, 0.0127, 0.0127, 0.0128, 0.0127, 0.0130,\n",
            "        0.0127, 0.0127, 0.0128, 0.0127, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0127, 0.0128, 0.0128, 0.0128, 0.0127, 0.0127],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [51]\n",
            "DEBUGGING: logits looks like: tensor([26.2790, 26.2078, 26.2077, 26.2085, 26.2061, 26.2054, 26.2039, 26.2045,\n",
            "        26.2036, 26.2034, 26.2075, 26.2056, 26.2092, 26.2042, 26.2084, 26.2093,\n",
            "        26.2078, 26.2041, 26.2031, 26.2079, 26.2077, 26.2073, 26.2036, 26.2053,\n",
            "        26.2060, 26.2077, 26.2069, 26.2049, 26.2056, 26.2051, 26.2091, 26.2069,\n",
            "        26.2041, 26.2060, 26.2053, 26.2066, 26.2073, 26.2072, 26.2089, 26.2001,\n",
            "        26.2050, 26.2058, 26.2013, 26.2010, 26.2023, 26.2063, 26.2039, 26.2061,\n",
            "        26.2067, 26.2052, 26.2052, 26.2038, 26.2047, 26.2094, 26.2101, 26.2036,\n",
            "        26.2094, 26.2065, 26.2056, 26.2048, 26.2061, 26.2052, 26.2099, 26.2056,\n",
            "        26.2054, 26.2060, 26.2053, 26.2057, 26.2074, 26.2064, 26.2065, 26.2076,\n",
            "        26.2052, 26.2059, 26.2063, 26.2058, 26.2056, 26.2044],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0116, 0.0107, 0.0110, 0.0117, 0.0116, 0.0119, 0.0117, 0.0117, 0.0119,\n",
            "        0.0100, 0.0118, 0.0119, 0.0114, 0.0117, 0.0118, 0.0110, 0.0112, 0.0120,\n",
            "        0.0118, 0.0115, 0.0119, 0.0110, 0.0120, 0.0113, 0.0119, 0.0119, 0.0116,\n",
            "        0.0117, 0.0126, 0.0118, 0.0119, 0.0123, 0.0115, 0.0117, 0.0115, 0.0117,\n",
            "        0.0112, 0.0109, 0.0109, 0.0118, 0.0122, 0.0123, 0.0117, 0.0115, 0.0115,\n",
            "        0.0119, 0.0113, 0.0116, 0.0115, 0.0118, 0.0120, 0.0114, 0.0119, 0.0120,\n",
            "        0.0119, 0.0114, 0.0117, 0.0114, 0.0111, 0.0115, 0.0116, 0.0117, 0.0119,\n",
            "        0.0119, 0.0118, 0.0118, 0.0121, 0.0105, 0.0120, 0.0116, 0.0117, 0.0116,\n",
            "        0.0115, 0.0111, 0.0114, 0.0116, 0.0114, 0.0115, 0.0119, 0.0118, 0.0115,\n",
            "        0.0120, 0.0115, 0.0118, 0.0120, 0.0122], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [70]\n",
            "DEBUGGING: logits looks like: tensor([26.2144, 26.1937, 26.2013, 26.2156, 26.2142, 26.2198, 26.2174, 26.2163,\n",
            "        26.2207, 26.1783, 26.2193, 26.2202, 26.2103, 26.2156, 26.2190, 26.2016,\n",
            "        26.2060, 26.2217, 26.2178, 26.2129, 26.2208, 26.2013, 26.2223, 26.2073,\n",
            "        26.2204, 26.2213, 26.2147, 26.2173, 26.2344, 26.2186, 26.2211, 26.2283,\n",
            "        26.2110, 26.2167, 26.2112, 26.2170, 26.2058, 26.1988, 26.1985, 26.2183,\n",
            "        26.2275, 26.2286, 26.2162, 26.2127, 26.2122, 26.2205, 26.2085, 26.2151,\n",
            "        26.2123, 26.2193, 26.2223, 26.2095, 26.2211, 26.2221, 26.2203, 26.2105,\n",
            "        26.2171, 26.2104, 26.2030, 26.2115, 26.2136, 26.2167, 26.2198, 26.2212,\n",
            "        26.2188, 26.2175, 26.2250, 26.1905, 26.2228, 26.2134, 26.2164, 26.2144,\n",
            "        26.2115, 26.2022, 26.2092, 26.2140, 26.2094, 26.2118, 26.2199, 26.2186,\n",
            "        26.2118, 26.2217, 26.2126, 26.2185, 26.2222, 26.2262],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0111, 0.0109, 0.0103, 0.0101, 0.0102, 0.0099, 0.0109, 0.0101, 0.0101,\n",
            "        0.0097, 0.0102, 0.0101, 0.0102, 0.0102, 0.0101, 0.0112, 0.0105, 0.0090,\n",
            "        0.0102, 0.0102, 0.0103, 0.0102, 0.0099, 0.0110, 0.0102, 0.0101, 0.0104,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0101, 0.0101, 0.0104, 0.0108, 0.0102,\n",
            "        0.0105, 0.0101, 0.0101, 0.0103, 0.0100, 0.0102, 0.0102, 0.0099, 0.0104,\n",
            "        0.0102, 0.0101, 0.0101, 0.0101, 0.0103, 0.0103, 0.0103, 0.0100, 0.0101,\n",
            "        0.0102, 0.0093, 0.0102, 0.0100, 0.0102, 0.0102, 0.0102, 0.0101, 0.0104,\n",
            "        0.0101, 0.0099, 0.0102, 0.0105, 0.0102, 0.0101, 0.0103, 0.0103, 0.0103,\n",
            "        0.0095, 0.0100, 0.0109, 0.0096, 0.0101, 0.0102, 0.0105, 0.0102, 0.0101,\n",
            "        0.0101, 0.0102, 0.0101, 0.0102, 0.0102, 0.0102, 0.0102, 0.0103, 0.0101,\n",
            "        0.0102, 0.0102, 0.0103, 0.0099, 0.0104, 0.0101, 0.0101, 0.0102],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [4]\n",
            "DEBUGGING: logits looks like: tensor([26.2534, 26.2481, 26.2340, 26.2300, 26.2304, 26.2226, 26.2480, 26.2291,\n",
            "        26.2298, 26.2191, 26.2315, 26.2284, 26.2318, 26.2318, 26.2299, 26.2557,\n",
            "        26.2376, 26.2013, 26.2317, 26.2304, 26.2325, 26.2315, 26.2230, 26.2503,\n",
            "        26.2303, 26.2290, 26.2356, 26.2323, 26.2301, 26.2314, 26.2312, 26.2294,\n",
            "        26.2285, 26.2369, 26.2446, 26.2316, 26.2390, 26.2280, 26.2282, 26.2336,\n",
            "        26.2265, 26.2300, 26.2304, 26.2240, 26.2357, 26.2313, 26.2288, 26.2295,\n",
            "        26.2284, 26.2337, 26.2348, 26.2329, 26.2269, 26.2292, 26.2308, 26.2094,\n",
            "        26.2302, 26.2255, 26.2303, 26.2312, 26.2303, 26.2278, 26.2368, 26.2289,\n",
            "        26.2229, 26.2320, 26.2386, 26.2311, 26.2286, 26.2327, 26.2333, 26.2343,\n",
            "        26.2134, 26.2268, 26.2469, 26.2159, 26.2284, 26.2321, 26.2383, 26.2319,\n",
            "        26.2292, 26.2296, 26.2309, 26.2295, 26.2319, 26.2315, 26.2319, 26.2320,\n",
            "        26.2332, 26.2284, 26.2322, 26.2324, 26.2348, 26.2247, 26.2364, 26.2293,\n",
            "        26.2292, 26.2325], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0106, 0.0100, 0.0095, 0.0091, 0.0100, 0.0102, 0.0094, 0.0106, 0.0092,\n",
            "        0.0086, 0.0091, 0.0100, 0.0090, 0.0092, 0.0078, 0.0099, 0.0093, 0.0087,\n",
            "        0.0077, 0.0114, 0.0093, 0.0090, 0.0085, 0.0112, 0.0089, 0.0097, 0.0088,\n",
            "        0.0099, 0.0084, 0.0090, 0.0094, 0.0094, 0.0095, 0.0086, 0.0092, 0.0082,\n",
            "        0.0100, 0.0085, 0.0099, 0.0104, 0.0105, 0.0095, 0.0098, 0.0102, 0.0094,\n",
            "        0.0092, 0.0090, 0.0094, 0.0101, 0.0094, 0.0085, 0.0094, 0.0086, 0.0105,\n",
            "        0.0097, 0.0097, 0.0092, 0.0097, 0.0099, 0.0096, 0.0092, 0.0101, 0.0080,\n",
            "        0.0088, 0.0092, 0.0089, 0.0083, 0.0094, 0.0103, 0.0093, 0.0096, 0.0096,\n",
            "        0.0096, 0.0097, 0.0101, 0.0079, 0.0098, 0.0096, 0.0094, 0.0097, 0.0095,\n",
            "        0.0096, 0.0088, 0.0097, 0.0099, 0.0094, 0.0093, 0.0094, 0.0104, 0.0098,\n",
            "        0.0090, 0.0096, 0.0095, 0.0099, 0.0094, 0.0095, 0.0090, 0.0096, 0.0095,\n",
            "        0.0097, 0.0100, 0.0086, 0.0088, 0.0099, 0.0090, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [104]\n",
            "DEBUGGING: logits looks like: tensor([26.2761, 26.2629, 26.2483, 26.2398, 26.2619, 26.2669, 26.2481, 26.2763,\n",
            "        26.2424, 26.2238, 26.2384, 26.2625, 26.2350, 26.2423, 26.1991, 26.2599,\n",
            "        26.2454, 26.2275, 26.1959, 26.2940, 26.2436, 26.2367, 26.2230, 26.2912,\n",
            "        26.2340, 26.2554, 26.2303, 26.2594, 26.2202, 26.2354, 26.2471, 26.2481,\n",
            "        26.2482, 26.2243, 26.2406, 26.2122, 26.2634, 26.2226, 26.2587, 26.2715,\n",
            "        26.2748, 26.2505, 26.2582, 26.2662, 26.2475, 26.2420, 26.2350, 26.2473,\n",
            "        26.2655, 26.2472, 26.2219, 26.2481, 26.2234, 26.2737, 26.2552, 26.2548,\n",
            "        26.2403, 26.2543, 26.2606, 26.2516, 26.2422, 26.2647, 26.2066, 26.2291,\n",
            "        26.2411, 26.2319, 26.2158, 26.2470, 26.2706, 26.2454, 26.2529, 26.2509,\n",
            "        26.2530, 26.2554, 26.2643, 26.2025, 26.2576, 26.2509, 26.2456, 26.2551,\n",
            "        26.2502, 26.2533, 26.2314, 26.2551, 26.2608, 26.2476, 26.2442, 26.2472,\n",
            "        26.2728, 26.2564, 26.2356, 26.2510, 26.2495, 26.2585, 26.2478, 26.2496,\n",
            "        26.2362, 26.2516, 26.2501, 26.2540, 26.2631, 26.2242, 26.2312, 26.2589,\n",
            "        26.2360, 26.2651], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.7082653196371211 and immediate abs rewards look like: [0.011022115781997854, 0.011282827910235937, 0.017809225102155324, 0.007576851168778376, 0.007985525429830886, 0.0011845784611068666, 0.0028682989664048364, 0.0002286971862304199, 0.00011396378977224231, 0.0017083363545680186, 0.007535841970820911, 0.00934559183087913, 0.00428853896801229, 0.0005096848126413533, 2.6590158086037263e-05, 0.0007355489619840228, 0.0003190349630131095, 0.00016956092940745293, 0.00011938227362406906, 0.0013282485178933712, 0.622106876082853, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 1.3642420526593924e-12, 9.094947017729282e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 2.2737367544323206e-12, 4.092726157978177e-12, 2.2737367544323206e-12, 0.0, 0.0, 0.0]\n",
            "DEBUGGING: the total relative reward of the trajectory = 37.469909216714754 and immediate relative rewards look like: [0.029723104556951728, 0.061033733991445537, 0.144949130911426, 0.0826231074700762, 0.10907476226585336, 0.01945872148759934, 0.054987319092520456, 0.005014552526256769, 0.002811369908923658, 0.04682691524651324, 0.22732649103005065, 0.30818541342169664, 0.15360081923966776, 0.019682669012912768, 0.0011003415209226244, 0.03246759187939083, 0.014965590136284186, 0.00842254176153184, 0.006259773916894022, 0.07331432589343753, 36.0680809393177, 3.3348139065012427e-11, 3.4863963567962245e-11, 7.27595761418453e-11, 7.579122514773254e-11, 3.941143707683287e-11, 4.0927261579781764e-11, 1.2732925824819067e-10, 8.791782117140972e-11, 0.0, 0.0, 4.850638409455617e-11, 5.002220859751864e-11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.214880462115009e-11, 6.366462912409532e-11, 6.518045362705985e-11, 0.0, 3.410605131648998e-10, 6.2755134422294e-10, 3.562187581946669e-10, 0.0, 0.0, 0.0]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 200 number of matrices\n",
            "DEBUGGING: ACT_MAT has 200 number of matrices\n",
            "DEBUGGING: VAL looks like: [[2.7679325766143923, 2.691551222863871, 2.2100681296411744, 2.0751186514926174, 2.0548457399487896, 2.040943918838719, 2.0190044360670214, 1.8510522464261792, 1.38583970213703, 1.3745927168048275, 1.3807441271044294, 1.3781298088020262, 1.29272252871039, 1.2220715459330564, 1.1207630529763255, 1.0157054220599813, 0.9993945437399905, 0.9845662954671418, 0.9936214162282526, 0.7739020743921664, 0.739769553080713, 0.7077604483933714, 0.5274048798221402, 0.4352550323989268, 0.3559746769899535, 0.35736630882521925, 0.30015051730762493, 0.2843212371439522, 0.2818441292318973, 0.2718834680631025, 0.24393369624425962, 0.23475474709858926, 0.2205383651763061, 0.2030571594594498, 0.1947800761061318, 0.16402002968994778, 0.15779617834374926, 0.09744422562243754, 0.08188044033794777, 0.05237182335500835, 0.050680926225034374, 0.04806454622365229, 0.04614765440845535, 0.046394840859185035, 0.04607860577892596, 0.04542794807974851, 0.04209190797647715, 0.03985113005677531, 0.032024671418697734, 0.001979728292119731], [4.167025424588126, 4.2003943519847144, 4.201293170937641, 4.238747965752471, 4.039175752879161, 4.0348902548918435, 3.9315191804990635, 3.9700262463052876, 3.9572650620726924, 3.993370254309182, 3.54760587908414, 3.582479268722649, 3.61850813148171, 3.202186815033933, 3.2112300077795117, 3.2057265950589193, 3.2281188904763214, 3.2591766077676447, 3.2414629738885017, 3.1832156321046634, 3.0919677031483794, 2.996375625379878, 2.78206945423187, 2.057485183198833, 2.062797867254655, 1.90777491864712, 1.885878421103198, 1.813068964178559, 1.656760315249683, 1.6702612809761317, 1.6782356832696466, 1.6931864330839814, 1.6818141104267386, 1.6008538145969118, 1.6075419334523346, 1.5072315012510635, 1.5222038171799794, 1.5369746352785933, 1.5488761690853006, 1.559804411666484, 1.5282814588361182, 1.5046065345190527, 1.0474696678307418, 1.0090273772502352, 0.6808234517059828, 0.6661192768834787, 0.6064211848140941, 0.518692102371932, 0.4466682589696793, 0.29685099129770665], [39.106102417785145, 39.49829985389484, 39.825583995722226, 39.976135747480484, 40.35270789663715, 40.71130628294021, 41.068393081932975, 41.32703101734049, 41.54653175100793, 41.889263234898834, 42.31058823300455, 42.7291687677763, 43.02481682936559, 43.39745581899297, 43.5856913337015, 1.9941547123358e-09, 2.0142976892280806e-09, 2.0346441305334148e-09, 2.055196091447894e-09, 2.0759556479271654e-09, 2.0969248968961265e-09, 2.1181059564607337e-09, 2.1395009661219534e-09, 2.1611120869918723e-09, 2.1278206109954513e-09, 1.9770609640536022e-09, 1.9970312768218203e-09, 1.8931813051337565e-09, 1.912304348619956e-09, 1.931620554161572e-09, 1.9511318728904767e-09, 1.970840275646946e-09, 1.9172532318233454e-09, 1.8608282009364604e-09, 1.8015365164502642e-09, 1.7393492222678292e-09, 1.674237069806329e-09, 1.521192474725632e-09, 1.4492833111688728e-09, 1.463922536534215e-09, 1.3868414811686082e-09, 1.3066851254918226e-09, 1.2234224058643082e-09, 1.038263681801185e-09, 8.466412600113911e-10, 6.48489850618716e-10, 6.550402531502181e-10, 5.537117431231059e-10, 4.49063009000358e-10, 3.410605131648481e-10], [30.787268142094284, 31.06822731064377, 31.320397552174065, 31.490351940669335, 31.72497861939319, 31.935256421340746, 32.23817949480116, 32.50827492495822, 32.831576133769666, 33.16036844834418, 33.44802175060371, 33.55625783795319, 33.5839115399308, 33.76799062696074, 34.08919995752306, 34.43242385454761, 34.74743056835174, 35.08329795779339, 35.42916708690087, 35.78071445755957, 36.0680809410769, 1.776967377113662e-09, 1.7612315535844945e-09, 1.7438056464813458e-09, 1.687925323575253e-09, 1.6284182812399198e-09, 1.6050574183465525e-09, 1.5799294512795665e-09, 1.467272922253915e-09, 1.3932879808914194e-09, 1.4073615968600196e-09, 1.4215773705656764e-09, 1.386940390374869e-09, 1.3504224058357074e-09, 1.3640630361976842e-09, 1.3778414507047315e-09, 1.3917590411158905e-09, 1.4058172132483742e-09, 1.42001738711957e-09, 1.4343609970904746e-09, 1.4488494920105804e-09, 1.4007077650398286e-09, 1.3505486221371044e-09, 1.2983516853636813e-09, 1.3114663488522032e-09, 9.8020791483566e-10, 3.562187581946669e-10, 0.0, 0.0, 0.0]]\n",
            "DEBUGGING: traj_returns = [2.7679325766143923, 4.167025424588126, 39.106102417785145, 30.787268142094284]\n",
            "DEBUGGING: actions = [[35], [20], [52], [11], [12], [1], [28], [53], [21], [21], [19], [6], [27], [48], [17], [26], [59], [53], [13], [1], [22], [23], [45], [6], [32], [42], [57], [71], [79], [6], [55], [6], [59], [92], [46], [92], [22], [2], [9], [37], [91], [12], [36], [26], [73], [63], [79], [74], [13], [96], [59], [44], [47], [2], [59], [62], [38], [58], [1], [8], [33], [18], [18], [59], [70], [54], [46], [4], [35], [48], [8], [46], [79], [25], [78], [43], [8], [80], [7], [57], [14], [38], [67], [61], [88], [23], [16], [73], [77], [12], [20], [8], [44], [69], [44], [59], [55], [64], [52], [3], [40], [59], [8], [1], [6], [17], [27], [11], [50], [37], [4], [30], [66], [68], [0], [11], [44], [40], [72], [23], [23], [44], [27], [28], [11], [53], [41], [32], [48], [18], [26], [86], [63], [39], [68], [32], [23], [9], [91], [80], [19], [61], [9], [28], [73], [57], [101], [92], [87], [34], [44], [31], [16], [39], [52], [19], [6], [1], [35], [9], [61], [20], [62], [5], [6], [58], [31], [42], [70], [51], [0], [38], [25], [41], [19], [27], [30], [55], [77], [70], [22], [53], [77], [49], [72], [57], [86], [1], [37], [4], [12], [33], [46], [23], [36], [97], [81], [29], [9], [104]]\n",
            "DEBUGGING: actions length = 200\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[ 2.9913,  0.9482,  0.0507,  ...,  2.2733, -0.1500,  1.6288],\n",
            "        [ 2.9285,  0.9129,  0.0500,  ...,  2.2115, -0.2208,  1.5443],\n",
            "        [ 2.9744,  0.9382,  0.0640,  ...,  2.2556, -0.1747,  1.6094],\n",
            "        ...,\n",
            "        [ 2.9453,  0.9097,  0.0602,  ...,  2.2175, -0.2209,  1.5639],\n",
            "        [ 2.9443,  0.9100,  0.0603,  ...,  2.2174, -0.2204,  1.5634],\n",
            "        [ 2.9434,  0.9112,  0.0603,  ...,  2.2180, -0.2189,  1.5628]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[ 2.9451,  0.9123,  0.0606,  ...,  2.2202, -0.2173,  1.5647],\n",
            "        [ 2.9445,  0.9114,  0.0605,  ...,  2.2188, -0.2189,  1.5637],\n",
            "        [ 2.9438,  0.9096,  0.0604,  ...,  2.2171, -0.2218,  1.5618],\n",
            "        ...,\n",
            "        [ 2.9446,  0.9108,  0.0607,  ...,  2.2184, -0.2197,  1.5637],\n",
            "        [ 2.9431,  0.9090,  0.0615,  ...,  2.2164, -0.2223,  1.5619],\n",
            "        [ 2.9445,  0.9114,  0.0596,  ...,  2.2186, -0.2198,  1.5627]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([26.2761, 26.2629, 26.2483, 26.2398, 26.2619, 26.2669, 26.2481, 26.2763,\n",
            "        26.2424, 26.2238, 26.2384, 26.2625, 26.2350, 26.2423, 26.1991, 26.2599,\n",
            "        26.2454, 26.2275, 26.1959, 26.2940, 26.2436, 26.2367, 26.2230, 26.2912,\n",
            "        26.2340, 26.2554, 26.2303, 26.2594, 26.2202, 26.2354, 26.2471, 26.2481,\n",
            "        26.2482, 26.2243, 26.2406, 26.2122, 26.2634, 26.2226, 26.2587, 26.2715,\n",
            "        26.2748, 26.2505, 26.2582, 26.2662, 26.2475, 26.2420, 26.2350, 26.2473,\n",
            "        26.2655, 26.2472, 26.2219, 26.2481, 26.2234, 26.2737, 26.2552, 26.2548,\n",
            "        26.2403, 26.2543, 26.2606, 26.2516, 26.2422, 26.2647, 26.2066, 26.2291,\n",
            "        26.2411, 26.2319, 26.2158, 26.2470, 26.2706, 26.2454, 26.2529, 26.2509,\n",
            "        26.2530, 26.2554, 26.2643, 26.2025, 26.2576, 26.2509, 26.2456, 26.2551,\n",
            "        26.2502, 26.2533, 26.2314, 26.2551, 26.2608, 26.2476, 26.2442, 26.2472,\n",
            "        26.2728, 26.2564, 26.2356, 26.2510, 26.2495, 26.2585, 26.2478, 26.2496,\n",
            "        26.2362, 26.2516, 26.2501, 26.2540, 26.2631, 26.2242, 26.2312, 26.2589,\n",
            "        26.2360, 26.2651], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[19.20708214 19.36461818 19.38933571 19.44508858 19.542927   19.68059922\n",
            "  19.81427405 19.91409611 19.93030316 20.10439866 20.17174    20.31150892\n",
            "  20.37998976 20.3974262  20.50172109  9.66346397  9.743736    9.83176022\n",
            "   9.91606287  9.93445804  9.97495455  0.92603402  0.82736858  0.62318505\n",
            "   0.60469314  0.56628531  0.54650724  0.52434755  0.48465111  0.48553619\n",
            "   0.48054235  0.4819853   0.47558812  0.45097774  0.4505805   0.41781288\n",
            "   0.42        0.40860472  0.40768915  0.40304406  0.3947406   0.38816777\n",
            "   0.27340433  0.26385556  0.18172551  0.17788681  0.16212827  0.13963581\n",
            "   0.11967323  0.07470768]]\n",
            "DEBUGGING: baseline2 looks like: 19.207082140270487\n",
            "DEBUGGING: ADS looks like: [-16.43914956 -16.67306696 -17.17926758 -17.36996992 -17.48808126\n",
            " -17.6396553  -17.79526961 -18.06304386 -18.54446346 -18.72980595\n",
            " -18.79099587 -18.93337911 -19.08726723 -19.17535466 -19.38095804\n",
            "  -8.64775855  -8.74434146  -8.84719392  -8.92244145  -9.16055597\n",
            "  -9.235185    -0.21827357  -0.2999637   -0.18793002  -0.24871846\n",
            "  -0.208919    -0.24635672  -0.24002631  -0.20280698  -0.21365272\n",
            "  -0.23660865  -0.24723055  -0.25504975  -0.24792058  -0.25580043\n",
            "  -0.25379285  -0.26220382  -0.31116049  -0.32580871  -0.35067224\n",
            "  -0.34405967  -0.34010322  -0.22725668  -0.21746071  -0.13564691\n",
            "  -0.13245886  -0.12003637  -0.09978468  -0.08764856  -0.07272795\n",
            " -15.04005672 -15.16422383 -15.18804254 -15.20634061 -15.50375125\n",
            " -15.64570896 -15.88275487 -15.94406986 -15.9730381  -16.11102841\n",
            " -16.62413412 -16.72902965 -16.76148163 -17.19523939 -17.29049108\n",
            "  -6.45773737  -6.51561711  -6.57258361  -6.6745999   -6.75124241\n",
            "  -6.88298685   2.07034161   1.95470087   1.43430013   1.45810473\n",
            "   1.34148961   1.33937119   1.28872141   1.1721092    1.18472509\n",
            "   1.19769334   1.21120114   1.20622599   1.14987607   1.15696143\n",
            "   1.08941862   1.10220382   1.12836992   1.14118702   1.15676035\n",
            "   1.13354086   1.11643876   0.77406534   0.74517182   0.49909794\n",
            "   0.48823247   0.44429291   0.37905629   0.32699503   0.22214331\n",
            "  19.89902028  20.13368167  20.43624828  20.53104717  20.80978089\n",
            "  21.03070706  21.25411903  21.41293491  21.61622859  21.78486457\n",
            "  22.13884824  22.41765985  22.64482707  23.00002962  23.08397025\n",
            "  -9.66346397  -9.743736    -9.83176021  -9.91606287  -9.93445804\n",
            "  -9.97495455  -0.92603402  -0.82736858  -0.62318505  -0.60469313\n",
            "  -0.56628531  -0.54650723  -0.52434755  -0.48465111  -0.48553619\n",
            "  -0.48054234  -0.48198529  -0.47558812  -0.45097774  -0.4505805\n",
            "  -0.41781288  -0.42        -0.40860471  -0.40768915  -0.40304406\n",
            "  -0.3947406   -0.38816777  -0.27340433  -0.26385555  -0.18172551\n",
            "  -0.17788681  -0.16212827  -0.13963581  -0.11967323  -0.07470768\n",
            "  11.580186    11.70360913  11.93106184  12.04526336  12.18205162\n",
            "  12.2546572   12.42390545  12.59417882  12.90127297  13.05596978\n",
            "  13.27628175  13.24474892  13.20392178  13.37056443  13.58747887\n",
            "  24.76895989  25.00369457  25.25153774  25.51310422  25.84625642\n",
            "  26.09312639  -0.92603402  -0.82736858  -0.62318505  -0.60469314\n",
            "  -0.56628531  -0.54650723  -0.52434755  -0.48465111  -0.48553619\n",
            "  -0.48054234  -0.48198529  -0.47558812  -0.45097774  -0.4505805\n",
            "  -0.41781288  -0.42        -0.40860471  -0.40768915  -0.40304406\n",
            "  -0.3947406   -0.38816777  -0.27340433  -0.26385555  -0.18172551\n",
            "  -0.17788681  -0.16212827  -0.13963581  -0.11967323  -0.07470768]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(-0.0465, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0385, -0.0350, -0.0313,  ...,  0.0613,  0.0027,  0.1948],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [-0.0306, -0.0279, -0.0250,  ...,  0.0486,  0.0021,  0.1315],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0101,  0.0092,  0.0082,  ..., -0.0164, -0.0008, -0.0855]])\n",
            "   Last layer:\n",
            "tensor([[ 0.0350,  0.0291,  0.0000,  0.0015,  0.0000,  0.0000,  0.0000,  0.1003,\n",
            "          0.0041,  0.0000,  0.0752,  0.0000,  0.0000, -0.0324,  0.0000,  0.0418,\n",
            "          0.0000,  0.0642,  0.0852,  0.0252],\n",
            "        [ 0.0149,  0.0233,  0.0000,  0.0036,  0.0000,  0.0000,  0.0000,  0.0540,\n",
            "          0.0120,  0.0000,  0.0501,  0.0000,  0.0000, -0.0082,  0.0000,  0.0339,\n",
            "          0.0000,  0.0410,  0.0411,  0.0097],\n",
            "        [-0.0024, -0.0112,  0.0000, -0.0026,  0.0000,  0.0000,  0.0000, -0.0163,\n",
            "         -0.0089,  0.0000, -0.0203,  0.0000,  0.0000, -0.0026,  0.0000, -0.0165,\n",
            "          0.0000, -0.0158, -0.0098, -0.0008],\n",
            "        [ 0.0137,  0.0167,  0.0000,  0.0020,  0.0000,  0.0000,  0.0000,  0.0448,\n",
            "          0.0065,  0.0000,  0.0382,  0.0000,  0.0000, -0.0100,  0.0000,  0.0242,\n",
            "          0.0000,  0.0318,  0.0358,  0.0094],\n",
            "        [ 0.0049,  0.0040,  0.0000,  0.0002,  0.0000,  0.0000,  0.0000,  0.0139,\n",
            "          0.0005,  0.0000,  0.0104,  0.0000,  0.0000, -0.0045,  0.0000,  0.0058,\n",
            "          0.0000,  0.0089,  0.0118,  0.0035],\n",
            "        [-0.0063, -0.0155,  0.0000, -0.0031,  0.0000,  0.0000,  0.0000, -0.0285,\n",
            "         -0.0104,  0.0000, -0.0304,  0.0000,  0.0000,  0.0005,  0.0000, -0.0227,\n",
            "          0.0000, -0.0243, -0.0197, -0.0035],\n",
            "        [ 0.0380,  0.0483,  0.0000,  0.0062,  0.0000,  0.0000,  0.0000,  0.1261,\n",
            "          0.0201,  0.0000,  0.1093,  0.0000,  0.0000, -0.0265,  0.0000,  0.0701,\n",
            "          0.0000,  0.0905,  0.0998,  0.0258],\n",
            "        [ 0.0289,  0.0310,  0.0000,  0.0031,  0.0000,  0.0000,  0.0000,  0.0900,\n",
            "          0.0099,  0.0000,  0.0737,  0.0000,  0.0000, -0.0232,  0.0000,  0.0448,\n",
            "          0.0000,  0.0618,  0.0734,  0.0202],\n",
            "        [ 0.0033,  0.0189,  0.0000,  0.0045,  0.0000,  0.0000,  0.0000,  0.0262,\n",
            "          0.0156,  0.0000,  0.0338,  0.0000,  0.0000,  0.0053,  0.0000,  0.0279,\n",
            "          0.0000,  0.0263,  0.0152,  0.0008],\n",
            "        [ 0.0189,  0.0162,  0.0000,  0.0009,  0.0000,  0.0000,  0.0000,  0.0547,\n",
            "          0.0026,  0.0000,  0.0414,  0.0000,  0.0000, -0.0173,  0.0000,  0.0233,\n",
            "          0.0000,  0.0353,  0.0463,  0.0136]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0089,  0.0072, -0.0077,  ..., -0.0042,  0.0177, -0.0942],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [ 0.0064,  0.0052, -0.0056,  ..., -0.0031,  0.0128, -0.0692],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0003, -0.0002,  0.0003,  ...,  0.0002, -0.0007,  0.0111]])\n",
            "   Last layer:\n",
            "tensor([[-4.5376e-03, -3.1288e-02,  0.0000e+00, -1.3917e-02,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -5.2305e-02, -2.5442e-02,  0.0000e+00,\n",
            "         -4.8447e-02,  0.0000e+00,  0.0000e+00,  4.6885e-03,  0.0000e+00,\n",
            "         -2.6232e-02,  0.0000e+00, -3.4613e-02, -4.0198e-02, -9.7947e-04],\n",
            "        [-1.8580e-03, -1.3138e-02,  0.0000e+00, -4.6353e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -1.9829e-02, -9.8312e-03,  0.0000e+00,\n",
            "         -1.9874e-02,  0.0000e+00,  0.0000e+00,  9.6158e-04,  0.0000e+00,\n",
            "         -1.2058e-02,  0.0000e+00, -1.4587e-02, -1.4936e-02, -7.5863e-04],\n",
            "        [ 2.1089e-04,  1.6858e-03,  0.0000e+00, -3.4259e-05,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  1.4613e-03,  8.0807e-04,  0.0000e+00,\n",
            "          2.3204e-03,  0.0000e+00,  0.0000e+00,  4.1324e-04,  0.0000e+00,\n",
            "          2.1146e-03,  0.0000e+00,  1.9168e-03,  9.1624e-04,  2.7659e-04],\n",
            "        [-1.4904e-03, -1.0227e-02,  0.0000e+00, -4.6106e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -1.7202e-02, -8.3616e-03,  0.0000e+00,\n",
            "         -1.5855e-02,  0.0000e+00,  0.0000e+00,  1.5966e-03,  0.0000e+00,\n",
            "         -8.5028e-03,  0.0000e+00, -1.1303e-02, -1.3237e-02, -3.0264e-04],\n",
            "        [-4.2702e-04, -2.9574e-03,  0.0000e+00, -1.2977e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -4.9168e-03, -2.3889e-03,  0.0000e+00,\n",
            "         -4.5727e-03,  0.0000e+00,  0.0000e+00,  4.2624e-04,  0.0000e+00,\n",
            "         -2.4979e-03,  0.0000e+00, -3.2744e-03, -3.7719e-03, -9.7129e-05],\n",
            "        [ 7.9368e-04,  5.8377e-03,  0.0000e+00,  1.2991e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  7.4824e-03,  3.8259e-03,  0.0000e+00,\n",
            "          8.5457e-03,  0.0000e+00,  0.0000e+00,  2.1854e-04,  0.0000e+00,\n",
            "          6.0307e-03,  0.0000e+00,  6.5278e-03,  5.4203e-03,  5.5427e-04],\n",
            "        [-4.9524e-03, -3.4523e-02,  0.0000e+00, -1.4006e-02,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -5.5340e-02, -2.7110e-02,  0.0000e+00,\n",
            "         -5.2949e-02,  0.0000e+00,  0.0000e+00,  4.0419e-03,  0.0000e+00,\n",
            "         -3.0125e-02,  0.0000e+00, -3.8258e-02, -4.2188e-02, -1.4702e-03],\n",
            "        [-3.7551e-03, -2.6126e-02,  0.0000e+00, -1.0727e-02,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -4.2093e-02, -2.0612e-02,  0.0000e+00,\n",
            "         -4.0104e-02,  0.0000e+00,  0.0000e+00,  3.1746e-03,  0.0000e+00,\n",
            "         -2.2667e-02,  0.0000e+00, -2.8937e-02, -3.2124e-02, -1.0736e-03],\n",
            "        [-1.6747e-03, -1.3031e-02,  0.0000e+00, -4.9082e-04,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -1.2474e-02, -6.8409e-03,  0.0000e+00,\n",
            "         -1.8159e-02,  0.0000e+00,  0.0000e+00, -2.5238e-03,  0.0000e+00,\n",
            "         -1.5576e-02,  0.0000e+00, -1.4711e-02, -8.2326e-03, -1.9199e-03],\n",
            "        [-3.4159e-03, -2.4295e-02,  0.0000e+00, -8.1424e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -3.5925e-02, -1.7873e-02,  0.0000e+00,\n",
            "         -3.6596e-02,  0.0000e+00,  0.0000e+00,  1.3948e-03,  0.0000e+00,\n",
            "         -2.2693e-02,  0.0000e+00, -2.7016e-02, -2.6933e-02, -1.5181e-03]])\n",
            "DEBUGGING: training for one iteration takes 0.004270 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 24\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0147, 0.0144, 0.0149, 0.0154, 0.0148, 0.0153, 0.0150, 0.0134, 0.0148,\n",
            "        0.0148, 0.0155, 0.0142, 0.0149, 0.0145, 0.0142, 0.0123, 0.0150, 0.0149,\n",
            "        0.0131, 0.0150, 0.0145, 0.0150, 0.0148, 0.0138, 0.0145, 0.0144, 0.0143,\n",
            "        0.0141, 0.0144, 0.0150, 0.0148, 0.0136, 0.0139, 0.0138, 0.0137, 0.0146,\n",
            "        0.0144, 0.0145, 0.0149, 0.0145, 0.0139, 0.0146, 0.0136, 0.0148, 0.0140,\n",
            "        0.0136, 0.0173, 0.0142, 0.0142, 0.0146, 0.0148, 0.0149, 0.0144, 0.0143,\n",
            "        0.0137, 0.0148, 0.0146, 0.0141, 0.0147, 0.0162, 0.0138, 0.0147, 0.0151,\n",
            "        0.0140, 0.0143, 0.0152, 0.0148, 0.0137, 0.0148],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [41]\n",
            "DEBUGGING: logits looks like: tensor([27.4290, 27.4240, 27.4337, 27.4415, 27.4313, 27.4391, 27.4343, 27.4062,\n",
            "        27.4311, 27.4307, 27.4432, 27.4215, 27.4325, 27.4264, 27.4205, 27.3858,\n",
            "        27.4346, 27.4335, 27.4003, 27.4346, 27.4262, 27.4354, 27.4320, 27.4140,\n",
            "        27.4266, 27.4242, 27.4222, 27.4185, 27.4239, 27.4354, 27.4307, 27.4105,\n",
            "        27.4153, 27.4140, 27.4128, 27.4273, 27.4242, 27.4263, 27.4327, 27.4257,\n",
            "        27.4163, 27.4285, 27.4105, 27.4320, 27.4176, 27.4099, 27.4709, 27.4204,\n",
            "        27.4207, 27.4286, 27.4318, 27.4324, 27.4247, 27.4231, 27.4125, 27.4307,\n",
            "        27.4287, 27.4195, 27.4292, 27.4534, 27.4141, 27.4291, 27.4362, 27.4174,\n",
            "        27.4226, 27.4373, 27.4318, 27.4128, 27.4311], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0134, 0.0126, 0.0126, 0.0131, 0.0128, 0.0127, 0.0128, 0.0127, 0.0126,\n",
            "        0.0126, 0.0127, 0.0125, 0.0127, 0.0127, 0.0126, 0.0126, 0.0128, 0.0127,\n",
            "        0.0126, 0.0127, 0.0131, 0.0125, 0.0124, 0.0126, 0.0125, 0.0127, 0.0125,\n",
            "        0.0128, 0.0127, 0.0128, 0.0128, 0.0127, 0.0126, 0.0127, 0.0126, 0.0126,\n",
            "        0.0126, 0.0126, 0.0127, 0.0126, 0.0126, 0.0127, 0.0126, 0.0126, 0.0127,\n",
            "        0.0126, 0.0126, 0.0128, 0.0125, 0.0126, 0.0126, 0.0127, 0.0126, 0.0125,\n",
            "        0.0127, 0.0127, 0.0127, 0.0126, 0.0126, 0.0125, 0.0126, 0.0126, 0.0126,\n",
            "        0.0127, 0.0126, 0.0126, 0.0127, 0.0126, 0.0127, 0.0127, 0.0127, 0.0126,\n",
            "        0.0127, 0.0126, 0.0126, 0.0128, 0.0126, 0.0126, 0.0126],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [76]\n",
            "DEBUGGING: logits looks like: tensor([27.4224, 27.4076, 27.4064, 27.4165, 27.4107, 27.4082, 27.4115, 27.4093,\n",
            "        27.4071, 27.4076, 27.4097, 27.4050, 27.4085, 27.4087, 27.4075, 27.4081,\n",
            "        27.4105, 27.4098, 27.4067, 27.4082, 27.4168, 27.4047, 27.4042, 27.4075,\n",
            "        27.4042, 27.4088, 27.4049, 27.4103, 27.4098, 27.4106, 27.4103, 27.4084,\n",
            "        27.4073, 27.4090, 27.4079, 27.4068, 27.4080, 27.4081, 27.4090, 27.4081,\n",
            "        27.4081, 27.4100, 27.4076, 27.4077, 27.4083, 27.4081, 27.4075, 27.4118,\n",
            "        27.4046, 27.4067, 27.4077, 27.4082, 27.4066, 27.4052, 27.4082, 27.4083,\n",
            "        27.4087, 27.4067, 27.4075, 27.4054, 27.4077, 27.4072, 27.4067, 27.4085,\n",
            "        27.4081, 27.4072, 27.4082, 27.4080, 27.4089, 27.4096, 27.4082, 27.4079,\n",
            "        27.4101, 27.4070, 27.4064, 27.4117, 27.4063, 27.4064, 27.4072],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0115, 0.0110, 0.0100, 0.0112, 0.0109, 0.0110, 0.0127, 0.0110, 0.0111,\n",
            "        0.0110, 0.0116, 0.0116, 0.0109, 0.0109, 0.0110, 0.0110, 0.0109, 0.0129,\n",
            "        0.0110, 0.0112, 0.0113, 0.0106, 0.0112, 0.0112, 0.0110, 0.0115, 0.0111,\n",
            "        0.0110, 0.0110, 0.0108, 0.0110, 0.0108, 0.0109, 0.0115, 0.0111, 0.0114,\n",
            "        0.0110, 0.0109, 0.0112, 0.0110, 0.0109, 0.0110, 0.0110, 0.0110, 0.0111,\n",
            "        0.0109, 0.0110, 0.0110, 0.0111, 0.0112, 0.0113, 0.0111, 0.0109, 0.0111,\n",
            "        0.0109, 0.0109, 0.0110, 0.0127, 0.0109, 0.0111, 0.0118, 0.0108, 0.0111,\n",
            "        0.0110, 0.0109, 0.0114, 0.0110, 0.0111, 0.0111, 0.0109, 0.0111, 0.0109,\n",
            "        0.0109, 0.0109, 0.0110, 0.0109, 0.0111, 0.0109, 0.0110, 0.0110, 0.0108,\n",
            "        0.0110, 0.0109, 0.0112, 0.0119, 0.0112, 0.0112, 0.0110, 0.0111, 0.0109],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [71]\n",
            "DEBUGGING: logits looks like: tensor([27.4172, 27.4063, 27.3823, 27.4104, 27.4034, 27.4053, 27.4414, 27.4067,\n",
            "        27.4076, 27.4054, 27.4196, 27.4189, 27.4038, 27.4026, 27.4066, 27.4053,\n",
            "        27.4045, 27.4449, 27.4068, 27.4105, 27.4133, 27.3967, 27.4095, 27.4110,\n",
            "        27.4065, 27.4172, 27.4087, 27.4060, 27.4061, 27.4024, 27.4063, 27.4017,\n",
            "        27.4035, 27.4168, 27.4079, 27.4141, 27.4065, 27.4043, 27.4110, 27.4067,\n",
            "        27.4045, 27.4069, 27.4071, 27.4071, 27.4076, 27.4039, 27.4055, 27.4052,\n",
            "        27.4074, 27.4103, 27.4125, 27.4080, 27.4036, 27.4074, 27.4038, 27.4036,\n",
            "        27.4070, 27.4427, 27.4028, 27.4083, 27.4242, 27.4008, 27.4081, 27.4059,\n",
            "        27.4042, 27.4141, 27.4064, 27.4079, 27.4083, 27.4035, 27.4090, 27.4035,\n",
            "        27.4033, 27.4048, 27.4065, 27.4034, 27.4081, 27.4046, 27.4058, 27.4066,\n",
            "        27.4019, 27.4052, 27.4048, 27.4102, 27.4266, 27.4097, 27.4104, 27.4055,\n",
            "        27.4085, 27.4038], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0107, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0100, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0100, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [52]\n",
            "DEBUGGING: logits looks like: tensor([27.4131, 27.3973, 27.3983, 27.3983, 27.3981, 27.3994, 27.3985, 27.3983,\n",
            "        27.3987, 27.3975, 27.3970, 27.3983, 27.3983, 27.3984, 27.3979, 27.3975,\n",
            "        27.3978, 27.3982, 27.3978, 27.3978, 27.3980, 27.3978, 27.3979, 27.3981,\n",
            "        27.3981, 27.3983, 27.3983, 27.3981, 27.3984, 27.3985, 27.3978, 27.3987,\n",
            "        27.3990, 27.3984, 27.3979, 27.3981, 27.3986, 27.3987, 27.3983, 27.3983,\n",
            "        27.3981, 27.3968, 27.3985, 27.3982, 27.3974, 27.3980, 27.3987, 27.3984,\n",
            "        27.3982, 27.3975, 27.3975, 27.3984, 27.3981, 27.3987, 27.3979, 27.3979,\n",
            "        27.3981, 27.3972, 27.3983, 27.3980, 27.3973, 27.3985, 27.3989, 27.3983,\n",
            "        27.3982, 27.3981, 27.3979, 27.3975, 27.3981, 27.3988, 27.3985, 27.3990,\n",
            "        27.3983, 27.3983, 27.3979, 27.3982, 27.3993, 27.3983, 27.3981, 27.3977,\n",
            "        27.3989, 27.3983, 27.3986, 27.3983, 27.3979, 27.3983, 27.3986, 27.3982,\n",
            "        27.3980, 27.3984, 27.3990, 27.3995, 27.3984, 27.3972, 27.3987, 27.3979,\n",
            "        27.3983, 27.3980, 27.3986], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0098, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0091, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0091, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [30]\n",
            "DEBUGGING: logits looks like: tensor([27.4096, 27.3938, 27.3943, 27.3940, 27.3942, 27.3940, 27.3941, 27.3941,\n",
            "        27.3941, 27.3943, 27.3943, 27.3938, 27.3939, 27.3940, 27.3941, 27.3944,\n",
            "        27.3940, 27.3939, 27.3942, 27.3949, 27.3942, 27.3943, 27.3941, 27.3943,\n",
            "        27.3941, 27.3942, 27.3943, 27.3941, 27.3945, 27.3943, 27.3942, 27.3943,\n",
            "        27.3939, 27.3941, 27.3942, 27.3944, 27.3943, 27.3942, 27.3942, 27.3942,\n",
            "        27.3941, 27.3945, 27.3940, 27.3940, 27.3940, 27.3940, 27.3940, 27.3943,\n",
            "        27.3943, 27.3939, 27.3938, 27.3943, 27.3940, 27.3943, 27.3941, 27.3939,\n",
            "        27.3942, 27.3942, 27.3944, 27.3941, 27.3938, 27.3942, 27.3940, 27.3942,\n",
            "        27.3944, 27.3941, 27.3941, 27.3938, 27.3941, 27.3936, 27.3940, 27.3944,\n",
            "        27.3943, 27.3942, 27.3945, 27.3943, 27.3940, 27.3939, 27.3943, 27.3942,\n",
            "        27.3941, 27.3942, 27.3942, 27.3936, 27.3941, 27.3940, 27.3941, 27.3940,\n",
            "        27.3943, 27.3944, 27.3941, 27.3938, 27.3941, 27.3941, 27.3939, 27.3942,\n",
            "        27.3945, 27.3938, 27.3941, 27.3934, 27.3946, 27.3942, 27.3939, 27.3943,\n",
            "        27.3937, 27.3942, 27.3943, 27.3945, 27.3941], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.03001234273187947 and immediate abs rewards look like: [0.004276747504718514, 0.0037826374532414775, 0.004452873635727883, 0.001172243637029169, 0.0014562950086656201, 0.0025089359678531764, 0.0037297437743291084, 8.225265492001199e-05, 0.002053503703336901, 0.00012516236938608927, 2.3371787847281666e-05, 0.00032117873615788994, 1.3566846064350102e-06, 0.0019376248696971743, 0.0005553875503210293, 0.00010927030689344974, 0.00025216374115188955, 9.575092235536431e-05, 4.5532416606874904e-05, 0.0003632167599789682, 0.00019502894838296925, 6.325081312752445e-05, 0.0002932917022917536, 0.00024124944593495457, 1.7994751488004113e-05, 0.00024575331326559535, 2.907205998781137e-05, 1.8077914319292177e-05, 0.0004664228481487953, 5.159885267858044e-05, 0.00017693207973934477, 1.3886738088331185e-06, 6.551372734975303e-06, 0.0001692829955572961, 0.00022432648665926536, 2.790412509057205e-06, 1.2019554560538381e-06, 1.0313588518329198e-05, 1.7087440937757492e-05, 1.998063180508325e-05, 5.19757513757213e-05, 0.00014692113018099917, 8.370123396161944e-06, 1.7905085769598372e-07, 5.126334644955932e-05, 1.2911159501527436e-06, 0.00010506928083486855, 1.231969963555457e-05, 2.7828573820443125e-05, 1.0278887202730402e-05]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.8764123468836217 and immediate relative rewards look like: [0.015805582676096372, 0.028003259501366742, 0.049516963592980794, 0.017409555352471552, 0.027046945887775477, 0.05594672707853143, 0.09712164724370953, 0.0024512177623511318, 0.0688483109329988, 0.00466617927585675, 0.000958500912245492, 0.01436943608948063, 6.576365642198029e-05, 0.10114901491199901, 0.03108602623949839, 0.0065251420114499584, 0.01599989360910687, 0.006433419661672584, 0.003229358314986902, 0.0271172216247752, 0.015290672158615855, 0.005195515206456981, 0.02518707266517685, 0.021620971829582007, 0.001680051647420779, 0.02386230646649066, 0.0029316972538103218, 0.0018905606777790765, 0.05052019433196991, 0.005782613445578281, 0.02048988446048218, 0.0001660160827979822, 0.0008076931759452359, 0.021502723988057755, 0.029334405680709344, 0.0003753496425405259, 0.00016617115777981908, 0.0014643980453586524, 0.0024900556354522604, 0.002986341010038126, 0.0079626580464848, 0.023057669925223387, 0.001344949650137402, 2.943988781314721e-05, 0.008620383057130846, 0.0002219414888879616, 0.018453945950728558, 0.002209907195141555, 0.0050959095210288035, 0.0019206812632270765]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0169, 0.0143, 0.0144, 0.0143, 0.0154, 0.0145, 0.0144, 0.0135, 0.0139,\n",
            "        0.0135, 0.0143, 0.0140, 0.0144, 0.0145, 0.0142, 0.0140, 0.0143, 0.0140,\n",
            "        0.0143, 0.0141, 0.0147, 0.0144, 0.0137, 0.0141, 0.0142, 0.0145, 0.0139,\n",
            "        0.0142, 0.0141, 0.0141, 0.0141, 0.0140, 0.0158, 0.0141, 0.0141, 0.0138,\n",
            "        0.0138, 0.0143, 0.0137, 0.0140, 0.0147, 0.0148, 0.0142, 0.0139, 0.0143,\n",
            "        0.0140, 0.0150, 0.0145, 0.0143, 0.0143, 0.0141, 0.0141, 0.0141, 0.0145,\n",
            "        0.0146, 0.0143, 0.0142, 0.0145, 0.0144, 0.0140, 0.0141, 0.0144, 0.0143,\n",
            "        0.0143, 0.0141, 0.0142, 0.0140, 0.0138, 0.0151, 0.0142],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [40]\n",
            "DEBUGGING: logits looks like: tensor([27.5099, 27.4689, 27.4699, 27.4689, 27.4871, 27.4713, 27.4693, 27.4546,\n",
            "        27.4604, 27.4534, 27.4682, 27.4632, 27.4708, 27.4711, 27.4657, 27.4635,\n",
            "        27.4687, 27.4622, 27.4674, 27.4652, 27.4749, 27.4694, 27.4581, 27.4640,\n",
            "        27.4672, 27.4717, 27.4616, 27.4669, 27.4650, 27.4641, 27.4652, 27.4628,\n",
            "        27.4938, 27.4642, 27.4644, 27.4587, 27.4590, 27.4678, 27.4577, 27.4638,\n",
            "        27.4753, 27.4772, 27.4659, 27.4606, 27.4692, 27.4625, 27.4805, 27.4722,\n",
            "        27.4674, 27.4688, 27.4642, 27.4645, 27.4650, 27.4715, 27.4730, 27.4675,\n",
            "        27.4673, 27.4717, 27.4692, 27.4630, 27.4654, 27.4701, 27.4684, 27.4692,\n",
            "        27.4650, 27.4671, 27.4636, 27.4597, 27.4815, 27.4664],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0152, 0.0128, 0.0129, 0.0128, 0.0129, 0.0128, 0.0128, 0.0128, 0.0127,\n",
            "        0.0128, 0.0127, 0.0128, 0.0128, 0.0128, 0.0128, 0.0127, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0127, 0.0128, 0.0127, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0127, 0.0127, 0.0130, 0.0128, 0.0128, 0.0129, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0126, 0.0128, 0.0127, 0.0127, 0.0127, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0132, 0.0128, 0.0128, 0.0128, 0.0127,\n",
            "        0.0128, 0.0127, 0.0129, 0.0128, 0.0127, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0127, 0.0128, 0.0128, 0.0128, 0.0128, 0.0127, 0.0128, 0.0129, 0.0128,\n",
            "        0.0128, 0.0129, 0.0128, 0.0128, 0.0128, 0.0127],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [44]\n",
            "DEBUGGING: logits looks like: tensor([27.5313, 27.4877, 27.4896, 27.4881, 27.4895, 27.4881, 27.4877, 27.4883,\n",
            "        27.4866, 27.4889, 27.4864, 27.4883, 27.4876, 27.4885, 27.4883, 27.4872,\n",
            "        27.4879, 27.4886, 27.4884, 27.4875, 27.4876, 27.4869, 27.4883, 27.4870,\n",
            "        27.4875, 27.4885, 27.4892, 27.4878, 27.4867, 27.4868, 27.4915, 27.4881,\n",
            "        27.4884, 27.4896, 27.4881, 27.4880, 27.4879, 27.4887, 27.4891, 27.4844,\n",
            "        27.4883, 27.4866, 27.4867, 27.4868, 27.4876, 27.4890, 27.4885, 27.4888,\n",
            "        27.4879, 27.4958, 27.4886, 27.4887, 27.4887, 27.4873, 27.4885, 27.4871,\n",
            "        27.4900, 27.4884, 27.4858, 27.4884, 27.4888, 27.4876, 27.4887, 27.4872,\n",
            "        27.4890, 27.4890, 27.4886, 27.4880, 27.4871, 27.4893, 27.4895, 27.4877,\n",
            "        27.4875, 27.4896, 27.4877, 27.4882, 27.4884, 27.4867],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0134, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0110, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [75]\n",
            "DEBUGGING: logits looks like: tensor([27.5488, 27.5024, 27.5024, 27.5024, 27.5019, 27.5025, 27.5024, 27.5021,\n",
            "        27.5021, 27.5025, 27.5023, 27.5015, 27.5020, 27.5025, 27.5024, 27.5016,\n",
            "        27.5024, 27.5029, 27.5022, 27.5026, 27.5029, 27.5022, 27.5023, 27.5026,\n",
            "        27.5022, 27.5019, 27.5027, 27.5024, 27.5027, 27.5017, 27.5023, 27.5027,\n",
            "        27.5026, 27.5020, 27.5023, 27.5024, 27.5018, 27.5024, 27.5022, 27.5023,\n",
            "        27.5023, 27.5021, 27.5020, 27.5030, 27.5019, 27.5023, 27.5021, 27.5024,\n",
            "        27.5028, 27.5019, 27.5022, 27.5025, 27.5022, 27.5035, 27.5024, 27.5030,\n",
            "        27.5023, 27.5027, 27.5021, 27.5023, 27.5030, 27.5021, 27.5028, 27.5022,\n",
            "        27.5019, 27.5025, 27.5022, 27.5022, 27.5021, 27.5020, 27.5023, 27.5022,\n",
            "        27.5027, 27.5026, 27.5022, 27.5025, 27.5017, 27.5019, 27.5025, 27.5020,\n",
            "        27.5017, 27.5020, 27.5022, 27.5026, 27.5023, 27.5030, 27.5023, 27.5026,\n",
            "        27.5023, 27.5023], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0124, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [87]\n",
            "DEBUGGING: logits looks like: tensor([27.5575, 27.5128, 27.5126, 27.5126, 27.5126, 27.5125, 27.5125, 27.5126,\n",
            "        27.5122, 27.5126, 27.5126, 27.5125, 27.5124, 27.5125, 27.5128, 27.5124,\n",
            "        27.5124, 27.5120, 27.5127, 27.5127, 27.5124, 27.5129, 27.5127, 27.5126,\n",
            "        27.5126, 27.5126, 27.5126, 27.5125, 27.5125, 27.5125, 27.5127, 27.5128,\n",
            "        27.5126, 27.5125, 27.5125, 27.5124, 27.5124, 27.5125, 27.5127, 27.5126,\n",
            "        27.5126, 27.5126, 27.5126, 27.5126, 27.5126, 27.5125, 27.5125, 27.5125,\n",
            "        27.5127, 27.5127, 27.5127, 27.5127, 27.5125, 27.5125, 27.5124, 27.5125,\n",
            "        27.5125, 27.5125, 27.5127, 27.5126, 27.5125, 27.5126, 27.5126, 27.5127,\n",
            "        27.5125, 27.5125, 27.5127, 27.5127, 27.5125, 27.5125, 27.5125, 27.5126,\n",
            "        27.5126, 27.5126, 27.5125, 27.5126, 27.5126, 27.5124, 27.5125, 27.5125,\n",
            "        27.5126, 27.5125, 27.5126, 27.5127, 27.5127, 27.5127, 27.5126, 27.5125,\n",
            "        27.5125, 27.5125, 27.5126, 27.5124, 27.5125, 27.5124, 27.5127, 27.5125],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0111, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [26]\n",
            "DEBUGGING: logits looks like: tensor([27.5646, 27.5196, 27.5195, 27.5195, 27.5195, 27.5195, 27.5195, 27.5195,\n",
            "        27.5196, 27.5196, 27.5195, 27.5194, 27.5195, 27.5195, 27.5195, 27.5195,\n",
            "        27.5196, 27.5195, 27.5195, 27.5195, 27.5195, 27.5196, 27.5196, 27.5195,\n",
            "        27.5195, 27.5195, 27.5195, 27.5195, 27.5196, 27.5195, 27.5196, 27.5195,\n",
            "        27.5195, 27.5196, 27.5195, 27.5196, 27.5195, 27.5196, 27.5195, 27.5195,\n",
            "        27.5196, 27.5196, 27.5195, 27.5195, 27.5195, 27.5195, 27.5195, 27.5196,\n",
            "        27.5196, 27.5195, 27.5196, 27.5195, 27.5195, 27.5196, 27.5195, 27.5194,\n",
            "        27.5197, 27.5196, 27.5195, 27.5196, 27.5195, 27.5196, 27.5195, 27.5195,\n",
            "        27.5196, 27.5196, 27.5195, 27.5196, 27.5195, 27.5195, 27.5196, 27.5196,\n",
            "        27.5196, 27.5196, 27.5195, 27.5196, 27.5195, 27.5195, 27.5196, 27.5195,\n",
            "        27.5195, 27.5195, 27.5195, 27.5194, 27.5195, 27.5195, 27.5195, 27.5196,\n",
            "        27.5195, 27.5196, 27.5195, 27.5195, 27.5195, 27.5195, 27.5196, 27.5195,\n",
            "        27.5196, 27.5196, 27.5196, 27.5195, 27.5195, 27.5195, 27.5196, 27.5195,\n",
            "        27.5195, 27.5195, 27.5195, 27.5196], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.25194311460154495 and immediate abs rewards look like: [0.04188488521799627, 0.05454613415213316, 0.043538830913803395, 0.02501152169861598, 0.0013196149450322991, 0.007812811478515869, 0.008848518150898599, 0.01747171255669855, 0.0008898331909676926, 0.022687343604957277, 0.00043279307533339306, 0.0085382158019911, 0.0038538793498901214, 0.00021916051423431782, 0.002852830816664209, 6.239113213268865e-05, 0.0004393501283175283, 0.00020394025682435313, 0.001066445300466512, 9.12907601104962e-05, 0.0006953708141281822, 0.003558347892067104, 1.596878996679152e-05, 0.0005581710659043893, 0.0006947492802282795, 0.0010419932773402252, 0.0008326437462073955, 0.0002672840059858572, 3.93425489164656e-06, 8.066496252467914e-05, 2.4187768303818302e-06, 4.6136114633554826e-05, 0.00010945808594442497, 0.00015833232009754283, 0.0007250358682995284, 0.00022453662700172572, 0.00014508226649923017, 0.0002843355837285344, 0.0001126966517404071, 9.901666385303542e-05, 2.7741660005631275e-05, 5.182716017770872e-05, 4.463028517420753e-06, 0.00020224463037266105, 8.948250570028904e-05, 4.914353667118121e-06, 5.281059907247254e-05, 7.359622918556852e-05, 5.1639744924614206e-08, 8.302701644424815e-06]\n",
            "DEBUGGING: the total relative reward of the trajectory = 4.038505082700489 and immediate relative rewards look like: [0.11611313515305924, 0.3059780187551465, 0.37204018503714514, 0.2885436011610492, 0.019167821111868842, 0.13623247123005316, 0.1804171549836761, 0.4081835490391112, 0.023507298138747773, 0.6661142166216988, 0.014071507226856291, 0.3028806736351433, 0.14847803225593756, 0.009103478923048353, 0.12697334523598441, 0.002964530450105328, 0.02218098317383948, 0.010903181691598662, 0.06018611012128062, 0.005424984495669759, 0.043389956655521346, 0.23265601884329376, 0.0010927040976548737, 0.03985504941857141, 0.05168268054920771, 0.08063154334463118, 0.06693055112641869, 0.02228640849497962, 0.0003397849536835483, 0.0072069308019883874, 0.00022331201377535264, 0.004396893059872797, 0.010757780132014096, 0.01603332238791598, 0.07558281267562933, 0.024081248784431315, 0.01599316496526358, 0.03219229540588279, 0.013096328124193233, 0.011802032123902401, 0.003389359432612471, 0.00648651786933248, 0.000571886362815832, 0.026518067820347796, 0.012000214975141121, 0.0006737118660889183, 0.007397237204034724, 0.010528202488945693, 7.541315715790206e-06, 0.001237246965603439]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0139, 0.0145, 0.0144, 0.0150, 0.0170, 0.0144, 0.0145, 0.0146, 0.0148,\n",
            "        0.0146, 0.0153, 0.0151, 0.0153, 0.0146, 0.0139, 0.0150, 0.0152, 0.0148,\n",
            "        0.0139, 0.0158, 0.0156, 0.0143, 0.0150, 0.0155, 0.0144, 0.0156, 0.0146,\n",
            "        0.0138, 0.0146, 0.0160, 0.0146, 0.0157, 0.0153, 0.0146, 0.0137, 0.0150,\n",
            "        0.0152, 0.0145, 0.0156, 0.0146, 0.0131, 0.0142, 0.0153, 0.0145, 0.0132,\n",
            "        0.0150, 0.0144, 0.0143, 0.0153, 0.0148, 0.0151, 0.0145, 0.0170, 0.0147,\n",
            "        0.0145, 0.0147, 0.0151, 0.0166, 0.0153, 0.0167, 0.0166, 0.0145, 0.0155,\n",
            "        0.0157, 0.0149, 0.0152, 0.0143], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [59]\n",
            "DEBUGGING: logits looks like: tensor([27.5180, 27.5285, 27.5266, 27.5383, 27.5686, 27.5266, 27.5283, 27.5299,\n",
            "        27.5345, 27.5304, 27.5423, 27.5389, 27.5417, 27.5311, 27.5190, 27.5369,\n",
            "        27.5408, 27.5338, 27.5190, 27.5499, 27.5475, 27.5262, 27.5378, 27.5457,\n",
            "        27.5272, 27.5475, 27.5307, 27.5164, 27.5300, 27.5533, 27.5312, 27.5482,\n",
            "        27.5418, 27.5301, 27.5151, 27.5381, 27.5410, 27.5287, 27.5476, 27.5315,\n",
            "        27.5030, 27.5240, 27.5432, 27.5287, 27.5059, 27.5378, 27.5265, 27.5262,\n",
            "        27.5429, 27.5348, 27.5387, 27.5298, 27.5694, 27.5330, 27.5292, 27.5323,\n",
            "        27.5395, 27.5633, 27.5424, 27.5650, 27.5631, 27.5293, 27.5464, 27.5485,\n",
            "        27.5363, 27.5401, 27.5258], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0119, 0.0128, 0.0131, 0.0142, 0.0151, 0.0121, 0.0109, 0.0139, 0.0125,\n",
            "        0.0118, 0.0119, 0.0104, 0.0108, 0.0127, 0.0129, 0.0168, 0.0127, 0.0127,\n",
            "        0.0115, 0.0117, 0.0132, 0.0121, 0.0138, 0.0131, 0.0125, 0.0129, 0.0133,\n",
            "        0.0130, 0.0146, 0.0150, 0.0113, 0.0117, 0.0123, 0.0141, 0.0127, 0.0137,\n",
            "        0.0122, 0.0122, 0.0123, 0.0130, 0.0121, 0.0131, 0.0106, 0.0132, 0.0139,\n",
            "        0.0128, 0.0119, 0.0123, 0.0104, 0.0132, 0.0131, 0.0129, 0.0138, 0.0107,\n",
            "        0.0123, 0.0129, 0.0143, 0.0127, 0.0131, 0.0126, 0.0123, 0.0125, 0.0122,\n",
            "        0.0113, 0.0131, 0.0127, 0.0127, 0.0112, 0.0123, 0.0115, 0.0119, 0.0116,\n",
            "        0.0123, 0.0131, 0.0132, 0.0128, 0.0121, 0.0143, 0.0130],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [78]\n",
            "DEBUGGING: logits looks like: tensor([27.5263, 27.5428, 27.5487, 27.5694, 27.5850, 27.5300, 27.5035, 27.5646,\n",
            "        27.5375, 27.5233, 27.5259, 27.4925, 27.5006, 27.5420, 27.5451, 27.6122,\n",
            "        27.5419, 27.5424, 27.5166, 27.5210, 27.5522, 27.5290, 27.5631, 27.5495,\n",
            "        27.5375, 27.5459, 27.5532, 27.5478, 27.5772, 27.5840, 27.5120, 27.5218,\n",
            "        27.5332, 27.5683, 27.5407, 27.5608, 27.5313, 27.5320, 27.5338, 27.5478,\n",
            "        27.5289, 27.5487, 27.4974, 27.5521, 27.5644, 27.5434, 27.5259, 27.5345,\n",
            "        27.4906, 27.5521, 27.5494, 27.5456, 27.5632, 27.4990, 27.5341, 27.5451,\n",
            "        27.5705, 27.5420, 27.5488, 27.5396, 27.5329, 27.5378, 27.5319, 27.5128,\n",
            "        27.5485, 27.5423, 27.5424, 27.5106, 27.5345, 27.5175, 27.5263, 27.5184,\n",
            "        27.5346, 27.5489, 27.5515, 27.5441, 27.5287, 27.5706, 27.5475],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0125, 0.0118, 0.0118, 0.0119, 0.0121, 0.0115, 0.0117, 0.0114, 0.0115,\n",
            "        0.0117, 0.0110, 0.0115, 0.0119, 0.0117, 0.0120, 0.0117, 0.0116, 0.0115,\n",
            "        0.0119, 0.0120, 0.0119, 0.0117, 0.0121, 0.0119, 0.0119, 0.0118, 0.0117,\n",
            "        0.0117, 0.0115, 0.0117, 0.0115, 0.0116, 0.0122, 0.0129, 0.0120, 0.0124,\n",
            "        0.0115, 0.0116, 0.0117, 0.0116, 0.0118, 0.0115, 0.0116, 0.0120, 0.0121,\n",
            "        0.0117, 0.0124, 0.0119, 0.0124, 0.0117, 0.0114, 0.0117, 0.0121, 0.0118,\n",
            "        0.0114, 0.0120, 0.0113, 0.0117, 0.0116, 0.0116, 0.0111, 0.0116, 0.0118,\n",
            "        0.0124, 0.0117, 0.0116, 0.0117, 0.0118, 0.0112, 0.0113, 0.0121, 0.0116,\n",
            "        0.0114, 0.0118, 0.0119, 0.0110, 0.0119, 0.0122, 0.0119, 0.0117, 0.0120,\n",
            "        0.0117, 0.0119, 0.0117, 0.0117], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [63]\n",
            "DEBUGGING: logits looks like: tensor([27.5623, 27.5492, 27.5491, 27.5510, 27.5536, 27.5424, 27.5462, 27.5392,\n",
            "        27.5426, 27.5458, 27.5297, 27.5415, 27.5504, 27.5464, 27.5519, 27.5451,\n",
            "        27.5437, 27.5422, 27.5511, 27.5529, 27.5510, 27.5463, 27.5540, 27.5492,\n",
            "        27.5498, 27.5489, 27.5452, 27.5452, 27.5426, 27.5460, 27.5409, 27.5432,\n",
            "        27.5561, 27.5703, 27.5517, 27.5596, 27.5419, 27.5444, 27.5454, 27.5447,\n",
            "        27.5477, 27.5410, 27.5448, 27.5523, 27.5546, 27.5462, 27.5612, 27.5501,\n",
            "        27.5600, 27.5467, 27.5400, 27.5468, 27.5542, 27.5491, 27.5392, 27.5531,\n",
            "        27.5366, 27.5469, 27.5449, 27.5431, 27.5326, 27.5441, 27.5471, 27.5607,\n",
            "        27.5458, 27.5449, 27.5461, 27.5489, 27.5360, 27.5382, 27.5544, 27.5449,\n",
            "        27.5403, 27.5475, 27.5493, 27.5299, 27.5509, 27.5559, 27.5500, 27.5451,\n",
            "        27.5524, 27.5457, 27.5503, 27.5455, 27.5455], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0111, 0.0102, 0.0102, 0.0102, 0.0103, 0.0101, 0.0101, 0.0102, 0.0103,\n",
            "        0.0102, 0.0101, 0.0103, 0.0103, 0.0101, 0.0103, 0.0102, 0.0102, 0.0102,\n",
            "        0.0101, 0.0102, 0.0101, 0.0101, 0.0102, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0103, 0.0102, 0.0102, 0.0102, 0.0102, 0.0103, 0.0102, 0.0102, 0.0103,\n",
            "        0.0102, 0.0102, 0.0102, 0.0101, 0.0102, 0.0101, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0103, 0.0103, 0.0102, 0.0102, 0.0103, 0.0102, 0.0103, 0.0102,\n",
            "        0.0102, 0.0101, 0.0101, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0101,\n",
            "        0.0101, 0.0101, 0.0102, 0.0102, 0.0101, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0103, 0.0102, 0.0101, 0.0101, 0.0104,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0101, 0.0102, 0.0101, 0.0102,\n",
            "        0.0102, 0.0102, 0.0103, 0.0103, 0.0102, 0.0102, 0.0102, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [3]\n",
            "DEBUGGING: logits looks like: tensor([27.5836, 27.5625, 27.5629, 27.5637, 27.5642, 27.5612, 27.5604, 27.5619,\n",
            "        27.5652, 27.5617, 27.5614, 27.5640, 27.5642, 27.5608, 27.5644, 27.5617,\n",
            "        27.5628, 27.5622, 27.5602, 27.5623, 27.5602, 27.5606, 27.5629, 27.5603,\n",
            "        27.5601, 27.5600, 27.5614, 27.5648, 27.5634, 27.5628, 27.5620, 27.5627,\n",
            "        27.5661, 27.5625, 27.5618, 27.5640, 27.5627, 27.5627, 27.5627, 27.5602,\n",
            "        27.5637, 27.5602, 27.5629, 27.5634, 27.5633, 27.5616, 27.5641, 27.5659,\n",
            "        27.5630, 27.5638, 27.5655, 27.5638, 27.5654, 27.5620, 27.5615, 27.5610,\n",
            "        27.5604, 27.5622, 27.5630, 27.5628, 27.5623, 27.5621, 27.5593, 27.5607,\n",
            "        27.5608, 27.5624, 27.5634, 27.5602, 27.5617, 27.5639, 27.5636, 27.5631,\n",
            "        27.5631, 27.5632, 27.5615, 27.5636, 27.5644, 27.5639, 27.5613, 27.5608,\n",
            "        27.5668, 27.5634, 27.5630, 27.5621, 27.5633, 27.5624, 27.5603, 27.5622,\n",
            "        27.5613, 27.5617, 27.5626, 27.5630, 27.5644, 27.5642, 27.5636, 27.5617,\n",
            "        27.5632, 27.5612], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0100, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0091, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0091, 0.0092, 0.0091, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0091, 0.0092, 0.0092,\n",
            "        0.0092, 0.0091, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0091, 0.0092, 0.0092, 0.0092,\n",
            "        0.0091, 0.0092, 0.0092, 0.0091, 0.0092, 0.0092, 0.0092, 0.0091, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0091, 0.0091, 0.0092, 0.0091, 0.0092,\n",
            "        0.0091, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0091, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [72]\n",
            "DEBUGGING: logits looks like: tensor([27.5896, 27.5692, 27.5685, 27.5693, 27.5687, 27.5690, 27.5696, 27.5680,\n",
            "        27.5691, 27.5693, 27.5691, 27.5689, 27.5687, 27.5686, 27.5689, 27.5686,\n",
            "        27.5689, 27.5687, 27.5689, 27.5698, 27.5690, 27.5692, 27.5684, 27.5693,\n",
            "        27.5691, 27.5689, 27.5691, 27.5687, 27.5685, 27.5688, 27.5692, 27.5686,\n",
            "        27.5689, 27.5693, 27.5688, 27.5685, 27.5688, 27.5687, 27.5690, 27.5691,\n",
            "        27.5688, 27.5679, 27.5690, 27.5681, 27.5687, 27.5693, 27.5692, 27.5687,\n",
            "        27.5688, 27.5689, 27.5689, 27.5683, 27.5687, 27.5689, 27.5692, 27.5680,\n",
            "        27.5690, 27.5685, 27.5688, 27.5687, 27.5694, 27.5698, 27.5685, 27.5693,\n",
            "        27.5687, 27.5694, 27.5689, 27.5693, 27.5677, 27.5685, 27.5685, 27.5688,\n",
            "        27.5670, 27.5688, 27.5692, 27.5681, 27.5689, 27.5692, 27.5688, 27.5683,\n",
            "        27.5691, 27.5688, 27.5685, 27.5685, 27.5696, 27.5683, 27.5681, 27.5691,\n",
            "        27.5682, 27.5693, 27.5684, 27.5690, 27.5699, 27.5689, 27.5692, 27.5689,\n",
            "        27.5686, 27.5692, 27.5691, 27.5681, 27.5698, 27.5694, 27.5689, 27.5690,\n",
            "        27.5690, 27.5687, 27.5688, 27.5694, 27.5689], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.14338516010047897 and immediate abs rewards look like: [0.027264142465810437, 0.07404694794968236, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 9.094947017729282e-13, 9.094947017729282e-13, 9.094947017729282e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 9.094947017729282e-13, 9.094947017729282e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0035192139516766474, 0.0001769692057678185, 0.00010760833083622856, 0.007823056088000158, 0.012777264494616247, 0.0003840088829747401, 0.000921474222195684, 0.005537843187539693, 0.00013379128540691454, 3.894024621331482e-05, 9.27533392314217e-05, 0.00160430799542155, 0.00023231786053656833, 0.0001859939898167795, 2.9736524084000848e-05, 0.0028555365388456266, 1.4861558611301007e-05, 3.9808288420317695e-05, 0.0006509013123832119, 0.0004409944394865306, 0.0002767557939478138, 0.00016641528918626136, 0.0001560481318847451, 7.005806764937006e-06, 0.00012188097753096372, 0.0005476215746966773, 4.782222322319285e-05, 0.003183138137046626]\n",
            "DEBUGGING: the total relative reward of the trajectory = 4.915224041415324 and immediate relative rewards look like: [0.08791166597270889, 0.48175547871224933, 4.547473508863952e-12, 6.063298011819521e-12, 1.5158245029546508e-11, 1.8189894035461324e-11, 2.122154304136511e-11, 2.4253192047281764e-11, 2.728484105318371e-11, 1.5158245029551102e-11, 0.0, 0.0, 0.0, 0.0, 0.0, 4.850638409455617e-11, 5.153803310048156e-11, 0.0, 2.8800665556142725e-11, 3.0316490059102205e-11, 0.0, 0.0, 0.2698064029618763, 0.014174163766385485, 0.0089784228681476, 0.6788589402462308, 1.1544279236842887, 0.03613477477540091, 0.0898179811390183, 0.5585711985512236, 0.013970614800339606, 0.004197534190888199, 0.010310856941074368, 0.18375175782731198, 0.02740629054969549, 0.02257018055953008, 0.00370896535955964, 0.36579395369826306, 0.0019557459068343165, 0.005373028433470493, 0.09005140224604666, 0.06251284691678743, 0.04017137492482144, 0.02471940392125305, 0.023707599436289004, 0.001088066301403851, 0.01934078903826744, 0.08875239064748802, 0.007913423788211476, 0.5374908629592197]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 3\n",
            "DEBUGGING: the action_prob is: tensor([0.0176, 0.0150, 0.0170, 0.0173, 0.0127, 0.0119, 0.0189, 0.0156, 0.0141,\n",
            "        0.0123, 0.0135, 0.0177, 0.0157, 0.0136, 0.0175, 0.0123, 0.0133, 0.0189,\n",
            "        0.0125, 0.0119, 0.0155, 0.0168, 0.0174, 0.0121, 0.0123, 0.0159, 0.0141,\n",
            "        0.0120, 0.0128, 0.0139, 0.0129, 0.0157, 0.0154, 0.0126, 0.0186, 0.0142,\n",
            "        0.0160, 0.0140, 0.0120, 0.0120, 0.0142, 0.0154, 0.0172, 0.0140, 0.0131,\n",
            "        0.0186, 0.0160, 0.0118, 0.0152, 0.0156, 0.0143, 0.0164, 0.0115, 0.0152,\n",
            "        0.0148, 0.0132, 0.0143, 0.0141, 0.0117, 0.0159, 0.0132, 0.0187, 0.0132,\n",
            "        0.0139, 0.0171, 0.0174, 0.0140, 0.0141], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [10]\n",
            "DEBUGGING: logits looks like: tensor([27.4623, 27.4229, 27.4535, 27.4585, 27.3809, 27.3641, 27.4800, 27.4316,\n",
            "        27.4061, 27.3731, 27.3959, 27.4640, 27.4343, 27.3981, 27.4605, 27.3730,\n",
            "        27.3913, 27.4794, 27.3772, 27.3634, 27.4301, 27.4513, 27.4599, 27.3690,\n",
            "        27.3718, 27.4376, 27.4061, 27.3658, 27.3827, 27.4025, 27.3846, 27.4338,\n",
            "        27.4296, 27.3781, 27.4766, 27.4092, 27.4389, 27.4049, 27.3670, 27.3673,\n",
            "        27.4086, 27.4283, 27.4562, 27.4055, 27.3881, 27.4766, 27.4384, 27.3630,\n",
            "        27.4262, 27.4326, 27.4109, 27.4442, 27.3568, 27.4260, 27.4188, 27.3909,\n",
            "        27.4109, 27.4064, 27.3599, 27.4372, 27.3897, 27.4776, 27.3910, 27.4025,\n",
            "        27.4546, 27.4594, 27.4058, 27.4066], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0137, 0.0119, 0.0122, 0.0125, 0.0119, 0.0115, 0.0129, 0.0114, 0.0120,\n",
            "        0.0100, 0.0126, 0.0127, 0.0133, 0.0129, 0.0134, 0.0132, 0.0166, 0.0130,\n",
            "        0.0110, 0.0134, 0.0116, 0.0134, 0.0124, 0.0135, 0.0138, 0.0115, 0.0112,\n",
            "        0.0137, 0.0149, 0.0126, 0.0112, 0.0136, 0.0118, 0.0120, 0.0144, 0.0128,\n",
            "        0.0116, 0.0113, 0.0125, 0.0146, 0.0168, 0.0138, 0.0116, 0.0102, 0.0115,\n",
            "        0.0128, 0.0124, 0.0114, 0.0125, 0.0125, 0.0142, 0.0149, 0.0119, 0.0166,\n",
            "        0.0130, 0.0154, 0.0131, 0.0140, 0.0123, 0.0140, 0.0107, 0.0121, 0.0113,\n",
            "        0.0124, 0.0124, 0.0127, 0.0147, 0.0143, 0.0154, 0.0137, 0.0123, 0.0121,\n",
            "        0.0123, 0.0151, 0.0108, 0.0130, 0.0127, 0.0103],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [40]\n",
            "DEBUGGING: logits looks like: tensor([27.4267, 27.3917, 27.3981, 27.4041, 27.3930, 27.3844, 27.4119, 27.3808,\n",
            "        27.3938, 27.3483, 27.4067, 27.4079, 27.4208, 27.4125, 27.4219, 27.4173,\n",
            "        27.4752, 27.4147, 27.3713, 27.4218, 27.3863, 27.4226, 27.4028, 27.4238,\n",
            "        27.4296, 27.3840, 27.3778, 27.4269, 27.4480, 27.4063, 27.3773, 27.4263,\n",
            "        27.3908, 27.3947, 27.4389, 27.4107, 27.3855, 27.3788, 27.4040, 27.4424,\n",
            "        27.4790, 27.4297, 27.3862, 27.3544, 27.3838, 27.4111, 27.4026, 27.3822,\n",
            "        27.4041, 27.4045, 27.4362, 27.4477, 27.3914, 27.4755, 27.4148, 27.4572,\n",
            "        27.4169, 27.4322, 27.3999, 27.4329, 27.3655, 27.3955, 27.3791, 27.4027,\n",
            "        27.4016, 27.4078, 27.4449, 27.4386, 27.4565, 27.4268, 27.4004, 27.3965,\n",
            "        27.3996, 27.4517, 27.3687, 27.4142, 27.4085, 27.3569],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0112, 0.0107, 0.0156, 0.0108, 0.0136, 0.0089, 0.0104, 0.0156, 0.0099,\n",
            "        0.0111, 0.0097, 0.0108, 0.0110, 0.0103, 0.0109, 0.0100, 0.0141, 0.0095,\n",
            "        0.0095, 0.0103, 0.0124, 0.0112, 0.0110, 0.0115, 0.0126, 0.0126, 0.0110,\n",
            "        0.0131, 0.0119, 0.0114, 0.0127, 0.0120, 0.0112, 0.0113, 0.0121, 0.0107,\n",
            "        0.0115, 0.0100, 0.0097, 0.0116, 0.0117, 0.0107, 0.0106, 0.0143, 0.0117,\n",
            "        0.0105, 0.0117, 0.0120, 0.0116, 0.0124, 0.0101, 0.0111, 0.0131, 0.0134,\n",
            "        0.0105, 0.0119, 0.0101, 0.0092, 0.0127, 0.0104, 0.0119, 0.0111, 0.0153,\n",
            "        0.0114, 0.0101, 0.0109, 0.0122, 0.0146, 0.0108, 0.0120, 0.0112, 0.0111,\n",
            "        0.0108, 0.0115, 0.0110, 0.0111, 0.0102, 0.0104, 0.0125, 0.0106, 0.0109,\n",
            "        0.0116, 0.0128, 0.0124, 0.0116, 0.0124, 0.0119],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [23]\n",
            "DEBUGGING: logits looks like: tensor([27.3969, 27.3851, 27.4798, 27.3886, 27.4463, 27.3397, 27.3789, 27.4805,\n",
            "        27.3667, 27.3959, 27.3616, 27.3873, 27.3924, 27.3751, 27.3907, 27.3682,\n",
            "        27.4556, 27.3549, 27.3548, 27.3761, 27.4235, 27.3978, 27.3932, 27.4027,\n",
            "        27.4263, 27.4274, 27.3932, 27.4370, 27.4119, 27.4024, 27.4294, 27.4154,\n",
            "        27.3982, 27.3986, 27.4170, 27.3867, 27.4035, 27.3679, 27.3618, 27.4065,\n",
            "        27.4083, 27.3864, 27.3837, 27.4582, 27.4088, 27.3801, 27.4082, 27.4148,\n",
            "        27.4059, 27.4231, 27.3723, 27.3954, 27.4366, 27.4416, 27.3818, 27.4115,\n",
            "        27.3722, 27.3491, 27.4291, 27.3777, 27.4133, 27.3949, 27.4744, 27.4014,\n",
            "        27.3710, 27.3911, 27.4191, 27.4635, 27.3887, 27.4152, 27.3972, 27.3945,\n",
            "        27.3892, 27.4043, 27.3928, 27.3960, 27.3734, 27.3779, 27.4240, 27.3841,\n",
            "        27.3907, 27.4055, 27.4299, 27.4227, 27.4055, 27.4236, 27.4132],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0136, 0.0091, 0.0171, 0.0089, 0.0111, 0.0124, 0.0085, 0.0098, 0.0090,\n",
            "        0.0064, 0.0108, 0.0100, 0.0099, 0.0110, 0.0113, 0.0099, 0.0110, 0.0099,\n",
            "        0.0097, 0.0100, 0.0112, 0.0146, 0.0092, 0.0100, 0.0087, 0.0105, 0.0095,\n",
            "        0.0112, 0.0107, 0.0096, 0.0099, 0.0106, 0.0101, 0.0107, 0.0088, 0.0099,\n",
            "        0.0084, 0.0093, 0.0121, 0.0097, 0.0129, 0.0108, 0.0106, 0.0098, 0.0105,\n",
            "        0.0123, 0.0123, 0.0085, 0.0100, 0.0114, 0.0089, 0.0129, 0.0114, 0.0097,\n",
            "        0.0111, 0.0110, 0.0113, 0.0097, 0.0105, 0.0100, 0.0114, 0.0068, 0.0110,\n",
            "        0.0091, 0.0075, 0.0109, 0.0096, 0.0109, 0.0104, 0.0106, 0.0098, 0.0126,\n",
            "        0.0118, 0.0098, 0.0092, 0.0112, 0.0089, 0.0095, 0.0109, 0.0122, 0.0092,\n",
            "        0.0124, 0.0119, 0.0145, 0.0106, 0.0100, 0.0118, 0.0107, 0.0110, 0.0102,\n",
            "        0.0114, 0.0110, 0.0098, 0.0110, 0.0107], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [12]\n",
            "DEBUGGING: logits looks like: tensor([27.4576, 27.3557, 27.5137, 27.3521, 27.4071, 27.4332, 27.3399, 27.3737,\n",
            "        27.3550, 27.2684, 27.3991, 27.3798, 27.3763, 27.4029, 27.4110, 27.3785,\n",
            "        27.4049, 27.3764, 27.3713, 27.3790, 27.4076, 27.4745, 27.3601, 27.3798,\n",
            "        27.3453, 27.3910, 27.3674, 27.4087, 27.3978, 27.3699, 27.3775, 27.3952,\n",
            "        27.3825, 27.3959, 27.3479, 27.3784, 27.3356, 27.3628, 27.4269, 27.3720,\n",
            "        27.4444, 27.3989, 27.3952, 27.3747, 27.3917, 27.4308, 27.4321, 27.3388,\n",
            "        27.3812, 27.4125, 27.3500, 27.4431, 27.4131, 27.3736, 27.4064, 27.4030,\n",
            "        27.4112, 27.3735, 27.3926, 27.3790, 27.4121, 27.2821, 27.4045, 27.3558,\n",
            "        27.3072, 27.4004, 27.3710, 27.4007, 27.3909, 27.3957, 27.3737, 27.4385,\n",
            "        27.4207, 27.3747, 27.3600, 27.4086, 27.3515, 27.3672, 27.4017, 27.4292,\n",
            "        27.3600, 27.4341, 27.4238, 27.4730, 27.3942, 27.3796, 27.4220, 27.3961,\n",
            "        27.4032, 27.3848, 27.4122, 27.4046, 27.3749, 27.4029, 27.3961],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0118, 0.0074, 0.0118, 0.0092, 0.0176, 0.0117, 0.0099, 0.0078, 0.0155,\n",
            "        0.0082, 0.0066, 0.0094, 0.0089, 0.0087, 0.0095, 0.0117, 0.0100, 0.0086,\n",
            "        0.0103, 0.0054, 0.0083, 0.0092, 0.0107, 0.0086, 0.0150, 0.0071, 0.0090,\n",
            "        0.0097, 0.0084, 0.0102, 0.0092, 0.0107, 0.0086, 0.0079, 0.0085, 0.0088,\n",
            "        0.0080, 0.0135, 0.0106, 0.0104, 0.0111, 0.0089, 0.0075, 0.0101, 0.0083,\n",
            "        0.0101, 0.0087, 0.0098, 0.0092, 0.0087, 0.0079, 0.0085, 0.0084, 0.0090,\n",
            "        0.0110, 0.0111, 0.0124, 0.0084, 0.0072, 0.0065, 0.0101, 0.0115, 0.0148,\n",
            "        0.0080, 0.0082, 0.0074, 0.0097, 0.0070, 0.0086, 0.0097, 0.0120, 0.0121,\n",
            "        0.0118, 0.0112, 0.0143, 0.0112, 0.0093, 0.0115, 0.0067, 0.0088, 0.0096,\n",
            "        0.0092, 0.0116, 0.0068, 0.0094, 0.0089, 0.0083, 0.0094, 0.0076, 0.0091,\n",
            "        0.0086, 0.0073, 0.0145, 0.0071, 0.0098, 0.0111, 0.0077, 0.0083, 0.0120,\n",
            "        0.0113, 0.0067, 0.0087, 0.0074, 0.0106], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [82]\n",
            "DEBUGGING: logits looks like: tensor([27.5207, 27.4031, 27.5194, 27.4580, 27.6206, 27.5172, 27.4768, 27.4157,\n",
            "        27.5881, 27.4276, 27.3735, 27.4641, 27.4495, 27.4433, 27.4670, 27.5171,\n",
            "        27.4791, 27.4417, 27.4869, 27.3227, 27.4326, 27.4568, 27.4966, 27.4396,\n",
            "        27.5807, 27.3922, 27.4513, 27.4715, 27.4356, 27.4832, 27.4571, 27.4956,\n",
            "        27.4399, 27.4188, 27.4382, 27.4466, 27.4217, 27.5539, 27.4939, 27.4878,\n",
            "        27.5054, 27.4496, 27.4063, 27.4803, 27.4330, 27.4805, 27.4431, 27.4748,\n",
            "        27.4588, 27.4431, 27.4195, 27.4383, 27.4357, 27.4515, 27.5022, 27.5037,\n",
            "        27.5325, 27.4363, 27.3948, 27.3710, 27.4800, 27.5142, 27.5770, 27.4217,\n",
            "        27.4293, 27.4042, 27.4714, 27.3901, 27.4421, 27.4700, 27.5237, 27.5268,\n",
            "        27.5194, 27.5058, 27.5672, 27.5070, 27.4594, 27.5127, 27.3800, 27.4452,\n",
            "        27.4679, 27.4573, 27.5163, 27.3819, 27.4629, 27.4490, 27.4323, 27.4642,\n",
            "        27.4111, 27.4563, 27.4410, 27.3999, 27.5723, 27.3929, 27.4745, 27.5047,\n",
            "        27.4137, 27.4307, 27.5240, 27.5081, 27.3778, 27.4425, 27.4042, 27.4924],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.886741328989956 and immediate abs rewards look like: [0.007054489052279678, 0.0026130851401831023, 0.0009174021815852029, 0.00948592443319285, 0.031225133400766936, 0.0016134525403685984, 0.004254434910762939, 0.8295774073130815, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 9.094947017729282e-13, 1.8189894035458565e-12, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 24.322387330606688 and immediate relative rewards look like: [0.024437551717840296, 0.018148397971324805, 0.009565992321826512, 0.13192501756202857, 0.5446237051524248, 0.03414178104612332, 0.10509107820584432, 23.454453804130438, 0.0, 2.2737367544318036e-11, 2.5011104298755527e-11, 2.7284841053181643e-11, 2.955857780762017e-11, 3.1832314562059726e-11, 3.410605131648481e-11, 0.0, 3.865352482534066e-11, 4.092726157978177e-11, 8.640199666840854e-11, 4.547473508865675e-11, 9.549694368615746e-11, 2.000888343901352e-10, 1.0459189070383918e-10, 5.4569682106375694e-11, 5.684341886082094e-11, 1.1823431123053444e-10, 6.139089236967266e-11, 6.366462912411945e-11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.412825991399586e-11, 1.7280399333681707e-10, 1.7735146684576133e-10, 9.094947017727214e-11, 0.0, 9.549694368615746e-11, 1.9554136088122403e-10, 1.0004441719499936e-10, 0.0, 0.0, 0.0, 1.0913936421275139e-10, 2.2282620193441808e-10, 1.1368683772159018e-10]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 200 number of matrices\n",
            "DEBUGGING: ACT_MAT has 200 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.7508049680528539, 0.7424236215926844, 0.7216367293851693, 0.678908854335544, 0.6681811100839116, 0.647610266864784, 0.5976399391780328, 0.5055740322568922, 0.5082048631257992, 0.4437944971644448, 0.44356395746322025, 0.44707621873835834, 0.4370775582311896, 0.4414260551260279, 0.3437141820343726, 0.31578601595441835, 0.31238472115451354, 0.29937861368222896, 0.2959042363844004, 0.29563118996910454, 0.27122623065083773, 0.25852076615375946, 0.25588409186596206, 0.23302729212200526, 0.2135417376691144, 0.2140017030522158, 0.19205999655123754, 0.1910386861590174, 0.19105871260731144, 0.14195809926802175, 0.13755099578024593, 0.11824354678764017, 0.11927023303519413, 0.11965913117095847, 0.09914788604333405, 0.07051866703295424, 0.07085183574789264, 0.07139966120213416, 0.0706416799563389, 0.06884002456655215, 0.0665188722793071, 0.05914769114426495, 0.03645456688792077, 0.03546425983614482, 0.03579274742255724, 0.02744683269234989, 0.027499890104507, 0.009137317327048929, 0.006997383971623609, 0.0019206812632270765], [3.633002460844251, 3.552413460294133, 3.2792277187262493, 2.9365530643324282, 2.6747570335064434, 2.682413345853106, 2.571899873356619, 2.4156391094676195, 2.0277328893217255, 2.0244702941242196, 1.3720768459621424, 1.371722564379077, 1.0796382734787209, 0.9405659002250337, 0.9408711326282682, 0.8221189771639229, 0.8274287340543611, 0.8133815665459815, 0.8105842271256393, 0.7579778959639987, 0.7601544560286151, 0.7240045448213068, 0.49631164240203346, 0.5002211498024026, 0.465016263013971, 0.4175086691563265, 0.3402799250623185, 0.2761104787231311, 0.2563879497254055, 0.25863450987042624, 0.2539672515842807, 0.2563070096671771, 0.25445466323970134, 0.24615846778554268, 0.23244964181578454, 0.15845134256581334, 0.13572736745594144, 0.1209436388794726, 0.08964782169049476, 0.07732474097606215, 0.06618455439612095, 0.06342948986212978, 0.05751815352807808, 0.05752148198511338, 0.03131657996440968, 0.01951147978713996, 0.01902804840510206, 0.011748294142492257, 0.0012324158116631947, 0.001237246965603439], [3.737803593247766, 3.686759522500058, 3.237377822007887, 3.2700786080841815, 3.3031097051294127, 3.336474449610358, 3.370176211709261, 3.404218395644484, 3.4386044400204354, 3.4733378181748997, 3.5084220385451936, 3.5438606449951453, 3.5796572171668135, 3.6158153708755694, 3.652338758460171, 3.689231069151688, 3.7264960293971527, 3.764137403379409, 3.8021589933125344, 3.8405646396805393, 3.879358221868912, 3.9185436584534465, 3.958124907528734, 3.72557424703723, 3.748888973000853, 3.77768742437647, 3.130129781949737, 1.9956584426923725, 1.9793168362797693, 1.9085847021623747, 1.3636500036476273, 1.3633125139871594, 1.3728434139356274, 1.37629551211571, 1.2045896507963618, 1.189074101259259, 1.1782867885855848, 1.1864422456828536, 0.8289376686713035, 0.8353352755196658, 0.83834570412747, 0.7558528301832559, 0.7003434174408771, 0.6668404469859148, 0.6486071142067291, 0.6312116310812527, 0.6364884492725745, 0.6233814749841486, 0.5400293781178389, 0.5374908629592197], [22.695443470326353, 22.900005978392436, 23.11298745497082, 23.336789356211106, 23.439256907726342, 23.125892123812037, 23.325000346228194, 23.45445380608318, 1.97246732364972e-09, 1.9923912360098186e-09, 1.9895493620863642e-09, 1.984382078573342e-09, 1.9768658964850105e-09, 1.9669770895731217e-09, 1.9546916919303656e-09, 1.9399854955695766e-09, 1.959581308656138e-09, 1.9403310947785833e-09, 1.9185897305038398e-09, 1.8506946806418495e-09, 1.8234544904577705e-09, 1.7454116634056697e-09, 1.560932150520742e-09, 1.471050767491821e-09, 1.4307889751368135e-09, 1.3878237942181742e-09, 1.282413619179434e-09, 1.2333562897068298e-09, 1.1815067278613236e-09, 1.1934411392538623e-09, 1.2054961002564266e-09, 1.2176728285418452e-09, 1.229972554082672e-09, 1.2423965192754262e-09, 1.254945979066087e-09, 1.2676222010768557e-09, 1.2804264657341976e-09, 1.2083820260810119e-09, 1.046038416913328e-09, 8.774615657248149e-10, 7.944566621692351e-10, 8.024814769386214e-10, 7.14125791164105e-10, 5.238226568513949e-10, 4.28058827935753e-10, 4.323826544805586e-10, 4.3675015604096827e-10, 4.411617737787558e-10, 3.3537617127879236e-10, 1.1368683772159018e-10]]\n",
            "DEBUGGING: traj_returns = [0.7508049680528539, 3.633002460844251, 3.737803593247766, 22.695443470326353]\n",
            "DEBUGGING: actions = [[23], [42], [46], [14], [14], [39], [44], [58], [8], [41], [47], [54], [70], [58], [19], [28], [10], [42], [66], [76], [4], [31], [41], [79], [65], [75], [8], [74], [11], [71], [21], [85], [55], [62], [87], [73], [55], [85], [7], [52], [97], [68], [47], [34], [87], [29], [49], [52], [59], [30], [59], [25], [41], [2], [50], [30], [45], [5], [25], [40], [39], [12], [47], [49], [58], [40], [12], [58], [74], [44], [60], [20], [68], [54], [28], [21], [64], [44], [42], [75], [46], [9], [57], [8], [37], [36], [63], [38], [58], [87], [44], [5], [67], [64], [90], [17], [5], [95], [96], [26], [45], [0], [44], [13], [9], [13], [11], [34], [27], [59], [49], [59], [20], [17], [53], [67], [63], [3], [36], [78], [11], [64], [26], [43], [77], [71], [26], [24], [68], [63], [32], [85], [85], [91], [18], [14], [87], [68], [3], [3], [91], [61], [82], [96], [77], [66], [84], [90], [9], [72], [15], [6], [33], [49], [13], [13], [27], [0], [42], [10], [30], [29], [10], [33], [59], [33], [46], [11], [3], [40], [45], [40], [13], [42], [69], [21], [62], [63], [28], [23], [55], [51], [60], [77], [25], [47], [5], [88], [76], [12], [52], [88], [93], [30], [93], [95], [34], [92], [107], [82]]\n",
            "DEBUGGING: actions length = 200\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[ 2.9507,  0.8184,  0.0848,  ...,  2.1898, -0.2865,  1.5246],\n",
            "        [ 2.9601,  0.8171,  0.0722,  ...,  2.1914, -0.2633,  1.5328],\n",
            "        [ 3.0154,  0.8416,  0.0598,  ...,  2.2335, -0.2406,  1.5847],\n",
            "        ...,\n",
            "        [ 3.0095,  0.8456,  0.0715,  ...,  2.2416, -0.2299,  1.5916],\n",
            "        [ 3.0108,  0.8463,  0.0709,  ...,  2.2427, -0.2292,  1.5922],\n",
            "        [ 3.0094,  0.8452,  0.0713,  ...,  2.2410, -0.2306,  1.5910]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[ 3.0140,  0.8469,  0.0727,  ...,  2.2445, -0.2264,  1.5978],\n",
            "        [ 3.0029,  0.8406,  0.0693,  ...,  2.2324, -0.2374,  1.5824],\n",
            "        [ 3.0114,  0.8474,  0.0688,  ...,  2.2432, -0.2264,  1.5926],\n",
            "        ...,\n",
            "        [ 3.0063,  0.8429,  0.0705,  ...,  2.2371, -0.2338,  1.5875],\n",
            "        [ 3.0029,  0.8397,  0.0702,  ...,  2.2325, -0.2372,  1.5827],\n",
            "        [ 3.0104,  0.8456,  0.0716,  ...,  2.2419, -0.2274,  1.5939]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([27.5207, 27.4031, 27.5194, 27.4580, 27.6206, 27.5172, 27.4768, 27.4157,\n",
            "        27.5881, 27.4276, 27.3735, 27.4641, 27.4495, 27.4433, 27.4670, 27.5171,\n",
            "        27.4791, 27.4417, 27.4869, 27.3227, 27.4326, 27.4568, 27.4966, 27.4396,\n",
            "        27.5807, 27.3922, 27.4513, 27.4715, 27.4356, 27.4832, 27.4571, 27.4956,\n",
            "        27.4399, 27.4188, 27.4382, 27.4466, 27.4217, 27.5539, 27.4939, 27.4878,\n",
            "        27.5054, 27.4496, 27.4063, 27.4803, 27.4330, 27.4805, 27.4431, 27.4748,\n",
            "        27.4588, 27.4431, 27.4195, 27.4383, 27.4357, 27.4515, 27.5022, 27.5037,\n",
            "        27.5325, 27.4363, 27.3948, 27.3710, 27.4800, 27.5142, 27.5770, 27.4217,\n",
            "        27.4293, 27.4042, 27.4714, 27.3901, 27.4421, 27.4700, 27.5237, 27.5268,\n",
            "        27.5194, 27.5058, 27.5672, 27.5070, 27.4594, 27.5127, 27.3800, 27.4452,\n",
            "        27.4679, 27.4573, 27.5163, 27.3819, 27.4629, 27.4490, 27.4323, 27.4642,\n",
            "        27.4111, 27.4563, 27.4410, 27.3999, 27.5723, 27.3929, 27.4745, 27.5047,\n",
            "        27.4137, 27.4307, 27.5240, 27.5081, 27.3778, 27.4425, 27.4042, 27.4924],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[7.70426362 7.72040065 7.58780743 7.55558247 7.52132619 7.44809755\n",
            "  7.46617909 7.44497134 1.49363555 1.48540065 1.33101571 1.34066486\n",
            "  1.27409326 1.24945183 1.23423102 1.20678402 1.21657737 1.2192244\n",
            "  1.22716186 1.22354343 1.22768473 1.22526724 1.17758016 1.11470567\n",
            "  1.10686174 1.10229945 0.91561743 0.6157019  0.60669087 0.57729433\n",
            "  0.43879206 0.43446577 0.43664208 0.43552828 0.38404679 0.35451103\n",
            "  0.3462165  0.34469639 0.24730679 0.24537501 0.24276228 0.2196075\n",
            "  0.19857903 0.18995655 0.17892911 0.16954249 0.1707541  0.16106677\n",
            "  0.13706479 0.1351622 ]]\n",
            "DEBUGGING: baseline2 looks like: 7.704263623117806\n",
            "DEBUGGING: ADS looks like: [-6.95345866 -6.97797702 -6.8661707  -6.87667362 -6.85314508 -6.80048728\n",
            " -6.86853915 -6.9393973  -0.98543069 -1.04160616 -0.88745175 -0.89358864\n",
            " -0.8370157  -0.80802578 -0.89051684 -0.890998   -0.90419265 -0.91984578\n",
            " -0.93125763 -0.92791224 -0.9564585  -0.96674648 -0.92169607 -0.88167838\n",
            " -0.89332001 -0.88829775 -0.72355743 -0.42466322 -0.41563216 -0.43533623\n",
            " -0.30124107 -0.31622222 -0.31737184 -0.31586915 -0.28489891 -0.28399236\n",
            " -0.27536466 -0.27329673 -0.17666511 -0.17653499 -0.17624341 -0.16045981\n",
            " -0.16212447 -0.15449229 -0.14313636 -0.14209565 -0.14325421 -0.15192945\n",
            " -0.13006741 -0.13324152 -4.07126116 -4.16798719 -4.30857971 -4.61902941\n",
            " -4.84656916 -4.7656842  -4.89427922 -5.02933223  0.53409734  0.53906964\n",
            "  0.04106113  0.03105771 -0.19445499 -0.30888593 -0.29335989 -0.38466504\n",
            " -0.38914864 -0.40584283 -0.41657764 -0.46556554 -0.46753027 -0.5012627\n",
            " -0.68126852 -0.61448452 -0.64184548 -0.68479078 -0.5753375  -0.33959142\n",
            " -0.35030293 -0.31865982 -0.18482481 -0.17815876 -0.18218741 -0.18936981\n",
            " -0.15159715 -0.19605969 -0.21048913 -0.22375275 -0.15765897 -0.16805027\n",
            " -0.17657773 -0.15617801 -0.14106088 -0.13243507 -0.14761253 -0.15003101\n",
            " -0.15172605 -0.14931848 -0.13583238 -0.13392495 -3.96646003 -4.03364112\n",
            " -4.35042961 -4.28550386 -4.21821648 -4.1116231  -4.09600288 -4.04075294\n",
            "  1.94496889  1.98793717  2.17740633  2.20319579  2.30556395  2.36636354\n",
            "  2.41810774  2.48244705  2.50991866  2.54491301  2.57499713  2.61702121\n",
            "  2.65167349  2.69327642  2.78054475  2.61086857  2.64202723  2.67538797\n",
            "  2.21451236  1.37995654  1.37262596  1.33129037  0.92485794  0.92884675\n",
            "  0.93620134  0.94076723  0.82054286  0.83456307  0.83207029  0.84174586\n",
            "  0.58163088  0.58996027  0.59558342  0.53624533  0.50176438  0.4768839\n",
            "  0.469678    0.46166915  0.46573435  0.4623147   0.40296458  0.40232867\n",
            " 14.99117985 15.17960533 15.52518002 15.78120689 15.91793072 15.67779458\n",
            " 15.85882125 16.00948247 -1.49363555 -1.48540065 -1.33101571 -1.34066486\n",
            " -1.27409326 -1.24945183 -1.23423102 -1.20678401 -1.21657737 -1.21922439\n",
            " -1.22716186 -1.22354343 -1.22768473 -1.22526724 -1.17758016 -1.11470567\n",
            " -1.10686174 -1.10229945 -0.91561742 -0.6157019  -0.60669087 -0.57729433\n",
            " -0.43879206 -0.43446577 -0.43664208 -0.43552828 -0.38404679 -0.35451103\n",
            " -0.3462165  -0.34469639 -0.24730679 -0.24537501 -0.24276228 -0.2196075\n",
            " -0.19857903 -0.18995655 -0.17892911 -0.16954249 -0.1707541  -0.16106677\n",
            " -0.13706479 -0.1351622 ]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(-0.0255, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0089,  0.0072, -0.0077,  ..., -0.0042,  0.0177, -0.0942],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [ 0.0064,  0.0052, -0.0056,  ..., -0.0031,  0.0128, -0.0692],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0003, -0.0002,  0.0003,  ...,  0.0002, -0.0007,  0.0111]])\n",
            "   Last layer:\n",
            "tensor([[-4.5376e-03, -3.1288e-02,  0.0000e+00, -1.3917e-02,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -5.2305e-02, -2.5442e-02,  0.0000e+00,\n",
            "         -4.8447e-02,  0.0000e+00,  0.0000e+00,  4.6885e-03,  0.0000e+00,\n",
            "         -2.6232e-02,  0.0000e+00, -3.4613e-02, -4.0198e-02, -9.7947e-04],\n",
            "        [-1.8580e-03, -1.3138e-02,  0.0000e+00, -4.6353e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -1.9829e-02, -9.8312e-03,  0.0000e+00,\n",
            "         -1.9874e-02,  0.0000e+00,  0.0000e+00,  9.6158e-04,  0.0000e+00,\n",
            "         -1.2058e-02,  0.0000e+00, -1.4587e-02, -1.4936e-02, -7.5863e-04],\n",
            "        [ 2.1089e-04,  1.6858e-03,  0.0000e+00, -3.4259e-05,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  1.4613e-03,  8.0807e-04,  0.0000e+00,\n",
            "          2.3204e-03,  0.0000e+00,  0.0000e+00,  4.1324e-04,  0.0000e+00,\n",
            "          2.1146e-03,  0.0000e+00,  1.9168e-03,  9.1624e-04,  2.7659e-04],\n",
            "        [-1.4904e-03, -1.0227e-02,  0.0000e+00, -4.6106e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -1.7202e-02, -8.3616e-03,  0.0000e+00,\n",
            "         -1.5855e-02,  0.0000e+00,  0.0000e+00,  1.5966e-03,  0.0000e+00,\n",
            "         -8.5028e-03,  0.0000e+00, -1.1303e-02, -1.3237e-02, -3.0264e-04],\n",
            "        [-4.2702e-04, -2.9574e-03,  0.0000e+00, -1.2977e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -4.9168e-03, -2.3889e-03,  0.0000e+00,\n",
            "         -4.5727e-03,  0.0000e+00,  0.0000e+00,  4.2624e-04,  0.0000e+00,\n",
            "         -2.4979e-03,  0.0000e+00, -3.2744e-03, -3.7719e-03, -9.7129e-05],\n",
            "        [ 7.9368e-04,  5.8377e-03,  0.0000e+00,  1.2991e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  7.4824e-03,  3.8259e-03,  0.0000e+00,\n",
            "          8.5457e-03,  0.0000e+00,  0.0000e+00,  2.1854e-04,  0.0000e+00,\n",
            "          6.0307e-03,  0.0000e+00,  6.5278e-03,  5.4203e-03,  5.5427e-04],\n",
            "        [-4.9524e-03, -3.4523e-02,  0.0000e+00, -1.4006e-02,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -5.5340e-02, -2.7110e-02,  0.0000e+00,\n",
            "         -5.2949e-02,  0.0000e+00,  0.0000e+00,  4.0419e-03,  0.0000e+00,\n",
            "         -3.0125e-02,  0.0000e+00, -3.8258e-02, -4.2188e-02, -1.4702e-03],\n",
            "        [-3.7551e-03, -2.6126e-02,  0.0000e+00, -1.0727e-02,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -4.2093e-02, -2.0612e-02,  0.0000e+00,\n",
            "         -4.0104e-02,  0.0000e+00,  0.0000e+00,  3.1746e-03,  0.0000e+00,\n",
            "         -2.2667e-02,  0.0000e+00, -2.8937e-02, -3.2124e-02, -1.0736e-03],\n",
            "        [-1.6747e-03, -1.3031e-02,  0.0000e+00, -4.9082e-04,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -1.2474e-02, -6.8409e-03,  0.0000e+00,\n",
            "         -1.8159e-02,  0.0000e+00,  0.0000e+00, -2.5238e-03,  0.0000e+00,\n",
            "         -1.5576e-02,  0.0000e+00, -1.4711e-02, -8.2326e-03, -1.9199e-03],\n",
            "        [-3.4159e-03, -2.4295e-02,  0.0000e+00, -8.1424e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -3.5925e-02, -1.7873e-02,  0.0000e+00,\n",
            "         -3.6596e-02,  0.0000e+00,  0.0000e+00,  1.3948e-03,  0.0000e+00,\n",
            "         -2.2693e-02,  0.0000e+00, -2.7016e-02, -2.6933e-02, -1.5181e-03]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 2.9677e-03,  3.9045e-03, -7.6581e-03,  ..., -2.7466e-03,\n",
            "          1.2655e-02, -4.8214e-02],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 2.2140e-03,  2.9092e-03, -5.7384e-03,  ..., -2.0650e-03,\n",
            "          9.4579e-03, -3.8759e-02],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-4.3995e-05, -5.6331e-05,  1.5841e-04,  ...,  6.9599e-05,\n",
            "         -2.2782e-04,  5.4905e-03]])\n",
            "   Last layer:\n",
            "tensor([[-2.2929e-03, -9.0203e-03,  0.0000e+00, -4.5068e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -3.1219e-02, -2.8970e-03,  0.0000e+00,\n",
            "         -1.9276e-02,  0.0000e+00,  0.0000e+00,  6.0238e-04,  0.0000e+00,\n",
            "         -1.2910e-02,  0.0000e+00, -1.7722e-02, -1.6859e-02,  4.0495e-03],\n",
            "        [-9.9052e-04, -6.2621e-03,  0.0000e+00, -1.4510e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -1.1916e-02, -2.5513e-03,  0.0000e+00,\n",
            "         -9.9446e-03,  0.0000e+00,  0.0000e+00, -2.6161e-04,  0.0000e+00,\n",
            "         -7.2303e-03,  0.0000e+00, -8.6005e-03, -7.1094e-03,  7.4243e-04],\n",
            "        [-1.0922e-04, -8.2997e-04,  0.0000e+00, -1.3580e-04,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -1.2487e-03, -3.5571e-04,  0.0000e+00,\n",
            "         -1.2021e-03,  0.0000e+00,  0.0000e+00, -5.7381e-05,  0.0000e+00,\n",
            "         -9.0190e-04,  0.0000e+00, -1.0127e-03, -7.8770e-04,  2.6134e-05],\n",
            "        [-1.0191e-03, -5.6960e-03,  0.0000e+00, -1.6484e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -1.2744e-02, -2.2169e-03,  0.0000e+00,\n",
            "         -9.7139e-03,  0.0000e+00,  0.0000e+00, -1.0494e-04,  0.0000e+00,\n",
            "         -6.9105e-03,  0.0000e+00, -8.5439e-03, -7.3646e-03,  1.0807e-03],\n",
            "        [-1.3051e-04, -5.6583e-04,  0.0000e+00, -2.4235e-04,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -1.7282e-03, -1.9377e-04,  0.0000e+00,\n",
            "         -1.1295e-03,  0.0000e+00,  0.0000e+00,  2.2109e-05,  0.0000e+00,\n",
            "         -7.6955e-04,  0.0000e+00, -1.0244e-03, -9.4998e-04,  2.0453e-04],\n",
            "        [ 2.8911e-04,  2.3418e-03,  0.0000e+00,  3.1524e-04,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  3.1334e-03,  1.0281e-03,  0.0000e+00,\n",
            "          3.2514e-03,  0.0000e+00,  0.0000e+00,  1.9019e-04,  0.0000e+00,\n",
            "          2.4679e-03,  0.0000e+00,  2.7145e-03,  2.0352e-03,  1.9682e-06],\n",
            "        [-2.3861e-03, -9.9742e-03,  0.0000e+00, -4.5760e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -3.2125e-02, -3.3421e-03,  0.0000e+00,\n",
            "         -2.0469e-02,  0.0000e+00,  0.0000e+00,  4.9612e-04,  0.0000e+00,\n",
            "         -1.3847e-02,  0.0000e+00, -1.8687e-02, -1.7513e-02,  3.9742e-03],\n",
            "        [-2.0379e-03, -1.0267e-02,  0.0000e+00, -3.5319e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -2.6242e-02, -3.8116e-03,  0.0000e+00,\n",
            "         -1.8666e-02,  0.0000e+00,  0.0000e+00,  3.8826e-05,  0.0000e+00,\n",
            "         -1.3044e-02,  0.0000e+00, -1.6646e-02, -1.4814e-02,  2.6398e-03],\n",
            "        [-7.5195e-04, -9.2163e-03,  0.0000e+00, -1.2946e-04,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -5.9438e-03, -4.3876e-03,  0.0000e+00,\n",
            "         -1.0551e-02,  0.0000e+00,  0.0000e+00, -1.1856e-03,  0.0000e+00,\n",
            "         -8.5853e-03,  0.0000e+00, -8.2630e-03, -5.0081e-03, -1.3687e-03],\n",
            "        [-1.7984e-03, -1.1045e-02,  0.0000e+00, -2.6776e-03,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00, -2.1762e-02, -4.4502e-03,  0.0000e+00,\n",
            "         -1.7804e-02,  0.0000e+00,  0.0000e+00, -4.0344e-04,  0.0000e+00,\n",
            "         -1.2889e-02,  0.0000e+00, -1.5455e-02, -1.2895e-02,  1.4628e-03]])\n",
            "DEBUGGING: training for one iteration takes 0.004875 min:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329,
          "referenced_widgets": [
            "039c9c17bafc40e0827108fbca81c032",
            "bc4e8b39d254488b8effcefa54538fdf",
            "c9951c0aa6104832b2076847a5b32354",
            "de1d993f297d4379890e58b111f43372",
            "50fa5111f16b426f877f98ac67e06cc0",
            "2225fdcbbb2f4de2ad8d8b17984b8d62",
            "ef0c3d8046e348818bb8e82a6692a1b8",
            "6cd1de4bb69d4c4c95db3ac2853cbebd"
          ]
        },
        "id": "Oe4fCU1V-kAs",
        "outputId": "2774c2d4-0e53-4228-80a0-6775371c49d3"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "039c9c17bafc40e0827108fbca81c032"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>training loss</td><td>â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–â–„â–ƒâ–‚â–„â–ƒâ–ƒ</td></tr><tr><td>training reward</td><td>â–†â–†â–â–‚â–‚â–‚â–†â–ˆâ–â–†â–†â–â–‚â–ˆâ–†â–ˆâ–ˆâ–†â–ˆâ–†â–†â–â–†â–†â–ˆ</td></tr><tr><td>training reward moving average</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>training loss</td><td>-0.02551</td></tr><tr><td>training reward</td><td>0.88674</td></tr><tr><td>training reward moving average</td><td>0.4233</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">true-wood-2736</strong>: <a href=\"https://wandb.ai/ieor4575-spring2022/finalproject/runs/3o250wsg\" target=\"_blank\">https://wandb.ai/ieor4575-spring2022/finalproject/runs/3o250wsg</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220508_235644-3o250wsg/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = [   54246.9141, 27220988.0000,  3948314.5000,  3902082.5000,\n",
        "        27219886.0000,  3816622.7500, 27175632.0000,  3976903.5000,\n",
        "         3977309.0000,  3961905.7500,  3904790.7500, 23420360.0000,\n",
        "        27218672.0000, 27206276.0000, 27190054.0000,  3976984.7500,\n",
        "         3931598.0000,  3963138.5000, 19419398.0000,  3931559.5000,\n",
        "         7862683.0000, 19299310.0000,  3915878.7500, 15562425.0000,\n",
        "        27141808.0000, 27323768.0000, 11622510.0000, 15451909.0000,\n",
        "        23389744.0000,  7781165.5000, 11871555.0000, 11657742.0000,\n",
        "        27473456.0000, 11592561.0000,  4054315.2500,  7718911.0000,\n",
        "        27136936.0000,  3914171.2500, 23255876.0000, 15599427.0000,\n",
        "        19661078.0000,  3915166.7500,  7554840.5000, 27196042.0000,\n",
        "        11791202.0000, 23477022.0000, 15708443.0000, 23297932.0000,\n",
        "        23511102.0000,  7689591.5000, 27405178.0000,  4021836.7500,\n",
        "        15558291.0000,  3948567.5000,  7915618.5000,  3864714.5000,\n",
        "         3995934.2500, 19536616.0000,  3890406.7500, 27284138.0000,\n",
        "        11731948.0000, 23232848.0000, 19200360.0000, 22949674.0000,\n",
        "        26724900.0000,  3268551.2500, 22647966.0000, 25925040.0000,\n",
        "        25380838.0000,  5669201.5000,  7810961.0000, 13092715.0000,\n",
        "        14364957.0000, 15622157.0000, 27473452.0000]"
      ],
      "metadata": {
        "id": "R5xFgG-ZGZkL"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_t = torch.FloatTensor(a)\n",
        "print(a_t)\n",
        "b_t = a_t /1000000\n",
        "print(b_t)\n",
        "print(torch.nn.functional.softmax(b_t, dim=0))\n",
        "b_t = a_t / a_t.max()\n",
        "print(b_t)\n",
        "torch.nn.functional.softmax(b_t, dim=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5GfEt5DGbcN",
        "outputId": "da271f0c-20f2-49ac-fc76-6b094b1dcc6c"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([   54246.9141, 27220988.0000,  3948314.5000,  3902082.5000,\n",
            "        27219886.0000,  3816622.7500, 27175632.0000,  3976903.5000,\n",
            "         3977309.0000,  3961905.7500,  3904790.7500, 23420360.0000,\n",
            "        27218672.0000, 27206276.0000, 27190054.0000,  3976984.7500,\n",
            "         3931598.0000,  3963138.5000, 19419398.0000,  3931559.5000,\n",
            "         7862683.0000, 19299310.0000,  3915878.7500, 15562425.0000,\n",
            "        27141808.0000, 27323768.0000, 11622510.0000, 15451909.0000,\n",
            "        23389744.0000,  7781165.5000, 11871555.0000, 11657742.0000,\n",
            "        27473456.0000, 11592561.0000,  4054315.2500,  7718911.0000,\n",
            "        27136936.0000,  3914171.2500, 23255876.0000, 15599427.0000,\n",
            "        19661078.0000,  3915166.7500,  7554840.5000, 27196042.0000,\n",
            "        11791202.0000, 23477022.0000, 15708443.0000, 23297932.0000,\n",
            "        23511102.0000,  7689591.5000, 27405178.0000,  4021836.7500,\n",
            "        15558291.0000,  3948567.5000,  7915618.5000,  3864714.5000,\n",
            "         3995934.2500, 19536616.0000,  3890406.7500, 27284138.0000,\n",
            "        11731948.0000, 23232848.0000, 19200360.0000, 22949674.0000,\n",
            "        26724900.0000,  3268551.2500, 22647966.0000, 25925040.0000,\n",
            "        25380838.0000,  5669201.5000,  7810961.0000, 13092715.0000,\n",
            "        14364957.0000, 15622157.0000, 27473452.0000])\n",
            "tensor([ 0.0542, 27.2210,  3.9483,  3.9021, 27.2199,  3.8166, 27.1756,  3.9769,\n",
            "         3.9773,  3.9619,  3.9048, 23.4204, 27.2187, 27.2063, 27.1901,  3.9770,\n",
            "         3.9316,  3.9631, 19.4194,  3.9316,  7.8627, 19.2993,  3.9159, 15.5624,\n",
            "        27.1418, 27.3238, 11.6225, 15.4519, 23.3897,  7.7812, 11.8716, 11.6577,\n",
            "        27.4735, 11.5926,  4.0543,  7.7189, 27.1369,  3.9142, 23.2559, 15.5994,\n",
            "        19.6611,  3.9152,  7.5548, 27.1960, 11.7912, 23.4770, 15.7084, 23.2979,\n",
            "        23.5111,  7.6896, 27.4052,  4.0218, 15.5583,  3.9486,  7.9156,  3.8647,\n",
            "         3.9959, 19.5366,  3.8904, 27.2841, 11.7319, 23.2328, 19.2004, 22.9497,\n",
            "        26.7249,  3.2686, 22.6480, 25.9250, 25.3808,  5.6692,  7.8110, 13.0927,\n",
            "        14.3650, 15.6222, 27.4735])\n",
            "tensor([1.0010e-13, 6.2920e-02, 4.9158e-12, 4.6937e-12, 6.2851e-02, 4.3093e-12,\n",
            "        6.0130e-02, 5.0584e-12, 5.0605e-12, 4.9831e-12, 4.7065e-12, 1.4067e-03,\n",
            "        6.2775e-02, 6.2001e-02, 6.1004e-02, 5.0588e-12, 4.8344e-12, 4.9893e-12,\n",
            "        2.5740e-05, 4.8342e-12, 2.4637e-10, 2.2827e-05, 4.7589e-12, 5.4393e-07,\n",
            "        5.8130e-02, 6.9731e-02, 1.0579e-08, 4.8702e-07, 1.3643e-03, 2.2708e-10,\n",
            "        1.3571e-08, 1.0959e-08, 8.0991e-02, 1.0267e-08, 5.4655e-12, 2.1338e-10,\n",
            "        5.7848e-02, 4.7508e-12, 1.1933e-03, 5.6443e-07, 3.2777e-05, 4.7556e-12,\n",
            "        1.8109e-10, 6.1370e-02, 1.2523e-08, 1.4887e-03, 6.2944e-07, 1.2446e-03,\n",
            "        1.5403e-03, 2.0721e-10, 7.5646e-02, 5.2909e-12, 5.4168e-07, 4.9171e-12,\n",
            "        2.5976e-10, 4.5216e-12, 5.1556e-12, 2.8941e-05, 4.6393e-12, 6.7022e-02,\n",
            "        1.1803e-08, 1.1662e-03, 2.0676e-05, 8.7858e-04, 3.8313e-02, 2.4910e-12,\n",
            "        6.4976e-04, 1.7217e-02, 9.9914e-03, 2.7477e-11, 2.3395e-10, 4.6021e-08,\n",
            "        1.6424e-07, 5.7741e-07, 8.0991e-02])\n",
            "tensor([0.0020, 0.9908, 0.1437, 0.1420, 0.9908, 0.1389, 0.9892, 0.1448, 0.1448,\n",
            "        0.1442, 0.1421, 0.8525, 0.9907, 0.9903, 0.9897, 0.1448, 0.1431, 0.1443,\n",
            "        0.7068, 0.1431, 0.2862, 0.7025, 0.1425, 0.5665, 0.9879, 0.9946, 0.4230,\n",
            "        0.5624, 0.8514, 0.2832, 0.4321, 0.4243, 1.0000, 0.4220, 0.1476, 0.2810,\n",
            "        0.9878, 0.1425, 0.8465, 0.5678, 0.7156, 0.1425, 0.2750, 0.9899, 0.4292,\n",
            "        0.8545, 0.5718, 0.8480, 0.8558, 0.2799, 0.9975, 0.1464, 0.5663, 0.1437,\n",
            "        0.2881, 0.1407, 0.1454, 0.7111, 0.1416, 0.9931, 0.4270, 0.8456, 0.6989,\n",
            "        0.8353, 0.9728, 0.1190, 0.8244, 0.9436, 0.9238, 0.2064, 0.2843, 0.4766,\n",
            "        0.5229, 0.5686, 1.0000])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0074, 0.0199, 0.0085, 0.0085, 0.0199, 0.0085, 0.0199, 0.0085, 0.0085,\n",
              "        0.0085, 0.0085, 0.0173, 0.0199, 0.0199, 0.0199, 0.0085, 0.0085, 0.0085,\n",
              "        0.0150, 0.0085, 0.0098, 0.0149, 0.0085, 0.0130, 0.0198, 0.0200, 0.0113,\n",
              "        0.0130, 0.0173, 0.0098, 0.0114, 0.0113, 0.0201, 0.0113, 0.0086, 0.0098,\n",
              "        0.0198, 0.0085, 0.0172, 0.0130, 0.0151, 0.0085, 0.0097, 0.0199, 0.0113,\n",
              "        0.0174, 0.0131, 0.0172, 0.0174, 0.0098, 0.0200, 0.0085, 0.0130, 0.0085,\n",
              "        0.0099, 0.0085, 0.0085, 0.0150, 0.0085, 0.0199, 0.0113, 0.0172, 0.0149,\n",
              "        0.0170, 0.0195, 0.0083, 0.0168, 0.0190, 0.0186, 0.0091, 0.0098, 0.0119,\n",
              "        0.0125, 0.0130, 0.0201])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYdGyAom2SMj",
        "outputId": "f2fc0e25-85c4-46a9-e617-24d438a8d491"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2., 0., 1., 3., 0., 0., 0., 3., 2., 3., 1., 1., 2., 0., 4., 4., 0.,\n",
              "       2., 1., 2., 2., 2., 4., 1., 3., 2., 0., 1., 2., 0., 3., 0., 3., 1.,\n",
              "       3., 0., 4., 1., 4., 4., 0., 0., 1., 2., 4., 0., 0., 1., 1., 1., 2.,\n",
              "       3., 4., 4., 3., 3., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s[1][-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WUaedhX198a",
        "outputId": "ef9c5994-7d29-423d-f03c-95ef17cdd1e7"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5113.0"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s[-2] / np.max(s[-2], axis=1, keepdims=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmmYGIMyEQNl",
        "outputId": "1b68748f-8102-4e9b-f42f-35cd6547dd55"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.6097561 , 0.75609756, 0.7804878 , ..., 0.7804878 , 0.85365854,\n",
              "        0.68292683],\n",
              "       [0.68181818, 0.79545455, 0.79545455, ..., 0.79545455, 0.86363636,\n",
              "        0.81818182],\n",
              "       [0.57142857, 0.8       , 0.8       , ..., 0.88571429, 0.85714286,\n",
              "        0.68571429],\n",
              "       ...,\n",
              "       [0.64102564, 0.82051282, 0.76923077, ..., 0.76923077, 0.87179487,\n",
              "        0.61538462],\n",
              "       [0.62162162, 0.72972973, 0.7027027 , ..., 0.78378378, 0.75675676,\n",
              "        0.72972973],\n",
              "       [0.66666667, 0.80701754, 0.8245614 , ..., 0.9122807 , 0.84210526,\n",
              "        0.70175439]])"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(s[-2]), len(s[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GEP-MOCEZKu",
        "outputId": "f8f8ab5a-3ede-416d-ccbd-507d1ca709c5"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(109, 110)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([0.0057, 0.0235, 0.0144, 0.0181, 0.0181, 0.0123, 0.0293, 0.0095, 0.0122,\n",
        "        0.0146, 0.0152, 0.0095, 0.0184, 0.0151, 0.0143, 0.0095, 0.0220, 0.0120,\n",
        "        0.0209, 0.0226, 0.0142, 0.0124, 0.0145, 0.0133, 0.0218, 0.0146, 0.0098,\n",
        "        0.0186, 0.0146, 0.0126, 0.0182, 0.0204, 0.0155, 0.0180, 0.0098, 0.0184,\n",
        "        0.0248, 0.0150, 0.0215, 0.0114, 0.0186, 0.0175, 0.0141, 0.0199, 0.0250,\n",
        "        0.0178, 0.0310, 0.0170, 0.0153, 0.0204, 0.0121, 0.0247, 0.0187, 0.0197,\n",
        "        0.0130, 0.0127, 0.0240, 0.0189, 0.0104, 0.0125])\n",
        "a.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlcbwh-Ts6Jt",
        "outputId": "cbf63915-ad14-4f75-c704-efda85f0fe67"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.env_now.env.oldobj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg-stUzCuAdm",
        "outputId": "20923695-9827-4ee8-e6b2-4770c2996e67"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-2102.0"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.env_now.env.newobj\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyF1UWObuLoZ",
        "outputId": "ca7229bb-8cce-4fba-fb9a-f6228df609f9"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-2102.0"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.env_now.env.ip_obj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPL2qJZ0nNrJ",
        "outputId": "cec87695-43a2-4af2-a33c-3a72aa22b8ee"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-2100.0"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OH notes:\n",
        "\n",
        "baseline is what you use to interprete the reward\n",
        "\n",
        "don't use nn, use \"mean\" of rewards in this episode (I think so)\n",
        "\n",
        "Maybe look in to the instances, and look at how your agent is solving them\n",
        "\n",
        "Cutting off early, but only after solved some LP's\n",
        "- counterfactual exploration\n",
        "  - more than to do it more random\n",
        "- all prob entirely the same\n",
        "- activation function ?\n",
        "\n",
        "not learning? \n",
        "\n",
        "reward shaping?\n",
        "- someone: amplify the reward (shouldn't, since every step wil GIVE a reward), compare it to other POSSIBLE states\n",
        "- pre-trained on the instances and pre-trained the baseline?\n",
        "- get the max reward from the LP solver (isn't this cheating lol)\n",
        "- recalcluate the max-gap-to-go (lol remaining max gap) every step\n",
        "- go in the environment to make if return the shaped new-gap reward\n",
        "- the original reward doesn't help you across LPs?\n",
        "- moving average of *returns* from all *previous* episodes\n",
        "  - return is the discounted sum of the reward\n",
        "- advantage? Q - running averaged RETURN (but different states are mixed in, how do you deal with that)\n",
        "- Think about what's wrong with this base line, and write it down\n",
        "  - something about the states\n",
        "\n",
        "\n",
        "differences between LPs\n",
        "\n",
        "mode: \n",
        "- standardize the constraints, and the b vector by itself\n",
        "  - do you normalize by row or columns?\n",
        "  - He thinks it's more sense to normalize by rows\n",
        "  - Normalize it twice? LOL\n",
        "- or a normalization layer\\\n",
        "\n",
        "\n",
        "When doing softmax you can use a lamda (parameter) to mitigate large score difference\n",
        "- softmax comes with a scalar parameter\n",
        "\n",
        "Professor didn't think normalizing the input is necessary & and it shouldn't make a difference anyways (but come numerical value can be lost after normalization)\n",
        "\n",
        "iteration: \n",
        "\n",
        "plot random policy together, (as a bseline to see if your model is really learning)\n",
        "\n",
        "For baseline, Prof is suggesting Q network can work too.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ha4GPGL8Dduc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sKnhC54KYNNL"
      },
      "execution_count": 92,
      "outputs": []
    }
  ]
}