{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_policy_gradient.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ced572e143cf4e6f9d7c42c31b2d0af1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_37b10496ae1d45b6b68f11b3eeb250fb",
              "IPY_MODEL_77a8e198db354f3d83bfc2f3011e97fe"
            ],
            "layout": "IPY_MODEL_0c6df2472cd84d3985d6407326d71fd9"
          }
        },
        "37b10496ae1d45b6b68f11b3eeb250fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98e43e3d6868406286236c8cd5d17b2c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4822cb1580fc4498bffa4c46cd7d387e",
            "value": "0.432 MB of 0.432 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "77a8e198db354f3d83bfc2f3011e97fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a760fbe60df403e94c9d84e72aac997",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2bd0dcdf9dde4376b34b6e44ee3fdbec",
            "value": 1
          }
        },
        "0c6df2472cd84d3985d6407326d71fd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98e43e3d6868406286236c8cd5d17b2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4822cb1580fc4498bffa4c46cd7d387e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a760fbe60df403e94c9d84e72aac997": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bd0dcdf9dde4376b34b6e44ee3fdbec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ClaireZixiWang/learn2cut/blob/main/Project_policy_gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## See README.md file for further details about the project and the environment.\n",
        "\n",
        "### State-Action Description\n",
        "\n",
        "### State\n",
        "State s is an array with give components\n",
        "\n",
        "* s[0]:  constraint matrix $A$of the current LP ($\\max  -c^Tx \\text{ s.t. }Ax \\le  b$) . Dimension is $m \\times n$. See by printing s[0].shape. Here $n$ is the (fixed) number of variables. For instances of size 60 by 60 used in the above command, $n$ will remain fixed as 60. And $m$ is the current number of constraints. Initially, $m$ is to the number of constraints in the IP instance. (For instances generated with --num-c=60, $m$ is 60 at the first step).  But $m$ will increase by one in every step of the episode as one new constraint (cut) is added on taking an action.\n",
        "* s[1]: rhs $b$ for the current LP ($Ax\\le b$). Dimension same as the number $m$ in matrix A.\n",
        "* s[2]: coefficient vector $c$ from the LP objective ($-c^Tx$). Dimension same as the number of variables, i.e., $n$.\n",
        "* s[3],  s[4]: Gomory cuts available in the current round of Gomory's cutting plane algorithm. Each cut $i$ is of the form $D_i x\\le d_i$.   s[3] gives the matrix $D$ (of dimension $k \\times n$) of cuts and s[4] gives the rhs $d$ (of dimension $k$). The number of cuts $k$ available in each round changes, you can find it out by printing the size of last component of state, i.e., s[4].size or s[-1].size.\n",
        "\n",
        "### Actions\n",
        "There are k=s[4].size actions available in each state $s$, with $i^{th}$ action corresponding to the $i^{th}$ cut with inequality $D_i x\\le d_i$ in $s[3], s[4]$."
      ],
      "metadata": {
        "id": "5TN-sMTvcG_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***QUESTIONS***:\n",
        "1. By \"current\" LP, you mean the LP that the agent was running in the last state? As in, the LP with all the added constraints? \n",
        "  * ==> I think so.\n",
        "1. What do you mean Gomory cuts *available*? As in, after doing Simplex methods, the *variables* that you can choose to cut?\n",
        "  * Yes I think so.\n",
        "2. Isn't the number of variables (n) changing? in the C-G cutting plane method?\n",
        "  * No, as the spec says, **$n$ is the fixed number of variables**.\n",
        "  * If you look that cuttng plane lecture notes, you can see that after each step, the dummy variable is not added in the constraint. They are merely there for the sake of the LP solver (simplex method), but not really relevant for us.\n",
        "    * This is not correct, I think they are still very much relevant, it's just that I think among the 60 variables a lot of them are space holders for dummy variables so that our $n$ is fixed, so that we don't have to worry about using LSTM. Since each time the sequence [a, b] will be of size n+1. And we can just use a fixed-input-size network to do that.\n",
        "    * But still need to verify with the TA about the place holder understanding.\n",
        "3. dimension of s[3] and s[4]? Where is the \"available all\" stored? In which dimension?\n",
        "  * Each row of D is an \"available cut\". Therefore each $D_i x\\le d_i$ is an \"available\" cut in CG method solved from the simplex method.\n",
        "4. pointing towards the slides: why does the number of constraints m increase 1 in each step, if you can choose *multiple* cuts in one step? (OR in the algorithm we just choose one cut each time? or is that a more vanilla version to start, but to expand on multiple cuts a time later?)\n",
        "5. What do you mean by each \"instance\"?"
      ],
      "metadata": {
        "id": "XJE0bz30UL1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYROdPaaZOVa",
        "outputId": "774f1cf3-3db4-4286-cb22-5eab2182e3bb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -i https://pypi.gurobi.com gurobipy"
      ],
      "metadata": {
        "id": "xSXTKB2zurrt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00c3f574-9c13-493b-e282-2107e478dad9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.gurobi.com\n",
            "Requirement already satisfied: gurobipy in /usr/local/lib/python3.7/dist-packages (9.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qqq"
      ],
      "metadata": {
        "id": "YULy9ymNvDxN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/IEOR_RL/Project_learn2cut\n",
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "lTnvB0_iZUrX",
        "outputId": "82774827-f46a-4f51-b86e-97a6a9fa9d56"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/IEOR_RL/Project_learn2cut\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/IEOR_RL/Project_learn2cut'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import time"
      ],
      "metadata": {
        "id": "Q8PiSPj5us0O"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(4, 40),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(40, 20), \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(20, 10)\n",
        "        )\n",
        "\n",
        "datapoint = torch.FloatTensor([\n",
        "                               [[1,2,3,4],\n",
        "                                [2,3,4,5]],\n",
        "                               [[3,4,5,6],\n",
        "                                [4,5,6,7]]\n",
        "                              ])\n"
      ],
      "metadata": {
        "id": "ojB8UvBAPAWk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(datapoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r_nraniPaKl",
        "outputId": "c4f45d05-c750-4d23-f9a9-ff0748473545"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0695, -0.0943, -0.0399, -0.0739, -0.7781,  0.3757, -0.4727,\n",
              "          -0.4254,  0.3087,  0.6647],\n",
              "         [ 0.1452, -0.0976, -0.1027, -0.1702, -1.0447,  0.4448, -0.5795,\n",
              "          -0.6108,  0.3231,  1.0118]],\n",
              "\n",
              "        [[ 0.2120, -0.1004, -0.1580, -0.2718, -1.2982,  0.4972, -0.6834,\n",
              "          -0.7912,  0.3371,  1.3318],\n",
              "         [ 0.2808, -0.1117, -0.2140, -0.3729, -1.5482,  0.5461, -0.7886,\n",
              "          -0.9763,  0.3587,  1.6468]]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Code for Policy Model\n",
        "\n",
        "class Policy(object):\n",
        "\n",
        "    # inputsize = n+1 = 61\n",
        "    def __init__(self, lr, input_size, attention_size=10, temperature=1) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_size, 40),\n",
        "            torch.nn.Sigmoid(),\n",
        "            torch.nn.Linear(40, 30), \n",
        "            torch.nn.Sigmoid(),\n",
        "            torch.nn.Linear(30, 20), \n",
        "            torch.nn.Sigmoid(),\n",
        "            torch.nn.Linear(20, attention_size)\n",
        "        )\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "        # RECORD HYPER-PARAMS\n",
        "        self.input_size = input_size\n",
        "        self.attention_size = attention_size\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def compute_logits_big_batch(self, obs_matrix, act_matrix, batchsize):\n",
        "        '''\n",
        "        Function that takes in a batch of observations and computes the action logits for each of observations in this batch\n",
        "        Args: obs_matrix: np.array(m * batchsize, n+1) # TODO maybe change this to tensor\n",
        "              act_matrix: np.array(k * batchsize, n+1)\n",
        "        Return: batch_logit: tensor(batchsize, k)\n",
        "        '''\n",
        "\n",
        "        # Get the batch result\n",
        "\n",
        "        # transform to tensor\n",
        "        obs_attention = self.model(torch.FloatTensor(obs_matrix)) # tensor(m * batchsize, u)\n",
        "        act_attention = self.model(torch.FloatTensor(act_matrix)) # tensor(k * batchsize, u)\n",
        "\n",
        "        assert obs_attention.shape == (obs_matrix.shape[0], self.attention_size)\n",
        "        assert act_attention.shape == (act_matrix.shape[0], self.attention_size)\n",
        "\n",
        "        # split a batch of output of size tensor(m * batchsize, u) into (batchsize, m, u)\n",
        "        batch_obs_output = torch.reshape(obs_attention, (batchsize, obs_attention.shape[0]/batchsize, obs_attention.shape[1]))\n",
        "        \n",
        "        # split a batch of output of size tensor(k * batchsize, u) into (batchsize, k, u)\n",
        "        batch_act_output = torch.reshape(act_attention, (batchsize, act_attention.shape[0]/batchsize, act_attention.shape[1]))\n",
        "\n",
        "        # To do batch matrix multiplication, transpose (batchsize, k, u) into (batchsize, u, k)\n",
        "        # (batchsize, m, u) @ (batchsize, u, k) =  (batchsize, m, k)\n",
        "        # (batchsize, m, k) == mean across all observation ==> (batchsize, k)\n",
        "        batch_logit = torch.bmm(batch_obs_output, batch_act_output.transpose(0, 2, 1)).mean(dim=1)\n",
        "\n",
        "        assert batch_logit.shape == (batchsize, act_matrix.shape[0]/batchsize)\n",
        "\n",
        "        return batch_logit\n",
        "\n",
        "    def compute_batch_selected_prob(self, batch_probs):\n",
        "        '''\n",
        "        Function that takes in a batch of probabilities and return the max probability for each \"datapoint\" in the batch\n",
        "        Args:    batch_logit: tensor(batchsize, k)\n",
        "        Return:  batch_selected_prob: tensor(batchsize,)\n",
        "        '''\n",
        "        # TODO\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def compute_one_step_logits(self, obs, act):\n",
        "        '''\n",
        "        Function that takes in ONE observation and action space, computes the action logits\n",
        "        Args:   obs_matrix: np.array(m, n+1)\n",
        "                act_matrix: np.array(k, n+1)\n",
        "        Return: logit: tensor(k)\n",
        "        '''\n",
        "\n",
        "        obs_attention = self.model(torch.FloatTensor(obs)) #-> (m, 10)\n",
        "        act_attention = self.model(torch.FloatTensor(act)) #-> (k, 10)\n",
        "        # print(\"DEBUGGING: obs_attention looks like:\", obs_attention)\n",
        "        # print(\"DEBUGGING: act_attention looks like:\", act_attention)\n",
        "\n",
        "        # attention matrix multiplication & mean to get the score\n",
        "        logits = torch.mm(obs_attention, act_attention.transpose(1, 0)).mean(dim=0)\n",
        "        # print(\"DEBUGGING: logits looks like:\", logits)\n",
        "\n",
        "        # print(\"DEBUGGING: act.shape =\", act_attention.shape)\n",
        "        # print(\"DEBUGGING: logits.shape =\", logits.shape)\n",
        "        assert logits.shape[0] == act_attention.shape[0]\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def predict_prob(self, obs, act):\n",
        "        # Function that uses softmax to transform logits to probabilities\n",
        "        # TODO: What shape should obs and act take?\n",
        "        # Args:   obs_matrix: np.array(m, n+1)\n",
        "        #         act_matrix: np.array(k, n+1)\n",
        "        # Return: probs: tensor(k)\n",
        "\n",
        "        logits = self.compute_one_step_logits(obs, act)\n",
        "\n",
        "        # TODO: softmax parameter lamda?\n",
        "        probs = torch.nn.functional.softmax(logits/self.temperature, dim=0)\n",
        "        return probs\n",
        "\n",
        "    def selected_prob(self, probs, action):\n",
        "\n",
        "\n",
        "        # TODO: do I need this function?\n",
        "        one_hot = torch.zeros()\n",
        "        return probs.max()\n",
        "\n",
        "    def choose_action(self, obs, act):\n",
        "\n",
        "        # TODO: is this returning a number?\n",
        "        return torch.argmax(self.predict_prob(obs, act)).item()\n",
        "\n",
        "\n",
        "    def train(self, obs_matrix, act_matrix, actions, Qs):\n",
        "        \"\"\"\n",
        "        Args: obs_matrix: np.array(batchsize * m, n+1) => changed to [np.array(m, n+1)] * batchsize, note that m are varied!\n",
        "              act_matrix: np.array(batchsize * k, n+1)  \n",
        "              actions: => [[action number]] * batchsize      \n",
        "              Qs: np.array(batchsize, )\n",
        "        \"\"\"\n",
        "        # Convert numpy array to tensor\n",
        "\n",
        "        # use compute_batch_prob to compute the batch logits for every datapoint in batch.\n",
        "        # use compute_batch_selected_prob the compute the max probabilty for every datapoint in batch.\n",
        "        start = time.time()\n",
        "        print(\"DEBUGGING: I'm inside the training now!\")\n",
        "        \n",
        "        Qs = torch.FloatTensor(Qs)\n",
        "\n",
        "\n",
        "        # Try using a for loop first, see whether it really cost too much time & whether it works at all\n",
        "        prob_selected = torch.zeros(len(actions))\n",
        "        for i in range(len(obs_matrix)):\n",
        "            probs = self.predict_prob(obs_matrix[i], act_matrix[i])\n",
        "            one_hot = torch.zeros_like(probs)\n",
        "            one_hot[actions[i][0]] = 1\n",
        "            prob_selected[i] = torch.sum(probs * one_hot)\n",
        "\n",
        "\n",
        "        # For robustness add in noise for prob_selected # TODO: why do this?\n",
        "\n",
        "        # define loss function as in lab 4\n",
        "        # TODO define loss function as described in the text above\n",
        "        loss = - (torch.sum(Qs * torch.log(prob_selected)) / (len(obs_matrix) + 1))\n",
        "        print(\"DEBUGGING: the loss =\", loss)\n",
        "\n",
        "        print(\"DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\")\n",
        "        print(\"   First layer:\")\n",
        "        print(policy.model[0].weight.grad)\n",
        "        print(\"   Last layer:\")\n",
        "        print(policy.model[6].weight.grad)\n",
        "\n",
        "        # backward pass\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "\n",
        "        print(\"DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\")\n",
        "        print(\"   First layer:\")\n",
        "        print(policy.model[0].weight.grad)\n",
        "        print(\"   Last layer:\")\n",
        "        print(policy.model[6].weight.grad)\n",
        "\n",
        "        # step\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "        print(\"DEBUGGING: training for one iteration takes %f min:\" % ((time.time() - start)/60))\n",
        "\n",
        "\n",
        "        # return detached loss (why?)\n",
        "        return loss.detach().cpu().data.numpy()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HViRnY1ssGfc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchsummary import summary\n",
        "# summary(policy.model, (61,))\n",
        "# print(policy.model)"
      ],
      "metadata": {
        "id": "OGm4an0pWPHr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(policy.model[6].weight.grad)"
      ],
      "metadata": {
        "id": "dfkXKSJWW6OR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP model for policy model:\n",
        "#   model.forward\n",
        "#   model.train --> What is in this function? what are the function arguments?\n",
        "# Baseline function b(s) ==> Okay maybe we stil need the V model as a proper baseline\n",
        "\n",
        "\n",
        "# Q value model ==> Discard this right now, just do vanilla policy gradient\n",
        "#   Can I just use the one in Lab4? What does it mean? what does the states and actions mean? --> Print out the s, r to check\n",
        "#   What is a Q-value in our set-up?\n",
        "#   How do I used this? \n",
        "#   (What's the baseline function??)"
      ],
      "metadata": {
        "id": "ACiQz-gESgOO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2xI0riE6md5V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "46daf959-7a56-4d36-a61f-5aa200166c84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mclaire-zixi-wang\u001b[0m (\u001b[33mieor4575-spring2022\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/IEOR_RL/Project_learn2cut/wandb/run-20220508_033729-vg5kvpbc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/ieor4575-spring2022/finalproject/runs/vg5kvpbc\" target=\"_blank\">toasty-sea-2318</a></strong> to <a href=\"https://wandb.ai/ieor4575-spring2022/finalproject\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# import gymenv_v2\n",
        "from gymenv_v2 import make_multiple_env\n",
        "\n",
        "\n",
        "import wandb\n",
        "wandb.login()\n",
        "run=wandb.init(project=\"finalproject\", entity=\"ieor4575-spring2022\", tags=[\"training-easy\"])\n",
        "#run=wandb.init(project=\"finalproject\", entity=\"ieor-4575\", tags=[\"training-hard\"])\n",
        "#run=wandb.init(project=\"finalproject\", entity=\"ieor-4575\", tags=[\"test\"])\n",
        "\n",
        "### TRAINING\n",
        "\n",
        "# Setup: You may generate your own instances on which you train the cutting agent.\n",
        "custom_config = {\n",
        "    \"load_dir\"        : 'instances/randomip_n60_m60',   # this is the location of the randomly generated instances (you may specify a different directory)\n",
        "    \"idx_list\"        : list(range(20)),                # take the first 20 instances from the directory\n",
        "    \"timelimit\"       : 50,                             # the maximum horizon length is 50\n",
        "    \"reward_type\"     : 'obj'                           # DO NOT CHANGE reward_type\n",
        "}\n",
        "\n",
        "# Easy Setup: Use the following environment settings. We will evaluate your agent with the same easy config below:\n",
        "easy_config = {\n",
        "    \"load_dir\"        : 'instances/train_10_n60_m60',\n",
        "    \"idx_list\"        : list(range(10)),\n",
        "    \"timelimit\"       : 50,\n",
        "    \"reward_type\"     : 'obj'\n",
        "}\n",
        "\n",
        "# Hard Setup: Use the following environment settings. We will evaluate your agent with the same hard config below:\n",
        "hard_config = {\n",
        "    \"load_dir\"        : 'instances/train_100_n60_m60',\n",
        "    \"idx_list\"        : list(range(99)),\n",
        "    \"timelimit\"       : 50,\n",
        "    \"reward_type\"     : 'obj'\n",
        "}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._C import dtype\n",
        "def discounted_rewards(r, gamma):\n",
        "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "    discounted_r = np.zeros_like(r, dtype=float)\n",
        "    running_sum = 0\n",
        "    for i in reversed(range(0,len(r))):\n",
        "        discounted_r[i] = running_sum * gamma + r[i]\n",
        "        running_sum = discounted_r[i]\n",
        "    return list(discounted_r)"
      ],
      "metadata": {
        "id": "COyIO0TEDRjt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = make_multiple_env(**easy_config) \n",
        "s = env.reset()   # samples a RANDOM INSTANCE every time env.reset() is called\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFhvgi_q53V4",
        "outputId": "7638d5b7-aa9b-45c9-ed42-064143882da0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading training instances, dir instances/train_10_n60_m60 idx 0\n",
            "Restricted license - for non-production use only - expires 2023-10-25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "A, b, c0, cuts_a, cuts_b = s\n",
        "concat = np.hstack((s[0], np.expand_dims(s[1], axis=1)))\n",
        "concat\n",
        "# np.linalg.norm(concat, axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJXouQZx9DQ7",
        "outputId": "a568797f-57f3-4af1-f6d0-61b544a474e9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  2,   0,   1, ...,   0,   0, 586],\n",
              "       [  2,   3,   0, ...,   1,   1, 580],\n",
              "       [  0,   4,   4, ...,   4,   4, 564],\n",
              "       ...,\n",
              "       [  4,   2,   3, ...,   1,   1, 598],\n",
              "       [  4,   4,   4, ...,   2,   0, 592],\n",
              "       [  1,   2,   2, ...,   2,   3, 580]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.expand_dims(s[1], axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSUhT5go6czx",
        "outputId": "b36f38a4-88ca-41d7-8df7-2751f6bd2579"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[586],\n",
              "       [580],\n",
              "       [564],\n",
              "       [575],\n",
              "       [562],\n",
              "       [563],\n",
              "       [568],\n",
              "       [594],\n",
              "       [585],\n",
              "       [585],\n",
              "       [543],\n",
              "       [552],\n",
              "       [569],\n",
              "       [598],\n",
              "       [548],\n",
              "       [547],\n",
              "       [544],\n",
              "       [557],\n",
              "       [542],\n",
              "       [547],\n",
              "       [582],\n",
              "       [542],\n",
              "       [562],\n",
              "       [578],\n",
              "       [548],\n",
              "       [557],\n",
              "       [588],\n",
              "       [540],\n",
              "       [573],\n",
              "       [580],\n",
              "       [589],\n",
              "       [557],\n",
              "       [568],\n",
              "       [571],\n",
              "       [597],\n",
              "       [566],\n",
              "       [568],\n",
              "       [581],\n",
              "       [562],\n",
              "       [556],\n",
              "       [543],\n",
              "       [554],\n",
              "       [592],\n",
              "       [589],\n",
              "       [597],\n",
              "       [561],\n",
              "       [587],\n",
              "       [576],\n",
              "       [587],\n",
              "       [587],\n",
              "       [590],\n",
              "       [544],\n",
              "       [589],\n",
              "       [586],\n",
              "       [588],\n",
              "       [573],\n",
              "       [581],\n",
              "       [598],\n",
              "       [592],\n",
              "       [580]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.hstack((s[0], np.expand_dims(s[1], axis=1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjIckPCE7n2z",
        "outputId": "e3341ca6-a926-43a3-ee82-11328f1f455d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  2,   0,   1, ...,   0,   0, 586],\n",
              "       [  2,   3,   0, ...,   1,   1, 580],\n",
              "       [  0,   4,   4, ...,   4,   4, 564],\n",
              "       ...,\n",
              "       [  4,   2,   3, ...,   1,   1, 598],\n",
              "       [  4,   4,   4, ...,   2,   0, 592],\n",
              "       [  1,   2,   2, ...,   2,   3, 580]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([\n",
        "              [1,2,3,4],\n",
        "              [3,3,3,3],\n",
        "              [2,2,2,2],\n",
        "              [1,1,1,1]\n",
        "])\n",
        "print(a.mean(axis=0, keepdims=True))\n",
        "a = a - a.mean(axis=0, keepdims=True)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMUMT8x_A8I0",
        "outputId": "bc8c646f-2560-4a69-dadc-3b9845756de9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.75 2.   2.25 2.5 ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.75,  0.  ,  0.75,  1.5 ],\n",
              "       [ 1.25,  1.  ,  0.75,  0.5 ],\n",
              "       [ 0.25,  0.  , -0.25, -0.5 ],\n",
              "       [-0.75, -1.  , -1.25, -1.5 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "empty = [np.array([1,2]), np.array([3,4])]\n",
        "a = []\n",
        "b = a + empty + empty\n",
        "\n",
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBFhGXfSJz3l",
        "outputId": "ab55b7aa-6608-46ef-cbd8-c98691a56436"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([1, 2]), array([3, 4]), array([1, 2]), array([3, 4])]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "empty = [np.array([10,20])]\n",
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJY9NttjZiM7",
        "outputId": "ea7849d2-ff16-4e68-aea5-ccf33a4caaa9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([1, 2]), array([3, 4]), array([1, 2]), array([3, 4])]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%wandb\n",
        "# create env\n",
        "env = make_multiple_env(**easy_config) \n",
        "# Parameter initialization\n",
        "numtrajs = 3  # num of trajecories from the current policy to collect in each iteration\n",
        "lr_pg = 1e-2  # learning rate for PG\n",
        "attention_size = 10\n",
        "iterations = 10\n",
        "discount_gamma = .99\n",
        "\n",
        "# Network initialize\n",
        "policy = Policy(lr_pg, 61, attention_size, 1)\n",
        "\n",
        "#To record training reward for logging and plotting purposes\n",
        "rrecord = []\n",
        "\n",
        "# For every training iteration, we roll out 5 random instances using the policy network\n",
        "# collect observation and action matrices from these 5 roll-outs, consider this a BATCH\n",
        "# train the network using this BATCH\n",
        "for ite in range(iterations):\n",
        "\n",
        "    print(\"==========================================================================================================\")\n",
        "    print(\"Outer iteration no\", ite)\n",
        "\n",
        "    # To record traectories generated from current policy\n",
        "    OBS_MAT = []  # observations: [obs_matrices_batch_traj1, obs_matrices_batch_traj2, ....]\n",
        "    ACT_MAT = []  # actions\n",
        "    ADS = []  # advantages (to compute policy gradient)\n",
        "    VAL = []  # Monte carlo value predictions (to compute baseline, and policy gradient) => 2 d list (numtrajs, #steps per traj)\n",
        "    traj_returns = []  # a list of 5 numbers, each number represents the RETURN of that roll-out\n",
        "    actions = []\n",
        "    \n",
        "    # collect some trajectories\n",
        "    for num in range(numtrajs):\n",
        "        print(\"-------------------------------------------------------------------------------------------\")\n",
        "        print(\"Running trajectories:\", num)\n",
        "        # Initialize a list of obs_matrices and act_matrices, to store all the obs_matrix and act_matrix in the trajectory\n",
        "        obs_matrices = []  # states: [obs_matrix_state1, obs_matrix_state2, ...]\n",
        "        act_matrices = []  # actions matrices\n",
        "        \n",
        "        # this is used to collect all the immedaite rewards in this trajectory\n",
        "        rews = []  # instant rewards\n",
        "        rel_rews = []\n",
        "\n",
        "        # gym loop\n",
        "        s = env.reset()   # samples a RANDOM INSTANCE every time env.reset() is called\n",
        "        done = False\n",
        "\n",
        "        # TODO: wandb logging -> what reward average should we log??\n",
        "        t = 0 # TODO: how is this used?\n",
        "        repisode = 0  # TODO: how is this used?\n",
        "\n",
        "        inner_step = 1\n",
        "        \n",
        "\n",
        "        # roll out ONE policy\n",
        "        while not done:\n",
        "\n",
        "            # TODO compute the running average RETURN as basline\n",
        "            A, b, c0, cuts_a, cuts_b = s\n",
        "\n",
        "            # choose the action according to the model output probabilities\n",
        "\n",
        "            # Concat [A,b] and [cuts_a, cuts_b]\n",
        "            assert A.shape[0] == b.shape[0]\n",
        "            assert cuts_a.shape[0] == cuts_b.shape[0]\n",
        "\n",
        "            obs_matrix = np.hstack((A, np.expand_dims(b, axis=1)))\n",
        "            act_matrix = np.hstack((cuts_a, np.expand_dims(cuts_b, axis=1)))\n",
        "\n",
        "            assert obs_matrix.shape == (A.shape[0], A.shape[1]+1)\n",
        "            assert act_matrix.shape == (cuts_a.shape[0], cuts_a.shape[1]+1)\n",
        "\n",
        "            # Normalize on a row (MIGHT NOT NEED THIS)\n",
        "            \n",
        "            # The reason we want to normalize a row: we want the numeric space of the model input to be just between 0, 1\n",
        "            # Right now I'm normalizing such that the largest number has value 1 --> each row divided by the largest num in that row\n",
        "            #   => This would result in b vector always be 1 (does it make sense?)\n",
        "            # another option is to normalize such that the SUM of the row is 1\n",
        "            #   => I think this makes more sense, consider this differentiates the max among datapoints\n",
        "            #   => According to prof in OH this might cause some information loss\n",
        "            \n",
        "            # obs_matrix = obs_matrix / obs_matrix.max(axis=1, keepdims=True)\n",
        "            # act_matrix = act_matrix / act_matrix.max(axis=1, keepdims=True)\n",
        "            \n",
        "            # print(\"DEBUGGING: obs_matrix.shape = \", obs_matrix.shape)\n",
        "            # print(\"DEBUGGING: act_matrix.shape = \", act_matrix.shape)\n",
        "\n",
        "            action_prob = policy.predict_prob(obs_matrix, act_matrix)\n",
        "            action = random.choices(range(0, len(cuts_b)), action_prob) # this returns a list\n",
        "            actions.append(action)\n",
        "\n",
        "            if  inner_step %10 == 0:\n",
        "                print(\"DEBUGGING: the action_prob is:\", action_prob)\n",
        "                print(\"DEBUGGING: the actual action to take is:\", action)\n",
        "\n",
        "            # take the action in the environment\n",
        "            # TODO: why does the environment.step function takes in a list?\n",
        "            # TODO: remember to go in the environment to change the returned r to the NORMALIZED r!!\n",
        "            s, r, done, _ = env.step(action)\n",
        "            \n",
        "            # Record the observed immediate reward & observed matrices along the trajectory\n",
        "            abs_reward, rel_reward = r\n",
        "            print(\"DEBUGGING: rel_reward looks like:\", rel_reward)\n",
        "            rews.append(abs_reward) # rews = list(len of trajectory)\n",
        "            rel_rews.append(rel_reward * 100 * inner_step)\n",
        "            obs_matrices.append(obs_matrix)\n",
        "            act_matrices.append(act_matrix)\n",
        "\n",
        "            inner_step += 1\n",
        "\n",
        "            # TODO: do we need to also record the one step of observation and action where the environment terminates?\n",
        "            #   ==> RN I'm thinking maybe don't need to\n",
        "        print(\"-------------------------------------------------------------------------------------------\")\n",
        "        #Below is for logging training performance\n",
        "        print(\"DEBUGGING: the total abs reward of the trajectory =\", np.sum(rews), \"and immediate abs rewards look like:\", rews)\n",
        "        print(\"DEBUGGING: the total relative reward of the trajectory =\", np.sum(rel_rews), \"and immediate relative rewards look like:\", rel_rews)\n",
        "\n",
        "        rrecord.append(np.sum(rews))\n",
        "\n",
        "        # After the policy roll out for this trajectory,\n",
        "        # compute the monte-carlo RETURN of this trajectory (i.e. discounted sum of rewards), add to big list\n",
        "        # TODO: one of the next steps could be: to make the basline state-dependent\n",
        "        v_hat = discounted_rewards(rel_rews, discount_gamma) # This is a list\n",
        "        traj_returns.append(v_hat[0])\n",
        "        # OBS_MAT.append(np.concatenate(obs_matrices, axis=1))\n",
        "        # ACT_MAT.append(np.concatenate(act_matrices, aixs=1))\n",
        "        VAL.append(v_hat) # VAL -> 2d list\n",
        "        OBS_MAT += obs_matrices\n",
        "        ACT_MAT += act_matrices\n",
        "\n",
        "        # TODO: do I need to specify batchsize somewhere?\n",
        "\n",
        "    print(\"+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\")\n",
        "    \n",
        "    # After collecting 5 (or however many) trajectories,\n",
        "    print(\"DEBUGGING: OBS_MAT has %d number of matrices\" % len(OBS_MAT))\n",
        "    print(\"DEBUGGING: ACT_MAT has %d number of matrices\" % len(ACT_MAT))\n",
        "    print(\"DEBUGGING: VAL looks like:\", VAL)\n",
        "    # print(\"DEBUGGING: OBS_MAT looks like:\", OBS_MAT)\n",
        "    # print(\"DEBUGGING: ACT_MAT looks like:\", ACT_MAT)\n",
        "    print(\"DEBUGGING: traj_returns =\", traj_returns)\n",
        "    print(\"DEBUGGING: actions =\", actions)\n",
        "    print(\"DEBUGGING: actions length =\", len(actions))\n",
        "\n",
        "    ## For debugging purposes, let's look at what does the model output\n",
        "    print(\"DEBUGGING: what does the model output in this round of roll-out?\")\n",
        "    obs_attention_dbg = policy.model(torch.FloatTensor(obs_matrix))\n",
        "    act_attention_dbg = policy.model(torch.FloatTensor(act_matrix))\n",
        "    logits_dbg = policy.compute_one_step_logits(obs_matrix, act_matrix)\n",
        "    print(\"DEBUGGING: obs_attention looks like:\", obs_attention_dbg)\n",
        "    print(\"DEBUGGING: act_attention looks like:\", act_attention_dbg)\n",
        "    print(\"DEBUGGING: logits looks like:\", logits_dbg)\n",
        "\n",
        "\n",
        "    assert len(traj_returns) == numtrajs\n",
        "    VAL = np.array(VAL)\n",
        "    # 1. calculate the baseline: average return of the trajectories\n",
        "    # TODO: potentially can make this into *running average*, take into account of all the previous trajectories as well\n",
        "    # TODO: potentially make the baseline the average *VALUE* of every *state*, \n",
        "    #       but I'm not sure whether that \"state\" is useful in this concept, since the first *step*\n",
        "    #       doesn't really mean the same thing among instances\n",
        "    #       I think prof says it makes sense in the OH, also it would make more sense if you engineered the reward\n",
        "    baseline = np.mean(traj_returns)\n",
        "    baseline_2 = VAL.mean(axis=0, keepdims=True)\n",
        "\n",
        "    # assert baseline_2.shape == VAL.shape\n",
        "\n",
        "    # 2. Update the policy\n",
        "    ADS = (VAL - baseline_2).flatten()\n",
        "    print(\"DEBUGGING: baseline2 looks like:\", baseline_2)\n",
        "    print(\"DEBUGGING: baseline2 looks like:\", baseline)\n",
        "    print(\"DEBUGGING: ADS looks like:\", ADS)\n",
        "\n",
        "\n",
        "    # Train the agent using the batch\n",
        "    # obs_batch = np.concatenate(OBS_MAT)\n",
        "    # act_batch = np.concatenate(ACT_MAT)\n",
        "\n",
        "    assert ADS.shape[0] == len(actions)\n",
        "\n",
        "    # scaling up the rewards to artificially make bigger loss, improve learning\n",
        "\n",
        "    policy.train(OBS_MAT, ACT_MAT, actions, ADS)\n",
        "\n",
        "\n",
        "    # TODO: wandb logging\n",
        "    wandb.log({\"Training reward (easy config)\" : rrecord[-1]})\n",
        "    #make sure to use the correct tag in wandb.init in the initialization on top\n"
      ],
      "metadata": {
        "id": "aeQHQnp1-8fR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bc942caf-475e-40bc-9f42-d478f7a24613"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<wandb.jupyter.IFrame at 0x7f95ba685a90>"
            ],
            "text/html": [
              "<iframe src=\"https://wandb.ai/ieor4575-spring2022/finalproject/runs/vg5kvpbc?jupyter=true\" style=\"border:none;width:100%;height:420px;\"></iframe>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "DEBUGGING: rel_reward looks like: 0.00022276548588722958\n",
            "DEBUGGING: the action_prob is: tensor([0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [62]\n",
            "DEBUGGING: rel_reward looks like: 2.6155694680375225e-05\n",
            "DEBUGGING: rel_reward looks like: 2.964187797658456e-05\n",
            "DEBUGGING: rel_reward looks like: 8.160294824707702e-06\n",
            "DEBUGGING: rel_reward looks like: 5.985510973449706e-05\n",
            "DEBUGGING: rel_reward looks like: 9.369418143833525e-05\n",
            "DEBUGGING: rel_reward looks like: 9.822741478947113e-05\n",
            "DEBUGGING: rel_reward looks like: 3.0374124790894107e-07\n",
            "DEBUGGING: rel_reward looks like: 2.951881997969185e-06\n",
            "DEBUGGING: rel_reward looks like: 6.032563801108495e-08\n",
            "DEBUGGING: rel_reward looks like: 5.380522393538753e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [1]\n",
            "DEBUGGING: rel_reward looks like: 4.525760241231756e-05\n",
            "DEBUGGING: rel_reward looks like: 8.428859601244929e-06\n",
            "DEBUGGING: rel_reward looks like: 0.0001727312678623822\n",
            "DEBUGGING: rel_reward looks like: 2.361023783728881e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00011740441879659079\n",
            "DEBUGGING: rel_reward looks like: 1.6149262093063104e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00015840141067095426\n",
            "DEBUGGING: rel_reward looks like: 1.6394681520466078e-05\n",
            "DEBUGGING: rel_reward looks like: 2.907997359758964e-05\n",
            "DEBUGGING: rel_reward looks like: 2.9185067402519133e-05\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.1440241994077951 and immediate abs rewards look like: [0.05024906289827413, 0.020345569882920245, 0.023425986910751817, 0.0016181599230549182, 0.02551371383106016, 0.0052361650664352055, 0.00023726044355498743, 0.00013414146678769612, 0.00029086511176501517, 0.00505899808013055, 0.00017790388483263087, 0.0011784564972003864, 5.1558270570239983e-05, 0.0018020418365267687, 0.0005897946703043999, 0.001640385879909445, 0.0010054722674794903, 0.00015979960562617634, 2.8906390525662573e-05, 0.0003616759254327917, 0.00037686419682358974, 0.00057707999349077, 3.6345204534882214e-06, 9.829009832174052e-05, 2.9084628749842523e-05, 8.798458566161571e-05, 0.00011498520188979455, 0.0001736106582939101, 0.0001296275868298835, 0.0006117238813203585, 7.180869488365715e-05, 8.137764734783559e-05, 2.240228877781192e-05, 0.00016431765516244923, 0.00025719920586197986, 0.0002696180872590048, 8.336378414242063e-07, 8.101631919998908e-06, 1.6556714399484918e-07, 0.00014767148786631878, 0.00012420537268553744, 2.313119148311671e-05, 0.0004740198014587804, 6.478147952293511e-05, 0.00032212519136010087, 4.4303897993813735e-05, 0.0004345515335444361, 4.496933115660795e-05, 7.976278902788181e-05, 8.004872051969869e-05]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.050918906647181086 and immediate relative rewards look like: [0.01740684639595215, 0.007172792299228705, 0.008318454057252346, 0.0005794206050313131, 0.009141087932054561, 0.0018933273808353916, 8.595294582972026e-05, 4.8599948502475184e-05, 0.00010538662529323723, 0.0018331759511240682, 6.458355353336784e-05, 0.00042783677290981185, 1.872616056442379e-05, 0.0006545207264242808, 0.0002143599888720507, 0.0005963236171835783, 0.00036573383617552794, 5.8147307976186495e-05, 1.0518965470169566e-05, 0.0001316143773471081, 0.00013715946279998739, 0.00021005667425992052, 1.3232406081880373e-06, 3.578508451097444e-05, 1.0589399765156564e-05, 3.2034578300664434e-05, 4.186665685681918e-05, 6.321510993119632e-05, 4.720297864281291e-05, 0.00022276548588722958, 2.6155694680375225e-05, 2.964187797658456e-05, 8.160294824707702e-06, 5.985510973449706e-05, 9.369418143833525e-05, 9.822741478947113e-05, 3.0374124790894107e-07, 2.951881997969185e-06, 6.032563801108495e-08, 5.380522393538753e-05, 4.525760241231756e-05, 8.428859601244929e-06, 0.0001727312678623822, 2.361023783728881e-05, 0.00011740441879659079, 1.6149262093063104e-05, 0.00015840141067095426, 1.6394681520466078e-05, 2.907997359758964e-05, 2.9185067402519133e-05]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 150 number of matrices\n",
            "DEBUGGING: ACT_MAT has 150 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.28041690749267056, 0.28285268478030184, 0.2839848366541597, 0.28628351329460877, 0.28912361135185294, 0.290777699412795, 0.2893492382146114, 0.28984335610941264, 0.27900093863393455, 0.2762251810340103, 0.2789247251161776, 0.28161291883903733, 0.2820074306417961, 0.2825682975241213, 5.805777272033118e-12, 5.86442148690214e-12, 5.923658067577919e-12, 5.7538226183178655e-12, 5.352601280233634e-12, 5.1769975805964185e-12, 5.229290485450928e-12, 5.052441222229996e-12, 4.8738056028148606e-12, 4.693365583203665e-12, 4.511102937131698e-12, 4.326999254230774e-12, 4.370706317404823e-12, 4.185184486829837e-12, 3.9977886983703085e-12, 3.3491592646876437e-12, 2.9236483977787606e-12, 2.9531801997765258e-12, 2.7533399235689836e-12, 2.551481058712828e-12, 2.3475832154238345e-12, 2.3712961771957926e-12, 2.165578284598598e-12, 1.9577824334902686e-12, 1.7478876343908938e-12, 1.535872685805719e-12, 1.3217161720833724e-12, 1.335066840488255e-12, 1.1188819848939627e-12, 6.708430646539345e-13, 4.479488779906608e-13, 4.524736141319806e-13, 2.2737367544318036e-13, 0.0, 0.0, 0.0], [0.28817345964937446, 0.28995035196558694, 0.2849460263306006, 0.2877327705557624, 0.28835235636420015, 0.2770641296804044, 0.2795286422741372, 0.281204905006178, 0.28155439618567424, 0.2841245176972044, 0.28450886820819954, 6.419816729792706e-12, 6.254992984191388e-12, 5.8588339730351725e-12, 5.688343734941406e-12, 5.745801752466067e-12, 5.57416977477054e-12, 5.400804140734602e-12, 5.225687338678152e-12, 5.048801680035325e-12, 4.87012929756777e-12, 4.689652143560192e-12, 4.737022367232518e-12, 4.7848710780126445e-12, 4.603532729868093e-12, 4.4203626812372854e-12, 4.465012809330591e-12, 4.280443569583191e-12, 3.864339614845075e-12, 3.903373348328358e-12, 3.7131309827123005e-12, 3.5209669770394634e-12, 3.556532300039862e-12, 3.3627864894915977e-12, 3.167083650553905e-12, 2.969404015263358e-12, 2.5400572367445462e-12, 2.5657143805500468e-12, 2.59163068742429e-12, 2.3881383959404624e-12, 2.412261006000467e-12, 2.2069568995527105e-12, 1.9995790142519985e-12, 1.3307656443661188e-12, 8.848669631105533e-13, 6.641346340073952e-13, 6.708430646539345e-13, 4.479488779906608e-13, 4.524736141319806e-13, 2.2737367544318036e-13], [0.049199672977971225, 0.032113966244463714, 0.025193104995186878, 0.017045101957509627, 0.01663200136613971, 0.007566579226348636, 0.005730557419710349, 0.005701620680687504, 0.005710121951702049, 0.0056613488145543555, 0.0038668412761922095, 0.0038406643663220622, 0.0034473005994063136, 0.0034632065038806966, 0.0028370563408650663, 0.0026491882343363795, 0.002073600623386668, 0.0017251179668799396, 0.0016838087463674272, 0.001690191697876018, 0.0015743205257867775, 0.0014516778414007982, 0.0012541627950917956, 0.0012654944994783916, 0.0012421307221893105, 0.0012439811337617715, 0.001224188439859704, 0.0011942644272756413, 0.001142474057923682, 0.0011063344235160294, 0.0008924938763927271, 0.0008750890724367191, 0.0008539870651112471, 0.0008543704750369085, 0.0008025407730327389, 0.0007160066581761653, 0.0006240194377643375, 0.0006300158550670995, 0.0006333979525950811, 0.0006397349767243132, 0.0005918482351403289, 0.0005521117502303145, 0.0005491746369990602, 0.00038024582741078586, 0.00036023796926615867, 0.00024528641461572516, 0.00023145166921481017, 7.378813994328882e-05, 5.797319032608358e-05, 2.9185067402519133e-05]]\n",
            "DEBUGGING: traj_returns = [0.28041690749267056, 0.28817345964937446, 0.049199672977971225]\n",
            "DEBUGGING: actions = [[35], [34], [21], [33], [16], [15], [35], [3], [29], [66], [40], [16], [67], [0], [54], [60], [44], [25], [72], [60], [51], [22], [51], [75], [58], [22], [1], [41], [11], [54], [28], [78], [67], [1], [9], [50], [50], [54], [5], [56], [41], [43], [26], [35], [48], [6], [61], [34], [67], [19], [21], [33], [30], [12], [42], [21], [7], [53], [21], [46], [0], [47], [1], [65], [16], [7], [32], [45], [16], [22], [26], [2], [59], [18], [79], [61], [59], [25], [65], [10], [81], [53], [32], [71], [17], [74], [17], [16], [92], [18], [78], [4], [18], [14], [44], [24], [90], [77], [92], [43], [19], [51], [29], [58], [62], [49], [7], [56], [15], [49], [41], [63], [67], [10], [10], [35], [67], [29], [66], [61], [36], [71], [59], [17], [81], [62], [45], [13], [36], [70], [62], [49], [86], [7], [69], [26], [69], [55], [36], [2], [1], [72], [42], [39], [61], [6], [90], [71], [20], [66]]\n",
            "DEBUGGING: actions length = 150\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        ...,\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        ...,\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[2.05930013e-01 2.01639001e-01 1.98041323e-01 1.97020462e-01\n",
            "  1.98035990e-01 1.91802803e-01 1.91536146e-01 1.92249961e-01\n",
            "  1.88755152e-01 1.88670349e-01 1.89100145e-01 9.51511944e-02\n",
            "  9.51515771e-02 9.53438347e-02 9.45685451e-04 8.83062749e-04\n",
            "  6.91200212e-04 5.75039326e-04 5.61269586e-04 5.63397236e-04\n",
            "  5.24773512e-04 4.83892617e-04 4.18054268e-04 4.21831503e-04\n",
            "  4.14043577e-04 4.14660381e-04 4.08062816e-04 3.98088145e-04\n",
            "  3.80824689e-04 3.68778144e-04 2.97497961e-04 2.91696360e-04\n",
            "  2.84662357e-04 2.84790160e-04 2.67513593e-04 2.38668888e-04\n",
            "  2.08006481e-04 2.10005287e-04 2.11132652e-04 2.13244994e-04\n",
            "  1.97282746e-04 1.84037251e-04 1.83058213e-04 1.26748610e-04\n",
            "  1.20079324e-04 8.17621386e-05 7.71505567e-05 2.45960468e-05\n",
            "  1.93243969e-05 9.72835588e-06]]\n",
            "DEBUGGING: baseline2 looks like: 0.20593001337333874\n",
            "DEBUGGING: ADS looks like: [2.80416907e-01 2.82852685e-01 2.83984837e-01 2.86283513e-01\n",
            " 2.89123611e-01 2.90777699e-01 2.89349238e-01 2.89843356e-01\n",
            " 2.79000939e-01 2.76225181e-01 2.78924725e-01 2.81612919e-01\n",
            " 2.82007431e-01 2.82568298e-01 5.80577727e-12 5.86442149e-12\n",
            " 5.92365807e-12 5.75382262e-12 5.35260128e-12 5.17699758e-12\n",
            " 5.22929049e-12 5.05244122e-12 4.87380560e-12 4.69336558e-12\n",
            " 4.51110294e-12 4.32699925e-12 4.37070632e-12 4.18518449e-12\n",
            " 3.99778870e-12 3.34915926e-12 2.92364840e-12 2.95318020e-12\n",
            " 2.75333992e-12 2.55148106e-12 2.34758322e-12 2.37129618e-12\n",
            " 2.16557828e-12 1.95778243e-12 1.74788763e-12 1.53587269e-12\n",
            " 1.32171617e-12 1.33506684e-12 1.11888198e-12 6.70843065e-13\n",
            " 4.47948878e-13 4.52473614e-13 2.27373675e-13 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 2.88173460e-01 2.89950352e-01\n",
            " 2.84946026e-01 2.87732771e-01 2.88352356e-01 2.77064130e-01\n",
            " 2.79528642e-01 2.81204905e-01 2.81554396e-01 2.84124518e-01\n",
            " 2.84508868e-01 6.41981673e-12 6.25499298e-12 5.85883397e-12\n",
            " 5.68834373e-12 5.74580175e-12 5.57416977e-12 5.40080414e-12\n",
            " 5.22568734e-12 5.04880168e-12 4.87012930e-12 4.68965214e-12\n",
            " 4.73702237e-12 4.78487108e-12 4.60353273e-12 4.42036268e-12\n",
            " 4.46501281e-12 4.28044357e-12 3.86433961e-12 3.90337335e-12\n",
            " 3.71313098e-12 3.52096698e-12 3.55653230e-12 3.36278649e-12\n",
            " 3.16708365e-12 2.96940402e-12 2.54005724e-12 2.56571438e-12\n",
            " 2.59163069e-12 2.38813840e-12 2.41226101e-12 2.20695690e-12\n",
            " 1.99957901e-12 1.33076564e-12 8.84866963e-13 6.64134634e-13\n",
            " 6.70843065e-13 4.47948878e-13 4.52473614e-13 2.27373675e-13\n",
            " 4.91996730e-02 3.21139662e-02 2.51931050e-02 1.70451020e-02\n",
            " 1.66320014e-02 7.56657923e-03 5.73055742e-03 5.70162068e-03\n",
            " 5.71012195e-03 5.66134881e-03 3.86684128e-03 3.84066437e-03\n",
            " 3.44730060e-03 3.46320650e-03 2.83705634e-03 2.64918823e-03\n",
            " 2.07360062e-03 1.72511797e-03 1.68380875e-03 1.69019170e-03\n",
            " 1.57432053e-03 1.45167784e-03 1.25416280e-03 1.26549450e-03\n",
            " 1.24213072e-03 1.24398113e-03 1.22418844e-03 1.19426443e-03\n",
            " 1.14247406e-03 1.10633442e-03 8.92493876e-04 8.75089072e-04\n",
            " 8.53987065e-04 8.54370475e-04 8.02540773e-04 7.16006658e-04\n",
            " 6.24019438e-04 6.30015855e-04 6.33397953e-04 6.39734977e-04\n",
            " 5.91848235e-04 5.52111750e-04 5.49174637e-04 3.80245827e-04\n",
            " 3.60237969e-04 2.45286415e-04 2.31451669e-04 7.37881399e-05\n",
            " 5.79731903e-05 2.91850674e-05]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(0.2026, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "None\n",
            "   Last layer:\n",
            "None\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 1.0943e-35,  1.1814e-35,  1.0911e-35,  ...,  1.1947e-35,\n",
            "          1.0458e-35,  3.2880e-33],\n",
            "        [-6.4653e-39, -7.3476e-39, -6.5101e-39,  ..., -1.3740e-38,\n",
            "         -9.0297e-39, -2.4386e-36],\n",
            "        ...,\n",
            "        [ 3.4318e-30,  3.8906e-30,  3.2308e-30,  ...,  5.9988e-30,\n",
            "          3.6561e-30,  1.1009e-27],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]])\n",
            "   Last layer:\n",
            "tensor([[-1.3914e-13, -8.7586e-14, -1.2129e-13, -3.3568e-13, -7.7389e-14,\n",
            "         -1.0666e-13, -9.7653e-14, -3.1280e-13, -1.1571e-13, -1.2047e-13,\n",
            "         -6.3340e-14, -1.2901e-13, -1.0577e-13, -2.7134e-13, -2.4815e-13,\n",
            "         -1.1156e-13, -1.8527e-13, -2.2226e-13, -8.6446e-14, -7.6773e-14],\n",
            "        [-5.8286e-14, -2.3955e-14, -4.5019e-14, -5.4718e-14, -1.5413e-13,\n",
            "         -1.7683e-14, -6.0873e-14,  3.2133e-14, -1.0890e-13, -6.3693e-14,\n",
            "         -2.9887e-14, -1.0043e-13, -9.1490e-14, -1.2519e-13, -5.5695e-14,\n",
            "         -1.4744e-13, -1.8411e-14, -2.8154e-14, -1.0154e-13, -1.5177e-13],\n",
            "        [-1.8582e-14,  6.4004e-14,  5.1626e-14,  6.8446e-14,  7.3183e-14,\n",
            "         -3.0821e-15,  6.2790e-14,  1.8129e-13,  1.1423e-14,  1.0254e-13,\n",
            "          4.9103e-14,  9.9752e-14,  1.3853e-14,  7.0468e-14,  6.5362e-14,\n",
            "          5.0355e-14,  8.0968e-14,  8.4776e-14,  1.0837e-13,  5.2765e-14],\n",
            "        [-2.0850e-14, -7.7542e-14, -5.1449e-14, -9.8151e-14,  4.7457e-16,\n",
            "         -2.5290e-14, -1.2425e-13, -1.0883e-13, -1.3213e-13,  6.1780e-15,\n",
            "         -6.3633e-15, -1.2295e-13, -2.8674e-14, -1.2086e-14, -7.3446e-14,\n",
            "         -9.1290e-14, -9.5162e-14, -6.3901e-14, -8.9979e-14, -6.6612e-14],\n",
            "        [ 4.8104e-14, -1.6479e-14, -6.9732e-14, -1.4346e-14,  1.3620e-13,\n",
            "          6.6062e-16,  6.1240e-15,  4.4050e-14,  2.4117e-14,  9.0927e-14,\n",
            "          8.3957e-14, -5.0247e-14,  5.0066e-14,  5.5508e-14,  2.1743e-14,\n",
            "         -3.6950e-14,  4.3151e-14, -3.7492e-14,  9.9227e-14, -1.8337e-14],\n",
            "        [-7.0755e-14, -2.1806e-13, -1.2336e-13, -8.8908e-14, -1.6895e-13,\n",
            "         -6.4098e-14, -1.3193e-13, -1.7758e-13, -1.1042e-13, -1.7639e-13,\n",
            "         -1.3282e-13, -1.7110e-13, -1.2076e-13, -2.1420e-14, -1.7824e-13,\n",
            "         -1.0906e-13, -9.0430e-15, -1.5939e-13, -1.0938e-13, -2.2490e-13],\n",
            "        [-1.0656e-13,  1.3886e-13, -3.1321e-16,  3.7808e-14,  1.4739e-13,\n",
            "          6.8219e-14, -2.6500e-14, -1.7550e-13,  4.9697e-14,  4.3884e-14,\n",
            "          3.3086e-14, -1.1834e-13, -3.5923e-14, -8.6384e-15, -1.1420e-13,\n",
            "         -7.5543e-14,  5.9403e-14,  1.0861e-14,  1.7090e-15,  5.1600e-14],\n",
            "        [ 1.1215e-15, -4.0338e-14, -4.1773e-15, -5.4068e-14, -4.5636e-15,\n",
            "         -1.7060e-14, -1.1732e-14, -2.7008e-14,  2.7418e-14,  1.7827e-14,\n",
            "         -1.5924e-14,  1.7895e-14, -1.8160e-15, -6.3443e-15, -4.5401e-15,\n",
            "         -1.6063e-14,  1.6903e-14, -4.7248e-15, -1.1603e-14,  1.7612e-14],\n",
            "        [ 7.8723e-15,  2.9960e-15, -3.0643e-15, -2.8035e-14, -3.8472e-15,\n",
            "         -1.7571e-15, -5.8523e-15, -6.1233e-15,  1.2118e-14,  4.2564e-15,\n",
            "          3.3384e-16, -1.3316e-14, -2.0574e-14, -2.3154e-14, -2.5906e-14,\n",
            "         -5.9440e-15, -1.6329e-14, -2.2254e-14,  1.2372e-14, -2.2984e-14],\n",
            "        [-8.1562e-15, -3.8203e-15,  6.2401e-15, -3.8636e-15, -1.5354e-14,\n",
            "         -1.1342e-14,  1.7950e-16, -2.1998e-14, -1.1202e-14, -3.3113e-15,\n",
            "         -8.1234e-15,  1.9286e-14,  6.5033e-15, -5.3282e-15, -2.3216e-14,\n",
            "         -1.1792e-14, -2.3866e-14, -1.6163e-14, -5.8979e-15, -4.0728e-15]])\n",
            "DEBUGGING: training for one iteration takes 0.006023 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 1\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [5]\n",
            "DEBUGGING: rel_reward looks like: 0.0015480179722601275\n",
            "DEBUGGING: rel_reward looks like: 0.0014976203612558745\n",
            "DEBUGGING: rel_reward looks like: 0.007326264022295792\n",
            "DEBUGGING: rel_reward looks like: 0.0014596764929949585\n",
            "DEBUGGING: rel_reward looks like: 0.00638166694068384\n",
            "DEBUGGING: rel_reward looks like: 0.003916151465403845\n",
            "DEBUGGING: rel_reward looks like: 0.005727424518159038\n",
            "DEBUGGING: rel_reward looks like: 0.0006991525737693111\n",
            "DEBUGGING: rel_reward looks like: 0.00043361879375984315\n",
            "DEBUGGING: rel_reward looks like: 9.322585840343026e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [1]\n",
            "DEBUGGING: rel_reward looks like: 0.00039173975373385907\n",
            "DEBUGGING: rel_reward looks like: 0.00018640601857231092\n",
            "DEBUGGING: rel_reward looks like: 0.00030586125757830324\n",
            "DEBUGGING: rel_reward looks like: 0.0006325857651126359\n",
            "DEBUGGING: rel_reward looks like: 9.779173791037919e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0042944416229798955\n",
            "DEBUGGING: rel_reward looks like: 0.002012795910840708\n",
            "DEBUGGING: rel_reward looks like: 0.00031736659756068087\n",
            "DEBUGGING: rel_reward looks like: 0.0003855282826416374\n",
            "DEBUGGING: rel_reward looks like: 0.0004597401635061796\n",
            "DEBUGGING: the action_prob is: tensor([0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [3]\n",
            "DEBUGGING: rel_reward looks like: 0.001045316885945094\n",
            "DEBUGGING: rel_reward looks like: 0.00017097660199683632\n",
            "DEBUGGING: rel_reward looks like: 0.0007123136157834825\n",
            "DEBUGGING: rel_reward looks like: 6.123867173732599e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0005141075888559242\n",
            "DEBUGGING: rel_reward looks like: 7.900572513047626e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00024949413166141333\n",
            "DEBUGGING: rel_reward looks like: 0.0006302196178811894\n",
            "DEBUGGING: rel_reward looks like: 2.741085252207553e-05\n",
            "DEBUGGING: rel_reward looks like: 6.026133464785427e-06\n",
            "DEBUGGING: the action_prob is: tensor([0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [35]\n",
            "DEBUGGING: rel_reward looks like: 1.3185551781065903e-05\n",
            "DEBUGGING: rel_reward looks like: 3.0671932876699356e-05\n",
            "DEBUGGING: rel_reward looks like: 2.6393539665914786e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0008659946874243108\n",
            "DEBUGGING: rel_reward looks like: 7.828296427312635e-05\n",
            "DEBUGGING: rel_reward looks like: 5.3263057394152285e-05\n",
            "DEBUGGING: rel_reward looks like: 0.000290451674483586\n",
            "DEBUGGING: rel_reward looks like: 0.00021428860567725299\n",
            "DEBUGGING: rel_reward looks like: 7.286605414207538e-07\n",
            "DEBUGGING: rel_reward looks like: 0.0001878131929105284\n",
            "DEBUGGING: the action_prob is: tensor([0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [29]\n",
            "DEBUGGING: rel_reward looks like: 3.9154575412519476e-05\n",
            "DEBUGGING: rel_reward looks like: 1.814013334463624e-05\n",
            "DEBUGGING: rel_reward looks like: 3.055678854995819e-06\n",
            "DEBUGGING: rel_reward looks like: 1.6782128461848284e-06\n",
            "DEBUGGING: rel_reward looks like: 2.0346790020721e-05\n",
            "DEBUGGING: rel_reward looks like: 4.720936813542324e-06\n",
            "DEBUGGING: rel_reward looks like: 3.256975167722231e-05\n",
            "DEBUGGING: rel_reward looks like: 1.858571670008341e-05\n",
            "DEBUGGING: rel_reward looks like: 7.342442795130129e-05\n",
            "DEBUGGING: rel_reward looks like: 3.093458702871074e-05\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.12358776949713501 and immediate abs rewards look like: [0.0044687274585157866, 0.004316550134717545, 0.021084665876969666, 0.004170108004927897, 0.01820498943425264, 0.01110031679218082, 0.016170787310329615, 0.001962678712516208, 0.0012164145437054685, 0.0002614096038087155, 0.0010983539204971748, 0.0005224375995567243, 0.0008570733712076617, 0.001772066815647122, 0.00027377136848372174, 0.012021262857615511, 0.005610144701222453, 0.0008827963124531379, 0.001072056528300891, 0.0012779281091752637, 0.0029043050799373304, 0.00047454427385673625, 0.001976683062821394, 0.0001698173632576072, 0.0014255542500904994, 0.0002189600918427459, 0.0006914048553881003, 0.001746045837080601, 7.589487995574018e-05, 1.6684637557773385e-05, 3.6506796277535614e-05, 8.492016559102922e-05, 7.307250643862062e-05, 0.0023975083668119623, 0.00021653889007211546, 0.00014731966984982137, 0.0008033141129999422, 0.0005924946476625337, 2.0142692847002763e-06, 0.0005191801237742766, 0.00010821635623869952, 5.013417330701486e-05, 8.44487476570066e-06, 4.638005066226469e-06, 5.623145898425719e-05, 1.3046763797319727e-05, 9.00092222764215e-05, 5.1361503665248165e-05, 0.0002029041438618151, 8.547965853722417e-05]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.04366687061207923 and immediate relative rewards look like: [0.0015480179722601275, 0.0014976203612558745, 0.007326264022295792, 0.0014596764929949585, 0.00638166694068384, 0.003916151465403845, 0.005727424518159038, 0.0006991525737693111, 0.00043361879375984315, 9.322585840343026e-05, 0.00039173975373385907, 0.00018640601857231092, 0.00030586125757830324, 0.0006325857651126359, 9.779173791037919e-05, 0.0042944416229798955, 0.002012795910840708, 0.00031736659756068087, 0.0003855282826416374, 0.0004597401635061796, 0.001045316885945094, 0.00017097660199683632, 0.0007123136157834825, 6.123867173732599e-05, 0.0005141075888559242, 7.900572513047626e-05, 0.00024949413166141333, 0.0006302196178811894, 2.741085252207553e-05, 6.026133464785427e-06, 1.3185551781065903e-05, 3.0671932876699356e-05, 2.6393539665914786e-05, 0.0008659946874243108, 7.828296427312635e-05, 5.3263057394152285e-05, 0.000290451674483586, 0.00021428860567725299, 7.286605414207538e-07, 0.0001878131929105284, 3.9154575412519476e-05, 1.814013334463624e-05, 3.055678854995819e-06, 1.6782128461848284e-06, 2.0346790020721e-05, 4.720936813542324e-06, 3.256975167722231e-05, 1.858571670008341e-05, 7.342442795130129e-05, 3.093458702871074e-05]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [46]\n",
            "DEBUGGING: rel_reward looks like: 0.0036128292517656558\n",
            "DEBUGGING: rel_reward looks like: 0.003328991825259228\n",
            "DEBUGGING: rel_reward looks like: 0.004254024818987241\n",
            "DEBUGGING: rel_reward looks like: 0.006727492857282037\n",
            "DEBUGGING: rel_reward looks like: 0.001893830337739953\n",
            "DEBUGGING: rel_reward looks like: 0.011359867733954287\n",
            "DEBUGGING: rel_reward looks like: 0.0179292107428372\n",
            "DEBUGGING: rel_reward looks like: 8.630278330386745e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00011006430159079354\n",
            "DEBUGGING: rel_reward looks like: 0.0007279459029231035\n",
            "DEBUGGING: the action_prob is: tensor([0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [28]\n",
            "DEBUGGING: rel_reward looks like: 0.002280936545820694\n",
            "DEBUGGING: rel_reward looks like: 8.06971720372529e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0008521100166388253\n",
            "DEBUGGING: rel_reward looks like: 0.0001054181116114845\n",
            "DEBUGGING: rel_reward looks like: 7.197501610399763e-06\n",
            "DEBUGGING: rel_reward looks like: 0.00016105308097795278\n",
            "DEBUGGING: rel_reward looks like: 3.1279735887558273e-06\n",
            "DEBUGGING: rel_reward looks like: 8.208445526416457e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00016009864312355274\n",
            "DEBUGGING: rel_reward looks like: 0.0005440514361871206\n",
            "DEBUGGING: the action_prob is: tensor([0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [26]\n",
            "DEBUGGING: rel_reward looks like: 0.0001240170833696641\n",
            "DEBUGGING: rel_reward looks like: 7.300068024531334e-05\n",
            "DEBUGGING: rel_reward looks like: 9.662071270041714e-06\n",
            "DEBUGGING: rel_reward looks like: 0.00011575145829503328\n",
            "DEBUGGING: rel_reward looks like: 1.4156424246098206e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00015525144594388826\n",
            "DEBUGGING: rel_reward looks like: 0.00021325013713640727\n",
            "DEBUGGING: rel_reward looks like: 2.0131613721440873e-05\n",
            "DEBUGGING: rel_reward looks like: 0.26777811616563335\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [76]\n",
            "DEBUGGING: rel_reward looks like: 4.547473508863607e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508865675e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: the action_prob is: tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [73]\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.886741328979042 and immediate abs rewards look like: [0.010429303515593347, 0.00957521921964144, 0.012195169523238292, 0.019203908471808973, 0.005369648607484123, 0.03214806307596518, 0.050162704697413574, 0.00023713047357887262, 0.0003023928834409162, 0.0019997531389890355, 0.0062614400494567235, 0.00022101798185758526, 0.0023336188073699304, 0.0002884557852667058, 1.9692461592057953e-05, 0.000440640232682199, 8.556738066545222e-06, 0.00022454568943430786, 0.00043792103133455385, 0.0014879165596539679, 0.0003389875801076414, 0.00019951489184677484, 2.6405046810396016e-05, 0.00031632897935196524, 3.8682613194396254e-05, 0.0004242205877744709, 0.000582610056426347, 5.49888482055394e-05, 0.7314124914250897, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.322810672575548 and immediate relative rewards look like: [0.0036128292517656558, 0.003328991825259228, 0.004254024818987241, 0.006727492857282037, 0.001893830337739953, 0.011359867733954287, 0.0179292107428372, 8.630278330386745e-05, 0.00011006430159079354, 0.0007279459029231035, 0.002280936545820694, 8.06971720372529e-05, 0.0008521100166388253, 0.0001054181116114845, 7.197501610399763e-06, 0.00016105308097795278, 3.1279735887558273e-06, 8.208445526416457e-05, 0.00016009864312355274, 0.0005440514361871206, 0.0001240170833696641, 7.300068024531334e-05, 9.662071270041714e-06, 0.00011575145829503328, 1.4156424246098206e-05, 0.00015525144594388826, 0.00021325013713640727, 2.0131613721440873e-05, 0.26777811616563335, 2.2737367544323206e-13, 4.547473508863607e-13, 2.2737367544328376e-13, 0.0, 0.0, 2.2737367544323206e-13, 4.547473508865675e-13, 0.0, 2.2737367544318036e-13, 2.2737367544323206e-13, 0.0, 0.0, 2.2737367544328376e-13, 2.2737367544323206e-13, 0.0, 2.2737367544318036e-13, 0.0, 2.2737367544323206e-13, 0.0, 0.0, 2.2737367544318036e-13]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [45]\n",
            "DEBUGGING: rel_reward looks like: 0.00025667461589194387\n",
            "DEBUGGING: rel_reward looks like: 0.0020330826361081235\n",
            "DEBUGGING: rel_reward looks like: 0.007382169230949076\n",
            "DEBUGGING: rel_reward looks like: 0.0004415357046217487\n",
            "DEBUGGING: rel_reward looks like: 0.0072715213718608\n",
            "DEBUGGING: rel_reward looks like: 0.0013167379623985876\n",
            "DEBUGGING: rel_reward looks like: 0.004881996700248979\n",
            "DEBUGGING: rel_reward looks like: 0.011559812247667728\n",
            "DEBUGGING: rel_reward looks like: 0.0005555781089609606\n",
            "DEBUGGING: rel_reward looks like: 0.004047061645182092\n",
            "DEBUGGING: the action_prob is: tensor([0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [39]\n",
            "DEBUGGING: rel_reward looks like: 3.3605779122581976e-05\n",
            "DEBUGGING: rel_reward looks like: 0.007999206318567912\n",
            "DEBUGGING: rel_reward looks like: 0.00299490165504157\n",
            "DEBUGGING: rel_reward looks like: 0.00035358839955470253\n",
            "DEBUGGING: rel_reward looks like: 0.0036062082883738422\n",
            "DEBUGGING: rel_reward looks like: 0.0014187745559372067\n",
            "DEBUGGING: rel_reward looks like: 0.0019261472343249874\n",
            "DEBUGGING: rel_reward looks like: 5.0926665277240774e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0020556432627510867\n",
            "DEBUGGING: rel_reward looks like: 8.824260625219654e-06\n",
            "DEBUGGING: the action_prob is: tensor([0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [7]\n",
            "DEBUGGING: rel_reward looks like: 0.0002887982982378222\n",
            "DEBUGGING: rel_reward looks like: 2.7616691262932605e-05\n",
            "DEBUGGING: rel_reward looks like: 2.332574684571824e-06\n",
            "DEBUGGING: rel_reward looks like: 0.00013764262038391134\n",
            "DEBUGGING: rel_reward looks like: 0.00023796271657188932\n",
            "DEBUGGING: rel_reward looks like: 6.756937937832594e-06\n",
            "DEBUGGING: rel_reward looks like: 8.058996862380279e-05\n",
            "DEBUGGING: rel_reward looks like: 4.788114929519764e-05\n",
            "DEBUGGING: rel_reward looks like: 3.2501781741337936e-05\n",
            "DEBUGGING: rel_reward looks like: 4.398483660845217e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [77]\n",
            "DEBUGGING: rel_reward looks like: 3.333468294878446e-06\n",
            "DEBUGGING: rel_reward looks like: 7.811480362355674e-05\n",
            "DEBUGGING: rel_reward looks like: 3.977272742796055e-05\n",
            "DEBUGGING: rel_reward looks like: 1.0214451580497497e-05\n",
            "DEBUGGING: rel_reward looks like: 2.049519303359347e-05\n",
            "DEBUGGING: rel_reward looks like: 9.559027901163645e-05\n",
            "DEBUGGING: rel_reward looks like: 1.0256273505249244e-05\n",
            "DEBUGGING: rel_reward looks like: 3.8888876105305956e-05\n",
            "DEBUGGING: rel_reward looks like: 4.5870574908484274e-06\n",
            "DEBUGGING: rel_reward looks like: 3.4920536676943416e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [92]\n",
            "DEBUGGING: rel_reward looks like: 0.00021666606132474911\n",
            "DEBUGGING: rel_reward looks like: 0.0003927628880073862\n",
            "DEBUGGING: rel_reward looks like: 1.8277684675413334e-06\n",
            "DEBUGGING: rel_reward looks like: 7.743322232591156e-06\n",
            "DEBUGGING: rel_reward looks like: 9.405586962364231e-05\n",
            "DEBUGGING: rel_reward looks like: 1.7273617610647292e-05\n",
            "DEBUGGING: rel_reward looks like: 2.4420624189270115e-05\n",
            "DEBUGGING: rel_reward looks like: 3.7017509336600445e-05\n",
            "DEBUGGING: rel_reward looks like: 2.3621558578039196e-05\n",
            "DEBUGGING: rel_reward looks like: 2.0135055001369076e-05\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.17480351885706114 and immediate abs rewards look like: [0.0007409532217934611, 0.005867477251740638, 0.02126162846434454, 0.0012622937406376877, 0.020779164743544243, 0.003735361505732726, 0.01383115775661281, 0.03259015433559398, 0.0015482145922760537, 0.011271572254372586, 9.321750303570298e-05, 0.02218787958690882, 0.008240688297519227, 0.0009700102177703229, 0.009889526971164742, 0.0038767617888879613, 0.005255676226624928, 0.0001386906037623703, 0.005597929583018413, 2.398083688603947e-05, 0.0007848321874917019, 7.502886546717491e-05, 6.336950264085317e-06, 0.0003739354674507922, 0.0006463873733082437, 1.834976546888356e-05, 0.00021885609203309286, 0.00013001911884202855, 8.825290842651157e-05, 0.00011942925539187854, 9.050758308148943e-06, 0.00021209016813372727, 0.0001079788353308686, 2.7730075089493766e-05, 5.563954482568079e-05, 0.0002594994152786967, 2.7840096208819887e-05, 0.00010556065490163746, 1.2450705980882049e-05, 9.478480296820635e-05, 0.000588076093208656, 0.0010658079950189858, 4.957915280101588e-06, 2.1004118480050238e-05, 0.00025512890624668216, 4.6850714170432184e-05, 6.62341813040257e-05, 0.00010039729249911034, 6.40630009911547e-05, 5.460611646412872e-05]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.06227176215993644 and immediate relative rewards look like: [0.00025667461589194387, 0.0020330826361081235, 0.007382169230949076, 0.0004415357046217487, 0.0072715213718608, 0.0013167379623985876, 0.004881996700248979, 0.011559812247667728, 0.0005555781089609606, 0.004047061645182092, 3.3605779122581976e-05, 0.007999206318567912, 0.00299490165504157, 0.00035358839955470253, 0.0036062082883738422, 0.0014187745559372067, 0.0019261472343249874, 5.0926665277240774e-05, 0.0020556432627510867, 8.824260625219654e-06, 0.0002887982982378222, 2.7616691262932605e-05, 2.332574684571824e-06, 0.00013764262038391134, 0.00023796271657188932, 6.756937937832594e-06, 8.058996862380279e-05, 4.788114929519764e-05, 3.2501781741337936e-05, 4.398483660845217e-05, 3.333468294878446e-06, 7.811480362355674e-05, 3.977272742796055e-05, 1.0214451580497497e-05, 2.049519303359347e-05, 9.559027901163645e-05, 1.0256273505249244e-05, 3.8888876105305956e-05, 4.5870574908484274e-06, 3.4920536676943416e-05, 0.00021666606132474911, 0.0003927628880073862, 1.8277684675413334e-06, 7.743322232591156e-06, 9.405586962364231e-05, 1.7273617610647292e-05, 2.4420624189270115e-05, 3.7017509336600445e-05, 2.3621558578039196e-05, 2.0135055001369076e-05]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 150 number of matrices\n",
            "DEBUGGING: ACT_MAT has 150 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.03999306781958436, 0.03883338368416589, 0.03771289224536365, 0.03069356386168471, 0.02952917916029268, 0.023381325474352364, 0.01966179192823083, 0.014075118596032114, 0.013511076790164446, 0.013209553531721822, 0.013248815831634739, 0.012986945533233213, 0.012929837893596871, 0.012751491551533907, 0.012241318976183102, 0.012266189129568408, 0.008052270208675265, 0.006100479088721775, 0.005841527768849591, 0.005511110592129246, 0.005102394372346533, 0.00409805806707216, 0.003966748954621539, 0.0032873084230687437, 0.00325865631447618, 0.0027722714400204604, 0.002720470419080792, 0.0024959356438579586, 0.0018845616424007768, 0.0018759098887663648, 0.0018887714700015954, 0.0018945312305257873, 0.001882686159241503, 0.0018750430500763518, 0.001019240770355597, 0.0009504624303863341, 0.0009062619929213958, 0.0006220306246846564, 0.00041186062526000344, 0.00041528481284705326, 0.00022976931306719682, 0.00019254013904512862, 0.0001761616219196893, 0.00017485448794413483, 0.00017492553040196972, 0.00015614014179924114, 0.00015294869190474628, 0.00012159488911871108, 0.00010404966910972492, 3.093458702871074e-05], [0.2544063964318591, 0.25332683553544794, 0.25252307445473604, 0.25077681781388766, 0.246514469653137, 0.24709155486403742, 0.23811281528291225, 0.22240768135361116, 0.22456704906091646, 0.2267242270296219, 0.22827907184515028, 0.22828094474679755, 0.23050530058056595, 0.23197291976154255, 0.23420959762619298, 0.2365680809339218, 0.2387949776292362, 0.24120388854105804, 0.24355737786443826, 0.2458558375972876, 0.2477896829910106, 0.2501673393006474, 0.2526205440610122, 0.2551625070603456, 0.25762298545661677, 0.26021093841653603, 0.26268251209150717, 0.26512046662057653, 0.2677781161685405, 2.9365598199890235e-12, 2.736551661157365e-12, 2.30485283865758e-12, 2.098463801226562e-12, 2.1196604052793556e-12, 2.1410711164437933e-12, 1.9330277181823853e-12, 1.4932124922179978e-12, 1.5082954466848462e-12, 1.2938603749915817e-12, 1.0772592924730805e-12, 1.0881406994677582e-12, 1.0991320196644023e-12, 8.805639840617358e-13, 6.597881905237411e-13, 6.664527177007486e-13, 4.4351418409855374e-13, 4.479941253520745e-13, 2.2284893930186105e-13, 2.2509993868874854e-13, 2.2737367544318036e-13], [0.05717119657710984, 0.057489416122442315, 0.05601649847104464, 0.04912558509100562, 0.04917580746099381, 0.042327561706194956, 0.0414250744886832, 0.036912199786297194, 0.025608472261241886, 0.025305953689172652, 0.02147362832726319, 0.021656588432465262, 0.013795335468583182, 0.010909529104587486, 0.010662566368719983, 0.007127634424592062, 0.005766525119853389, 0.0038791695813418206, 0.0038669120364288687, 0.0018295644178563455, 0.0018391314719506321, 0.0015659931047604142, 0.0015539155691893754, 0.001567255550004852, 0.0014440534642635766, 0.0012182734825168559, 0.0012237540854333569, 0.0011547112291005598, 0.001118010181621578, 0.0010964731311921617, 0.0010631194894784944, 0.0010704909304885008, 0.001002400128146408, 0.0009723509098166137, 0.0009718550083193093, 0.000960969510389612, 0.0008741204357353288, 0.0008725900628586663, 0.0008421224108619801, 0.0008459953064354866, 0.0008192674442005486, 0.0006086882655311106, 0.00021810644194315587, 0.00021846330654102478, 0.00021284846899841779, 0.00011999252462098532, 0.00010375647172761418, 8.013721973570108e-05, 4.355526302939458e-05, 2.0135055001369076e-05]]\n",
            "DEBUGGING: traj_returns = [0.03999306781958436, 0.2544063964318591, 0.05717119657710984]\n",
            "DEBUGGING: actions = [[5], [53], [59], [56], [41], [8], [3], [31], [23], [53], [1], [13], [24], [30], [18], [52], [15], [14], [48], [66], [3], [48], [34], [32], [25], [83], [41], [46], [63], [44], [35], [70], [39], [92], [70], [56], [86], [46], [57], [82], [29], [23], [39], [41], [101], [24], [94], [60], [99], [53], [46], [30], [34], [2], [24], [31], [40], [66], [17], [51], [28], [30], [55], [65], [73], [59], [24], [13], [38], [39], [26], [45], [16], [75], [75], [84], [64], [66], [0], [68], [76], [31], [16], [12], [42], [54], [36], [88], [65], [18], [73], [87], [88], [70], [4], [78], [23], [89], [104], [94], [45], [6], [28], [20], [18], [46], [6], [6], [11], [50], [39], [30], [16], [63], [8], [24], [18], [55], [71], [65], [7], [33], [3], [8], [71], [44], [61], [35], [53], [42], [77], [41], [32], [28], [64], [76], [89], [7], [88], [17], [92], [67], [39], [36], [47], [42], [35], [49], [65], [84]]\n",
            "DEBUGGING: actions length = 150\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        ...,\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        ...,\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[1.17190220e-01 1.16549878e-01 1.15417488e-01 1.10198656e-01\n",
            "  1.08406485e-01 1.04266814e-01 9.97332272e-02 9.11316666e-02\n",
            "  8.78955327e-02 8.84132448e-02 8.76671720e-02 8.76414929e-02\n",
            "  8.57434913e-02 8.52113135e-02 8.57044943e-02 8.53206348e-02\n",
            "  8.42045910e-02 8.37278457e-02 8.44219392e-02 8.43988375e-02\n",
            "  8.49104029e-02 8.52771302e-02 8.60470695e-02 8.66723570e-02\n",
            "  8.74418984e-02 8.80671611e-02 8.88755789e-02 8.95903712e-02\n",
            "  9.02602293e-02 9.90794341e-04 9.83963654e-04 9.88340721e-04\n",
            "  9.61695430e-04 9.49131321e-04 6.63698594e-04 6.37143981e-04\n",
            "  5.93460810e-04 4.98206896e-04 4.17994346e-04 4.20426707e-04\n",
            "  3.49678919e-04 2.67076135e-04 1.31422688e-04 1.31105932e-04\n",
            "  1.29258000e-04 9.20442223e-05 8.55683880e-05 6.72440364e-05\n",
            "  4.92016441e-05 1.70232141e-05]]\n",
            "DEBUGGING: baseline2 looks like: 0.11719022027618442\n",
            "DEBUGGING: ADS looks like: [3.99930678e-02 3.88333837e-02 3.77128922e-02 3.06935639e-02\n",
            " 2.95291792e-02 2.33813255e-02 1.96617919e-02 1.40751186e-02\n",
            " 1.35110768e-02 1.32095535e-02 1.32488158e-02 1.29869455e-02\n",
            " 1.29298379e-02 1.27514916e-02 1.22413190e-02 1.22661891e-02\n",
            " 8.05227021e-03 6.10047909e-03 5.84152777e-03 5.51111059e-03\n",
            " 5.10239437e-03 4.09805807e-03 3.96674895e-03 3.28730842e-03\n",
            " 3.25865631e-03 2.77227144e-03 2.72047042e-03 2.49593564e-03\n",
            " 1.88456164e-03 1.87590989e-03 1.88877147e-03 1.89453123e-03\n",
            " 1.88268616e-03 1.87504305e-03 1.01924077e-03 9.50462430e-04\n",
            " 9.06261993e-04 6.22030625e-04 4.11860625e-04 4.15284813e-04\n",
            " 2.29769313e-04 1.92540139e-04 1.76161622e-04 1.74854488e-04\n",
            " 1.74925530e-04 1.56140142e-04 1.52948692e-04 1.21594889e-04\n",
            " 1.04049669e-04 3.09345870e-05 2.54406396e-01 2.53326836e-01\n",
            " 2.52523074e-01 2.50776818e-01 2.46514470e-01 2.47091555e-01\n",
            " 2.38112815e-01 2.22407681e-01 2.24567049e-01 2.26724227e-01\n",
            " 2.28279072e-01 2.28280945e-01 2.30505301e-01 2.31972920e-01\n",
            " 2.34209598e-01 2.36568081e-01 2.38794978e-01 2.41203889e-01\n",
            " 2.43557378e-01 2.45855838e-01 2.47789683e-01 2.50167339e-01\n",
            " 2.52620544e-01 2.55162507e-01 2.57622985e-01 2.60210938e-01\n",
            " 2.62682512e-01 2.65120467e-01 2.67778116e-01 2.93655982e-12\n",
            " 2.73655166e-12 2.30485284e-12 2.09846380e-12 2.11966041e-12\n",
            " 2.14107112e-12 1.93302772e-12 1.49321249e-12 1.50829545e-12\n",
            " 1.29386037e-12 1.07725929e-12 1.08814070e-12 1.09913202e-12\n",
            " 8.80563984e-13 6.59788191e-13 6.66452718e-13 4.43514184e-13\n",
            " 4.47994125e-13 2.22848939e-13 2.25099939e-13 2.27373675e-13\n",
            " 5.71711966e-02 5.74894161e-02 5.60164985e-02 4.91255851e-02\n",
            " 4.91758075e-02 4.23275617e-02 4.14250745e-02 3.69121998e-02\n",
            " 2.56084723e-02 2.53059537e-02 2.14736283e-02 2.16565884e-02\n",
            " 1.37953355e-02 1.09095291e-02 1.06625664e-02 7.12763442e-03\n",
            " 5.76652512e-03 3.87916958e-03 3.86691204e-03 1.82956442e-03\n",
            " 1.83913147e-03 1.56599310e-03 1.55391557e-03 1.56725555e-03\n",
            " 1.44405346e-03 1.21827348e-03 1.22375409e-03 1.15471123e-03\n",
            " 1.11801018e-03 1.09647313e-03 1.06311949e-03 1.07049093e-03\n",
            " 1.00240013e-03 9.72350910e-04 9.71855008e-04 9.60969510e-04\n",
            " 8.74120436e-04 8.72590063e-04 8.42122411e-04 8.45995306e-04\n",
            " 8.19267444e-04 6.08688266e-04 2.18106442e-04 2.18463307e-04\n",
            " 2.12848469e-04 1.19992525e-04 1.03756472e-04 8.01372197e-05\n",
            " 4.35552630e-05 2.01350550e-05]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(0.2287, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 1.0943e-35,  1.1814e-35,  1.0911e-35,  ...,  1.1947e-35,\n",
            "          1.0458e-35,  3.2880e-33],\n",
            "        [-6.4653e-39, -7.3476e-39, -6.5101e-39,  ..., -1.3740e-38,\n",
            "         -9.0297e-39, -2.4386e-36],\n",
            "        ...,\n",
            "        [ 3.4318e-30,  3.8906e-30,  3.2308e-30,  ...,  5.9988e-30,\n",
            "          3.6561e-30,  1.1009e-27],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]])\n",
            "   Last layer:\n",
            "tensor([[-1.3914e-13, -8.7586e-14, -1.2129e-13, -3.3568e-13, -7.7389e-14,\n",
            "         -1.0666e-13, -9.7653e-14, -3.1280e-13, -1.1571e-13, -1.2047e-13,\n",
            "         -6.3340e-14, -1.2901e-13, -1.0577e-13, -2.7134e-13, -2.4815e-13,\n",
            "         -1.1156e-13, -1.8527e-13, -2.2226e-13, -8.6446e-14, -7.6773e-14],\n",
            "        [-5.8286e-14, -2.3955e-14, -4.5019e-14, -5.4718e-14, -1.5413e-13,\n",
            "         -1.7683e-14, -6.0873e-14,  3.2133e-14, -1.0890e-13, -6.3693e-14,\n",
            "         -2.9887e-14, -1.0043e-13, -9.1490e-14, -1.2519e-13, -5.5695e-14,\n",
            "         -1.4744e-13, -1.8411e-14, -2.8154e-14, -1.0154e-13, -1.5177e-13],\n",
            "        [-1.8582e-14,  6.4004e-14,  5.1626e-14,  6.8446e-14,  7.3183e-14,\n",
            "         -3.0821e-15,  6.2790e-14,  1.8129e-13,  1.1423e-14,  1.0254e-13,\n",
            "          4.9103e-14,  9.9752e-14,  1.3853e-14,  7.0468e-14,  6.5362e-14,\n",
            "          5.0355e-14,  8.0968e-14,  8.4776e-14,  1.0837e-13,  5.2765e-14],\n",
            "        [-2.0850e-14, -7.7542e-14, -5.1449e-14, -9.8151e-14,  4.7457e-16,\n",
            "         -2.5290e-14, -1.2425e-13, -1.0883e-13, -1.3213e-13,  6.1780e-15,\n",
            "         -6.3633e-15, -1.2295e-13, -2.8674e-14, -1.2086e-14, -7.3446e-14,\n",
            "         -9.1290e-14, -9.5162e-14, -6.3901e-14, -8.9979e-14, -6.6612e-14],\n",
            "        [ 4.8104e-14, -1.6479e-14, -6.9732e-14, -1.4346e-14,  1.3620e-13,\n",
            "          6.6062e-16,  6.1240e-15,  4.4050e-14,  2.4117e-14,  9.0927e-14,\n",
            "          8.3957e-14, -5.0247e-14,  5.0066e-14,  5.5508e-14,  2.1743e-14,\n",
            "         -3.6950e-14,  4.3151e-14, -3.7492e-14,  9.9227e-14, -1.8337e-14],\n",
            "        [-7.0755e-14, -2.1806e-13, -1.2336e-13, -8.8908e-14, -1.6895e-13,\n",
            "         -6.4098e-14, -1.3193e-13, -1.7758e-13, -1.1042e-13, -1.7639e-13,\n",
            "         -1.3282e-13, -1.7110e-13, -1.2076e-13, -2.1420e-14, -1.7824e-13,\n",
            "         -1.0906e-13, -9.0430e-15, -1.5939e-13, -1.0938e-13, -2.2490e-13],\n",
            "        [-1.0656e-13,  1.3886e-13, -3.1321e-16,  3.7808e-14,  1.4739e-13,\n",
            "          6.8219e-14, -2.6500e-14, -1.7550e-13,  4.9697e-14,  4.3884e-14,\n",
            "          3.3086e-14, -1.1834e-13, -3.5923e-14, -8.6384e-15, -1.1420e-13,\n",
            "         -7.5543e-14,  5.9403e-14,  1.0861e-14,  1.7090e-15,  5.1600e-14],\n",
            "        [ 1.1215e-15, -4.0338e-14, -4.1773e-15, -5.4068e-14, -4.5636e-15,\n",
            "         -1.7060e-14, -1.1732e-14, -2.7008e-14,  2.7418e-14,  1.7827e-14,\n",
            "         -1.5924e-14,  1.7895e-14, -1.8160e-15, -6.3443e-15, -4.5401e-15,\n",
            "         -1.6063e-14,  1.6903e-14, -4.7248e-15, -1.1603e-14,  1.7612e-14],\n",
            "        [ 7.8723e-15,  2.9960e-15, -3.0643e-15, -2.8035e-14, -3.8472e-15,\n",
            "         -1.7571e-15, -5.8523e-15, -6.1233e-15,  1.2118e-14,  4.2564e-15,\n",
            "          3.3384e-16, -1.3316e-14, -2.0574e-14, -2.3154e-14, -2.5906e-14,\n",
            "         -5.9440e-15, -1.6329e-14, -2.2254e-14,  1.2372e-14, -2.2984e-14],\n",
            "        [-8.1562e-15, -3.8203e-15,  6.2401e-15, -3.8636e-15, -1.5354e-14,\n",
            "         -1.1342e-14,  1.7950e-16, -2.1998e-14, -1.1202e-14, -3.3113e-15,\n",
            "         -8.1234e-15,  1.9286e-14,  6.5033e-15, -5.3282e-15, -2.3216e-14,\n",
            "         -1.1792e-14, -2.3866e-14, -1.6163e-14, -5.8979e-15, -4.0728e-15]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 6.7208e-36,  7.2491e-36,  6.6951e-36,  ...,  7.3430e-36,\n",
            "          6.4276e-36,  2.0191e-33],\n",
            "        [-3.0049e-39, -3.4156e-39, -3.0227e-39,  ..., -6.3959e-39,\n",
            "         -4.2026e-39, -1.1340e-36],\n",
            "        ...,\n",
            "        [ 9.0594e-31,  1.0088e-30,  8.4872e-31,  ...,  1.6157e-30,\n",
            "          9.7973e-31,  2.9168e-28],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]])\n",
            "   Last layer:\n",
            "tensor([[-2.0814e-13, -7.5544e-14, -1.2510e-13, -4.2455e-14, -7.4384e-15,\n",
            "          1.2469e-14, -8.9094e-14, -4.6395e-14, -1.8891e-13, -1.2304e-13,\n",
            "         -5.9158e-14, -2.2622e-13, -1.0802e-13, -1.3239e-13, -2.2587e-13,\n",
            "         -1.4144e-13, -9.9659e-14, -9.3598e-14, -1.0510e-13, -9.9330e-15],\n",
            "        [ 1.7002e-13,  3.6572e-14,  2.4426e-14,  1.5432e-13,  1.4410e-13,\n",
            "          9.0540e-14, -4.3881e-14,  1.8607e-13,  6.4824e-14,  9.5687e-15,\n",
            "          1.6456e-15,  4.3334e-14,  3.0115e-14, -6.5403e-14,  1.0379e-13,\n",
            "          4.0569e-14,  1.0163e-13,  7.8081e-14,  5.6114e-15,  9.5664e-14],\n",
            "        [ 9.7517e-15, -7.4044e-14,  7.8039e-15, -7.5990e-14, -1.6477e-13,\n",
            "         -8.1724e-14, -2.6970e-14, -1.5087e-13, -4.9228e-14, -7.2314e-14,\n",
            "         -2.0519e-15, -5.5119e-14, -1.2529e-13, -1.6683e-14, -6.2063e-14,\n",
            "         -3.8030e-15, -1.1450e-14, -5.4000e-14, -5.3082e-14, -2.9080e-14],\n",
            "        [-2.0060e-13, -2.0070e-13, -1.4927e-13, -2.8353e-13, -1.6510e-13,\n",
            "         -1.8857e-13, -1.7136e-13, -2.6387e-13, -1.8408e-13, -2.3502e-13,\n",
            "         -1.8710e-13, -2.0545e-13, -1.5484e-13, -1.5246e-13, -2.2525e-13,\n",
            "         -1.4643e-13, -2.1914e-13, -1.6675e-13, -1.8692e-13, -1.8478e-13],\n",
            "        [ 5.3117e-14, -5.6292e-14,  9.2419e-14,  5.9655e-14, -8.8332e-14,\n",
            "         -2.1883e-14, -6.8430e-14, -1.6382e-13,  1.5620e-14, -8.6751e-14,\n",
            "          9.3868e-15, -8.8955e-15, -1.1848e-13,  1.1316e-14,  2.4852e-14,\n",
            "         -1.5666e-13, -2.2830e-14,  6.5374e-14, -4.3662e-14, -1.1077e-13],\n",
            "        [-1.8871e-13, -1.5825e-13, -1.2937e-13, -2.0392e-13, -1.9662e-13,\n",
            "         -9.4002e-14, -1.7338e-13, -2.4436e-13, -1.1392e-13, -2.2553e-13,\n",
            "         -1.1975e-13, -8.8323e-14, -2.4534e-13, -1.7407e-13, -9.7383e-14,\n",
            "         -2.0183e-13, -1.5210e-13, -1.3211e-13, -1.3310e-13, -1.5436e-13],\n",
            "        [-1.8405e-13, -8.2987e-14, -9.3612e-14, -1.5021e-13, -9.5877e-14,\n",
            "         -8.8179e-14, -1.2793e-13, -1.9689e-14, -1.7550e-13, -3.5699e-15,\n",
            "          7.8888e-15, -8.3401e-14, -2.4475e-13, -8.2353e-14, -1.9056e-13,\n",
            "         -1.7914e-13, -2.2503e-13, -8.6213e-14, -1.0658e-13, -1.4225e-13],\n",
            "        [-8.5119e-15, -2.7563e-14, -2.9249e-14, -1.2249e-13, -7.2569e-14,\n",
            "         -2.4590e-14, -2.6721e-14, -6.9823e-14, -6.4726e-14, -4.3134e-14,\n",
            "         -5.3324e-14, -3.7806e-14, -3.2149e-14, -5.3462e-14, -1.1555e-13,\n",
            "         -7.7362e-14, -5.2196e-14, -4.7616e-14, -4.7197e-14, -7.2093e-14],\n",
            "        [-2.5666e-15, -5.3662e-14, -3.4290e-14, -3.7374e-15, -3.9966e-14,\n",
            "         -2.7707e-14, -2.8493e-14, -4.4280e-14, -1.9728e-15, -3.8158e-14,\n",
            "         -6.1421e-15, -2.3059e-14, -2.0366e-14, -2.0519e-14, -4.3391e-15,\n",
            "         -2.4637e-14, -2.2335e-14, -1.9688e-14, -2.4520e-14, -2.4808e-14],\n",
            "        [ 7.4762e-15, -2.7499e-16, -1.0617e-14, -5.1857e-16,  8.0497e-15,\n",
            "         -1.5037e-15,  8.7563e-15,  2.0310e-15, -7.8396e-15,  1.8606e-14,\n",
            "          5.4288e-15,  7.8879e-15, -9.0837e-16,  7.6887e-15,  5.1681e-15,\n",
            "         -1.7852e-15, -2.2492e-15, -4.3967e-17,  2.7475e-15, -4.5591e-15]])\n",
            "DEBUGGING: training for one iteration takes 0.006120 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 2\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [57]\n",
            "DEBUGGING: rel_reward looks like: 0.0008211357149821215\n",
            "DEBUGGING: rel_reward looks like: 0.0004570648700201211\n",
            "DEBUGGING: rel_reward looks like: 0.00339169073711228\n",
            "DEBUGGING: rel_reward looks like: 0.003065979261182583\n",
            "DEBUGGING: rel_reward looks like: 0.011721140281518877\n",
            "DEBUGGING: rel_reward looks like: 0.0027259850582341703\n",
            "DEBUGGING: rel_reward looks like: 0.002384754567223064\n",
            "DEBUGGING: rel_reward looks like: 0.00282932833922181\n",
            "DEBUGGING: rel_reward looks like: 0.0007283917248216128\n",
            "DEBUGGING: rel_reward looks like: 0.0001926724178278805\n",
            "DEBUGGING: the action_prob is: tensor([0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [38]\n",
            "DEBUGGING: rel_reward looks like: 9.022544745208329e-06\n",
            "DEBUGGING: rel_reward looks like: 0.0022268298835285086\n",
            "DEBUGGING: rel_reward looks like: 0.0015495856390406676\n",
            "DEBUGGING: rel_reward looks like: 0.001494714099813913\n",
            "DEBUGGING: rel_reward looks like: 8.868816872662354e-05\n",
            "DEBUGGING: rel_reward looks like: 5.795935807606017e-05\n",
            "DEBUGGING: rel_reward looks like: 1.5082015711187526e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0001226616776290897\n",
            "DEBUGGING: rel_reward looks like: 4.308146358501518e-05\n",
            "DEBUGGING: rel_reward looks like: 7.134638500076736e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [4]\n",
            "DEBUGGING: rel_reward looks like: 0.0003393004615760741\n",
            "DEBUGGING: rel_reward looks like: 0.00013510477048880225\n",
            "DEBUGGING: rel_reward looks like: 0.0005453415718886765\n",
            "DEBUGGING: rel_reward looks like: 0.00012736792718612118\n",
            "DEBUGGING: rel_reward looks like: 1.1058168709968718e-05\n",
            "DEBUGGING: rel_reward looks like: 1.9209869919884953e-06\n",
            "DEBUGGING: rel_reward looks like: 0.000575169985394791\n",
            "DEBUGGING: rel_reward looks like: 2.7471786930383234e-05\n",
            "DEBUGGING: rel_reward looks like: 5.2653831620864734e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00014372701779964993\n",
            "DEBUGGING: the action_prob is: tensor([0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [21]\n",
            "DEBUGGING: rel_reward looks like: 0.00029520445836249355\n",
            "DEBUGGING: rel_reward looks like: 8.598660418679835e-09\n",
            "DEBUGGING: rel_reward looks like: 0.00025947928153225976\n",
            "DEBUGGING: rel_reward looks like: 5.758952278801445e-07\n",
            "DEBUGGING: rel_reward looks like: 3.904199958280722e-06\n",
            "DEBUGGING: rel_reward looks like: 4.4969423096050303e-08\n",
            "DEBUGGING: rel_reward looks like: 5.6187055109706565e-06\n",
            "DEBUGGING: rel_reward looks like: 4.086559940737426e-05\n",
            "DEBUGGING: rel_reward looks like: 4.2530935952026203e-05\n",
            "DEBUGGING: rel_reward looks like: 1.3076569617319998e-06\n",
            "DEBUGGING: the action_prob is: tensor([0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [42]\n",
            "DEBUGGING: rel_reward looks like: 5.902144294651501e-06\n",
            "DEBUGGING: rel_reward looks like: 0.00039319479203537075\n",
            "DEBUGGING: rel_reward looks like: 4.886758927424696e-05\n",
            "DEBUGGING: rel_reward looks like: 5.120795204415492e-05\n",
            "DEBUGGING: rel_reward looks like: 8.530219875443516e-05\n",
            "DEBUGGING: rel_reward looks like: 1.0758726481932816e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00018644194493370217\n",
            "DEBUGGING: rel_reward looks like: 2.295515768108979e-05\n",
            "DEBUGGING: rel_reward looks like: 3.522204074634026e-05\n",
            "DEBUGGING: rel_reward looks like: 4.053394561376939e-05\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.10647778554175602 and immediate abs rewards look like: [0.0023704064051344176, 0.00131834462081315, 0.009778422723229596, 0.008809398971607152, 0.03357479323949519, 0.007716963740676874, 0.006732574704074068, 0.007968634708959144, 0.0020456676111280103, 0.0005407209064287599, 2.5316226583527168e-05, 0.0062481730865329155, 0.004338238413765794, 0.004178134972789849, 0.00024753715206315974, 0.00016175575865418068, 4.208917380310595e-05, 0.0003423050939090899, 0.00012021028760500485, 0.00019906937359337462, 0.0009466423744015628, 0.0003768120591303159, 0.0015207717465273163, 0.00035499203386279987, 3.0816720482107485e-05, 5.353315373213263e-06, 0.0016028533368626086, 7.65128916100366e-05, 0.000146644491906045, 0.00040026841497819987, 0.0008220029853873712, 2.3936081561259925e-08, 0.0007223121847346192, 1.6027029232645873e-06, 1.0865290732908761e-05, 1.2514828995335847e-07, 1.5636655007256195e-05, 0.00011372685185051523, 0.00011835656050607213, 3.6388382795848884e-06, 1.6423971374024404e-05, 0.001094141649446101, 0.00013593018638857757, 0.00014243318355511292, 0.00023725302571619977, 2.9920948236394906e-05, 0.0005185055847505282, 6.382768833645969e-05, 9.79339747573249e-05, 0.00011269961942161899]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.03748615750944511 and immediate relative rewards look like: [0.0008211357149821215, 0.0004570648700201211, 0.00339169073711228, 0.003065979261182583, 0.011721140281518877, 0.0027259850582341703, 0.002384754567223064, 0.00282932833922181, 0.0007283917248216128, 0.0001926724178278805, 9.022544745208329e-06, 0.0022268298835285086, 0.0015495856390406676, 0.001494714099813913, 8.868816872662354e-05, 5.795935807606017e-05, 1.5082015711187526e-05, 0.0001226616776290897, 4.308146358501518e-05, 7.134638500076736e-05, 0.0003393004615760741, 0.00013510477048880225, 0.0005453415718886765, 0.00012736792718612118, 1.1058168709968718e-05, 1.9209869919884953e-06, 0.000575169985394791, 2.7471786930383234e-05, 5.2653831620864734e-05, 0.00014372701779964993, 0.00029520445836249355, 8.598660418679835e-09, 0.00025947928153225976, 5.758952278801445e-07, 3.904199958280722e-06, 4.4969423096050303e-08, 5.6187055109706565e-06, 4.086559940737426e-05, 4.2530935952026203e-05, 1.3076569617319998e-06, 5.902144294651501e-06, 0.00039319479203537075, 4.886758927424696e-05, 5.120795204415492e-05, 8.530219875443516e-05, 1.0758726481932816e-05, 0.00018644194493370217, 2.295515768108979e-05, 3.522204074634026e-05, 4.053394561376939e-05]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [58]\n",
            "DEBUGGING: rel_reward looks like: 7.062391050380656e-05\n",
            "DEBUGGING: rel_reward looks like: 0.000322148400890266\n",
            "DEBUGGING: rel_reward looks like: 0.014527386142577748\n",
            "DEBUGGING: rel_reward looks like: 0.2966877718400505\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508863607e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [28]\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [18]\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 4.547473508863607e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508865675e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: the action_prob is: tensor([0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
            "        0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
            "        0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
            "        0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
            "        0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
            "        0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
            "        0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
            "        0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
            "        0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
            "        0.0116, 0.0116, 0.0116, 0.0116, 0.0116], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [58]\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [59]\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508863607e-13\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.8867413289840442 and immediate abs rewards look like: [0.00020387296126500587, 0.0009298934255639324, 0.04192033531762718, 0.8436872272682194, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13, 0.0, 0.0, 0.0, 9.094947017729282e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.3116079302997067 and immediate relative rewards look like: [7.062391050380656e-05, 0.000322148400890266, 0.014527386142577748, 0.2966877718400505, 0.0, 0.0, 2.2737367544323206e-13, 2.2737367544328376e-13, 2.2737367544323206e-13, 4.547473508863607e-13, 2.2737367544328376e-13, 0.0, 0.0, 0.0, 2.2737367544323206e-13, 2.2737367544318036e-13, 0.0, 0.0, 0.0, 2.2737367544323206e-13, 0.0, 0.0, 0.0, 4.547473508863607e-13, 4.547473508865675e-13, 2.2737367544318036e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 2.2737367544323206e-13, 2.2737367544318036e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.2737367544323206e-13, 0.0, 2.2737367544318036e-13, 0.0, 2.2737367544323206e-13, 2.2737367544328376e-13, 2.2737367544323206e-13, 0.0, 2.2737367544318036e-13, 2.2737367544323206e-13, 4.547473508863607e-13]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [48]\n",
            "DEBUGGING: rel_reward looks like: 0.002649605640713791\n",
            "DEBUGGING: rel_reward looks like: 0.003397535619800145\n",
            "DEBUGGING: rel_reward looks like: 0.00030838788489215995\n",
            "DEBUGGING: rel_reward looks like: 0.0014473691297585494\n",
            "DEBUGGING: rel_reward looks like: 0.0020187093575463346\n",
            "DEBUGGING: rel_reward looks like: 0.0006990066154927014\n",
            "DEBUGGING: rel_reward looks like: 0.0020315464842777923\n",
            "DEBUGGING: rel_reward looks like: 0.003918497313578641\n",
            "DEBUGGING: rel_reward looks like: 0.0003480363477145254\n",
            "DEBUGGING: rel_reward looks like: 2.3676701752452275e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [51]\n",
            "DEBUGGING: rel_reward looks like: 0.0009141506905672244\n",
            "DEBUGGING: rel_reward looks like: 0.0007302317739544612\n",
            "DEBUGGING: rel_reward looks like: 0.001052619713388457\n",
            "DEBUGGING: rel_reward looks like: 0.00039271230134672283\n",
            "DEBUGGING: rel_reward looks like: 2.5995472255700967e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00025400633945007236\n",
            "DEBUGGING: rel_reward looks like: 0.0009402682411036781\n",
            "DEBUGGING: rel_reward looks like: 0.00021362965105756795\n",
            "DEBUGGING: rel_reward looks like: 0.2921979477240524\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [68]\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [45]\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 4.547473508864641e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544333546e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [76]\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508865675e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508863607e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.88674132898268 and immediate abs rewards look like: [0.007648726108527626, 0.009781819670934055, 0.0008848606835272221, 0.004151671131239709, 0.005782137233836693, 0.0019981048981208005, 0.0058031003618452814, 0.011170424668307533, 0.0009882563385872345, 6.720708097418537e-05, 0.002594784758912283, 0.0020708425531665853, 0.002982913105370244, 0.0011116964546999952, 7.355951220233692e-05, 0.0007187442693066259, 0.0026599365619404125, 0.0006037713446858106, 0.8256487722364909, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.3135639330077056 and immediate relative rewards look like: [0.002649605640713791, 0.003397535619800145, 0.00030838788489215995, 0.0014473691297585494, 0.0020187093575463346, 0.0006990066154927014, 0.0020315464842777923, 0.003918497313578641, 0.0003480363477145254, 2.3676701752452275e-05, 0.0009141506905672244, 0.0007302317739544612, 0.001052619713388457, 0.00039271230134672283, 2.5995472255700967e-05, 0.00025400633945007236, 0.0009402682411036781, 0.00021362965105756795, 0.2921979477240524, 2.2737367544323206e-13, 2.2737367544318036e-13, 0.0, 0.0, 2.2737367544323206e-13, 2.2737367544328376e-13, 0.0, 0.0, 0.0, 2.2737367544323206e-13, 2.2737367544328376e-13, 0.0, 0.0, 4.547473508864641e-13, 2.2737367544333546e-13, 2.2737367544328376e-13, 0.0, 2.2737367544323206e-13, 2.2737367544318036e-13, 2.2737367544323206e-13, 2.2737367544318036e-13, 0.0, 0.0, 0.0, 2.2737367544323206e-13, 4.547473508865675e-13, 4.547473508863607e-13, 2.2737367544328376e-13, 2.2737367544323206e-13, 2.2737367544328376e-13, 0.0]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 150 number of matrices\n",
            "DEBUGGING: ACT_MAT has 150 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.03476755342657387, 0.03428931081978965, 0.034173985807848015, 0.03109322734417751, 0.028310351598984776, 0.016756779108551414, 0.01417251924274469, 0.011906833005577402, 0.009169196632682416, 0.008526065563495762, 0.00841756883400796, 0.008493481100265406, 0.006329950723976664, 0.004828651600945451, 0.003367613637506604, 0.0033120459280605867, 0.0032869561312975012, 0.0033049233490770845, 0.0032144057287353486, 0.0032033578435861952, 0.0031636479379650788, 0.0028528762387767725, 0.0027452237053413845, 0.0022221031651037454, 0.0021158941797147723, 0.0021260969808129327, 0.0021456323169908526, 0.0015863255874707693, 0.0015745997985256424, 0.0015373191584896744, 0.0014076688289798227, 0.0011237013844619488, 0.0011350432179813436, 0.0008844080166152361, 0.0008927597185730869, 0.0008978338571866729, 0.0009068574623874513, 0.0009103421786631117, 0.0008782591709653914, 0.0008441699343569346, 0.000851376037772932, 0.0008540140338164449, 0.00046547398159704463, 0.00042081453769979564, 0.0003733399855107482, 0.00029094725934981116, 0.00028301872006856397, 9.755229811602201e-05, 7.535064690397195e-05, 4.053394561376939e-05], [0.3025036903187216, 0.30548794586688666, 0.30824828026868323, 0.296687771844551, 4.545913672657182e-12, 4.591831992583012e-12, 4.6382141339222345e-12, 4.455394402504043e-12, 4.27072800713208e-12, 4.0841962946352e-12, 3.6661100441907466e-12, 3.473471079542892e-12, 3.5085566460029213e-12, 3.543996612124163e-12, 3.579794557701175e-12, 3.3862837194524677e-12, 3.1908182262720076e-12, 3.2230487134060683e-12, 3.2556047610162305e-12, 3.288489657592152e-12, 3.09203634560497e-12, 3.123269035964616e-12, 3.1548172080450666e-12, 3.1866840485303705e-12, 2.759532017822232e-12, 2.328065320137035e-12, 2.1219107522160147e-12, 2.1433441941575907e-12, 2.164994135512718e-12, 2.1868627631441594e-12, 2.2089522860042013e-12, 2.231264935357779e-12, 2.0241325857722698e-12, 1.8149079902314033e-12, 1.8332403941731348e-12, 1.8517579739122573e-12, 1.870462599911371e-12, 1.889356161526637e-12, 1.9084405671986236e-12, 1.9277177446450743e-12, 1.7175192618200427e-12, 1.7348679412323664e-12, 1.5227214805951376e-12, 1.538102505651654e-12, 1.3239685153620425e-12, 1.1076715554734936e-12, 8.891897778083452e-13, 8.981714927357022e-13, 6.775735528207291e-13, 4.547473508863607e-13], [0.26405455816797196, 0.26404540659319004, 0.26328067775089886, 0.26562857562222897, 0.2668497035277479, 0.2675060547173753, 0.2695020687897804, 0.2701722447530329, 0.268943179231772, 0.27130822513541164, 0.27402479639763555, 0.27586933909804884, 0.27791829022635794, 0.279662293447444, 0.2820904860061588, 0.2849136268019224, 0.2875349701641135, 0.28948959790203016, 0.29219794772825514, 4.245181425896224e-12, 4.058391667124234e-12, 3.869715143112175e-12, 3.908803174860783e-12, 3.948286035212912e-12, 3.7584973331006874e-12, 3.5667915733913164e-12, 3.60281977110234e-12, 3.639211890002364e-12, 3.675971606062994e-12, 3.4834322531512745e-12, 3.2889480582908997e-12, 3.3221697558493936e-12, 3.3557270261104986e-12, 2.9302825002262975e-12, 2.730210934124204e-12, 2.5281184431120406e-12, 2.5536549930424653e-12, 2.3497791086860944e-12, 2.1438438719625396e-12, 1.935828481332634e-12, 1.7257119251408625e-12, 1.743143358728144e-12, 1.7607508674021656e-12, 1.7785362296991572e-12, 1.5668308628847729e-12, 1.123316678786066e-12, 6.753225534340459e-13, 4.52473614132083e-13, 2.2737367544328376e-13, 0.0]]\n",
            "DEBUGGING: traj_returns = [0.03476755342657387, 0.3025036903187216, 0.26405455816797196]\n",
            "DEBUGGING: actions = [[57], [48], [37], [9], [50], [50], [15], [22], [31], [49], [38], [14], [15], [71], [50], [59], [16], [47], [65], [33], [4], [25], [23], [10], [30], [14], [8], [7], [33], [78], [21], [20], [64], [11], [19], [11], [46], [67], [39], [37], [42], [63], [82], [15], [87], [93], [8], [57], [70], [5], [58], [35], [44], [0], [0], [52], [61], [36], [31], [3], [28], [31], [67], [43], [32], [40], [2], [71], [33], [6], [18], [6], [63], [56], [73], [47], [45], [46], [42], [67], [58], [73], [7], [80], [3], [31], [25], [49], [86], [1], [59], [71], [40], [5], [21], [94], [21], [81], [76], [55], [48], [16], [56], [45], [37], [23], [54], [15], [48], [63], [51], [29], [16], [25], [7], [49], [39], [71], [0], [13], [68], [29], [75], [37], [1], [5], [47], [42], [13], [29], [45], [86], [60], [50], [49], [79], [68], [25], [19], [74], [76], [27], [63], [72], [50], [81], [96], [19], [14], [102]]\n",
            "DEBUGGING: actions length = 150\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        ...,\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        ...,\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[2.00441934e-01 2.01274221e-01 2.01900981e-01 1.97803192e-01\n",
            "  9.83866850e-02 9.47542779e-02 9.45581960e-02 9.40263593e-02\n",
            "  9.27041253e-02 9.32780969e-02 9.41474551e-02 9.47876067e-02\n",
            "  9.47494137e-02 9.48303150e-02 9.51526999e-02 9.60752242e-02\n",
            "  9.69406421e-02 9.75981738e-02 9.84707845e-02 1.06778595e-03\n",
            "  1.05454932e-03 9.50958749e-04 9.15074571e-04 7.40701057e-04\n",
            "  7.05298062e-04 7.08698996e-04 7.15210774e-04 5.28775198e-04\n",
            "  5.24866601e-04 5.12439721e-04 4.69222945e-04 3.74567130e-04\n",
            "  3.78347741e-04 2.94802674e-04 2.97586574e-04 2.99277954e-04\n",
            "  3.02285822e-04 3.03447394e-04 2.92753058e-04 2.81389979e-04\n",
            "  2.83792014e-04 2.84671346e-04 1.55157995e-04 1.40271514e-04\n",
            "  1.24446663e-04 9.69824205e-05 9.43395739e-05 3.25174332e-05\n",
            "  2.51168826e-05 1.35113154e-05]]\n",
            "DEBUGGING: baseline2 looks like: 0.20044193397108914\n",
            "DEBUGGING: ADS looks like: [3.47675534e-02 3.42893108e-02 3.41739858e-02 3.10932273e-02\n",
            " 2.83103516e-02 1.67567791e-02 1.41725192e-02 1.19068330e-02\n",
            " 9.16919663e-03 8.52606556e-03 8.41756883e-03 8.49348110e-03\n",
            " 6.32995072e-03 4.82865160e-03 3.36761364e-03 3.31204593e-03\n",
            " 3.28695613e-03 3.30492335e-03 3.21440573e-03 3.20335784e-03\n",
            " 3.16364794e-03 2.85287624e-03 2.74522371e-03 2.22210317e-03\n",
            " 2.11589418e-03 2.12609698e-03 2.14563232e-03 1.58632559e-03\n",
            " 1.57459980e-03 1.53731916e-03 1.40766883e-03 1.12370138e-03\n",
            " 1.13504322e-03 8.84408017e-04 8.92759719e-04 8.97833857e-04\n",
            " 9.06857462e-04 9.10342179e-04 8.78259171e-04 8.44169934e-04\n",
            " 8.51376038e-04 8.54014034e-04 4.65473982e-04 4.20814538e-04\n",
            " 3.73339986e-04 2.90947259e-04 2.83018720e-04 9.75522981e-05\n",
            " 7.53506469e-05 4.05339456e-05 3.02503690e-01 3.05487946e-01\n",
            " 3.08248280e-01 2.96687772e-01 4.54591367e-12 4.59183199e-12\n",
            " 4.63821413e-12 4.45539440e-12 4.27072801e-12 4.08419629e-12\n",
            " 3.66611004e-12 3.47347108e-12 3.50855665e-12 3.54399661e-12\n",
            " 3.57979456e-12 3.38628372e-12 3.19081823e-12 3.22304871e-12\n",
            " 3.25560476e-12 3.28848966e-12 3.09203635e-12 3.12326904e-12\n",
            " 3.15481721e-12 3.18668405e-12 2.75953202e-12 2.32806532e-12\n",
            " 2.12191075e-12 2.14334419e-12 2.16499414e-12 2.18686276e-12\n",
            " 2.20895229e-12 2.23126494e-12 2.02413259e-12 1.81490799e-12\n",
            " 1.83324039e-12 1.85175797e-12 1.87046260e-12 1.88935616e-12\n",
            " 1.90844057e-12 1.92771774e-12 1.71751926e-12 1.73486794e-12\n",
            " 1.52272148e-12 1.53810251e-12 1.32396852e-12 1.10767156e-12\n",
            " 8.89189778e-13 8.98171493e-13 6.77573553e-13 4.54747351e-13\n",
            " 2.64054558e-01 2.64045407e-01 2.63280678e-01 2.65628576e-01\n",
            " 2.66849704e-01 2.67506055e-01 2.69502069e-01 2.70172245e-01\n",
            " 2.68943179e-01 2.71308225e-01 2.74024796e-01 2.75869339e-01\n",
            " 2.77918290e-01 2.79662293e-01 2.82090486e-01 2.84913627e-01\n",
            " 2.87534970e-01 2.89489598e-01 2.92197948e-01 4.24518143e-12\n",
            " 4.05839167e-12 3.86971514e-12 3.90880317e-12 3.94828604e-12\n",
            " 3.75849733e-12 3.56679157e-12 3.60281977e-12 3.63921189e-12\n",
            " 3.67597161e-12 3.48343225e-12 3.28894806e-12 3.32216976e-12\n",
            " 3.35572703e-12 2.93028250e-12 2.73021093e-12 2.52811844e-12\n",
            " 2.55365499e-12 2.34977911e-12 2.14384387e-12 1.93582848e-12\n",
            " 1.72571193e-12 1.74314336e-12 1.76075087e-12 1.77853623e-12\n",
            " 1.56683086e-12 1.12331668e-12 6.75322553e-13 4.52473614e-13\n",
            " 2.27373675e-13 0.00000000e+00]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(0.1873, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 6.7208e-36,  7.2491e-36,  6.6951e-36,  ...,  7.3430e-36,\n",
            "          6.4276e-36,  2.0191e-33],\n",
            "        [-3.0049e-39, -3.4156e-39, -3.0227e-39,  ..., -6.3959e-39,\n",
            "         -4.2026e-39, -1.1340e-36],\n",
            "        ...,\n",
            "        [ 9.0594e-31,  1.0088e-30,  8.4872e-31,  ...,  1.6157e-30,\n",
            "          9.7973e-31,  2.9168e-28],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]])\n",
            "   Last layer:\n",
            "tensor([[-2.0814e-13, -7.5544e-14, -1.2510e-13, -4.2455e-14, -7.4384e-15,\n",
            "          1.2469e-14, -8.9094e-14, -4.6395e-14, -1.8891e-13, -1.2304e-13,\n",
            "         -5.9158e-14, -2.2622e-13, -1.0802e-13, -1.3239e-13, -2.2587e-13,\n",
            "         -1.4144e-13, -9.9659e-14, -9.3598e-14, -1.0510e-13, -9.9330e-15],\n",
            "        [ 1.7002e-13,  3.6572e-14,  2.4426e-14,  1.5432e-13,  1.4410e-13,\n",
            "          9.0540e-14, -4.3881e-14,  1.8607e-13,  6.4824e-14,  9.5687e-15,\n",
            "          1.6456e-15,  4.3334e-14,  3.0115e-14, -6.5403e-14,  1.0379e-13,\n",
            "          4.0569e-14,  1.0163e-13,  7.8081e-14,  5.6114e-15,  9.5664e-14],\n",
            "        [ 9.7517e-15, -7.4044e-14,  7.8039e-15, -7.5990e-14, -1.6477e-13,\n",
            "         -8.1724e-14, -2.6970e-14, -1.5087e-13, -4.9228e-14, -7.2314e-14,\n",
            "         -2.0519e-15, -5.5119e-14, -1.2529e-13, -1.6683e-14, -6.2063e-14,\n",
            "         -3.8030e-15, -1.1450e-14, -5.4000e-14, -5.3082e-14, -2.9080e-14],\n",
            "        [-2.0060e-13, -2.0070e-13, -1.4927e-13, -2.8353e-13, -1.6510e-13,\n",
            "         -1.8857e-13, -1.7136e-13, -2.6387e-13, -1.8408e-13, -2.3502e-13,\n",
            "         -1.8710e-13, -2.0545e-13, -1.5484e-13, -1.5246e-13, -2.2525e-13,\n",
            "         -1.4643e-13, -2.1914e-13, -1.6675e-13, -1.8692e-13, -1.8478e-13],\n",
            "        [ 5.3117e-14, -5.6292e-14,  9.2419e-14,  5.9655e-14, -8.8332e-14,\n",
            "         -2.1883e-14, -6.8430e-14, -1.6382e-13,  1.5620e-14, -8.6751e-14,\n",
            "          9.3868e-15, -8.8955e-15, -1.1848e-13,  1.1316e-14,  2.4852e-14,\n",
            "         -1.5666e-13, -2.2830e-14,  6.5374e-14, -4.3662e-14, -1.1077e-13],\n",
            "        [-1.8871e-13, -1.5825e-13, -1.2937e-13, -2.0392e-13, -1.9662e-13,\n",
            "         -9.4002e-14, -1.7338e-13, -2.4436e-13, -1.1392e-13, -2.2553e-13,\n",
            "         -1.1975e-13, -8.8323e-14, -2.4534e-13, -1.7407e-13, -9.7383e-14,\n",
            "         -2.0183e-13, -1.5210e-13, -1.3211e-13, -1.3310e-13, -1.5436e-13],\n",
            "        [-1.8405e-13, -8.2987e-14, -9.3612e-14, -1.5021e-13, -9.5877e-14,\n",
            "         -8.8179e-14, -1.2793e-13, -1.9689e-14, -1.7550e-13, -3.5699e-15,\n",
            "          7.8888e-15, -8.3401e-14, -2.4475e-13, -8.2353e-14, -1.9056e-13,\n",
            "         -1.7914e-13, -2.2503e-13, -8.6213e-14, -1.0658e-13, -1.4225e-13],\n",
            "        [-8.5119e-15, -2.7563e-14, -2.9249e-14, -1.2249e-13, -7.2569e-14,\n",
            "         -2.4590e-14, -2.6721e-14, -6.9823e-14, -6.4726e-14, -4.3134e-14,\n",
            "         -5.3324e-14, -3.7806e-14, -3.2149e-14, -5.3462e-14, -1.1555e-13,\n",
            "         -7.7362e-14, -5.2196e-14, -4.7616e-14, -4.7197e-14, -7.2093e-14],\n",
            "        [-2.5666e-15, -5.3662e-14, -3.4290e-14, -3.7374e-15, -3.9966e-14,\n",
            "         -2.7707e-14, -2.8493e-14, -4.4280e-14, -1.9728e-15, -3.8158e-14,\n",
            "         -6.1421e-15, -2.3059e-14, -2.0366e-14, -2.0519e-14, -4.3391e-15,\n",
            "         -2.4637e-14, -2.2335e-14, -1.9688e-14, -2.4520e-14, -2.4808e-14],\n",
            "        [ 7.4762e-15, -2.7499e-16, -1.0617e-14, -5.1857e-16,  8.0497e-15,\n",
            "         -1.5037e-15,  8.7563e-15,  2.0310e-15, -7.8396e-15,  1.8606e-14,\n",
            "          5.4288e-15,  7.8879e-15, -9.0837e-16,  7.6887e-15,  5.1681e-15,\n",
            "         -1.7852e-15, -2.2492e-15, -4.3967e-17,  2.7475e-15, -4.5591e-15]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-5.3497e-36, -5.7637e-36, -5.3348e-36,  ..., -5.8453e-36,\n",
            "         -5.1167e-36, -1.6074e-33],\n",
            "        [ 3.3841e-39,  3.8431e-39,  3.4072e-39,  ...,  7.1929e-39,\n",
            "          4.7271e-39,  1.2763e-36],\n",
            "        ...,\n",
            "        [-1.1404e-30, -1.2853e-30, -1.0644e-30,  ..., -2.0284e-30,\n",
            "         -1.2305e-30, -3.6694e-28],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]])\n",
            "   Last layer:\n",
            "tensor([[ 4.5864e-14,  1.3622e-13, -4.9234e-15,  2.6300e-13,  2.6299e-14,\n",
            "          7.6426e-14,  9.1693e-14, -3.1471e-14,  9.2573e-14,  1.6865e-13,\n",
            "          1.1626e-13, -1.3196e-14,  8.7977e-14,  8.5553e-14,  1.5782e-13,\n",
            "          9.2034e-14, -1.6989e-14,  1.2371e-13,  1.1295e-13,  1.3996e-13],\n",
            "        [ 1.5202e-13,  1.0649e-13,  9.1107e-14,  1.6111e-13,  2.7186e-13,\n",
            "          5.2704e-14,  1.9176e-13,  7.9985e-14,  1.8139e-13,  1.7107e-13,\n",
            "          9.9473e-14,  9.4574e-14,  1.4555e-13,  1.6987e-13,  9.8791e-14,\n",
            "          1.0135e-13,  1.7620e-13,  1.3288e-13,  1.3124e-13,  2.4644e-13],\n",
            "        [-1.8450e-14,  3.8747e-15,  1.0522e-14,  7.5031e-14,  2.9264e-14,\n",
            "         -1.9408e-14, -3.8559e-14,  1.0596e-13, -1.3313e-14,  3.7051e-14,\n",
            "          2.1968e-14,  4.1015e-14,  4.8766e-14,  2.6967e-14, -3.1432e-15,\n",
            "          5.4679e-14, -3.3787e-14,  2.0117e-14,  2.6503e-14,  1.7895e-14],\n",
            "        [ 6.0075e-14,  5.2723e-14,  3.3727e-14,  1.9271e-14,  4.4742e-14,\n",
            "          1.0234e-14,  3.1688e-14,  3.7802e-14,  3.9891e-14,  6.1884e-14,\n",
            "          4.1938e-14,  2.5438e-15, -5.8303e-14,  2.6156e-14,  2.5535e-15,\n",
            "         -2.9918e-14,  1.3038e-15, -1.8999e-14, -5.1887e-15,  3.3516e-14],\n",
            "        [-1.2106e-13, -1.1274e-13,  2.6734e-14, -4.7738e-14, -9.1045e-14,\n",
            "         -9.0795e-14, -2.5961e-14, -9.7497e-14, -1.0595e-13, -5.2691e-14,\n",
            "         -6.6899e-15, -6.6969e-14, -8.8794e-14, -1.4800e-13, -1.4546e-13,\n",
            "         -8.4534e-14, -1.0686e-13, -1.1443e-13, -9.3777e-14, -1.5052e-13],\n",
            "        [ 3.6594e-14,  1.0971e-13,  1.0680e-13,  6.3964e-14,  1.5285e-13,\n",
            "          6.2323e-14,  7.0629e-14,  1.5159e-13,  7.9745e-14,  9.8133e-14,\n",
            "          7.9410e-14,  9.6094e-14,  9.5865e-14,  9.9962e-14,  1.5483e-13,\n",
            "          6.0493e-14,  9.5589e-14,  6.3844e-14,  1.0791e-13,  4.8634e-14],\n",
            "        [-1.4715e-13, -1.6265e-13, -1.5316e-13, -1.7030e-13, -2.2734e-14,\n",
            "         -5.4672e-14, -1.1952e-15, -9.1191e-14, -7.2960e-14, -2.6034e-13,\n",
            "         -2.4930e-14, -1.6188e-13, -1.3457e-13, -8.3698e-14, -1.3636e-13,\n",
            "         -2.2134e-13,  3.8377e-14, -1.4105e-13, -1.1224e-13, -4.7888e-14],\n",
            "        [ 6.0032e-14,  6.5476e-14,  8.2512e-14,  1.7440e-14,  5.9986e-14,\n",
            "          5.3277e-14,  3.8193e-14,  5.9843e-14,  2.5594e-14,  9.7111e-14,\n",
            "          4.6053e-14,  2.4480e-14,  2.0317e-14,  3.3029e-14,  5.7972e-14,\n",
            "          4.3701e-14,  5.5011e-14,  4.3033e-14,  3.0872e-14,  7.7510e-14],\n",
            "        [-1.1552e-14, -2.4619e-14, -5.2439e-15, -2.2164e-14, -2.2680e-14,\n",
            "         -5.0343e-15, -1.3553e-14, -3.5536e-14, -1.4408e-14, -9.6479e-15,\n",
            "          1.0503e-15,  1.2872e-14, -4.9119e-15, -1.0211e-14, -1.2426e-14,\n",
            "         -1.4447e-14,  1.3410e-14, -1.9943e-15, -2.5541e-15, -2.2704e-14],\n",
            "        [-7.8100e-15, -4.8477e-15, -1.5779e-14, -2.2416e-14, -2.0776e-14,\n",
            "         -1.0224e-14, -2.5085e-14, -3.3724e-14, -1.8287e-14, -2.7168e-14,\n",
            "         -1.5745e-14, -2.7542e-14, -9.7785e-15,  1.4000e-15, -2.7270e-14,\n",
            "         -3.2888e-14, -3.0592e-14, -3.0222e-14, -2.4746e-14, -2.5661e-14]])\n",
            "DEBUGGING: training for one iteration takes 0.004582 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 3\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [56]\n",
            "DEBUGGING: rel_reward looks like: 0.0003597018465518302\n",
            "DEBUGGING: rel_reward looks like: 0.014213206274883221\n",
            "DEBUGGING: rel_reward looks like: 0.005348542926889081\n",
            "DEBUGGING: rel_reward looks like: 0.00024042009671138177\n",
            "DEBUGGING: rel_reward looks like: 0.002510720240650794\n",
            "DEBUGGING: rel_reward looks like: 0.00829347073753899\n",
            "DEBUGGING: rel_reward looks like: 0.0019425879041588886\n",
            "DEBUGGING: rel_reward looks like: 0.00030532381588152486\n",
            "DEBUGGING: rel_reward looks like: 0.005787030867637255\n",
            "DEBUGGING: rel_reward looks like: 0.00024258674579752867\n",
            "DEBUGGING: the action_prob is: tensor([0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [60]\n",
            "DEBUGGING: rel_reward looks like: 0.0032573306822434294\n",
            "DEBUGGING: rel_reward looks like: 0.00029803577096225605\n",
            "DEBUGGING: rel_reward looks like: 0.0011068436019646022\n",
            "DEBUGGING: rel_reward looks like: 0.005079962766733797\n",
            "DEBUGGING: rel_reward looks like: 0.0006237400548376932\n",
            "DEBUGGING: rel_reward looks like: 0.00010992793857856225\n",
            "DEBUGGING: rel_reward looks like: 9.173477482881336e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00013672382977939642\n",
            "DEBUGGING: rel_reward looks like: 2.7587023026797847e-05\n",
            "DEBUGGING: rel_reward looks like: 5.7043546034292775e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [3]\n",
            "DEBUGGING: rel_reward looks like: 0.0004833816580664075\n",
            "DEBUGGING: rel_reward looks like: 7.018430055021109e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00032281896644286275\n",
            "DEBUGGING: rel_reward looks like: 0.00040987814852893716\n",
            "DEBUGGING: rel_reward looks like: 2.575502620268378e-06\n",
            "DEBUGGING: rel_reward looks like: 0.0002877870705233977\n",
            "DEBUGGING: rel_reward looks like: 5.235034412294737e-06\n",
            "DEBUGGING: rel_reward looks like: 1.3202992622514852e-05\n",
            "DEBUGGING: rel_reward looks like: 7.652512330782947e-05\n",
            "DEBUGGING: rel_reward looks like: 4.164845411149535e-06\n",
            "DEBUGGING: the action_prob is: tensor([0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [86]\n",
            "DEBUGGING: rel_reward looks like: 2.6861821183458337e-06\n",
            "DEBUGGING: rel_reward looks like: 4.1475622041359873e-05\n",
            "DEBUGGING: rel_reward looks like: 1.2467526546550445e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0001490913210248468\n",
            "DEBUGGING: rel_reward looks like: 2.641168025925286e-05\n",
            "DEBUGGING: rel_reward looks like: 1.414402528013059e-05\n",
            "DEBUGGING: rel_reward looks like: 2.737440137599441e-05\n",
            "DEBUGGING: rel_reward looks like: 1.1180533074660251e-05\n",
            "DEBUGGING: rel_reward looks like: 3.220872032708898e-05\n",
            "DEBUGGING: rel_reward looks like: 5.398930462685032e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [55]\n",
            "DEBUGGING: rel_reward looks like: 1.530015547751688e-06\n",
            "DEBUGGING: rel_reward looks like: 3.178441593161557e-07\n",
            "DEBUGGING: rel_reward looks like: 1.2407470790401694e-05\n",
            "DEBUGGING: rel_reward looks like: 2.4028799014022156e-05\n",
            "DEBUGGING: rel_reward looks like: 4.677041414382207e-05\n",
            "DEBUGGING: rel_reward looks like: 3.910905613060586e-06\n",
            "DEBUGGING: rel_reward looks like: 8.874376905284388e-07\n",
            "DEBUGGING: rel_reward looks like: 3.161865416801585e-06\n",
            "DEBUGGING: rel_reward looks like: 2.1388012849915602e-06\n",
            "DEBUGGING: rel_reward looks like: 2.8430311890702594e-06\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.1472873961552068 and immediate abs rewards look like: [0.0010383661865489557, 0.04101509145812088, 0.01521493519339856, 0.0006802621569477196, 0.007102307023160392, 0.023401606488732796, 0.005435921898424567, 0.0008527244858669292, 0.016157390842636232, 0.0006733826530762599, 0.009039644241511269, 0.00082440559663155, 0.003060760468542867, 0.014032099366431794, 0.0017141701723630831, 0.0003019168921127857, 0.0002519217032386223, 0.0003754360377570265, 7.574207484140061e-05, 0.00015661267025279813, 0.0013270453728182474, 0.0001925863934957306, 0.0008857561679178616, 0.0011242675795983814, 7.061531050567282e-06, 0.0007890545653026493, 1.4349286175274756e-05, 3.618935670601786e-05, 0.0002097523265547352, 1.1414802429499105e-05, 7.362124506471446e-06, 0.00011367355591573869, 3.416872914385749e-05, 0.00040859728142095264, 7.237263525894377e-05, 3.875608581438428e-05, 7.500761785195209e-05, 3.06345400531427e-05, 8.825055965644424e-05, 0.00014792369393035187, 4.191817879473092e-06, 8.708034329174552e-07, 3.399296292627696e-05, 6.583130061699194e-05, 0.00012813304601877462, 1.0713884421420516e-05, 2.43111662712181e-06, 8.661855190439383e-06, 5.859176781086717e-06, 7.78837511461461e-06]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.05217730098970085 and immediate relative rewards look like: [0.0003597018465518302, 0.014213206274883221, 0.005348542926889081, 0.00024042009671138177, 0.002510720240650794, 0.00829347073753899, 0.0019425879041588886, 0.00030532381588152486, 0.005787030867637255, 0.00024258674579752867, 0.0032573306822434294, 0.00029803577096225605, 0.0011068436019646022, 0.005079962766733797, 0.0006237400548376932, 0.00010992793857856225, 9.173477482881336e-05, 0.00013672382977939642, 2.7587023026797847e-05, 5.7043546034292775e-05, 0.0004833816580664075, 7.018430055021109e-05, 0.00032281896644286275, 0.00040987814852893716, 2.575502620268378e-06, 0.0002877870705233977, 5.235034412294737e-06, 1.3202992622514852e-05, 7.652512330782947e-05, 4.164845411149535e-06, 2.6861821183458337e-06, 4.1475622041359873e-05, 1.2467526546550445e-05, 0.0001490913210248468, 2.641168025925286e-05, 1.414402528013059e-05, 2.737440137599441e-05, 1.1180533074660251e-05, 3.220872032708898e-05, 5.398930462685032e-05, 1.530015547751688e-06, 3.178441593161557e-07, 1.2407470790401694e-05, 2.4028799014022156e-05, 4.677041414382207e-05, 3.910905613060586e-06, 8.874376905284388e-07, 3.161865416801585e-06, 2.1388012849915602e-06, 2.8430311890702594e-06]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [44]\n",
            "DEBUGGING: rel_reward looks like: 0.0073686166379679586\n",
            "DEBUGGING: rel_reward looks like: 0.00141698525240936\n",
            "DEBUGGING: rel_reward looks like: 0.000340811259174877\n",
            "DEBUGGING: rel_reward looks like: 0.012106147532069182\n",
            "DEBUGGING: rel_reward looks like: 0.0017078735920970053\n",
            "DEBUGGING: rel_reward looks like: 0.004010996174912289\n",
            "DEBUGGING: rel_reward looks like: 0.0005727952543422567\n",
            "DEBUGGING: rel_reward looks like: 0.0051155145012783945\n",
            "DEBUGGING: rel_reward looks like: 0.0001403826065005745\n",
            "DEBUGGING: rel_reward looks like: 3.900526904086796e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [21]\n",
            "DEBUGGING: rel_reward looks like: 1.225376388428278e-05\n",
            "DEBUGGING: rel_reward looks like: 1.9338451773148822e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00020597471183205474\n",
            "DEBUGGING: rel_reward looks like: 0.0017820913703408766\n",
            "DEBUGGING: rel_reward looks like: 3.96599285787306e-05\n",
            "DEBUGGING: rel_reward looks like: 2.8797829998291622e-05\n",
            "DEBUGGING: rel_reward looks like: 6.268462573039949e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0006168523690502202\n",
            "DEBUGGING: rel_reward looks like: 0.0009030738022210916\n",
            "DEBUGGING: rel_reward looks like: 0.0016519948053161152\n",
            "DEBUGGING: the action_prob is: tensor([0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [77]\n",
            "DEBUGGING: rel_reward looks like: 7.72280361371317e-05\n",
            "DEBUGGING: rel_reward looks like: 1.0749542744123754e-05\n",
            "DEBUGGING: rel_reward looks like: 3.955426643720289e-05\n",
            "DEBUGGING: rel_reward looks like: 5.508155795854803e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00016524631119368887\n",
            "DEBUGGING: rel_reward looks like: 1.3853227589806779e-06\n",
            "DEBUGGING: rel_reward looks like: 0.00010081683825518832\n",
            "DEBUGGING: rel_reward looks like: 0.00035820305531290803\n",
            "DEBUGGING: rel_reward looks like: 0.0005993353641915058\n",
            "DEBUGGING: rel_reward looks like: 3.5417605534558702e-06\n",
            "DEBUGGING: the action_prob is: tensor([0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [25]\n",
            "DEBUGGING: rel_reward looks like: 2.6361135658074127e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00013919900701259713\n",
            "DEBUGGING: rel_reward looks like: 0.0004001746900115617\n",
            "DEBUGGING: rel_reward looks like: 0.0007455299058579548\n",
            "DEBUGGING: rel_reward looks like: 2.055200457963061e-05\n",
            "DEBUGGING: rel_reward looks like: 4.853243495439889e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0001052053945273765\n",
            "DEBUGGING: rel_reward looks like: 0.00017378654167182373\n",
            "DEBUGGING: rel_reward looks like: 2.522840769267106e-06\n",
            "DEBUGGING: rel_reward looks like: 0.000129760379327654\n",
            "DEBUGGING: the action_prob is: tensor([0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [12]\n",
            "DEBUGGING: rel_reward looks like: 2.7551051543241278e-05\n",
            "DEBUGGING: rel_reward looks like: 6.101172837945787e-05\n",
            "DEBUGGING: rel_reward looks like: 3.0306061691779205e-05\n",
            "DEBUGGING: rel_reward looks like: 7.2265474702362805e-06\n",
            "DEBUGGING: rel_reward looks like: 3.0344527306838183e-06\n",
            "DEBUGGING: rel_reward looks like: 7.67394140811463e-05\n",
            "DEBUGGING: rel_reward looks like: 1.9253946632320163e-05\n",
            "DEBUGGING: rel_reward looks like: 6.747438520141541e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00018646898724322542\n",
            "DEBUGGING: rel_reward looks like: 1.6315928307854488e-05\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.1186473824773202 and immediate abs rewards look like: [0.021271290186177794, 0.004060328786181344, 0.0009752006462804275, 0.03462884217606188, 0.004826118875371321, 0.011314938177747536, 0.001609362554972904, 0.014364647339334624, 0.00039218556821651873, 0.00010895335617533419, 3.4227084142912645e-05, 5.40152977919206e-05, 0.0005753082186856773, 0.004976536326921632, 0.00011055400909754098, 8.027218927963986e-05, 0.00017472452736910782, 0.0017192809445987223, 0.0025154800555355905, 0.004597417394961667, 0.0002145666412616265, 2.9863703275623266e-05, 0.00010988599296979373, 0.00015301642088161316, 0.0004590285257108917, 3.8475750443467405e-06, 0.00028000681504636304, 0.0009947662169906835, 0.0016638188981232815, 9.826412224356318e-06, 7.313720516322064e-05, 0.00038618814051005756, 0.0011100740625806793, 0.0020672527525675832, 5.694541960110655e-05, 0.00013447073069983162, 0.00029148258681743755, 0.0004814431968043209, 6.987845154071692e-06, 0.0003594135473576898, 7.630169011463295e-05, 0.00016896522902243305, 8.39241629364551e-05, 2.0011295873700874e-05, 8.4027528828301e-06, 0.0002124997267856088, 5.331216652848525e-05, 0.00018682592553886934, 0.0005162683646631194, 4.516475928539876e-05]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.041839998631712415 and immediate relative rewards look like: [0.0073686166379679586, 0.00141698525240936, 0.000340811259174877, 0.012106147532069182, 0.0017078735920970053, 0.004010996174912289, 0.0005727952543422567, 0.0051155145012783945, 0.0001403826065005745, 3.900526904086796e-05, 1.225376388428278e-05, 1.9338451773148822e-05, 0.00020597471183205474, 0.0017820913703408766, 3.96599285787306e-05, 2.8797829998291622e-05, 6.268462573039949e-05, 0.0006168523690502202, 0.0009030738022210916, 0.0016519948053161152, 7.72280361371317e-05, 1.0749542744123754e-05, 3.955426643720289e-05, 5.508155795854803e-05, 0.00016524631119368887, 1.3853227589806779e-06, 0.00010081683825518832, 0.00035820305531290803, 0.0005993353641915058, 3.5417605534558702e-06, 2.6361135658074127e-05, 0.00013919900701259713, 0.0004001746900115617, 0.0007455299058579548, 2.055200457963061e-05, 4.853243495439889e-05, 0.0001052053945273765, 0.00017378654167182373, 2.522840769267106e-06, 0.000129760379327654, 2.7551051543241278e-05, 6.101172837945787e-05, 3.0306061691779205e-05, 7.2265474702362805e-06, 3.0344527306838183e-06, 7.67394140811463e-05, 1.9253946632320163e-05, 6.747438520141541e-05, 0.00018646898724322542, 1.6315928307854488e-05]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [29]\n",
            "DEBUGGING: rel_reward looks like: 0.009654505119186309\n",
            "DEBUGGING: rel_reward looks like: 0.0001589936399658115\n",
            "DEBUGGING: rel_reward looks like: 0.000322463995118008\n",
            "DEBUGGING: rel_reward looks like: 0.030941809674093337\n",
            "DEBUGGING: rel_reward looks like: 0.003829226700344953\n",
            "DEBUGGING: rel_reward looks like: 4.8302488138098986e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0027034203813025297\n",
            "DEBUGGING: rel_reward looks like: 0.0017870596205143987\n",
            "DEBUGGING: rel_reward looks like: 0.00012426966954110283\n",
            "DEBUGGING: rel_reward looks like: 0.00019265729014948587\n",
            "DEBUGGING: the action_prob is: tensor([0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [0]\n",
            "DEBUGGING: rel_reward looks like: 0.2714288509007282\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508865675e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: the action_prob is: tensor([0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [39]\n",
            "DEBUGGING: rel_reward looks like: 4.547473508863607e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508865675e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508863607e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: the action_prob is: tensor([0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [27]\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [51]\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.88674132898268 and immediate abs rewards look like: [0.027870058938333386, 0.00045454234941644245, 0.0009217364777214243, 0.08841606616260833, 0.010603430955143267, 0.00013324122983249254, 0.007456958664988633, 0.004915996029922098, 0.0003412406972529425, 0.000528965261310077, 0.7450990922061465, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 0.0, 0.0, 9.094947017729282e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.3211915594840844 and immediate relative rewards look like: [0.009654505119186309, 0.0001589936399658115, 0.000322463995118008, 0.030941809674093337, 0.003829226700344953, 4.8302488138098986e-05, 0.0027034203813025297, 0.0017870596205143987, 0.00012426966954110283, 0.00019265729014948587, 0.2714288509007282, 2.2737367544323206e-13, 0.0, 0.0, 2.2737367544318036e-13, 0.0, 2.2737367544323206e-13, 4.547473508865675e-13, 0.0, 0.0, 4.547473508863607e-13, 4.547473508865675e-13, 2.2737367544318036e-13, 0.0, 0.0, 0.0, 2.2737367544323206e-13, 4.547473508863607e-13, 2.2737367544328376e-13, 0.0, 0.0, 0.0, 0.0, 2.2737367544323206e-13, 2.2737367544328376e-13, 2.2737367544323206e-13, 2.2737367544328376e-13, 0.0, 0.0, 2.2737367544323206e-13, 0.0, 0.0, 2.2737367544318036e-13, 0.0, 2.2737367544323206e-13, 2.2737367544318036e-13, 0.0, 0.0, 0.0, 0.0]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 150 number of matrices\n",
            "DEBUGGING: ACT_MAT has 150 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.04909405958566232, 0.04922662397889948, 0.03536708858991541, 0.030321763295986195, 0.030385195150782644, 0.028156035262759442, 0.02006319649012167, 0.018303645036326043, 0.018180122444893452, 0.012518274320460807, 0.012399684418851796, 0.009234700744048856, 0.00902693431624909, 0.008000091630590392, 0.0029496251150066617, 0.002349378848655524, 0.0022620716263403655, 0.0021922594459712645, 0.002076298602214008, 0.0020694056355426366, 0.0020326889793013574, 0.0015649568901363131, 0.0015098713026122242, 0.001199042763807436, 0.0007971359750287866, 0.0008025863357661799, 0.0005199992578209921, 0.0005199638620289873, 0.000511879666067144, 0.0004397520633932469, 0.00043998708887080544, 0.00044171808762874705, 0.00040428531877513855, 0.0003957755477056446, 0.0002491759865462604, 0.0002250144507949571, 0.0002130004298129561, 0.00018750103882521382, 0.00017810152096015512, 0.0001473664652859254, 9.432036430209602e-05, 9.372762500438822e-05, 9.435331398492127e-05, 8.277357898436321e-05, 5.9338161586203084e-05, 1.2694694386243447e-05, 8.87251391230592e-06, 8.065733557350991e-06, 4.953402162171117e-06, 2.8430311890702594e-06], [0.038980491019584264, 0.03193118624405687, 0.030822425244088392, 0.03078950907567022, 0.018872082367273776, 0.017337584621390678, 0.01346120045098827, 0.013018591107723246, 0.007982905663075609, 0.007921740461186903, 0.007962358779945491, 0.008030409107132534, 0.008091990560969075, 0.00796567257488588, 0.006246041620752529, 0.00626907241633717, 0.006303307662968564, 0.006303659633573904, 0.005744249762145135, 0.004890076727196003, 0.0032707898200806953, 0.003225819983781377, 0.0032475459000376296, 0.0032403955894953805, 0.003217488920744275, 0.00308307334298039, 0.003112816182041828, 0.003042423579582464, 0.0027113338628985415, 0.0021333318168757936, 0.0021513030871942807, 0.002146406011652734, 0.002027481822868825, 0.0016437445786437002, 0.0009072875482684298, 0.0008956924683725245, 0.0008557172054728541, 0.0007580927383287652, 0.0005902082794514561, 0.0005936216552345344, 0.00046854674334028323, 0.00044545019373438586, 0.00038832168217669497, 0.0003616319398837533, 0.0003579852448621384, 0.00035853615366813595, 0.00028464317129998955, 0.00026806992390673677, 0.00020262175626800137, 1.6315928307854488e-05], [0.2938519631504187, 0.2870681394254873, 0.2898072179651732, 0.29240884239399517, 0.2641081138584867, 0.26290796682640577, 0.26551481246289665, 0.2654660526076708, 0.26634241715874385, 0.2689072196860634, 0.27142885090496355, 4.278116222912574e-12, 4.091659138857922e-12, 4.132989029149416e-12, 4.174736393080218e-12, 3.98723506832024e-12, 4.027510170020445e-12, 3.838521711694155e-12, 3.4179538998056435e-12, 3.4524786866723673e-12, 3.487352208759967e-12, 3.06323723017536e-12, 2.634838261907871e-12, 2.4317824105703946e-12, 2.456345869263025e-12, 2.4811574437000253e-12, 2.5062196401010355e-12, 2.301864610765458e-12, 1.865775009978886e-12, 1.6549508429652546e-12, 1.6716675181467219e-12, 1.6885530486330524e-12, 1.7056091400333862e-12, 1.7228375151852386e-12, 1.510569535092936e-12, 1.2961574339895476e-12, 1.0795795540871873e-12, 8.608140188322259e-13, 8.695091099315413e-13, 8.782920302338801e-13, 6.574932876673212e-13, 6.641346340073952e-13, 6.708430646539345e-13, 4.479488779906608e-13, 4.524736141319806e-13, 2.2737367544318036e-13, 0.0, 0.0, 0.0, 0.0]]\n",
            "DEBUGGING: traj_returns = [0.04909405958566232, 0.038980491019584264, 0.2938519631504187]\n",
            "DEBUGGING: actions = [[56], [41], [26], [14], [1], [33], [5], [31], [1], [58], [60], [61], [69], [53], [54], [50], [64], [21], [18], [18], [3], [51], [73], [29], [52], [44], [32], [60], [18], [58], [86], [85], [62], [72], [18], [81], [75], [74], [41], [18], [55], [52], [14], [102], [84], [92], [45], [56], [12], [65], [44], [61], [11], [34], [45], [40], [34], [47], [30], [12], [21], [35], [48], [60], [30], [64], [50], [8], [1], [44], [77], [46], [50], [5], [29], [11], [40], [41], [61], [56], [25], [83], [22], [52], [78], [87], [78], [27], [81], [46], [12], [29], [9], [99], [19], [98], [74], [5], [102], [68], [29], [50], [45], [60], [27], [29], [55], [39], [14], [52], [0], [46], [50], [40], [61], [20], [61], [32], [44], [41], [39], [56], [63], [63], [3], [12], [45], [70], [72], [47], [27], [14], [58], [19], [79], [83], [40], [33], [85], [49], [51], [84], [54], [6], [4], [87], [5], [23], [72], [103]]\n",
            "DEBUGGING: actions length = 150\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        ...,\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        ...,\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[1.27308838e-01 1.22741983e-01 1.18665577e-01 1.17840038e-01\n",
            "  1.04455130e-01 1.02800529e-01 9.96797365e-02 9.89294296e-02\n",
            "  9.75018151e-02 9.64490782e-02 9.72636314e-02 5.75503662e-03\n",
            "  5.70630829e-03 5.32192140e-03 3.06522225e-03 2.87281709e-03\n",
            "  2.85512643e-03 2.83197303e-03 2.60684946e-03 2.31982746e-03\n",
            "  1.76782627e-03 1.59692563e-03 1.58580574e-03 1.47981279e-03\n",
            "  1.33820830e-03 1.29521989e-03 1.21093848e-03 1.18746248e-03\n",
            "  1.07440451e-03 8.57694627e-04 8.63763393e-04 8.62708034e-04\n",
            "  8.10589048e-04 6.79840043e-04 3.85487845e-04 3.73568973e-04\n",
            "  3.56239212e-04 3.15197926e-04 2.56103267e-04 2.46996040e-04\n",
            "  1.87622369e-04 1.79725940e-04 1.60891666e-04 1.48135173e-04\n",
            "  1.39107802e-04 1.23743616e-04 9.78385617e-05 9.20452192e-05\n",
            "  6.91917195e-05 6.38631983e-06]]\n",
            "DEBUGGING: baseline2 looks like: 0.1273088379185551\n",
            "DEBUGGING: ADS looks like: [4.90940596e-02 4.92266240e-02 3.53670886e-02 3.03217633e-02\n",
            " 3.03851952e-02 2.81560353e-02 2.00631965e-02 1.83036450e-02\n",
            " 1.81801224e-02 1.25182743e-02 1.23996844e-02 9.23470074e-03\n",
            " 9.02693432e-03 8.00009163e-03 2.94962512e-03 2.34937885e-03\n",
            " 2.26207163e-03 2.19225945e-03 2.07629860e-03 2.06940564e-03\n",
            " 2.03268898e-03 1.56495689e-03 1.50987130e-03 1.19904276e-03\n",
            " 7.97135975e-04 8.02586336e-04 5.19999258e-04 5.19963862e-04\n",
            " 5.11879666e-04 4.39752063e-04 4.39987089e-04 4.41718088e-04\n",
            " 4.04285319e-04 3.95775548e-04 2.49175987e-04 2.25014451e-04\n",
            " 2.13000430e-04 1.87501039e-04 1.78101521e-04 1.47366465e-04\n",
            " 9.43203643e-05 9.37276250e-05 9.43533140e-05 8.27735790e-05\n",
            " 5.93381616e-05 1.26946944e-05 8.87251391e-06 8.06573356e-06\n",
            " 4.95340216e-06 2.84303119e-06 3.89804910e-02 3.19311862e-02\n",
            " 3.08224252e-02 3.07895091e-02 1.88720824e-02 1.73375846e-02\n",
            " 1.34612005e-02 1.30185911e-02 7.98290566e-03 7.92174046e-03\n",
            " 7.96235878e-03 8.03040911e-03 8.09199056e-03 7.96567257e-03\n",
            " 6.24604162e-03 6.26907242e-03 6.30330766e-03 6.30365963e-03\n",
            " 5.74424976e-03 4.89007673e-03 3.27078982e-03 3.22581998e-03\n",
            " 3.24754590e-03 3.24039559e-03 3.21748892e-03 3.08307334e-03\n",
            " 3.11281618e-03 3.04242358e-03 2.71133386e-03 2.13333182e-03\n",
            " 2.15130309e-03 2.14640601e-03 2.02748182e-03 1.64374458e-03\n",
            " 9.07287548e-04 8.95692468e-04 8.55717205e-04 7.58092738e-04\n",
            " 5.90208279e-04 5.93621655e-04 4.68546743e-04 4.45450194e-04\n",
            " 3.88321682e-04 3.61631940e-04 3.57985245e-04 3.58536154e-04\n",
            " 2.84643171e-04 2.68069924e-04 2.02621756e-04 1.63159283e-05\n",
            " 2.93851963e-01 2.87068139e-01 2.89807218e-01 2.92408842e-01\n",
            " 2.64108114e-01 2.62907967e-01 2.65514812e-01 2.65466053e-01\n",
            " 2.66342417e-01 2.68907220e-01 2.71428851e-01 4.27811622e-12\n",
            " 4.09165914e-12 4.13298903e-12 4.17473639e-12 3.98723507e-12\n",
            " 4.02751017e-12 3.83852171e-12 3.41795390e-12 3.45247869e-12\n",
            " 3.48735221e-12 3.06323723e-12 2.63483826e-12 2.43178241e-12\n",
            " 2.45634587e-12 2.48115744e-12 2.50621964e-12 2.30186461e-12\n",
            " 1.86577501e-12 1.65495084e-12 1.67166752e-12 1.68855305e-12\n",
            " 1.70560914e-12 1.72283752e-12 1.51056954e-12 1.29615743e-12\n",
            " 1.07957955e-12 8.60814019e-13 8.69509110e-13 8.78292030e-13\n",
            " 6.57493288e-13 6.64134634e-13 6.70843065e-13 4.47948878e-13\n",
            " 4.52473614e-13 2.27373675e-13 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(0.1025, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-5.3497e-36, -5.7637e-36, -5.3348e-36,  ..., -5.8453e-36,\n",
            "         -5.1167e-36, -1.6074e-33],\n",
            "        [ 3.3841e-39,  3.8431e-39,  3.4072e-39,  ...,  7.1929e-39,\n",
            "          4.7271e-39,  1.2763e-36],\n",
            "        ...,\n",
            "        [-1.1404e-30, -1.2853e-30, -1.0644e-30,  ..., -2.0284e-30,\n",
            "         -1.2305e-30, -3.6694e-28],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]])\n",
            "   Last layer:\n",
            "tensor([[ 4.5864e-14,  1.3622e-13, -4.9234e-15,  2.6300e-13,  2.6299e-14,\n",
            "          7.6426e-14,  9.1693e-14, -3.1471e-14,  9.2573e-14,  1.6865e-13,\n",
            "          1.1626e-13, -1.3196e-14,  8.7977e-14,  8.5553e-14,  1.5782e-13,\n",
            "          9.2034e-14, -1.6989e-14,  1.2371e-13,  1.1295e-13,  1.3996e-13],\n",
            "        [ 1.5202e-13,  1.0649e-13,  9.1107e-14,  1.6111e-13,  2.7186e-13,\n",
            "          5.2704e-14,  1.9176e-13,  7.9985e-14,  1.8139e-13,  1.7107e-13,\n",
            "          9.9473e-14,  9.4574e-14,  1.4555e-13,  1.6987e-13,  9.8791e-14,\n",
            "          1.0135e-13,  1.7620e-13,  1.3288e-13,  1.3124e-13,  2.4644e-13],\n",
            "        [-1.8450e-14,  3.8747e-15,  1.0522e-14,  7.5031e-14,  2.9264e-14,\n",
            "         -1.9408e-14, -3.8559e-14,  1.0596e-13, -1.3313e-14,  3.7051e-14,\n",
            "          2.1968e-14,  4.1015e-14,  4.8766e-14,  2.6967e-14, -3.1432e-15,\n",
            "          5.4679e-14, -3.3787e-14,  2.0117e-14,  2.6503e-14,  1.7895e-14],\n",
            "        [ 6.0075e-14,  5.2723e-14,  3.3727e-14,  1.9271e-14,  4.4742e-14,\n",
            "          1.0234e-14,  3.1688e-14,  3.7802e-14,  3.9891e-14,  6.1884e-14,\n",
            "          4.1938e-14,  2.5438e-15, -5.8303e-14,  2.6156e-14,  2.5535e-15,\n",
            "         -2.9918e-14,  1.3038e-15, -1.8999e-14, -5.1887e-15,  3.3516e-14],\n",
            "        [-1.2106e-13, -1.1274e-13,  2.6734e-14, -4.7738e-14, -9.1045e-14,\n",
            "         -9.0795e-14, -2.5961e-14, -9.7497e-14, -1.0595e-13, -5.2691e-14,\n",
            "         -6.6899e-15, -6.6969e-14, -8.8794e-14, -1.4800e-13, -1.4546e-13,\n",
            "         -8.4534e-14, -1.0686e-13, -1.1443e-13, -9.3777e-14, -1.5052e-13],\n",
            "        [ 3.6594e-14,  1.0971e-13,  1.0680e-13,  6.3964e-14,  1.5285e-13,\n",
            "          6.2323e-14,  7.0629e-14,  1.5159e-13,  7.9745e-14,  9.8133e-14,\n",
            "          7.9410e-14,  9.6094e-14,  9.5865e-14,  9.9962e-14,  1.5483e-13,\n",
            "          6.0493e-14,  9.5589e-14,  6.3844e-14,  1.0791e-13,  4.8634e-14],\n",
            "        [-1.4715e-13, -1.6265e-13, -1.5316e-13, -1.7030e-13, -2.2734e-14,\n",
            "         -5.4672e-14, -1.1952e-15, -9.1191e-14, -7.2960e-14, -2.6034e-13,\n",
            "         -2.4930e-14, -1.6188e-13, -1.3457e-13, -8.3698e-14, -1.3636e-13,\n",
            "         -2.2134e-13,  3.8377e-14, -1.4105e-13, -1.1224e-13, -4.7888e-14],\n",
            "        [ 6.0032e-14,  6.5476e-14,  8.2512e-14,  1.7440e-14,  5.9986e-14,\n",
            "          5.3277e-14,  3.8193e-14,  5.9843e-14,  2.5594e-14,  9.7111e-14,\n",
            "          4.6053e-14,  2.4480e-14,  2.0317e-14,  3.3029e-14,  5.7972e-14,\n",
            "          4.3701e-14,  5.5011e-14,  4.3033e-14,  3.0872e-14,  7.7510e-14],\n",
            "        [-1.1552e-14, -2.4619e-14, -5.2439e-15, -2.2164e-14, -2.2680e-14,\n",
            "         -5.0343e-15, -1.3553e-14, -3.5536e-14, -1.4408e-14, -9.6479e-15,\n",
            "          1.0503e-15,  1.2872e-14, -4.9119e-15, -1.0211e-14, -1.2426e-14,\n",
            "         -1.4447e-14,  1.3410e-14, -1.9943e-15, -2.5541e-15, -2.2704e-14],\n",
            "        [-7.8100e-15, -4.8477e-15, -1.5779e-14, -2.2416e-14, -2.0776e-14,\n",
            "         -1.0224e-14, -2.5085e-14, -3.3724e-14, -1.8287e-14, -2.7168e-14,\n",
            "         -1.5745e-14, -2.7542e-14, -9.7785e-15,  1.4000e-15, -2.7270e-14,\n",
            "         -3.2888e-14, -3.0592e-14, -3.0222e-14, -2.4746e-14, -2.5661e-14]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 3.3391e-36,  3.6052e-36,  3.3295e-36,  ...,  3.6452e-36,\n",
            "          3.1910e-36,  1.0033e-33],\n",
            "        [-3.4784e-39, -3.9530e-39, -3.5023e-39,  ..., -7.3916e-39,\n",
            "         -4.8574e-39, -1.3119e-36],\n",
            "        ...,\n",
            "        [ 2.8526e-30,  3.2352e-30,  2.6858e-30,  ...,  4.9838e-30,\n",
            "          3.0381e-30,  9.1498e-28],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]])\n",
            "   Last layer:\n",
            "tensor([[ 3.8711e-15, -3.3062e-14,  1.8480e-14,  6.4958e-14,  4.0592e-14,\n",
            "          3.5461e-14,  7.9038e-15,  5.6924e-14,  2.3906e-14,  3.6693e-15,\n",
            "          1.6381e-14,  4.7816e-14,  1.3835e-13,  2.3103e-14, -4.0018e-15,\n",
            "         -2.5411e-14,  1.0368e-13, -3.6186e-14,  8.6276e-15,  7.0569e-14],\n",
            "        [ 5.8392e-14,  9.1421e-14,  8.2556e-14,  1.1118e-13,  8.1726e-14,\n",
            "          1.7037e-14,  3.2854e-14,  2.5432e-14,  5.6369e-14,  1.1003e-13,\n",
            "          1.0222e-13,  2.2461e-13,  7.0932e-14,  7.4508e-14,  1.0396e-13,\n",
            "          6.7832e-14,  1.1449e-13,  2.6485e-14,  3.3941e-14,  1.2485e-13],\n",
            "        [-1.9167e-14, -1.3957e-14, -1.0694e-14, -1.4141e-14, -2.0330e-14,\n",
            "         -2.6080e-14, -3.6465e-14, -2.3663e-15, -8.1794e-15,  3.4307e-14,\n",
            "          5.3268e-15, -1.3908e-14,  1.9212e-14,  3.3649e-16, -1.3085e-14,\n",
            "         -2.0719e-14,  2.7110e-15, -3.5239e-14, -7.3303e-15, -2.0232e-15],\n",
            "        [-1.2443e-15, -3.9574e-15, -3.0732e-14, -1.6435e-14, -1.5808e-14,\n",
            "         -2.8719e-14, -2.4232e-14, -3.1463e-14, -1.1689e-14,  3.2013e-14,\n",
            "         -1.8201e-14, -3.3628e-14, -2.3531e-14,  7.5208e-15,  3.0092e-14,\n",
            "         -3.9540e-14,  3.6970e-14, -2.5366e-15, -1.3172e-15,  4.3146e-15],\n",
            "        [-6.5905e-14, -2.6840e-15, -3.7403e-14, -3.1415e-14,  1.3039e-14,\n",
            "         -1.0103e-14,  2.0033e-14, -4.2948e-14, -2.1722e-14,  3.6322e-14,\n",
            "          1.9486e-14, -9.5788e-15, -4.9690e-15,  4.6296e-14,  2.3242e-14,\n",
            "          5.4842e-14,  2.5944e-14,  2.8605e-16, -4.7306e-14, -3.6829e-14],\n",
            "        [-1.1958e-13, -1.3236e-13, -7.2919e-14, -9.7182e-14, -1.3695e-13,\n",
            "         -7.1907e-14, -8.7806e-14, -1.9087e-13, -9.4240e-14, -1.1313e-13,\n",
            "         -6.7037e-14, -1.4968e-13, -1.0367e-13, -1.4060e-13, -8.4433e-14,\n",
            "         -1.3876e-13, -7.1353e-14, -1.1791e-13, -8.6192e-14, -7.4385e-14],\n",
            "        [ 1.0194e-14, -9.8683e-14, -8.4164e-14, -1.1635e-13, -3.3745e-14,\n",
            "         -7.9213e-14,  5.1653e-14, -3.3546e-14, -3.1136e-14,  1.3390e-14,\n",
            "         -4.0975e-14, -5.9733e-14, -5.4428e-14,  1.9945e-14,  1.9134e-14,\n",
            "         -2.5068e-14, -2.3611e-14, -6.6436e-14,  5.0306e-15, -1.7244e-14],\n",
            "        [-1.4996e-15,  1.5163e-15,  4.0808e-15,  2.3743e-15,  2.7846e-14,\n",
            "         -1.1807e-14, -4.8796e-15, -6.5266e-15,  1.9738e-14,  2.6101e-16,\n",
            "          3.2968e-15, -1.5584e-15,  2.3975e-14,  1.9218e-15,  2.4019e-14,\n",
            "          1.3859e-14,  2.4669e-14,  2.2542e-14,  2.3158e-14,  3.5164e-14],\n",
            "        [ 8.7244e-15, -5.3845e-15, -1.1222e-14, -1.9739e-14, -1.9608e-14,\n",
            "         -8.7514e-15,  4.5138e-15, -2.4529e-15, -6.9092e-15, -2.2572e-14,\n",
            "         -1.0126e-14,  1.0416e-15,  8.7068e-15, -1.4167e-14, -9.2550e-15,\n",
            "         -9.9150e-15, -1.1001e-14, -3.4303e-15, -7.9011e-16, -6.1227e-15],\n",
            "        [ 1.3342e-14, -6.8706e-16,  1.6371e-15, -7.5102e-15, -7.3208e-15,\n",
            "         -6.1990e-15,  2.1194e-15, -7.1471e-15,  9.1782e-15,  1.9125e-15,\n",
            "          2.8857e-15,  4.8096e-15, -1.7750e-15,  8.0432e-16, -4.0548e-15,\n",
            "         -2.3238e-15, -2.9981e-16,  1.0954e-15,  7.6349e-15,  1.0075e-14]])\n",
            "DEBUGGING: training for one iteration takes 0.005611 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 4\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [36]\n",
            "DEBUGGING: rel_reward looks like: 0.0012293375073774732\n",
            "DEBUGGING: rel_reward looks like: 0.0006764587314343817\n",
            "DEBUGGING: rel_reward looks like: 0.30585495111495226\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: the action_prob is: tensor([0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [3]\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [16]\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 4.547473508863607e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 4.547473508864641e-13\n",
            "DEBUGGING: rel_reward looks like: 6.821210263300064e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [77]\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [63]\n",
            "DEBUGGING: rel_reward looks like: 4.547473508863607e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508865675e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.8867413289890465 and immediate abs rewards look like: [0.003548779389802803, 0.001950360774571891, 0.8812421888083009, 0.0, 0.0, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 9.094947017729282e-13, 1.3642420526593924e-12, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 0.0]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.3077607473619496 and immediate relative rewards look like: [0.0012293375073774732, 0.0006764587314343817, 0.30585495111495226, 0.0, 0.0, 0.0, 2.2737367544323206e-13, 0.0, 2.2737367544318036e-13, 0.0, 2.2737367544323206e-13, 2.2737367544328376e-13, 2.2737367544323206e-13, 2.2737367544328376e-13, 0.0, 0.0, 0.0, 0.0, 2.2737367544323206e-13, 2.2737367544328376e-13, 0.0, 2.2737367544323206e-13, 0.0, 4.547473508863607e-13, 2.2737367544328376e-13, 0.0, 4.547473508864641e-13, 6.821210263300064e-13, 2.2737367544318036e-13, 2.2737367544323206e-13, 2.2737367544328376e-13, 2.2737367544323206e-13, 2.2737367544318036e-13, 2.2737367544323206e-13, 2.2737367544318036e-13, 0.0, 2.2737367544323206e-13, 2.2737367544318036e-13, 0.0, 2.2737367544323206e-13, 4.547473508863607e-13, 2.2737367544328376e-13, 0.0, 2.2737367544323206e-13, 2.2737367544318036e-13, 2.2737367544323206e-13, 4.547473508865675e-13, 2.2737367544318036e-13, 0.0, 0.0]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [55]\n",
            "DEBUGGING: rel_reward looks like: 0.016543954105431736\n",
            "DEBUGGING: rel_reward looks like: 0.011431802042001695\n",
            "DEBUGGING: rel_reward looks like: 0.003587354302401978\n",
            "DEBUGGING: rel_reward looks like: 4.55081376327941e-05\n",
            "DEBUGGING: rel_reward looks like: 0.003280088620144113\n",
            "DEBUGGING: rel_reward looks like: 0.0015123420843091656\n",
            "DEBUGGING: rel_reward looks like: 0.0019303653806666667\n",
            "DEBUGGING: rel_reward looks like: 0.0014729100911247846\n",
            "DEBUGGING: rel_reward looks like: 5.808311969274753e-06\n",
            "DEBUGGING: rel_reward looks like: 1.9567488807109925e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [38]\n",
            "DEBUGGING: rel_reward looks like: 0.001028266350052054\n",
            "DEBUGGING: rel_reward looks like: 0.0004484449356042684\n",
            "DEBUGGING: rel_reward looks like: 2.422488701038299e-06\n",
            "DEBUGGING: rel_reward looks like: 0.00024507650061635506\n",
            "DEBUGGING: rel_reward looks like: 0.00010921631557667908\n",
            "DEBUGGING: rel_reward looks like: 0.0001497314763035582\n",
            "DEBUGGING: rel_reward looks like: 0.27743465383954985\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [7]\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508863607e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [12]\n",
            "DEBUGGING: rel_reward looks like: 6.82121026329386e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: the action_prob is: tensor([0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [65]\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.886741328980861 and immediate abs rewards look like: [0.047758116060776956, 0.032454694090574776, 0.010068012156807526, 0.00012726170962196193, 0.00917222085490721, 0.004215140912492643, 0.0053721023891739605, 0.004091116450581467, 1.6109252555907005e-05, 5.426977577371872e-05, 0.002851806527814915, 0.0012424438154994277, 6.7086416493111756e-06, 0.0006786931289752829, 0.00030237985538406065, 0.0004145061757299118, 0.7679157471743565, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 1.3642420526593924e-12, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.3192475124749859 and immediate relative rewards look like: [0.016543954105431736, 0.011431802042001695, 0.003587354302401978, 4.55081376327941e-05, 0.003280088620144113, 0.0015123420843091656, 0.0019303653806666667, 0.0014729100911247846, 5.808311969274753e-06, 1.9567488807109925e-05, 0.001028266350052054, 0.0004484449356042684, 2.422488701038299e-06, 0.00024507650061635506, 0.00010921631557667908, 0.0001497314763035582, 0.27743465383954985, 2.2737367544323206e-13, 0.0, 2.2737367544328376e-13, 2.2737367544323206e-13, 4.547473508863607e-13, 2.2737367544328376e-13, 0.0, 0.0, 2.2737367544323206e-13, 2.2737367544328376e-13, 0.0, 2.2737367544323206e-13, 2.2737367544318036e-13, 6.82121026329386e-13, 2.2737367544328376e-13, 2.2737367544323206e-13, 2.2737367544318036e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.2737367544323206e-13, 2.2737367544318036e-13, 0.0]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [18]\n",
            "DEBUGGING: rel_reward looks like: 0.0016269024171890467\n",
            "DEBUGGING: rel_reward looks like: 0.002314516003094028\n",
            "DEBUGGING: rel_reward looks like: 0.003051524681018253\n",
            "DEBUGGING: rel_reward looks like: 0.010114735735418405\n",
            "DEBUGGING: rel_reward looks like: 0.02309099245513858\n",
            "DEBUGGING: rel_reward looks like: 0.0006915776165308591\n",
            "DEBUGGING: rel_reward looks like: 0.0011761997783141219\n",
            "DEBUGGING: rel_reward looks like: 0.0013988278178476406\n",
            "DEBUGGING: rel_reward looks like: 0.011401587688017053\n",
            "DEBUGGING: rel_reward looks like: 0.0002602246798627289\n",
            "DEBUGGING: the action_prob is: tensor([0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [48]\n",
            "DEBUGGING: rel_reward looks like: 0.00025776390615103294\n",
            "DEBUGGING: rel_reward looks like: 0.004986184665074378\n",
            "DEBUGGING: rel_reward looks like: 0.001155815970054265\n",
            "DEBUGGING: rel_reward looks like: 0.0013316559638235218\n",
            "DEBUGGING: rel_reward looks like: 0.004036476859968058\n",
            "DEBUGGING: rel_reward looks like: 0.003943539455301979\n",
            "DEBUGGING: rel_reward looks like: 0.00021265312230503444\n",
            "DEBUGGING: rel_reward looks like: 0.00013591582718070608\n",
            "DEBUGGING: rel_reward looks like: 0.0004518271557864208\n",
            "DEBUGGING: rel_reward looks like: 0.0012870732571378359\n",
            "DEBUGGING: the action_prob is: tensor([0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [48]\n",
            "DEBUGGING: rel_reward looks like: 0.00014010831942837213\n",
            "DEBUGGING: rel_reward looks like: 0.000299975090546401\n",
            "DEBUGGING: rel_reward looks like: 3.8402764989519284e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0004867141359943327\n",
            "DEBUGGING: rel_reward looks like: 4.967502447305648e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0004665784884294988\n",
            "DEBUGGING: rel_reward looks like: 3.6220268584082684e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00024534951980696577\n",
            "DEBUGGING: rel_reward looks like: 5.909597561340673e-05\n",
            "DEBUGGING: rel_reward looks like: 7.894879080651296e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [72]\n",
            "DEBUGGING: rel_reward looks like: 0.0001661834273721383\n",
            "DEBUGGING: rel_reward looks like: 0.0001247529608766095\n",
            "DEBUGGING: rel_reward looks like: 7.003785793457262e-05\n",
            "DEBUGGING: rel_reward looks like: 6.003067694701663e-07\n",
            "DEBUGGING: rel_reward looks like: 8.523676191819883e-05\n",
            "DEBUGGING: rel_reward looks like: 1.0959869459159735e-05\n",
            "DEBUGGING: rel_reward looks like: 3.997513033995663e-05\n",
            "DEBUGGING: rel_reward looks like: 7.540585479052582e-05\n",
            "DEBUGGING: rel_reward looks like: 1.9372268054297736e-05\n",
            "DEBUGGING: rel_reward looks like: 4.5785802196261305e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [79]\n",
            "DEBUGGING: rel_reward looks like: 4.5347633518309865e-05\n",
            "DEBUGGING: rel_reward looks like: 2.9233155252735886e-06\n",
            "DEBUGGING: rel_reward looks like: 6.379001284190728e-06\n",
            "DEBUGGING: rel_reward looks like: 1.4419259950067863e-06\n",
            "DEBUGGING: rel_reward looks like: 1.8871732297982676e-05\n",
            "DEBUGGING: rel_reward looks like: 1.3844048432688281e-05\n",
            "DEBUGGING: rel_reward looks like: 1.6646983451739982e-07\n",
            "DEBUGGING: rel_reward looks like: 1.2645307084316084e-05\n",
            "DEBUGGING: rel_reward looks like: 4.2084511607293174e-07\n",
            "DEBUGGING: rel_reward looks like: 4.472071263128869e-06\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.21126701022330963 and immediate abs rewards look like: [0.004696446445905167, 0.0066705390022434585, 0.00877427577643175, 0.028994902144404477, 0.06552311946961709, 0.0019171098942933895, 0.003258267383898783, 0.003870426017783757, 0.03150300000788775, 0.0007108123636498931, 0.0007039074566819181, 0.013612875113267364, 0.0031397806105815107, 0.003613269839206623, 0.010937853655832441, 0.01064288242469047, 0.0005716481396120798, 0.00036528740611174726, 0.0012141657134634443, 0.0034571054884509067, 0.0003758494854082528, 0.0008045895297073002, 0.00010297252947566449, 0.0013050169968664704, 0.0001331278349425702, 0.0012503566754276108, 9.70193123066565e-05, 0.0006571673939106404, 0.0001582494228387077, 0.000211399543331936, 0.00044495079328044085, 0.0003339665399835212, 0.00018746956311588292, 1.6067219803517219e-06, 0.00022813618579675676, 2.9331589303183137e-05, 0.00010698314054025104, 0.00020179628199912258, 5.183890425541904e-05, 0.00012251739599378197, 0.0001213393525176798, 7.821734016033588e-06, 1.706784814814455e-05, 3.858036052406533e-06, 5.0493380967964185e-05, 3.704056462083827e-05, 4.4539365262608044e-07, 3.383278772162157e-05, 1.1259658094786573e-06, 1.1964965324295918e-05]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.07557188602394876 and immediate relative rewards look like: [0.0016269024171890467, 0.002314516003094028, 0.003051524681018253, 0.010114735735418405, 0.02309099245513858, 0.0006915776165308591, 0.0011761997783141219, 0.0013988278178476406, 0.011401587688017053, 0.0002602246798627289, 0.00025776390615103294, 0.004986184665074378, 0.001155815970054265, 0.0013316559638235218, 0.004036476859968058, 0.003943539455301979, 0.00021265312230503444, 0.00013591582718070608, 0.0004518271557864208, 0.0012870732571378359, 0.00014010831942837213, 0.000299975090546401, 3.8402764989519284e-05, 0.0004867141359943327, 4.967502447305648e-05, 0.0004665784884294988, 3.6220268584082684e-05, 0.00024534951980696577, 5.909597561340673e-05, 7.894879080651296e-05, 0.0001661834273721383, 0.0001247529608766095, 7.003785793457262e-05, 6.003067694701663e-07, 8.523676191819883e-05, 1.0959869459159735e-05, 3.997513033995663e-05, 7.540585479052582e-05, 1.9372268054297736e-05, 4.5785802196261305e-05, 4.5347633518309865e-05, 2.9233155252735886e-06, 6.379001284190728e-06, 1.4419259950067863e-06, 1.8871732297982676e-05, 1.3844048432688281e-05, 1.6646983451739982e-07, 1.2645307084316084e-05, 4.2084511607293174e-07, 4.472071263128869e-06]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 150 number of matrices\n",
            "DEBUGGING: ACT_MAT has 150 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.30166746924542365, 0.3034728603414608, 0.3058549511212388, 6.350049944282145e-12, 6.414191862911257e-12, 6.47898167970834e-12, 6.544425939099334e-12, 6.380860872379901e-12, 6.44531401250495e-12, 6.280747815213909e-12, 6.344189712337282e-12, 6.178602057468737e-12, 6.011341800025711e-12, 5.842392045032807e-12, 5.671735726858104e-12, 5.729025986725358e-12, 5.78689493608622e-12, 5.8453484202891105e-12, 5.904392343726374e-12, 5.734362291195093e-12, 5.562614763385666e-12, 5.618802791298653e-12, 5.4458879958135565e-12, 5.500896965468239e-12, 5.097120822809978e-12, 4.918936512491611e-12, 4.968622739890516e-12, 4.559470089903082e-12, 3.91651420562937e-12, 3.726404575945646e-12, 3.5343746469721355e-12, 3.340405021746315e-12, 3.1444761073768513e-12, 2.9465681130643142e-12, 2.7466610481021033e-12, 2.544734719857498e-12, 2.5704391109671697e-12, 2.3667327631554925e-12, 2.160968775466982e-12, 2.1827967428959416e-12, 1.9751748156087975e-12, 1.5357853179014512e-12, 1.3216279216749166e-12, 1.334977698661532e-12, 1.1187919426447474e-12, 9.004224921227949e-13, 6.79847289575316e-13, 2.2737367544318036e-13, 0.0, 0.0], [0.27722032047524175, 0.26330946097960606, 0.2544218777147519, 0.2533682054670201, 0.25588151245392654, 0.25515295336745697, 0.25620263765974527, 0.25684067906977637, 0.25794724139257735, 0.26054690210162434, 0.26315892385133055, 0.26477844192048333, 0.26699999695442334, 0.26969451966234576, 0.2721711547088176, 0.2748100387810514, 0.27743465384317967, 3.6664903037330486e-12, 3.4738551800907235e-12, 3.5089446263542664e-12, 3.314718132233316e-12, 3.118529754333418e-12, 2.690689296411169e-12, 2.488197596937258e-12, 2.51333090599723e-12, 2.538718086865889e-12, 2.3346913246693503e-12, 2.1286036860869357e-12, 2.150104733421147e-12, 1.942152583816076e-12, 1.7320999074473694e-12, 1.0605847284020034e-12, 8.416273262209291e-13, 6.204582331087849e-13, 3.970551087531359e-13, 4.01065766417309e-13, 4.0511693577505957e-13, 4.092090260354137e-13, 4.133424505408219e-13, 4.1751762680891103e-13, 4.217349765746576e-13, 4.259949258329875e-13, 4.3029790488180555e-13, 4.3464434836546015e-13, 4.3903469531864664e-13, 4.434693892107542e-13, 4.479488779906608e-13, 4.524736141319806e-13, 2.2737367544318036e-13, 0.0], [0.07027876795617763, 0.06934531872625109, 0.06770788153854249, 0.0653094513712366, 0.05575223801597798, 0.03299115713216101, 0.03262583789457591, 0.03176731122854726, 0.03067523576838345, 0.01946833139430949, 0.01940212799439067, 0.01933774150327236, 0.014496522058785842, 0.013475460695688462, 0.012266469426126202, 0.008313123804200146, 0.004413721564543604, 0.0042435034770086566, 0.004149078434169647, 0.003734597250892148, 0.002472246458337689, 0.002355695089807391, 0.0020764848477383734, 0.002058668770453388, 0.0015878329641000563, 0.001553694888512121, 0.0010980973738208305, 0.0010726031366027756, 0.0008356097139351616, 0.0007843573114361161, 0.0007125338592218214, 0.0005518691230804879, 0.0004314304668726045, 0.0003650430393313453, 0.00036812397228472234, 0.00028574465693588237, 0.00027756039139062894, 0.00023998511217239628, 0.00016624167412310147, 0.00014835293542303407, 0.00010360316487552804, 5.884397106789715e-05, 5.648551064911471e-05, 5.061263572214543e-05, 4.966738356276632e-05, 3.11067184492764e-05, 1.743704042079608e-05, 1.744502079422089e-05, 4.848195666570512e-06, 4.472071263128869e-06]]\n",
            "DEBUGGING: traj_returns = [0.30166746924542365, 0.27722032047524175, 0.07027876795617763]\n",
            "DEBUGGING: actions = [[36], [34], [0], [18], [28], [63], [13], [50], [12], [42], [3], [28], [52], [6], [17], [45], [12], [32], [50], [7], [16], [60], [45], [77], [61], [15], [75], [6], [78], [10], [77], [78], [25], [35], [76], [5], [65], [29], [8], [26], [63], [50], [10], [80], [94], [5], [54], [14], [85], [83], [55], [42], [45], [59], [3], [4], [54], [62], [37], [67], [38], [59], [44], [28], [16], [15], [0], [1], [16], [4], [7], [46], [25], [35], [72], [19], [41], [61], [73], [24], [12], [55], [61], [28], [43], [83], [49], [58], [46], [62], [65], [36], [19], [2], [64], [96], [40], [29], [82], [31], [18], [34], [23], [52], [5], [4], [3], [23], [58], [19], [48], [48], [55], [44], [8], [22], [5], [29], [18], [1], [48], [64], [78], [9], [14], [31], [77], [10], [82], [46], [72], [31], [15], [13], [62], [19], [64], [34], [32], [96], [79], [30], [29], [30], [94], [65], [11], [3], [54], [15]]\n",
            "DEBUGGING: actions length = 150\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        ...,\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        ...,\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[2.16388853e-01 2.12042547e-01 2.09328237e-01 1.06225886e-01\n",
            "  1.03877917e-01 9.60480368e-02 9.62761585e-02 9.62026634e-02\n",
            "  9.62074924e-02 9.33384112e-02 9.41870173e-02 9.47053945e-02\n",
            "  9.38321730e-02 9.43899935e-02 9.48125414e-02 9.43743875e-02\n",
            "  9.39494585e-02 1.41450116e-03 1.38302615e-03 1.24486575e-03\n",
            "  8.24082156e-04 7.85231700e-04 6.92161619e-04 6.86222926e-04\n",
            "  5.29277657e-04 5.17898299e-04 3.66032460e-04 3.57534381e-04\n",
            "  2.78536573e-04 2.61452439e-04 2.37511288e-04 1.83956376e-04\n",
            "  1.43810157e-04 1.21681014e-04 1.22707992e-04 9.52482200e-05\n",
            "  9.25201315e-05 7.99950383e-05 5.54138922e-05 4.94509793e-05\n",
            "  3.45343891e-05 1.96146577e-05 1.88285041e-05 1.68708792e-05\n",
            "  1.65557950e-05 1.03689066e-05 5.81234718e-06 5.81500716e-06\n",
            "  1.61606530e-06 1.49069042e-06]]\n",
            "DEBUGGING: baseline2 looks like: 0.2163888525589477\n",
            "DEBUGGING: ADS looks like: [3.01667469e-01 3.03472860e-01 3.05854951e-01 6.35004994e-12\n",
            " 6.41419186e-12 6.47898168e-12 6.54442594e-12 6.38086087e-12\n",
            " 6.44531401e-12 6.28074782e-12 6.34418971e-12 6.17860206e-12\n",
            " 6.01134180e-12 5.84239205e-12 5.67173573e-12 5.72902599e-12\n",
            " 5.78689494e-12 5.84534842e-12 5.90439234e-12 5.73436229e-12\n",
            " 5.56261476e-12 5.61880279e-12 5.44588800e-12 5.50089697e-12\n",
            " 5.09712082e-12 4.91893651e-12 4.96862274e-12 4.55947009e-12\n",
            " 3.91651421e-12 3.72640458e-12 3.53437465e-12 3.34040502e-12\n",
            " 3.14447611e-12 2.94656811e-12 2.74666105e-12 2.54473472e-12\n",
            " 2.57043911e-12 2.36673276e-12 2.16096878e-12 2.18279674e-12\n",
            " 1.97517482e-12 1.53578532e-12 1.32162792e-12 1.33497770e-12\n",
            " 1.11879194e-12 9.00422492e-13 6.79847290e-13 2.27373675e-13\n",
            " 0.00000000e+00 0.00000000e+00 2.77220320e-01 2.63309461e-01\n",
            " 2.54421878e-01 2.53368205e-01 2.55881512e-01 2.55152953e-01\n",
            " 2.56202638e-01 2.56840679e-01 2.57947241e-01 2.60546902e-01\n",
            " 2.63158924e-01 2.64778442e-01 2.66999997e-01 2.69694520e-01\n",
            " 2.72171155e-01 2.74810039e-01 2.77434654e-01 3.66649030e-12\n",
            " 3.47385518e-12 3.50894463e-12 3.31471813e-12 3.11852975e-12\n",
            " 2.69068930e-12 2.48819760e-12 2.51333091e-12 2.53871809e-12\n",
            " 2.33469132e-12 2.12860369e-12 2.15010473e-12 1.94215258e-12\n",
            " 1.73209991e-12 1.06058473e-12 8.41627326e-13 6.20458233e-13\n",
            " 3.97055109e-13 4.01065766e-13 4.05116936e-13 4.09209026e-13\n",
            " 4.13342451e-13 4.17517627e-13 4.21734977e-13 4.25994926e-13\n",
            " 4.30297905e-13 4.34644348e-13 4.39034695e-13 4.43469389e-13\n",
            " 4.47948878e-13 4.52473614e-13 2.27373675e-13 0.00000000e+00\n",
            " 7.02787680e-02 6.93453187e-02 6.77078815e-02 6.53094514e-02\n",
            " 5.57522380e-02 3.29911571e-02 3.26258379e-02 3.17673112e-02\n",
            " 3.06752358e-02 1.94683314e-02 1.94021280e-02 1.93377415e-02\n",
            " 1.44965221e-02 1.34754607e-02 1.22664694e-02 8.31312380e-03\n",
            " 4.41372156e-03 4.24350348e-03 4.14907843e-03 3.73459725e-03\n",
            " 2.47224646e-03 2.35569509e-03 2.07648485e-03 2.05866877e-03\n",
            " 1.58783296e-03 1.55369489e-03 1.09809737e-03 1.07260314e-03\n",
            " 8.35609714e-04 7.84357311e-04 7.12533859e-04 5.51869123e-04\n",
            " 4.31430467e-04 3.65043039e-04 3.68123972e-04 2.85744657e-04\n",
            " 2.77560391e-04 2.39985112e-04 1.66241674e-04 1.48352935e-04\n",
            " 1.03603165e-04 5.88439711e-05 5.64855106e-05 5.06126357e-05\n",
            " 4.96673836e-05 3.11067184e-05 1.74370404e-05 1.74450208e-05\n",
            " 4.84819567e-06 4.47207126e-06]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(0.1659, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 3.3391e-36,  3.6052e-36,  3.3295e-36,  ...,  3.6452e-36,\n",
            "          3.1910e-36,  1.0033e-33],\n",
            "        [-3.4784e-39, -3.9530e-39, -3.5023e-39,  ..., -7.3916e-39,\n",
            "         -4.8574e-39, -1.3119e-36],\n",
            "        ...,\n",
            "        [ 2.8526e-30,  3.2352e-30,  2.6858e-30,  ...,  4.9838e-30,\n",
            "          3.0381e-30,  9.1498e-28],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]])\n",
            "   Last layer:\n",
            "tensor([[ 3.8711e-15, -3.3062e-14,  1.8480e-14,  6.4958e-14,  4.0592e-14,\n",
            "          3.5461e-14,  7.9038e-15,  5.6924e-14,  2.3906e-14,  3.6693e-15,\n",
            "          1.6381e-14,  4.7816e-14,  1.3835e-13,  2.3103e-14, -4.0018e-15,\n",
            "         -2.5411e-14,  1.0368e-13, -3.6186e-14,  8.6276e-15,  7.0569e-14],\n",
            "        [ 5.8392e-14,  9.1421e-14,  8.2556e-14,  1.1118e-13,  8.1726e-14,\n",
            "          1.7037e-14,  3.2854e-14,  2.5432e-14,  5.6369e-14,  1.1003e-13,\n",
            "          1.0222e-13,  2.2461e-13,  7.0932e-14,  7.4508e-14,  1.0396e-13,\n",
            "          6.7832e-14,  1.1449e-13,  2.6485e-14,  3.3941e-14,  1.2485e-13],\n",
            "        [-1.9167e-14, -1.3957e-14, -1.0694e-14, -1.4141e-14, -2.0330e-14,\n",
            "         -2.6080e-14, -3.6465e-14, -2.3663e-15, -8.1794e-15,  3.4307e-14,\n",
            "          5.3268e-15, -1.3908e-14,  1.9212e-14,  3.3649e-16, -1.3085e-14,\n",
            "         -2.0719e-14,  2.7110e-15, -3.5239e-14, -7.3303e-15, -2.0232e-15],\n",
            "        [-1.2443e-15, -3.9574e-15, -3.0732e-14, -1.6435e-14, -1.5808e-14,\n",
            "         -2.8719e-14, -2.4232e-14, -3.1463e-14, -1.1689e-14,  3.2013e-14,\n",
            "         -1.8201e-14, -3.3628e-14, -2.3531e-14,  7.5208e-15,  3.0092e-14,\n",
            "         -3.9540e-14,  3.6970e-14, -2.5366e-15, -1.3172e-15,  4.3146e-15],\n",
            "        [-6.5905e-14, -2.6840e-15, -3.7403e-14, -3.1415e-14,  1.3039e-14,\n",
            "         -1.0103e-14,  2.0033e-14, -4.2948e-14, -2.1722e-14,  3.6322e-14,\n",
            "          1.9486e-14, -9.5788e-15, -4.9690e-15,  4.6296e-14,  2.3242e-14,\n",
            "          5.4842e-14,  2.5944e-14,  2.8605e-16, -4.7306e-14, -3.6829e-14],\n",
            "        [-1.1958e-13, -1.3236e-13, -7.2919e-14, -9.7182e-14, -1.3695e-13,\n",
            "         -7.1907e-14, -8.7806e-14, -1.9087e-13, -9.4240e-14, -1.1313e-13,\n",
            "         -6.7037e-14, -1.4968e-13, -1.0367e-13, -1.4060e-13, -8.4433e-14,\n",
            "         -1.3876e-13, -7.1353e-14, -1.1791e-13, -8.6192e-14, -7.4385e-14],\n",
            "        [ 1.0194e-14, -9.8683e-14, -8.4164e-14, -1.1635e-13, -3.3745e-14,\n",
            "         -7.9213e-14,  5.1653e-14, -3.3546e-14, -3.1136e-14,  1.3390e-14,\n",
            "         -4.0975e-14, -5.9733e-14, -5.4428e-14,  1.9945e-14,  1.9134e-14,\n",
            "         -2.5068e-14, -2.3611e-14, -6.6436e-14,  5.0306e-15, -1.7244e-14],\n",
            "        [-1.4996e-15,  1.5163e-15,  4.0808e-15,  2.3743e-15,  2.7846e-14,\n",
            "         -1.1807e-14, -4.8796e-15, -6.5266e-15,  1.9738e-14,  2.6101e-16,\n",
            "          3.2968e-15, -1.5584e-15,  2.3975e-14,  1.9218e-15,  2.4019e-14,\n",
            "          1.3859e-14,  2.4669e-14,  2.2542e-14,  2.3158e-14,  3.5164e-14],\n",
            "        [ 8.7244e-15, -5.3845e-15, -1.1222e-14, -1.9739e-14, -1.9608e-14,\n",
            "         -8.7514e-15,  4.5138e-15, -2.4529e-15, -6.9092e-15, -2.2572e-14,\n",
            "         -1.0126e-14,  1.0416e-15,  8.7068e-15, -1.4167e-14, -9.2550e-15,\n",
            "         -9.9150e-15, -1.1001e-14, -3.4303e-15, -7.9011e-16, -6.1227e-15],\n",
            "        [ 1.3342e-14, -6.8706e-16,  1.6371e-15, -7.5102e-15, -7.3208e-15,\n",
            "         -6.1990e-15,  2.1194e-15, -7.1471e-15,  9.1782e-15,  1.9125e-15,\n",
            "          2.8857e-15,  4.8096e-15, -1.7750e-15,  8.0432e-16, -4.0548e-15,\n",
            "         -2.3238e-15, -2.9981e-16,  1.0954e-15,  7.6349e-15,  1.0075e-14]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-5.6228e-36, -6.0835e-36, -5.6082e-36,  ..., -6.1332e-36,\n",
            "         -5.3685e-36, -1.6896e-33],\n",
            "        [ 5.4388e-39,  6.1825e-39,  5.4761e-39,  ...,  1.1555e-38,\n",
            "          7.5938e-39,  2.0511e-36],\n",
            "        ...,\n",
            "        [-3.0163e-30, -3.4188e-30, -2.8399e-30,  ..., -5.2726e-30,\n",
            "         -3.2138e-30, -9.6758e-28],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]])\n",
            "   Last layer:\n",
            "tensor([[-6.4156e-14,  4.5420e-14, -8.7575e-14, -2.7562e-14,  5.6448e-14,\n",
            "         -4.2831e-15,  8.7711e-14,  3.1284e-14, -1.2292e-13,  6.6048e-14,\n",
            "          6.0957e-14, -5.7364e-14, -6.2265e-14, -3.2706e-14, -7.1139e-14,\n",
            "         -4.7346e-14, -6.8661e-14,  3.5472e-14,  1.4829e-14, -3.6443e-15],\n",
            "        [-2.9339e-14, -4.7101e-14, -2.1356e-14, -2.0971e-14,  5.1274e-14,\n",
            "          8.7392e-15, -4.0539e-14, -5.8835e-14, -8.6734e-14, -1.2354e-13,\n",
            "         -1.0072e-13, -4.9470e-14, -6.6724e-14, -1.9079e-14, -1.0067e-13,\n",
            "         -8.9628e-15,  9.4666e-14, -4.6731e-14, -2.1360e-14, -4.0079e-14],\n",
            "        [ 3.9896e-14,  3.6727e-14,  2.4830e-14,  6.2886e-14,  1.4567e-14,\n",
            "          9.9480e-15,  2.2830e-14,  5.9971e-14, -2.4448e-14,  4.2703e-14,\n",
            "          1.7919e-14,  3.8515e-14,  1.3967e-14,  1.5725e-14,  5.0387e-14,\n",
            "         -1.6082e-14, -2.3624e-14, -2.5762e-14,  7.9915e-14,  2.7397e-14],\n",
            "        [-5.0858e-14, -4.0270e-14, -6.1610e-14, -9.6522e-14, -6.2364e-14,\n",
            "         -6.7976e-14, -4.4775e-14, -9.3825e-14, -5.8564e-14, -5.7401e-14,\n",
            "         -3.2527e-14, -7.9643e-14, -3.6346e-14, -3.1569e-14,  9.8021e-15,\n",
            "         -2.9739e-14, -9.2882e-14, -7.2493e-14,  1.4104e-14, -4.2203e-14],\n",
            "        [-2.8119e-14, -3.4146e-14, -1.2572e-14, -8.9110e-14,  1.9951e-14,\n",
            "         -7.7110e-14,  7.7493e-15,  4.7520e-15, -5.2639e-14,  1.8624e-14,\n",
            "         -7.4473e-15, -7.6794e-14,  8.0809e-15,  3.4167e-15, -6.7543e-14,\n",
            "         -9.5307e-15,  1.1441e-14, -1.2297e-13, -4.2016e-15, -7.7192e-15],\n",
            "        [ 2.0012e-14,  1.4701e-13,  8.4148e-14,  9.0393e-14,  9.3228e-14,\n",
            "          9.4885e-14,  4.1564e-14,  7.6945e-14,  8.9541e-14,  7.9915e-14,\n",
            "         -2.4718e-14,  4.4846e-14,  3.3123e-14,  5.2924e-14, -8.2479e-14,\n",
            "         -4.7809e-15,  4.9269e-14, -1.0489e-14, -5.3750e-14, -7.7629e-15],\n",
            "        [ 1.4883e-13,  5.5118e-14, -2.9123e-14,  4.1514e-14, -3.1965e-14,\n",
            "         -1.5297e-14,  8.9419e-15,  4.5264e-14,  3.3934e-14, -4.2484e-14,\n",
            "          2.2216e-14,  1.0109e-13,  1.2818e-14,  8.7798e-14,  5.2137e-14,\n",
            "         -1.7619e-14,  7.3948e-14,  4.9278e-14, -8.4957e-14,  7.5018e-14],\n",
            "        [ 1.8735e-14,  7.4539e-14,  5.5167e-15,  9.3177e-14,  3.4062e-14,\n",
            "          1.1991e-13,  2.5644e-14,  1.3531e-13,  1.0072e-15,  6.5563e-14,\n",
            "          3.7722e-14,  5.3883e-14,  9.1435e-14,  7.8991e-14,  5.4372e-14,\n",
            "          5.1530e-14,  6.5635e-14,  6.0017e-14,  4.9485e-14,  7.9107e-14],\n",
            "        [ 1.0822e-14, -3.3970e-16, -3.8977e-15, -9.9860e-15, -1.1099e-14,\n",
            "         -1.2972e-14, -1.7207e-17, -2.7497e-14, -5.9938e-15, -2.4222e-14,\n",
            "         -3.9772e-17,  9.8156e-15, -7.8467e-15, -1.6729e-14,  4.5354e-16,\n",
            "         -1.7431e-14, -6.7154e-15,  8.4455e-16,  1.0515e-14, -1.5768e-14],\n",
            "        [ 3.9310e-14,  3.2038e-14,  6.6060e-15,  4.2212e-14,  2.8409e-14,\n",
            "          2.1028e-14,  1.0763e-14,  1.4947e-14,  5.8653e-15,  4.4012e-14,\n",
            "          1.8079e-14,  2.8652e-14,  2.8045e-14,  2.7817e-14,  3.2041e-14,\n",
            "          3.1247e-14,  2.9905e-14,  2.0168e-14,  1.2249e-14,  1.2554e-14]])\n",
            "DEBUGGING: training for one iteration takes 0.005855 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 5\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [14]\n",
            "DEBUGGING: rel_reward looks like: 0.00297118823605673\n",
            "DEBUGGING: rel_reward looks like: 0.003896858496780195\n",
            "DEBUGGING: rel_reward looks like: 0.0014119456530056948\n",
            "DEBUGGING: rel_reward looks like: 0.007340353999609599\n",
            "DEBUGGING: rel_reward looks like: 0.0037217254601503245\n",
            "DEBUGGING: rel_reward looks like: 0.000898796112699882\n",
            "DEBUGGING: rel_reward looks like: 0.012353145747626824\n",
            "DEBUGGING: rel_reward looks like: 0.0039238406919024586\n",
            "DEBUGGING: rel_reward looks like: 0.0005322597001794293\n",
            "DEBUGGING: rel_reward looks like: 5.958342348207342e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [47]\n",
            "DEBUGGING: rel_reward looks like: 0.00044894180962645335\n",
            "DEBUGGING: rel_reward looks like: 0.003157655009592618\n",
            "DEBUGGING: rel_reward looks like: 0.001080178804234059\n",
            "DEBUGGING: rel_reward looks like: 8.987549319035738e-05\n",
            "DEBUGGING: rel_reward looks like: 0.277441457843469\n",
            "DEBUGGING: rel_reward looks like: 4.547473508863607e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508863607e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [34]\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [53]\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [51]\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.886741328984499 and immediate abs rewards look like: [0.008577051877182384, 0.011215798918328801, 0.00404797544115354, 0.02101470315574261, 0.010576718735137547, 0.0025447695957154792, 0.03494413782254924, 0.010962505282350321, 0.0014812029880886257, 0.0001657239317864878, 0.0012486017831179197, 0.008778161376085336, 0.0029933741957393067, 0.00024879249576770235, 0.7679418113734755, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.3193278064877447 and immediate relative rewards look like: [0.00297118823605673, 0.003896858496780195, 0.0014119456530056948, 0.007340353999609599, 0.0037217254601503245, 0.000898796112699882, 0.012353145747626824, 0.0039238406919024586, 0.0005322597001794293, 5.958342348207342e-05, 0.00044894180962645335, 0.003157655009592618, 0.001080178804234059, 8.987549319035738e-05, 0.277441457843469, 4.547473508863607e-13, 2.2737367544328376e-13, 2.2737367544323206e-13, 4.547473508863607e-13, 2.2737367544328376e-13, 0.0, 0.0, 0.0, 2.2737367544323206e-13, 2.2737367544328376e-13, 0.0, 0.0, 2.2737367544323206e-13, 2.2737367544318036e-13, 2.2737367544323206e-13, 0.0, 2.2737367544318036e-13, 0.0, 2.2737367544323206e-13, 2.2737367544318036e-13, 2.2737367544323206e-13, 2.2737367544318036e-13, 2.2737367544323206e-13, 0.0, 2.2737367544328376e-13, 2.2737367544323206e-13, 2.2737367544318036e-13, 0.0, 2.2737367544323206e-13, 2.2737367544328376e-13, 0.0, 2.2737367544323206e-13, 2.2737367544328376e-13, 2.2737367544323206e-13, 2.2737367544328376e-13]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [52]\n",
            "DEBUGGING: rel_reward looks like: 0.005835945626064869\n",
            "DEBUGGING: rel_reward looks like: 0.006176648477866176\n",
            "DEBUGGING: rel_reward looks like: 0.0024698431312446647\n",
            "DEBUGGING: rel_reward looks like: 0.015535843040952258\n",
            "DEBUGGING: rel_reward looks like: 0.0009650784826507018\n",
            "DEBUGGING: rel_reward looks like: 7.198075761056785e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00026878798690821193\n",
            "DEBUGGING: rel_reward looks like: 0.00029887157137957037\n",
            "DEBUGGING: rel_reward looks like: 0.0011395738393491777\n",
            "DEBUGGING: rel_reward looks like: 0.00066348198155357\n",
            "DEBUGGING: the action_prob is: tensor([0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [14]\n",
            "DEBUGGING: rel_reward looks like: 0.0005476373570629904\n",
            "DEBUGGING: rel_reward looks like: 0.0003983511549130973\n",
            "DEBUGGING: rel_reward looks like: 0.002233364293474199\n",
            "DEBUGGING: rel_reward looks like: 2.1736537247139517e-06\n",
            "DEBUGGING: rel_reward looks like: 2.853093822455781e-06\n",
            "DEBUGGING: rel_reward looks like: 1.1966627114585296e-05\n",
            "DEBUGGING: rel_reward looks like: 3.3099061795588696e-05\n",
            "DEBUGGING: rel_reward looks like: 7.489907803344483e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00029343975255347027\n",
            "DEBUGGING: rel_reward looks like: 0.0003242349061758798\n",
            "DEBUGGING: the action_prob is: tensor([0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [20]\n",
            "DEBUGGING: rel_reward looks like: 0.0003152696622566278\n",
            "DEBUGGING: rel_reward looks like: 0.00044167197224573323\n",
            "DEBUGGING: rel_reward looks like: 0.0004954109519051764\n",
            "DEBUGGING: rel_reward looks like: 0.00023965930178358085\n",
            "DEBUGGING: rel_reward looks like: 7.889452709787377e-06\n",
            "DEBUGGING: rel_reward looks like: 3.6945766578514406e-06\n",
            "DEBUGGING: rel_reward looks like: 0.00012593100792833808\n",
            "DEBUGGING: rel_reward looks like: 3.461844245789103e-06\n",
            "DEBUGGING: rel_reward looks like: 0.00028789519755880155\n",
            "DEBUGGING: rel_reward looks like: 3.6984589084277035e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [73]\n",
            "DEBUGGING: rel_reward looks like: 0.00042460749722203885\n",
            "DEBUGGING: rel_reward looks like: 0.0002722363531532111\n",
            "DEBUGGING: rel_reward looks like: 3.0989756724550583e-07\n",
            "DEBUGGING: rel_reward looks like: 0.00022987416339594128\n",
            "DEBUGGING: rel_reward looks like: 5.134835980909694e-05\n",
            "DEBUGGING: rel_reward looks like: 1.9314266598704208e-05\n",
            "DEBUGGING: rel_reward looks like: 8.761935489982028e-06\n",
            "DEBUGGING: rel_reward looks like: 1.2946617616149358e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0001902859956797871\n",
            "DEBUGGING: rel_reward looks like: 1.5069946104037457e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [41]\n",
            "DEBUGGING: rel_reward looks like: 0.0001802777538061871\n",
            "DEBUGGING: rel_reward looks like: 5.234195805275702e-07\n",
            "DEBUGGING: rel_reward looks like: 5.851746608374199e-06\n",
            "DEBUGGING: rel_reward looks like: 3.451711469095674e-06\n",
            "DEBUGGING: rel_reward looks like: 2.2511113346898763e-05\n",
            "DEBUGGING: rel_reward looks like: 6.326748473183658e-05\n",
            "DEBUGGING: rel_reward looks like: 2.059749257698626e-05\n",
            "DEBUGGING: rel_reward looks like: 9.87854827434867e-06\n",
            "DEBUGGING: rel_reward looks like: 5.480736152814768e-05\n",
            "DEBUGGING: rel_reward looks like: 1.3229312573702366e-05\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.11615999679361266 and immediate abs rewards look like: [0.016846865432398772, 0.017726329269862617, 0.007044407875582692, 0.0442013956453593, 0.002703109872982168, 0.00020141793947914266, 0.0007520735725847771, 0.000836023204101366, 0.003186738136719214, 0.001853266379384877, 0.0015286692787412903, 0.0011113443642898346, 0.00622829397661917, 6.048237992217764e-06, 7.938777343952097e-06, 3.3297228583251126e-05, 9.209728250425542e-05, 0.00020839784747295198, 0.0008164003370438877, 0.0009018130954245862, 0.0008765932352616801, 0.0012276620541342709, 0.001376425585931429, 0.0006655278225480288, 2.190356008213712e-05, 1.0257206213282188e-05, 0.00034961936489708023, 9.609828339307569e-06, 0.0007991734041752352, 0.00010263662215947988, 0.0011782925953411905, 0.0007551394596703176, 8.593715392635204e-07, 0.0006374598360707751, 0.0001423604644514853, 5.354497761800303e-05, 2.4290261080750497e-05, 3.589092466427246e-05, 0.0005275085832181503, 4.176877655481803e-05, 0.0004996612256036315, 1.450458057661308e-06, 1.6215878986258758e-05, 9.565043001202866e-06, 6.238036576178274e-05, 0.00017531607318233, 5.707265063392697e-05, 2.7371454052627087e-05, 0.00015185858455879497, 3.665337135316804e-05]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.04090509340775845 and immediate relative rewards look like: [0.005835945626064869, 0.006176648477866176, 0.0024698431312446647, 0.015535843040952258, 0.0009650784826507018, 7.198075761056785e-05, 0.00026878798690821193, 0.00029887157137957037, 0.0011395738393491777, 0.00066348198155357, 0.0005476373570629904, 0.0003983511549130973, 0.002233364293474199, 2.1736537247139517e-06, 2.853093822455781e-06, 1.1966627114585296e-05, 3.3099061795588696e-05, 7.489907803344483e-05, 0.00029343975255347027, 0.0003242349061758798, 0.0003152696622566278, 0.00044167197224573323, 0.0004954109519051764, 0.00023965930178358085, 7.889452709787377e-06, 3.6945766578514406e-06, 0.00012593100792833808, 3.461844245789103e-06, 0.00028789519755880155, 3.6984589084277035e-05, 0.00042460749722203885, 0.0002722363531532111, 3.0989756724550583e-07, 0.00022987416339594128, 5.134835980909694e-05, 1.9314266598704208e-05, 8.761935489982028e-06, 1.2946617616149358e-05, 0.0001902859956797871, 1.5069946104037457e-05, 0.0001802777538061871, 5.234195805275702e-07, 5.851746608374199e-06, 3.451711469095674e-06, 2.2511113346898763e-05, 6.326748473183658e-05, 2.059749257698626e-05, 9.87854827434867e-06, 5.480736152814768e-05, 1.3229312573702366e-05]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [56]\n",
            "DEBUGGING: rel_reward looks like: 0.0003597018465518302\n",
            "DEBUGGING: rel_reward looks like: 0.0032749897754226417\n",
            "DEBUGGING: rel_reward looks like: 0.0042158785256811585\n",
            "DEBUGGING: rel_reward looks like: 0.007716801148331187\n",
            "DEBUGGING: rel_reward looks like: 0.005623663342333769\n",
            "DEBUGGING: rel_reward looks like: 0.00013937743968549143\n",
            "DEBUGGING: rel_reward looks like: 0.005354229464114569\n",
            "DEBUGGING: rel_reward looks like: 0.00036488499414752136\n",
            "DEBUGGING: rel_reward looks like: 0.0002089516809346335\n",
            "DEBUGGING: rel_reward looks like: 0.00012691580913071337\n",
            "DEBUGGING: the action_prob is: tensor([0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [15]\n",
            "DEBUGGING: rel_reward looks like: 4.412849924044808e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00022421423627094137\n",
            "DEBUGGING: rel_reward looks like: 0.0005777028616876865\n",
            "DEBUGGING: rel_reward looks like: 0.00024254902403732416\n",
            "DEBUGGING: rel_reward looks like: 0.00021488408482077382\n",
            "DEBUGGING: rel_reward looks like: 0.000404478017290888\n",
            "DEBUGGING: rel_reward looks like: 2.137350211617695e-05\n",
            "DEBUGGING: rel_reward looks like: 0.000894213234634756\n",
            "DEBUGGING: rel_reward looks like: 0.0001782960582485624\n",
            "DEBUGGING: rel_reward looks like: 2.8436692771693886e-06\n",
            "DEBUGGING: the action_prob is: tensor([0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [28]\n",
            "DEBUGGING: rel_reward looks like: 0.0004999465610982547\n",
            "DEBUGGING: rel_reward looks like: 4.3736772569046507e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0010838324392054637\n",
            "DEBUGGING: rel_reward looks like: 0.000223129306621182\n",
            "DEBUGGING: rel_reward looks like: 4.7071595723230996e-05\n",
            "DEBUGGING: rel_reward looks like: 1.619784938446104e-05\n",
            "DEBUGGING: rel_reward looks like: 6.734631872175293e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00013223095708071893\n",
            "DEBUGGING: rel_reward looks like: 6.291077048602251e-06\n",
            "DEBUGGING: rel_reward looks like: 1.4809524659932765e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [78]\n",
            "DEBUGGING: rel_reward looks like: 6.114838533635635e-06\n",
            "DEBUGGING: rel_reward looks like: 0.00011717926414545808\n",
            "DEBUGGING: rel_reward looks like: 0.00014793897589238827\n",
            "DEBUGGING: rel_reward looks like: 0.00028471171572772713\n",
            "DEBUGGING: rel_reward looks like: 0.00011135658128564639\n",
            "DEBUGGING: rel_reward looks like: 4.650255532100161e-07\n",
            "DEBUGGING: rel_reward looks like: 0.0001553721313347936\n",
            "DEBUGGING: rel_reward looks like: 3.2618141619270736e-06\n",
            "DEBUGGING: rel_reward looks like: 4.182183658529822e-05\n",
            "DEBUGGING: rel_reward looks like: 5.865920745334958e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [35]\n",
            "DEBUGGING: rel_reward looks like: 0.00013036838440268214\n",
            "DEBUGGING: rel_reward looks like: 0.00014098691513869741\n",
            "DEBUGGING: rel_reward looks like: 0.00022558258350598452\n",
            "DEBUGGING: rel_reward looks like: 6.337926191706366e-05\n",
            "DEBUGGING: rel_reward looks like: 2.1806408586376304e-06\n",
            "DEBUGGING: rel_reward looks like: 1.823916799907834e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00020084761619345166\n",
            "DEBUGGING: rel_reward looks like: 3.35804488163318e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00023029634346561606\n",
            "DEBUGGING: rel_reward looks like: 5.313876348910216e-06\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.09755570754623477 and immediate abs rewards look like: [0.0010383661865489557, 0.00945064769803139, 0.012125930369620619, 0.022101893774561177, 0.015982588947736076, 0.00039388648474414367, 0.015129167859868176, 0.0010255161532768398, 0.0005870483541912108, 0.00035649461369757773, 0.00012393708720992436, 0.000629689057404903, 0.001622071878045972, 0.0006806347869314777, 0.0006028558937032358, 0.0011345165885359165, 5.992608657834353e-05, 0.002507102209619916, 0.0004994409669052402, 7.964235010149423e-06, 0.0014001911595187266, 0.00012243153651070315, 0.0030338193560055515, 0.0006238974788175256, 0.00013158870751794893, 4.527897772277356e-05, 0.00018825481083695195, 0.0003696035332723113, 1.7582091913936893e-05, 4.1388904264749726e-05, 1.7089185803342843e-05, 0.0003274797791164019, 0.000413395207033318, 0.0007954701982271217, 0.00031103610035643214, 1.2987434274691623e-06, 0.00043392978977863095, 9.108315680350643e-06, 0.00011678324653985328, 0.0001637930504330143, 0.00036400394265001523, 0.0003936008160962956, 0.0006296823412412778, 0.00017687446734271361, 6.085196218919009e-06, 5.0897273467853665e-05, 0.0005604646876236075, 9.368732389702927e-05, 0.0006424906264328456, 1.4821466265857453e-05]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.03430232624539078 and immediate relative rewards look like: [0.0003597018465518302, 0.0032749897754226417, 0.0042158785256811585, 0.007716801148331187, 0.005623663342333769, 0.00013937743968549143, 0.005354229464114569, 0.00036488499414752136, 0.0002089516809346335, 0.00012691580913071337, 4.412849924044808e-05, 0.00022421423627094137, 0.0005777028616876865, 0.00024254902403732416, 0.00021488408482077382, 0.000404478017290888, 2.137350211617695e-05, 0.000894213234634756, 0.0001782960582485624, 2.8436692771693886e-06, 0.0004999465610982547, 4.3736772569046507e-05, 0.0010838324392054637, 0.000223129306621182, 4.7071595723230996e-05, 1.619784938446104e-05, 6.734631872175293e-05, 0.00013223095708071893, 6.291077048602251e-06, 1.4809524659932765e-05, 6.114838533635635e-06, 0.00011717926414545808, 0.00014793897589238827, 0.00028471171572772713, 0.00011135658128564639, 4.650255532100161e-07, 0.0001553721313347936, 3.2618141619270736e-06, 4.182183658529822e-05, 5.865920745334958e-05, 0.00013036838440268214, 0.00014098691513869741, 0.00022558258350598452, 6.337926191706366e-05, 2.1806408586376304e-06, 1.823916799907834e-05, 0.00020084761619345166, 3.35804488163318e-05, 0.00023029634346561606, 5.313876348910216e-06]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 150 number of matrices\n",
            "DEBUGGING: ACT_MAT has 150 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.2808937835876099, 0.28072989429449813, 0.279629329088604, 0.2810276600359579, 0.2764518242791397, 0.2754849483020095, 0.27735974968617133, 0.26768343832176217, 0.26642383598975733, 0.2685773497874524, 0.27123006703431346, 0.27351628810574447, 0.2730895283799513, 0.2747569187633508, 0.27744145784864693, 5.230189050090025e-12, 4.823678484044105e-12, 4.64273212989982e-12, 4.459958034804634e-12, 4.045667357493205e-12, 3.856862305100931e-12, 3.89582051020296e-12, 3.935172232528243e-12, 3.9749214469982256e-12, 3.785401789449489e-12, 3.5939677919254597e-12, 3.6302704968944036e-12, 3.666939895852933e-12, 3.474309313545152e-12, 3.2797329677797693e-12, 3.0831912043803408e-12, 3.114334549879132e-12, 2.9161220953898505e-12, 2.9455778741311623e-12, 2.745660806755485e-12, 2.543724375062934e-12, 2.3397481814340424e-12, 2.133711622212992e-12, 1.92559388562602e-12, 1.945044328915172e-12, 1.7350208620928162e-12, 1.522875946110691e-12, 1.3085881521894046e-12, 1.321806214332732e-12, 1.105487413019697e-12, 8.869835733095082e-13, 8.959430033429375e-13, 6.753225534340459e-13, 4.52473614132083e-13, 2.2737367544328376e-13], [0.03876403720751849, 0.03326069856712487, 0.027357626352786563, 0.025139174971254445, 0.009700335283133524, 0.008823491717659417, 0.008839910060655404, 0.008657699064391101, 0.008443260093951042, 0.007377460863234206, 0.0067817968501824605, 0.006297130801130778, 0.005958363279007758, 0.003762625237912686, 0.003798435943624214, 0.003833922070506827, 0.003860561053931557, 0.003866123224379766, 0.0038295193397437587, 0.003571797562818473, 0.0032803663198410033, 0.0029950471288731064, 0.0025791668248761343, 0.002104803912091877, 0.0018839844548568649, 0.0018950454567142195, 0.0019104554344003717, 0.001802549925729327, 0.00181726068836721, 0.0015448136270792005, 0.0015230596343383065, 0.0011095476132487552, 0.0008457689495914587, 0.0008539990424487002, 0.0006304291707603625, 0.0005849301120719855, 0.0005713291368416982, 0.0005682496983350668, 0.0005609122027463813, 0.0003743699061278729, 0.0003629292525493288, 0.00018449646337691077, 0.00018583135737008404, 0.00018179758662798974, 0.00018014734864534753, 0.0001592285205034836, 9.693033916327981e-05, 7.710388544070056e-05, 6.790438097611302e-05, 1.3229312573702366e-05], [0.031912916172007824, 0.03187193366207676, 0.02888580190571123, 0.024919114525282903, 0.017376074118133047, 0.011871121995756847, 0.011850247026334703, 0.006561633901232459, 0.006259342330388826, 0.006111505706519387, 0.0060450403003926, 0.0060615270718708605, 0.00589627559151507, 0.005372295686694326, 0.005181562285512123, 0.005016846667364999, 0.0046589582323980925, 0.004684429020486784, 0.003828500793789927, 0.003687075490445823, 0.003721446284008741, 0.0032540401241520065, 0.0032427306581646062, 0.002180705271675901, 0.0019773494596512318, 0.001949775620129294, 0.001953108859338215, 0.0019048106470873354, 0.0017904845353602186, 0.0018022156144561782, 0.0018054606967638843, 0.0018175210689194432, 0.0017175169745191768, 0.0015854323218452411, 0.001313859198098499, 0.0012146491078917705, 0.0012264485680187481, 0.001081895390589853, 0.001089528865078713, 0.001058289927771126, 0.0010097280003209862, 0.0008882420362811152, 0.0007548031526691089, 0.0005345662314779034, 0.00047594643389983803, 0.0004785513061022226, 0.00046496175565974166, 0.00026678195905685857, 0.00023555708105103717, 5.313876348910216e-06]]\n",
            "DEBUGGING: traj_returns = [0.2808937835876099, 0.03876403720751849, 0.031912916172007824]\n",
            "DEBUGGING: actions = [[14], [50], [28], [45], [37], [25], [44], [33], [7], [56], [47], [26], [70], [23], [0], [63], [38], [45], [30], [25], [34], [38], [56], [68], [21], [12], [47], [58], [53], [10], [53], [71], [80], [41], [32], [59], [81], [81], [60], [64], [51], [24], [28], [10], [84], [8], [102], [46], [14], [80], [52], [33], [20], [41], [18], [40], [47], [54], [54], [30], [14], [30], [60], [18], [12], [47], [68], [74], [26], [59], [20], [53], [5], [81], [55], [14], [21], [85], [40], [55], [73], [17], [52], [43], [29], [9], [37], [60], [14], [4], [41], [88], [3], [66], [56], [50], [77], [104], [90], [82], [56], [42], [47], [10], [1], [25], [55], [51], [39], [16], [15], [6], [6], [69], [42], [58], [48], [62], [37], [12], [28], [64], [38], [23], [38], [78], [33], [50], [32], [59], [78], [31], [34], [39], [37], [31], [36], [41], [65], [25], [35], [96], [73], [42], [65], [49], [58], [35], [66], [5]]\n",
            "DEBUGGING: actions length = 150\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        ...,\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        ...,\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[1.17190246e-01 1.15287509e-01 1.11957586e-01 1.10361983e-01\n",
            "  1.01176078e-01 9.87265207e-02 9.93499689e-02 9.43009238e-02\n",
            "  9.37088128e-02 9.40221055e-02 9.46856347e-02 9.52916487e-02\n",
            "  9.49813891e-02 9.46306132e-02 9.54738187e-02 2.95025625e-03\n",
            "  2.83983976e-03 2.85018408e-03 2.55267338e-03 2.41962435e-03\n",
            "  2.33393754e-03 2.08302909e-03 1.94063250e-03 1.42850306e-03\n",
            "  1.28711131e-03 1.28160703e-03 1.28785477e-03 1.23578686e-03\n",
            "  1.20258174e-03 1.11567641e-03 1.10950678e-03 9.75689562e-04\n",
            "  8.54428642e-04 8.13143789e-04 6.48096124e-04 5.99859741e-04\n",
            "  5.99259236e-04 5.50048364e-04 5.50147023e-04 4.77553279e-04\n",
            "  4.57552418e-04 3.57579500e-04 3.13544837e-04 2.38787940e-04\n",
            "  2.18697928e-04 2.12593276e-04 1.87297365e-04 1.14628615e-04\n",
            "  1.01153821e-04 6.18106305e-06]]\n",
            "DEBUGGING: baseline2 looks like: 0.11719024565571207\n",
            "DEBUGGING: ADS looks like: [2.80893784e-01 2.80729894e-01 2.79629329e-01 2.81027660e-01\n",
            " 2.76451824e-01 2.75484948e-01 2.77359750e-01 2.67683438e-01\n",
            " 2.66423836e-01 2.68577350e-01 2.71230067e-01 2.73516288e-01\n",
            " 2.73089528e-01 2.74756919e-01 2.77441458e-01 5.23018905e-12\n",
            " 4.82367848e-12 4.64273213e-12 4.45995803e-12 4.04566736e-12\n",
            " 3.85686231e-12 3.89582051e-12 3.93517223e-12 3.97492145e-12\n",
            " 3.78540179e-12 3.59396779e-12 3.63027050e-12 3.66693990e-12\n",
            " 3.47430931e-12 3.27973297e-12 3.08319120e-12 3.11433455e-12\n",
            " 2.91612210e-12 2.94557787e-12 2.74566081e-12 2.54372438e-12\n",
            " 2.33974818e-12 2.13371162e-12 1.92559389e-12 1.94504433e-12\n",
            " 1.73502086e-12 1.52287595e-12 1.30858815e-12 1.32180621e-12\n",
            " 1.10548741e-12 8.86983573e-13 8.95943003e-13 6.75322553e-13\n",
            " 4.52473614e-13 2.27373675e-13 3.87640372e-02 3.32606986e-02\n",
            " 2.73576264e-02 2.51391750e-02 9.70033528e-03 8.82349172e-03\n",
            " 8.83991006e-03 8.65769906e-03 8.44326009e-03 7.37746086e-03\n",
            " 6.78179685e-03 6.29713080e-03 5.95836328e-03 3.76262524e-03\n",
            " 3.79843594e-03 3.83392207e-03 3.86056105e-03 3.86612322e-03\n",
            " 3.82951934e-03 3.57179756e-03 3.28036632e-03 2.99504713e-03\n",
            " 2.57916682e-03 2.10480391e-03 1.88398445e-03 1.89504546e-03\n",
            " 1.91045543e-03 1.80254993e-03 1.81726069e-03 1.54481363e-03\n",
            " 1.52305963e-03 1.10954761e-03 8.45768950e-04 8.53999042e-04\n",
            " 6.30429171e-04 5.84930112e-04 5.71329137e-04 5.68249698e-04\n",
            " 5.60912203e-04 3.74369906e-04 3.62929253e-04 1.84496463e-04\n",
            " 1.85831357e-04 1.81797587e-04 1.80147349e-04 1.59228521e-04\n",
            " 9.69303392e-05 7.71038854e-05 6.79043810e-05 1.32293126e-05\n",
            " 3.19129162e-02 3.18719337e-02 2.88858019e-02 2.49191145e-02\n",
            " 1.73760741e-02 1.18711220e-02 1.18502470e-02 6.56163390e-03\n",
            " 6.25934233e-03 6.11150571e-03 6.04504030e-03 6.06152707e-03\n",
            " 5.89627559e-03 5.37229569e-03 5.18156229e-03 5.01684667e-03\n",
            " 4.65895823e-03 4.68442902e-03 3.82850079e-03 3.68707549e-03\n",
            " 3.72144628e-03 3.25404012e-03 3.24273066e-03 2.18070527e-03\n",
            " 1.97734946e-03 1.94977562e-03 1.95310886e-03 1.90481065e-03\n",
            " 1.79048454e-03 1.80221561e-03 1.80546070e-03 1.81752107e-03\n",
            " 1.71751697e-03 1.58543232e-03 1.31385920e-03 1.21464911e-03\n",
            " 1.22644857e-03 1.08189539e-03 1.08952887e-03 1.05828993e-03\n",
            " 1.00972800e-03 8.88242036e-04 7.54803153e-04 5.34566231e-04\n",
            " 4.75946434e-04 4.78551306e-04 4.64961756e-04 2.66781959e-04\n",
            " 2.35557081e-04 5.31387635e-06]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(0.1288, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-5.6228e-36, -6.0835e-36, -5.6082e-36,  ..., -6.1332e-36,\n",
            "         -5.3685e-36, -1.6896e-33],\n",
            "        [ 5.4388e-39,  6.1825e-39,  5.4761e-39,  ...,  1.1555e-38,\n",
            "          7.5938e-39,  2.0511e-36],\n",
            "        ...,\n",
            "        [-3.0163e-30, -3.4188e-30, -2.8399e-30,  ..., -5.2726e-30,\n",
            "         -3.2138e-30, -9.6758e-28],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]])\n",
            "   Last layer:\n",
            "tensor([[-6.4156e-14,  4.5420e-14, -8.7575e-14, -2.7562e-14,  5.6448e-14,\n",
            "         -4.2831e-15,  8.7711e-14,  3.1284e-14, -1.2292e-13,  6.6048e-14,\n",
            "          6.0957e-14, -5.7364e-14, -6.2265e-14, -3.2706e-14, -7.1139e-14,\n",
            "         -4.7346e-14, -6.8661e-14,  3.5472e-14,  1.4829e-14, -3.6443e-15],\n",
            "        [-2.9339e-14, -4.7101e-14, -2.1356e-14, -2.0971e-14,  5.1274e-14,\n",
            "          8.7392e-15, -4.0539e-14, -5.8835e-14, -8.6734e-14, -1.2354e-13,\n",
            "         -1.0072e-13, -4.9470e-14, -6.6724e-14, -1.9079e-14, -1.0067e-13,\n",
            "         -8.9628e-15,  9.4666e-14, -4.6731e-14, -2.1360e-14, -4.0079e-14],\n",
            "        [ 3.9896e-14,  3.6727e-14,  2.4830e-14,  6.2886e-14,  1.4567e-14,\n",
            "          9.9480e-15,  2.2830e-14,  5.9971e-14, -2.4448e-14,  4.2703e-14,\n",
            "          1.7919e-14,  3.8515e-14,  1.3967e-14,  1.5725e-14,  5.0387e-14,\n",
            "         -1.6082e-14, -2.3624e-14, -2.5762e-14,  7.9915e-14,  2.7397e-14],\n",
            "        [-5.0858e-14, -4.0270e-14, -6.1610e-14, -9.6522e-14, -6.2364e-14,\n",
            "         -6.7976e-14, -4.4775e-14, -9.3825e-14, -5.8564e-14, -5.7401e-14,\n",
            "         -3.2527e-14, -7.9643e-14, -3.6346e-14, -3.1569e-14,  9.8021e-15,\n",
            "         -2.9739e-14, -9.2882e-14, -7.2493e-14,  1.4104e-14, -4.2203e-14],\n",
            "        [-2.8119e-14, -3.4146e-14, -1.2572e-14, -8.9110e-14,  1.9951e-14,\n",
            "         -7.7110e-14,  7.7493e-15,  4.7520e-15, -5.2639e-14,  1.8624e-14,\n",
            "         -7.4473e-15, -7.6794e-14,  8.0809e-15,  3.4167e-15, -6.7543e-14,\n",
            "         -9.5307e-15,  1.1441e-14, -1.2297e-13, -4.2016e-15, -7.7192e-15],\n",
            "        [ 2.0012e-14,  1.4701e-13,  8.4148e-14,  9.0393e-14,  9.3228e-14,\n",
            "          9.4885e-14,  4.1564e-14,  7.6945e-14,  8.9541e-14,  7.9915e-14,\n",
            "         -2.4718e-14,  4.4846e-14,  3.3123e-14,  5.2924e-14, -8.2479e-14,\n",
            "         -4.7809e-15,  4.9269e-14, -1.0489e-14, -5.3750e-14, -7.7629e-15],\n",
            "        [ 1.4883e-13,  5.5118e-14, -2.9123e-14,  4.1514e-14, -3.1965e-14,\n",
            "         -1.5297e-14,  8.9419e-15,  4.5264e-14,  3.3934e-14, -4.2484e-14,\n",
            "          2.2216e-14,  1.0109e-13,  1.2818e-14,  8.7798e-14,  5.2137e-14,\n",
            "         -1.7619e-14,  7.3948e-14,  4.9278e-14, -8.4957e-14,  7.5018e-14],\n",
            "        [ 1.8735e-14,  7.4539e-14,  5.5167e-15,  9.3177e-14,  3.4062e-14,\n",
            "          1.1991e-13,  2.5644e-14,  1.3531e-13,  1.0072e-15,  6.5563e-14,\n",
            "          3.7722e-14,  5.3883e-14,  9.1435e-14,  7.8991e-14,  5.4372e-14,\n",
            "          5.1530e-14,  6.5635e-14,  6.0017e-14,  4.9485e-14,  7.9107e-14],\n",
            "        [ 1.0822e-14, -3.3970e-16, -3.8977e-15, -9.9860e-15, -1.1099e-14,\n",
            "         -1.2972e-14, -1.7207e-17, -2.7497e-14, -5.9938e-15, -2.4222e-14,\n",
            "         -3.9772e-17,  9.8156e-15, -7.8467e-15, -1.6729e-14,  4.5354e-16,\n",
            "         -1.7431e-14, -6.7154e-15,  8.4455e-16,  1.0515e-14, -1.5768e-14],\n",
            "        [ 3.9310e-14,  3.2038e-14,  6.6060e-15,  4.2212e-14,  2.8409e-14,\n",
            "          2.1028e-14,  1.0763e-14,  1.4947e-14,  5.8653e-15,  4.4012e-14,\n",
            "          1.8079e-14,  2.8652e-14,  2.8045e-14,  2.7817e-14,  3.2041e-14,\n",
            "          3.1247e-14,  2.9905e-14,  2.0168e-14,  1.2249e-14,  1.2554e-14]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-1.7176e-36, -1.8524e-36, -1.7126e-36,  ..., -1.8773e-36,\n",
            "         -1.6430e-36, -5.1619e-34],\n",
            "        [ 2.8285e-39,  3.2141e-39,  2.8478e-39,  ...,  6.0123e-39,\n",
            "          3.9507e-39,  1.0669e-36],\n",
            "        ...,\n",
            "        [-3.8871e-30, -4.4063e-30, -3.6589e-30,  ..., -6.7966e-30,\n",
            "         -4.1423e-30, -1.2470e-27],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]])\n",
            "   Last layer:\n",
            "tensor([[ 1.0546e-13, -1.3269e-14,  7.3893e-14,  1.3409e-14,  4.1515e-14,\n",
            "          3.7243e-14,  2.1333e-15,  3.3163e-14,  3.0197e-15, -5.2903e-15,\n",
            "          1.6926e-14,  1.6888e-13,  5.1114e-14,  2.8465e-14,  8.7558e-14,\n",
            "          1.0955e-14,  3.5569e-14,  1.3641e-14,  1.7261e-15, -1.2362e-14],\n",
            "        [ 2.0608e-14,  6.3029e-15, -4.8692e-15,  3.1036e-15,  2.1408e-14,\n",
            "          8.1213e-14,  2.7368e-14, -1.7477e-14,  5.4630e-15,  6.6189e-14,\n",
            "         -2.4345e-14,  3.3150e-14,  9.5205e-14,  8.9104e-14,  8.6053e-15,\n",
            "          9.0232e-14, -5.0112e-14,  3.0932e-14,  1.0355e-13,  1.2426e-13],\n",
            "        [-8.9092e-15,  1.8285e-15, -4.5031e-16, -3.5275e-14,  2.1681e-15,\n",
            "         -2.7460e-14, -3.5817e-14,  2.9873e-14, -1.1071e-14,  1.1255e-14,\n",
            "          2.3174e-14, -3.4697e-14, -1.6686e-14, -7.5978e-15, -3.1424e-15,\n",
            "          2.9753e-14, -2.4730e-14, -1.9959e-14, -2.1229e-14,  2.4371e-14],\n",
            "        [ 1.0601e-14, -5.2110e-14, -2.8481e-14, -7.7279e-14, -1.6080e-14,\n",
            "         -5.8197e-14, -3.4463e-14, -4.1020e-14, -3.9682e-14, -1.0420e-13,\n",
            "         -9.6475e-15, -3.6009e-14, -2.3542e-15, -3.7712e-14, -1.3242e-14,\n",
            "         -3.9489e-14, -3.7650e-14, -2.4969e-14, -1.6583e-14, -3.2778e-14],\n",
            "        [ 4.9631e-14,  7.6278e-14,  8.7014e-14,  5.9228e-14,  6.7993e-14,\n",
            "          2.8359e-14,  8.4146e-14,  6.0811e-14,  1.2301e-13,  6.8669e-14,\n",
            "          7.8938e-14,  1.7261e-13,  1.3783e-13,  5.0734e-14,  4.2381e-14,\n",
            "          9.5258e-14,  1.1382e-13,  4.9444e-14,  8.2784e-14,  9.6779e-14],\n",
            "        [ 2.8417e-14,  5.4509e-14,  2.8820e-15,  6.4210e-14, -6.0149e-15,\n",
            "         -7.7600e-15,  2.5273e-14,  4.5915e-14,  5.9170e-14,  3.0091e-15,\n",
            "         -3.7359e-14,  1.1209e-14, -7.1422e-15,  8.0529e-15,  5.3871e-14,\n",
            "          1.4293e-14,  1.4817e-14, -1.6132e-14, -4.4356e-14, -3.1531e-14],\n",
            "        [ 1.7921e-13,  2.1503e-13,  5.6803e-14,  1.5299e-13,  1.4340e-13,\n",
            "          1.2399e-13,  2.2717e-14,  1.1842e-13,  1.5679e-13,  2.3453e-13,\n",
            "          1.2054e-13,  6.1133e-14,  8.2582e-14,  5.4014e-14,  1.6941e-13,\n",
            "          1.2766e-13,  3.5289e-14,  8.2392e-14,  1.0241e-13,  3.7536e-14],\n",
            "        [-2.3422e-14, -1.1004e-14,  1.5725e-14, -1.3385e-14,  4.1514e-15,\n",
            "          5.4442e-15,  3.8009e-15, -8.6576e-15, -1.4139e-14,  4.9669e-14,\n",
            "          2.6640e-15,  2.7686e-14,  1.3242e-14,  2.1596e-14,  1.6326e-15,\n",
            "         -4.9864e-15, -2.5652e-16, -1.0717e-14,  4.6989e-15, -1.6435e-14],\n",
            "        [-3.7253e-15, -2.6226e-14, -9.2784e-15, -2.7354e-14, -8.1875e-15,\n",
            "         -1.5662e-14, -2.5985e-15, -2.9260e-14, -1.8560e-14, -1.7060e-14,\n",
            "         -8.9156e-15, -1.2785e-14, -2.9587e-14, -1.0765e-14, -2.7921e-14,\n",
            "         -1.4842e-14, -3.3057e-15, -2.3109e-15, -6.7366e-15, -2.1467e-14],\n",
            "        [ 1.0084e-14,  1.2150e-14,  1.2543e-14,  5.8101e-15,  1.4907e-14,\n",
            "          1.5554e-14,  5.4951e-15,  1.3057e-14,  4.4195e-15,  1.7696e-14,\n",
            "          1.3709e-15,  1.3455e-14,  2.2429e-15,  1.4868e-14,  1.1860e-14,\n",
            "          8.2761e-15, -1.0013e-15,  1.0657e-14,  9.8316e-15,  1.1074e-14]])\n",
            "DEBUGGING: training for one iteration takes 0.007384 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 6\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [58]\n",
            "DEBUGGING: rel_reward looks like: 7.062391050380656e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0010123886644820888\n",
            "DEBUGGING: rel_reward looks like: 0.011460783539724418\n",
            "DEBUGGING: rel_reward looks like: 0.00024861680574143804\n",
            "DEBUGGING: rel_reward looks like: 0.001603407388793415\n",
            "DEBUGGING: rel_reward looks like: 0.004621317620422307\n",
            "DEBUGGING: rel_reward looks like: 0.005629794784682132\n",
            "DEBUGGING: rel_reward looks like: 0.004847535393427354\n",
            "DEBUGGING: rel_reward looks like: 0.0007645412762105147\n",
            "DEBUGGING: rel_reward looks like: 0.2858165351308038\n",
            "DEBUGGING: the action_prob is: tensor([0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [7]\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508865675e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [58]\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 4.547473508864641e-13\n",
            "DEBUGGING: rel_reward looks like: 6.821210263300064e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [69]\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [0]\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508865675e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508863607e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.886741328988137 and immediate abs rewards look like: [0.00020387296126500587, 0.002922297800068918, 0.03304848914012837, 0.000708698779817496, 0.0045694833265770285, 0.013148981819995242, 0.01594436421555656, 0.013651605620452756, 0.0021426602111205284, 0.8004008750976936, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 9.094947017729282e-13, 1.3642420526593924e-12, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.316075544522522 and immediate relative rewards look like: [7.062391050380656e-05, 0.0010123886644820888, 0.011460783539724418, 0.00024861680574143804, 0.001603407388793415, 0.004621317620422307, 0.005629794784682132, 0.004847535393427354, 0.0007645412762105147, 0.2858165351308038, 2.2737367544323206e-13, 2.2737367544318036e-13, 0.0, 2.2737367544323206e-13, 4.547473508865675e-13, 0.0, 0.0, 2.2737367544318036e-13, 2.2737367544323206e-13, 2.2737367544328376e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13, 6.821210263300064e-13, 0.0, 2.2737367544318036e-13, 2.2737367544323206e-13, 2.2737367544318036e-13, 2.2737367544323206e-13, 2.2737367544318036e-13, 0.0, 0.0, 0.0, 0.0, 2.2737367544323206e-13, 2.2737367544318036e-13, 2.2737367544323206e-13, 2.2737367544328376e-13, 2.2737367544323206e-13, 2.2737367544318036e-13, 2.2737367544323206e-13, 2.2737367544318036e-13, 0.0, 2.2737367544323206e-13, 4.547473508865675e-13, 4.547473508863607e-13, 2.2737367544328376e-13, 2.2737367544323206e-13, 2.2737367544328376e-13]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [54]\n",
            "DEBUGGING: rel_reward looks like: 0.01702132535251133\n",
            "DEBUGGING: rel_reward looks like: 0.0023868663303190664\n",
            "DEBUGGING: rel_reward looks like: 0.002326244316545989\n",
            "DEBUGGING: rel_reward looks like: 0.009641276376497039\n",
            "DEBUGGING: rel_reward looks like: 0.004240103625739487\n",
            "DEBUGGING: rel_reward looks like: 0.00046357187400549707\n",
            "DEBUGGING: rel_reward looks like: 0.0017195021926715533\n",
            "DEBUGGING: rel_reward looks like: 0.0003362690362924087\n",
            "DEBUGGING: rel_reward looks like: 0.0010543588840789477\n",
            "DEBUGGING: rel_reward looks like: 0.2793354565427253\n",
            "DEBUGGING: the action_prob is: tensor([0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [11]\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508865675e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [53]\n",
            "DEBUGGING: rel_reward looks like: 4.547473508865675e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [74]\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [38]\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508865675e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544338716e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508866709e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508863607e-13\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.8867413289840442 and immediate abs rewards look like: [0.04913616336898485, 0.006772984228518908, 0.006585207273019478, 0.027229345632804325, 0.011859644587730145, 0.0012911209059893736, 0.004786865174992272, 0.0009345185526399291, 0.0029291623673088907, 0.7752163168806874, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 9.094947017729282e-13, 0.0, 4.547473508864641e-13, 9.094947017729282e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.31852497453707096 and immediate relative rewards look like: [0.01702132535251133, 0.0023868663303190664, 0.002326244316545989, 0.009641276376497039, 0.004240103625739487, 0.00046357187400549707, 0.0017195021926715533, 0.0003362690362924087, 0.0010543588840789477, 0.2793354565427253, 0.0, 2.2737367544323206e-13, 4.547473508865675e-13, 2.2737367544318036e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 2.2737367544323206e-13, 4.547473508865675e-13, 2.2737367544318036e-13, 0.0, 0.0, 2.2737367544323206e-13, 2.2737367544328376e-13, 0.0, 0.0, 2.2737367544323206e-13, 2.2737367544328376e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.2737367544323206e-13, 0.0, 0.0, 2.2737367544318036e-13, 0.0, 2.2737367544323206e-13, 2.2737367544318036e-13, 2.2737367544323206e-13, 4.547473508865675e-13, 2.2737367544338716e-13, 4.547473508866709e-13, 0.0, 2.2737367544323206e-13, 4.547473508863607e-13]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [4]\n",
            "DEBUGGING: rel_reward looks like: 0.0023432197301608434\n",
            "DEBUGGING: rel_reward looks like: 0.002912709518919005\n",
            "DEBUGGING: rel_reward looks like: 0.011405187350741799\n",
            "DEBUGGING: rel_reward looks like: 0.0014684740112568123\n",
            "DEBUGGING: rel_reward looks like: 0.0011323013067452814\n",
            "DEBUGGING: rel_reward looks like: 0.0028298651534603486\n",
            "DEBUGGING: rel_reward looks like: 0.0006413047908321914\n",
            "DEBUGGING: rel_reward looks like: 0.00027625117959584854\n",
            "DEBUGGING: rel_reward looks like: 0.007768735913575148\n",
            "DEBUGGING: rel_reward looks like: 0.000673188268449163\n",
            "DEBUGGING: the action_prob is: tensor([0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [9]\n",
            "DEBUGGING: rel_reward looks like: 0.00010250890497378673\n",
            "DEBUGGING: rel_reward looks like: 0.008962761601402138\n",
            "DEBUGGING: rel_reward looks like: 0.2784222339854042\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508865675e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508865675e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [31]\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508864641e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 4.547473508866709e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: the action_prob is: tensor([0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [18]\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106,\n",
            "        0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106,\n",
            "        0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106,\n",
            "        0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106,\n",
            "        0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106,\n",
            "        0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106,\n",
            "        0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106,\n",
            "        0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106,\n",
            "        0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106,\n",
            "        0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106,\n",
            "        0.0106, 0.0106, 0.0106, 0.0106], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [60]\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 6.821210263298513e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508862573e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.8867413289867727 and immediate abs rewards look like: [0.006764269237919507, 0.008388536596157792, 0.032751005100635666, 0.00416875911741954, 0.003209699139915756, 0.0080126473267228, 0.0018106897109646525, 0.0007794801967975218, 0.021914487198500865, 0.0018842147537725396, 0.0002867233056349505, 0.025066790089113056, 0.7717040271991209, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 0.0, 9.094947017729282e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 1.3642420526593924e-12, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.3189387417225652 and immediate relative rewards look like: [0.0023432197301608434, 0.002912709518919005, 0.011405187350741799, 0.0014684740112568123, 0.0011323013067452814, 0.0028298651534603486, 0.0006413047908321914, 0.00027625117959584854, 0.007768735913575148, 0.000673188268449163, 0.00010250890497378673, 0.008962761601402138, 0.2784222339854042, 2.2737367544323206e-13, 4.547473508865675e-13, 2.2737367544318036e-13, 2.2737367544323206e-13, 4.547473508865675e-13, 2.2737367544318036e-13, 2.2737367544323206e-13, 2.2737367544328376e-13, 4.547473508864641e-13, 0.0, 4.547473508866709e-13, 0.0, 0.0, 2.2737367544323206e-13, 2.2737367544318036e-13, 0.0, 0.0, 0.0, 2.2737367544323206e-13, 2.2737367544318036e-13, 0.0, 0.0, 2.2737367544323206e-13, 2.2737367544318036e-13, 0.0, 0.0, 2.2737367544323206e-13, 0.0, 2.2737367544328376e-13, 2.2737367544323206e-13, 6.821210263298513e-13, 4.547473508862573e-13, 2.2737367544323206e-13, 2.2737367544318036e-13, 0.0, 0.0, 2.2737367544323206e-13]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 150 number of matrices\n",
            "DEBUGGING: ACT_MAT has 150 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.29010425975609766, 0.2929632685309029, 0.29489987865295025, 0.28630211627598573, 0.2889429287578225, 0.2902419407767971, 0.2885056799559342, 0.28573321734469903, 0.28372291106189057, 0.2858165351370506, 6.309887463631507e-12, 6.143953321402298e-12, 5.976343076726381e-12, 6.0367101785114965e-12, 5.868016669765924e-12, 5.467948806948845e-12, 5.5231806130796414e-12, 5.578970316242062e-12, 5.4056531725241235e-12, 5.230585350586759e-12, 5.053749166811591e-12, 5.1047971381935265e-12, 5.156360745650027e-12, 5.2084451976262895e-12, 4.801714996706894e-12, 4.161206030683725e-12, 4.2032384148320456e-12, 4.016024989281682e-12, 3.826920519028737e-12, 3.635905902611674e-12, 3.4429618456246887e-12, 3.2480688587692002e-12, 3.2808776351204044e-12, 3.314017813252934e-12, 3.3474927406595293e-12, 3.3813057986459892e-12, 3.1857900234371286e-12, 2.9882993414080287e-12, 2.788813804004845e-12, 2.587313261173294e-12, 2.383777359323295e-12, 2.178185539272843e-12, 1.970517034171324e-12, 1.7607508674021656e-12, 1.7785362296991572e-12, 1.5668308628847729e-12, 1.123316678786066e-12, 6.753225534340459e-13, 4.52473614132083e-13, 2.2737367544328376e-13], [0.2936160381189825, 0.2793885987540113, 0.2797997297209012, 0.2802762478831871, 0.2733686580875657, 0.27184702470891536, 0.27412469983324234, 0.27515676529350586, 0.27759646086587214, 0.2793354565472658, 4.586406239827522e-12, 4.632733575583356e-12, 4.449858484990024e-12, 4.035465792023693e-12, 3.84655769351567e-12, 3.88541181163199e-12, 3.924658395587868e-12, 3.9643014096847154e-12, 4.004344858267389e-12, 4.044792786128676e-12, 3.855978899682267e-12, 3.435587423025959e-12, 3.2406199470533116e-12, 3.273353481872032e-12, 3.306417658456598e-12, 3.1101454373872383e-12, 2.9118906686302573e-12, 2.9413037056871286e-12, 2.9710138441284128e-12, 2.771353705742607e-12, 2.5696767982821448e-12, 2.595633129577924e-12, 2.6218516460383072e-12, 2.64833499599829e-12, 2.6750858545437274e-12, 2.7021069237815427e-12, 2.7294009331126694e-12, 2.5273002602721588e-12, 2.5528285457294532e-12, 2.5786146926560134e-12, 2.374990926477609e-12, 2.3989807338157666e-12, 2.1935424832045805e-12, 1.986029098748889e-12, 1.7764196195006638e-12, 1.3350224935495922e-12, 1.1188371900062677e-12, 6.707978172925219e-13, 6.775735528207291e-13, 4.547473508863607e-13], [0.2851601983857441, 0.2856737158137205, 0.28561717807555703, 0.27698180881294465, 0.27829629777948267, 0.2799636328007448, 0.2799330986336207, 0.28211292307352376, 0.28468350696356354, 0.27971188994948326, 0.28185727442528696, 0.2846007732528416, 0.278422233991353, 6.008881581178286e-12, 5.839906975489953e-12, 5.439555176367056e-12, 5.264829798913006e-12, 5.088339518656337e-12, 4.680396129060373e-12, 4.4980024784012045e-12, 4.313766467634316e-12, 4.127669487061649e-12, 3.710022359772914e-12, 3.7474973331039535e-12, 3.3260100830477597e-12, 3.3596061444926867e-12, 3.3935415600936227e-12, 3.198149378434738e-12, 3.0007835383753107e-12, 3.0310944832073845e-12, 3.0617115991993784e-12, 3.0926379789892712e-12, 2.8942063672182216e-12, 2.6937703957323648e-12, 2.7209801977094594e-12, 2.7484648461711713e-12, 2.5465567381090296e-12, 2.3426091542079285e-12, 2.3662718729373016e-12, 2.3901736090275775e-12, 2.184646397559945e-12, 2.2067135328888336e-12, 1.999333189338939e-12, 1.7898580948441488e-12, 1.1189263318326238e-12, 6.708878595417843e-13, 4.4799412535207294e-13, 2.2284893930191173e-13, 2.2509993868879974e-13, 2.2737367544323206e-13]]\n",
            "DEBUGGING: traj_returns = [0.29010425975609766, 0.2936160381189825, 0.2851601983857441]\n",
            "DEBUGGING: actions = [[58], [60], [32], [44], [20], [15], [10], [62], [47], [0], [7], [0], [70], [29], [52], [65], [56], [64], [30], [8], [58], [51], [0], [21], [57], [39], [12], [38], [11], [28], [69], [79], [28], [0], [5], [2], [5], [61], [65], [25], [0], [97], [31], [1], [87], [75], [105], [54], [52], [93], [54], [46], [16], [27], [10], [31], [59], [41], [53], [0], [11], [23], [48], [45], [41], [44], [69], [1], [47], [75], [53], [19], [2], [28], [72], [60], [22], [75], [49], [32], [74], [40], [17], [4], [57], [62], [90], [29], [90], [88], [38], [4], [34], [94], [62], [43], [49], [58], [92], [38], [4], [47], [41], [1], [48], [23], [26], [13], [33], [23], [9], [65], [0], [32], [61], [5], [36], [32], [13], [6], [31], [11], [3], [51], [39], [77], [22], [28], [39], [48], [18], [31], [21], [31], [80], [33], [24], [63], [15], [65], [60], [64], [97], [80], [69], [46], [66], [4], [32], [92]]\n",
            "DEBUGGING: actions length = 150\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        ...,\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        ...,\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[2.89626832e-01 2.86008528e-01 2.86772262e-01 2.81186724e-01\n",
            "  2.80202628e-01 2.80684199e-01 2.80854493e-01 2.81000969e-01\n",
            "  2.82000960e-01 2.81621294e-01 9.39524248e-02 9.48669244e-02\n",
            "  9.28074113e-02 5.36035252e-12 5.18482711e-12 4.93097193e-12\n",
            "  4.90422294e-12 4.87720375e-12 4.69679805e-12 4.59112687e-12\n",
            "  4.40783151e-12 4.22268468e-12 4.03566768e-12 4.07643200e-12\n",
            "  3.81138091e-12 3.54365254e-12 3.50289021e-12 3.38515936e-12\n",
            "  3.26623930e-12 3.14611803e-12 3.02478341e-12 2.97877999e-12\n",
            "  2.93231188e-12 2.88537440e-12 2.91451960e-12 2.94395919e-12\n",
            "  2.82058256e-12 2.61940292e-12 2.56930474e-12 2.51870052e-12\n",
            "  2.31447156e-12 2.26129327e-12 2.05446424e-12 1.84554602e-12\n",
            "  1.55796073e-12 1.19091374e-12 8.96715998e-13 5.22989770e-13\n",
            "  4.51715702e-13 3.03164901e-13]]\n",
            "DEBUGGING: baseline2 looks like: 0.28962683208694145\n",
            "DEBUGGING: ADS looks like: [2.90104260e-01 2.92963269e-01 2.94899879e-01 2.86302116e-01\n",
            " 2.88942929e-01 2.90241941e-01 2.88505680e-01 2.85733217e-01\n",
            " 2.83722911e-01 2.85816535e-01 6.30988746e-12 6.14395332e-12\n",
            " 5.97634308e-12 6.03671018e-12 5.86801667e-12 5.46794881e-12\n",
            " 5.52318061e-12 5.57897032e-12 5.40565317e-12 5.23058535e-12\n",
            " 5.05374917e-12 5.10479714e-12 5.15636075e-12 5.20844520e-12\n",
            " 4.80171500e-12 4.16120603e-12 4.20323841e-12 4.01602499e-12\n",
            " 3.82692052e-12 3.63590590e-12 3.44296185e-12 3.24806886e-12\n",
            " 3.28087764e-12 3.31401781e-12 3.34749274e-12 3.38130580e-12\n",
            " 3.18579002e-12 2.98829934e-12 2.78881380e-12 2.58731326e-12\n",
            " 2.38377736e-12 2.17818554e-12 1.97051703e-12 1.76075087e-12\n",
            " 1.77853623e-12 1.56683086e-12 1.12331668e-12 6.75322553e-13\n",
            " 4.52473614e-13 2.27373675e-13 2.93616038e-01 2.79388599e-01\n",
            " 2.79799730e-01 2.80276248e-01 2.73368658e-01 2.71847025e-01\n",
            " 2.74124700e-01 2.75156765e-01 2.77596461e-01 2.79335457e-01\n",
            " 4.58640624e-12 4.63273358e-12 4.44985848e-12 4.03546579e-12\n",
            " 3.84655769e-12 3.88541181e-12 3.92465840e-12 3.96430141e-12\n",
            " 4.00434486e-12 4.04479279e-12 3.85597890e-12 3.43558742e-12\n",
            " 3.24061995e-12 3.27335348e-12 3.30641766e-12 3.11014544e-12\n",
            " 2.91189067e-12 2.94130371e-12 2.97101384e-12 2.77135371e-12\n",
            " 2.56967680e-12 2.59563313e-12 2.62185165e-12 2.64833500e-12\n",
            " 2.67508585e-12 2.70210692e-12 2.72940093e-12 2.52730026e-12\n",
            " 2.55282855e-12 2.57861469e-12 2.37499093e-12 2.39898073e-12\n",
            " 2.19354248e-12 1.98602910e-12 1.77641962e-12 1.33502249e-12\n",
            " 1.11883719e-12 6.70797817e-13 6.77573553e-13 4.54747351e-13\n",
            " 2.85160198e-01 2.85673716e-01 2.85617178e-01 2.76981809e-01\n",
            " 2.78296298e-01 2.79963633e-01 2.79933099e-01 2.82112923e-01\n",
            " 2.84683507e-01 2.79711890e-01 2.81857274e-01 2.84600773e-01\n",
            " 2.78422234e-01 6.00888158e-12 5.83990698e-12 5.43955518e-12\n",
            " 5.26482980e-12 5.08833952e-12 4.68039613e-12 4.49800248e-12\n",
            " 4.31376647e-12 4.12766949e-12 3.71002236e-12 3.74749733e-12\n",
            " 3.32601008e-12 3.35960614e-12 3.39354156e-12 3.19814938e-12\n",
            " 3.00078354e-12 3.03109448e-12 3.06171160e-12 3.09263798e-12\n",
            " 2.89420637e-12 2.69377040e-12 2.72098020e-12 2.74846485e-12\n",
            " 2.54655674e-12 2.34260915e-12 2.36627187e-12 2.39017361e-12\n",
            " 2.18464640e-12 2.20671353e-12 1.99933319e-12 1.78985809e-12\n",
            " 1.11892633e-12 6.70887860e-13 4.47994125e-13 2.22848939e-13\n",
            " 2.25099939e-13 2.27373675e-13]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(0.2576, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-1.7176e-36, -1.8524e-36, -1.7126e-36,  ..., -1.8773e-36,\n",
            "         -1.6430e-36, -5.1619e-34],\n",
            "        [ 2.8285e-39,  3.2141e-39,  2.8478e-39,  ...,  6.0123e-39,\n",
            "          3.9507e-39,  1.0669e-36],\n",
            "        ...,\n",
            "        [-3.8871e-30, -4.4063e-30, -3.6589e-30,  ..., -6.7966e-30,\n",
            "         -4.1423e-30, -1.2470e-27],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]])\n",
            "   Last layer:\n",
            "tensor([[ 1.0546e-13, -1.3269e-14,  7.3893e-14,  1.3409e-14,  4.1515e-14,\n",
            "          3.7243e-14,  2.1333e-15,  3.3163e-14,  3.0197e-15, -5.2903e-15,\n",
            "          1.6926e-14,  1.6888e-13,  5.1114e-14,  2.8465e-14,  8.7558e-14,\n",
            "          1.0955e-14,  3.5569e-14,  1.3641e-14,  1.7261e-15, -1.2362e-14],\n",
            "        [ 2.0608e-14,  6.3029e-15, -4.8692e-15,  3.1036e-15,  2.1408e-14,\n",
            "          8.1213e-14,  2.7368e-14, -1.7477e-14,  5.4630e-15,  6.6189e-14,\n",
            "         -2.4345e-14,  3.3150e-14,  9.5205e-14,  8.9104e-14,  8.6053e-15,\n",
            "          9.0232e-14, -5.0112e-14,  3.0932e-14,  1.0355e-13,  1.2426e-13],\n",
            "        [-8.9092e-15,  1.8285e-15, -4.5031e-16, -3.5275e-14,  2.1681e-15,\n",
            "         -2.7460e-14, -3.5817e-14,  2.9873e-14, -1.1071e-14,  1.1255e-14,\n",
            "          2.3174e-14, -3.4697e-14, -1.6686e-14, -7.5978e-15, -3.1424e-15,\n",
            "          2.9753e-14, -2.4730e-14, -1.9959e-14, -2.1229e-14,  2.4371e-14],\n",
            "        [ 1.0601e-14, -5.2110e-14, -2.8481e-14, -7.7279e-14, -1.6080e-14,\n",
            "         -5.8197e-14, -3.4463e-14, -4.1020e-14, -3.9682e-14, -1.0420e-13,\n",
            "         -9.6475e-15, -3.6009e-14, -2.3542e-15, -3.7712e-14, -1.3242e-14,\n",
            "         -3.9489e-14, -3.7650e-14, -2.4969e-14, -1.6583e-14, -3.2778e-14],\n",
            "        [ 4.9631e-14,  7.6278e-14,  8.7014e-14,  5.9228e-14,  6.7993e-14,\n",
            "          2.8359e-14,  8.4146e-14,  6.0811e-14,  1.2301e-13,  6.8669e-14,\n",
            "          7.8938e-14,  1.7261e-13,  1.3783e-13,  5.0734e-14,  4.2381e-14,\n",
            "          9.5258e-14,  1.1382e-13,  4.9444e-14,  8.2784e-14,  9.6779e-14],\n",
            "        [ 2.8417e-14,  5.4509e-14,  2.8820e-15,  6.4210e-14, -6.0149e-15,\n",
            "         -7.7600e-15,  2.5273e-14,  4.5915e-14,  5.9170e-14,  3.0091e-15,\n",
            "         -3.7359e-14,  1.1209e-14, -7.1422e-15,  8.0529e-15,  5.3871e-14,\n",
            "          1.4293e-14,  1.4817e-14, -1.6132e-14, -4.4356e-14, -3.1531e-14],\n",
            "        [ 1.7921e-13,  2.1503e-13,  5.6803e-14,  1.5299e-13,  1.4340e-13,\n",
            "          1.2399e-13,  2.2717e-14,  1.1842e-13,  1.5679e-13,  2.3453e-13,\n",
            "          1.2054e-13,  6.1133e-14,  8.2582e-14,  5.4014e-14,  1.6941e-13,\n",
            "          1.2766e-13,  3.5289e-14,  8.2392e-14,  1.0241e-13,  3.7536e-14],\n",
            "        [-2.3422e-14, -1.1004e-14,  1.5725e-14, -1.3385e-14,  4.1514e-15,\n",
            "          5.4442e-15,  3.8009e-15, -8.6576e-15, -1.4139e-14,  4.9669e-14,\n",
            "          2.6640e-15,  2.7686e-14,  1.3242e-14,  2.1596e-14,  1.6326e-15,\n",
            "         -4.9864e-15, -2.5652e-16, -1.0717e-14,  4.6989e-15, -1.6435e-14],\n",
            "        [-3.7253e-15, -2.6226e-14, -9.2784e-15, -2.7354e-14, -8.1875e-15,\n",
            "         -1.5662e-14, -2.5985e-15, -2.9260e-14, -1.8560e-14, -1.7060e-14,\n",
            "         -8.9156e-15, -1.2785e-14, -2.9587e-14, -1.0765e-14, -2.7921e-14,\n",
            "         -1.4842e-14, -3.3057e-15, -2.3109e-15, -6.7366e-15, -2.1467e-14],\n",
            "        [ 1.0084e-14,  1.2150e-14,  1.2543e-14,  5.8101e-15,  1.4907e-14,\n",
            "          1.5554e-14,  5.4951e-15,  1.3057e-14,  4.4195e-15,  1.7696e-14,\n",
            "          1.3709e-15,  1.3455e-14,  2.2429e-15,  1.4868e-14,  1.1860e-14,\n",
            "          8.2761e-15, -1.0013e-15,  1.0657e-14,  9.8316e-15,  1.1074e-14]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-4.5338e-36, -4.8954e-36, -4.5207e-36,  ..., -4.9497e-36,\n",
            "         -4.3329e-36, -1.3623e-33],\n",
            "        [ 1.0616e-39,  1.2065e-39,  1.0690e-39,  ...,  2.2559e-39,\n",
            "          1.4826e-39,  4.0040e-37],\n",
            "        ...,\n",
            "        [-2.6891e-30, -3.0498e-30, -2.5319e-30,  ..., -4.6981e-30,\n",
            "         -2.8639e-30, -8.6254e-28],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]])\n",
            "   Last layer:\n",
            "tensor([[-1.2097e-13, -8.2623e-14, -1.3830e-14, -3.8261e-14, -8.2796e-14,\n",
            "         -1.0093e-13,  4.2727e-14, -7.3476e-14,  4.3518e-14,  1.2327e-13,\n",
            "          4.2637e-14, -7.1841e-15, -3.7011e-14, -9.9649e-14, -9.6028e-15,\n",
            "          1.1452e-14, -5.3601e-14,  5.0579e-16, -6.1162e-14, -6.1730e-14],\n",
            "        [ 4.5396e-14,  6.5415e-16,  1.3847e-13,  1.6011e-13,  2.7982e-13,\n",
            "          5.7417e-14,  7.3268e-14,  4.3289e-14,  1.6735e-13,  1.8078e-13,\n",
            "          3.4207e-14,  2.4656e-13,  7.7681e-14, -2.9447e-14,  9.2808e-14,\n",
            "          1.0912e-14,  1.7694e-13,  7.3670e-14,  1.6643e-13, -5.7688e-15],\n",
            "        [ 1.2592e-14,  3.5645e-14,  5.4574e-14, -3.5486e-14,  3.0110e-14,\n",
            "         -1.3376e-14, -2.8559e-14,  9.0746e-15, -1.3529e-14, -1.6797e-14,\n",
            "          5.9886e-15, -3.6158e-15,  4.9027e-14,  2.0340e-15, -7.6925e-14,\n",
            "         -6.5736e-14,  5.5331e-14, -2.3767e-14, -3.7745e-15, -2.0171e-14],\n",
            "        [ 5.6511e-14,  8.8288e-14,  1.3370e-14,  1.2692e-13,  9.1061e-14,\n",
            "          6.2144e-14,  7.2918e-14,  1.5159e-13,  6.3988e-14,  5.3194e-14,\n",
            "          3.9190e-15,  5.6580e-14,  1.0595e-13,  8.0399e-14,  1.7508e-14,\n",
            "          1.7693e-13,  9.0375e-14,  2.1245e-14,  1.2422e-13,  8.1315e-14],\n",
            "        [-9.6390e-14,  4.6853e-14, -6.8311e-14, -3.8921e-14, -1.9625e-14,\n",
            "         -9.8246e-14, -1.5068e-13, -2.5594e-13, -8.3428e-14, -2.2948e-13,\n",
            "         -1.3903e-13, -3.2985e-14, -1.3364e-13, -5.3840e-14, -1.1450e-13,\n",
            "         -1.4465e-13, -5.9538e-14, -1.1393e-13, -1.4212e-13, -4.8266e-14],\n",
            "        [ 7.4135e-14,  5.6254e-14,  6.0083e-14,  1.1571e-13,  6.1954e-14,\n",
            "          1.4788e-13,  7.3321e-14,  2.2140e-13,  8.8502e-14,  4.4871e-14,\n",
            "          1.7922e-13,  1.0705e-13,  8.7512e-14,  8.3891e-14,  1.8962e-14,\n",
            "          5.0909e-14,  2.1121e-13,  1.4773e-13, -2.0249e-14,  1.0011e-13],\n",
            "        [-1.5298e-14, -1.0707e-13, -3.9099e-15,  1.6767e-13, -1.8431e-13,\n",
            "         -1.0366e-13, -8.1899e-14, -1.5114e-13,  1.9117e-13, -2.9246e-13,\n",
            "         -7.9340e-14, -5.9208e-14, -1.6609e-13, -5.2510e-14, -1.9669e-14,\n",
            "         -1.0144e-14, -1.7508e-13,  1.2108e-14, -7.9434e-15,  1.0487e-14],\n",
            "        [ 5.9431e-14,  2.7698e-15,  4.0709e-14,  1.2322e-14,  4.3796e-14,\n",
            "          6.6857e-15, -1.0383e-14, -9.8864e-15, -1.1297e-14,  3.3919e-14,\n",
            "          6.9173e-15, -5.8140e-14, -8.9345e-15,  6.5332e-14, -5.0264e-14,\n",
            "          4.8253e-14,  3.5334e-14,  7.8112e-14,  1.4386e-14, -1.5189e-14],\n",
            "        [-5.3431e-15, -2.1140e-14,  1.2404e-14,  7.5289e-15, -1.7262e-14,\n",
            "         -1.1561e-14, -2.1849e-14, -1.0575e-14, -3.6633e-16,  1.5757e-14,\n",
            "          3.5453e-15,  1.6859e-14,  1.9294e-14, -2.6286e-14, -1.8319e-15,\n",
            "         -1.9892e-15, -7.6519e-15,  7.2397e-15, -1.0039e-14,  8.2309e-15],\n",
            "        [-2.6281e-14, -5.4648e-15,  1.4636e-14, -6.0249e-15,  8.1168e-15,\n",
            "         -1.6261e-14, -6.0536e-17, -1.2274e-14,  2.2685e-14,  3.3953e-16,\n",
            "         -1.5255e-14, -1.2915e-14, -3.3749e-14, -9.1337e-16,  5.9085e-15,\n",
            "          8.9087e-16, -1.2016e-14, -2.3173e-14, -1.2326e-14, -5.0776e-15]])\n",
            "DEBUGGING: training for one iteration takes 0.004858 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 7\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [41]\n",
            "DEBUGGING: rel_reward looks like: 4.264757095704378e-05\n",
            "DEBUGGING: rel_reward looks like: 8.37860479070127e-05\n",
            "DEBUGGING: rel_reward looks like: 0.004582325805182455\n",
            "DEBUGGING: rel_reward looks like: 0.008565751304464521\n",
            "DEBUGGING: rel_reward looks like: 0.0097271708765512\n",
            "DEBUGGING: rel_reward looks like: 4.253199079980282e-05\n",
            "DEBUGGING: rel_reward looks like: 2.3363628218219957e-05\n",
            "DEBUGGING: rel_reward looks like: 3.7765180645192925e-05\n",
            "DEBUGGING: rel_reward looks like: 2.067307618150772e-05\n",
            "DEBUGGING: rel_reward looks like: 2.1644516744644295e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [14]\n",
            "DEBUGGING: rel_reward looks like: 0.0024815973101786327\n",
            "DEBUGGING: rel_reward looks like: 0.0024742361532241177\n",
            "DEBUGGING: rel_reward looks like: 0.0002694337440329806\n",
            "DEBUGGING: rel_reward looks like: 0.0003082576753320998\n",
            "DEBUGGING: rel_reward looks like: 0.0007492153039067685\n",
            "DEBUGGING: rel_reward looks like: 0.00019474987263055427\n",
            "DEBUGGING: rel_reward looks like: 0.002006283029386517\n",
            "DEBUGGING: rel_reward looks like: 0.0009925423717776687\n",
            "DEBUGGING: rel_reward looks like: 0.28412739040441015\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: the action_prob is: tensor([0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [21]\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: the action_prob is: tensor([0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [42]\n",
            "DEBUGGING: rel_reward looks like: 4.547473508865675e-13\n",
            "DEBUGGING: rel_reward looks like: 6.821210263295411e-13\n",
            "DEBUGGING: rel_reward looks like: 9.094947017733418e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508862573e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544312866e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [12]\n",
            "DEBUGGING: rel_reward looks like: 1.1368683772161603e-12\n",
            "DEBUGGING: rel_reward looks like: 6.821210263289207e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508862573e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.8867413289890465 and immediate abs rewards look like: [0.00012311250566199305, 0.00024185833217416075, 0.01322631686934983, 0.02461068871389216, 0.027708229030849907, 0.00011997556339338189, 6.590204657186405e-05, 0.00010652217724782531, 5.8309211908635916e-05, 6.104793556005461e-05, 0.006999145113695704, 0.006961066026178742, 0.000756154797727504, 0.0008648794846521923, 0.0021014276453570346, 0.0005458326140796999, 0.005621987969334441, 0.0027757131019825465, 0.7937931598330579, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 9.094947017729282e-13, 1.3642420526593924e-12, 1.8189894035458565e-12, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 2.2737367544323206e-12, 1.3642420526593924e-12, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.31675136587071656 and immediate relative rewards look like: [4.264757095704378e-05, 8.37860479070127e-05, 0.004582325805182455, 0.008565751304464521, 0.0097271708765512, 4.253199079980282e-05, 2.3363628218219957e-05, 3.7765180645192925e-05, 2.067307618150772e-05, 2.1644516744644295e-05, 0.0024815973101786327, 0.0024742361532241177, 0.0002694337440329806, 0.0003082576753320998, 0.0007492153039067685, 0.00019474987263055427, 0.002006283029386517, 0.0009925423717776687, 0.28412739040441015, 0.0, 0.0, 2.2737367544323206e-13, 2.2737367544318036e-13, 2.2737367544323206e-13, 2.2737367544328376e-13, 2.2737367544323206e-13, 2.2737367544328376e-13, 0.0, 2.2737367544323206e-13, 0.0, 4.547473508865675e-13, 6.821210263295411e-13, 9.094947017733418e-13, 4.547473508862573e-13, 2.2737367544323206e-13, 0.0, 2.2737367544318036e-13, 2.2737367544312866e-13, 0.0, 2.2737367544318036e-13, 1.1368683772161603e-12, 6.821210263289207e-13, 4.547473508862573e-13, 2.2737367544323206e-13, 0.0, 2.2737367544318036e-13, 0.0, 2.2737367544323206e-13, 2.2737367544328376e-13, 0.0]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [10]\n",
            "DEBUGGING: rel_reward looks like: 0.022389719633495044\n",
            "DEBUGGING: rel_reward looks like: 0.0008936184361799202\n",
            "DEBUGGING: rel_reward looks like: 0.0038888712367526846\n",
            "DEBUGGING: rel_reward looks like: 0.0027189015434267304\n",
            "DEBUGGING: rel_reward looks like: 0.00016296307644948516\n",
            "DEBUGGING: rel_reward looks like: 0.0005162695425633089\n",
            "DEBUGGING: rel_reward looks like: 0.0016842866618948667\n",
            "DEBUGGING: rel_reward looks like: 0.00022982399669500944\n",
            "DEBUGGING: rel_reward looks like: 0.001987742563560883\n",
            "DEBUGGING: rel_reward looks like: 0.00022255772361473008\n",
            "DEBUGGING: the action_prob is: tensor([0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [14]\n",
            "DEBUGGING: rel_reward looks like: 0.0002855211278244161\n",
            "DEBUGGING: rel_reward looks like: 0.00123182535748019\n",
            "DEBUGGING: rel_reward looks like: 0.0026166956778342194\n",
            "DEBUGGING: rel_reward looks like: 0.0014031586305934141\n",
            "DEBUGGING: rel_reward looks like: 0.0008015879424868916\n",
            "DEBUGGING: rel_reward looks like: 0.002709801316935963\n",
            "DEBUGGING: rel_reward looks like: 0.00039187868130463594\n",
            "DEBUGGING: rel_reward looks like: 0.0015821096807271515\n",
            "DEBUGGING: rel_reward looks like: 0.0014962549235038535\n",
            "DEBUGGING: rel_reward looks like: 4.252079143651842e-06\n",
            "DEBUGGING: the action_prob is: tensor([0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [37]\n",
            "DEBUGGING: rel_reward looks like: 0.0003695508027822932\n",
            "DEBUGGING: rel_reward looks like: 0.0004920834412633554\n",
            "DEBUGGING: rel_reward looks like: 0.00014872320890176217\n",
            "DEBUGGING: rel_reward looks like: 0.00016416531022291243\n",
            "DEBUGGING: rel_reward looks like: 0.0003766773347388883\n",
            "DEBUGGING: rel_reward looks like: 2.1584092084955603e-06\n",
            "DEBUGGING: rel_reward looks like: 8.223895262966613e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0001577874089982603\n",
            "DEBUGGING: rel_reward looks like: 1.3402047756547052e-06\n",
            "DEBUGGING: rel_reward looks like: 9.15823845109266e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [23]\n",
            "DEBUGGING: rel_reward looks like: 0.00020061516402448536\n",
            "DEBUGGING: rel_reward looks like: 0.0001533492617527637\n",
            "DEBUGGING: rel_reward looks like: 2.7605673726847e-06\n",
            "DEBUGGING: rel_reward looks like: 0.2718429375272319\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: the action_prob is: tensor([0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [76]\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508865675e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.8867413289776778 and immediate abs rewards look like: [0.06463332901012109, 0.00252188773765738, 0.010965007331378729, 0.007636364256995876, 0.00045645709042219096, 0.001445827455881954, 0.0047144572463366785, 0.0006422128217309364, 0.005553207655793813, 0.000620529342086229, 0.0007959050049066718, 0.003432796883316769, 0.0072831101724659675, 0.0038952248173700355, 0.0022221180165615806, 0.0075059407063235994, 0.0010825323129211029, 0.004368733919363876, 0.00412512348339078, 1.170529594674008e-05, 0.0010173101145483088, 0.0013541208963943063, 0.0004090568545507267, 0.0004514625438787334, 0.0010357108349126065, 5.932519798079738e-06, 0.00022603830620937515, 0.0004336517476986046, 3.682742772070924e-06, 0.0002516584918339504, 0.0005512182960956125, 0.0004212640724290395, 7.582361376989866e-06, 0.7466601686292051, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.3213038098133822 and immediate relative rewards look like: [0.022389719633495044, 0.0008936184361799202, 0.0038888712367526846, 0.0027189015434267304, 0.00016296307644948516, 0.0005162695425633089, 0.0016842866618948667, 0.00022982399669500944, 0.001987742563560883, 0.00022255772361473008, 0.0002855211278244161, 0.00123182535748019, 0.0026166956778342194, 0.0014031586305934141, 0.0008015879424868916, 0.002709801316935963, 0.00039187868130463594, 0.0015821096807271515, 0.0014962549235038535, 4.252079143651842e-06, 0.0003695508027822932, 0.0004920834412633554, 0.00014872320890176217, 0.00016416531022291243, 0.0003766773347388883, 2.1584092084955603e-06, 8.223895262966613e-05, 0.0001577874089982603, 1.3402047756547052e-06, 9.15823845109266e-05, 0.00020061516402448536, 0.0001533492617527637, 2.7605673726847e-06, 0.2718429375272319, 2.2737367544323206e-13, 0.0, 2.2737367544328376e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 2.2737367544323206e-13, 4.547473508865675e-13, 2.2737367544318036e-13, 2.2737367544323206e-13, 2.2737367544318036e-13, 2.2737367544323206e-13, 2.2737367544318036e-13, 2.2737367544323206e-13]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [27]\n",
            "DEBUGGING: rel_reward looks like: 0.005020466020408541\n",
            "DEBUGGING: rel_reward looks like: 0.009595503483672503\n",
            "DEBUGGING: rel_reward looks like: 0.0004440284186696286\n",
            "DEBUGGING: rel_reward looks like: 0.00040032544244147553\n",
            "DEBUGGING: rel_reward looks like: 0.0011807291859167051\n",
            "DEBUGGING: rel_reward looks like: 0.0020865399214657627\n",
            "DEBUGGING: rel_reward looks like: 0.008160548276036886\n",
            "DEBUGGING: rel_reward looks like: 0.00047845562553380935\n",
            "DEBUGGING: rel_reward looks like: 2.432087777241106e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0006108973983724716\n",
            "DEBUGGING: the action_prob is: tensor([0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [16]\n",
            "DEBUGGING: rel_reward looks like: 5.027631791757378e-05\n",
            "DEBUGGING: rel_reward looks like: 8.803972449282282e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0018364995717387994\n",
            "DEBUGGING: rel_reward looks like: 5.31288886446763e-06\n",
            "DEBUGGING: rel_reward looks like: 0.0029407011878758915\n",
            "DEBUGGING: rel_reward looks like: 0.0003321665750328585\n",
            "DEBUGGING: rel_reward looks like: 0.00012610729129195638\n",
            "DEBUGGING: rel_reward looks like: 0.0013128380019356515\n",
            "DEBUGGING: rel_reward looks like: 0.0015664173477833872\n",
            "DEBUGGING: rel_reward looks like: 6.909416599411236e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [27]\n",
            "DEBUGGING: rel_reward looks like: 5.62949723423409e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00011991174730006375\n",
            "DEBUGGING: rel_reward looks like: 3.258843016492727e-05\n",
            "DEBUGGING: rel_reward looks like: 6.016257114061071e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00020227107408056218\n",
            "DEBUGGING: rel_reward looks like: 2.3827744461081944e-05\n",
            "DEBUGGING: rel_reward looks like: 6.468367240399394e-05\n",
            "DEBUGGING: rel_reward looks like: 3.959808976278197e-06\n",
            "DEBUGGING: rel_reward looks like: 0.0005856410017355039\n",
            "DEBUGGING: rel_reward looks like: 0.00011705156336874175\n",
            "DEBUGGING: the action_prob is: tensor([0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [4]\n",
            "DEBUGGING: rel_reward looks like: 0.0008547875309844184\n",
            "DEBUGGING: rel_reward looks like: 7.321534488978582e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0001319080227424282\n",
            "DEBUGGING: rel_reward looks like: 6.426239936913507e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0001391484601134541\n",
            "DEBUGGING: rel_reward looks like: 8.11897183493585e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00010673100171984888\n",
            "DEBUGGING: rel_reward looks like: 0.0004346140392288079\n",
            "DEBUGGING: rel_reward looks like: 0.0006579937871583284\n",
            "DEBUGGING: rel_reward looks like: 1.6232523694722543e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [70]\n",
            "DEBUGGING: rel_reward looks like: 2.262237854661907e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0001373613440308626\n",
            "DEBUGGING: rel_reward looks like: 7.662265471201952e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0009224208037295154\n",
            "DEBUGGING: rel_reward looks like: 4.827993019370127e-06\n",
            "DEBUGGING: rel_reward looks like: 2.144524092539384e-06\n",
            "DEBUGGING: rel_reward looks like: 5.610760764036131e-05\n",
            "DEBUGGING: rel_reward looks like: 6.975620588745701e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00015803498585630738\n",
            "DEBUGGING: rel_reward looks like: 0.0011077562106978518\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.12099944202645929 and immediate abs rewards look like: [0.014492786751816311, 0.027560670892853523, 0.0012631222571144463, 0.001138295270720846, 0.0033559705707375542, 0.0059235417875243, 0.023118889911984297, 0.0013444067453747266, 6.830624670328689e-05, 0.0017156902777060168, 0.00014111354039414437, 0.0002470939225531765, 0.0051538999491640425, 1.4882557479722891e-05, 0.008237499984716123, 0.000927729670820554, 0.0003520962773109204, 0.0036650305937655503, 0.004367203366655303, 0.0001923341892506869, 0.00015669484673708212, 0.00033375085422449047, 9.069263387573301e-05, 0.00016742519346735207, 0.0005628621856885729, 6.629234349020408e-05, 0.0001799553483579075, 1.1015804830094567e-05, 0.0016291900615215127, 0.0003254341172578279, 0.0023762558698763314, 0.00020336010084065492, 0.0003663557804429729, 0.0001784561354725156, 0.0003863892561639659, 0.00022541729640579433, 0.00029630673680003383, 0.0012064472452948394, 0.0018257343494951783, 4.5010722260485636e-05, 6.272795826589572e-05, 0.0003808707256212074, 0.00021242743105176487, 0.00255710883948268, 1.3371679870033404e-05, 5.9394765230535995e-06, 0.00015539536479991511, 0.00019318561817271984, 0.0004376378615233989, 0.0030671654239995405]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.04271339784565501 and immediate relative rewards look like: [0.005020466020408541, 0.009595503483672503, 0.0004440284186696286, 0.00040032544244147553, 0.0011807291859167051, 0.0020865399214657627, 0.008160548276036886, 0.00047845562553380935, 2.432087777241106e-05, 0.0006108973983724716, 5.027631791757378e-05, 8.803972449282282e-05, 0.0018364995717387994, 5.31288886446763e-06, 0.0029407011878758915, 0.0003321665750328585, 0.00012610729129195638, 0.0013128380019356515, 0.0015664173477833872, 6.909416599411236e-05, 5.62949723423409e-05, 0.00011991174730006375, 3.258843016492727e-05, 6.016257114061071e-05, 0.00020227107408056218, 2.3827744461081944e-05, 6.468367240399394e-05, 3.959808976278197e-06, 0.0005856410017355039, 0.00011705156336874175, 0.0008547875309844184, 7.321534488978582e-05, 0.0001319080227424282, 6.426239936913507e-05, 0.0001391484601134541, 8.11897183493585e-05, 0.00010673100171984888, 0.0004346140392288079, 0.0006579937871583284, 1.6232523694722543e-05, 2.262237854661907e-05, 0.0001373613440308626, 7.662265471201952e-05, 0.0009224208037295154, 4.827993019370127e-06, 2.144524092539384e-06, 5.610760764036131e-05, 6.975620588745701e-05, 0.00015803498585630738, 0.0011077562106978518]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 150 number of matrices\n",
            "DEBUGGING: ACT_MAT has 150 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.2678488751739718, 0.2705113410131462, 0.2731591464295346, 0.2712897178023759, 0.2653777439372842, 0.25823290208154853, 0.2607983536270189, 0.2634090808068694, 0.266031631945681, 0.2686979382520197, 0.271390195692197, 0.2716248468505237, 0.271869303734646, 0.27434330302082127, 0.27680307610655475, 0.2788422836390384, 0.2814621553196039, 0.2822786588790075, 0.2841273904113433, 7.0031658738386695e-12, 7.073904923069363e-12, 7.1453585081508715e-12, 6.987863467381454e-12, 6.828777567614418e-12, 6.668084739566854e-12, 6.50576875163997e-12, 6.341813208279533e-12, 6.176201548319444e-12, 6.238587422544893e-12, 6.071933077880466e-12, 6.133265735232794e-12, 5.7358771559052795e-12, 5.1048041712886246e-12, 4.23768633284372e-12, 3.82115048682572e-12, 3.630077587255039e-12, 3.666745037631353e-12, 3.4741124870587596e-12, 3.2795341531471018e-12, 3.3126607607546483e-12, 3.116451601324715e-12, 1.9995790142510655e-12, 1.3307656443658025e-12, 8.848669631106517e-13, 6.641346340074946e-13, 6.708430646540349e-13, 4.4794887799076216e-13, 4.52473614132083e-13, 2.2737367544328376e-13, 0.0], [0.2419407719611569, 0.22176873972491098, 0.22310618311993038, 0.22143162816482595, 0.22092194608222143, 0.22298887172300197, 0.22471980018226126, 0.22528839749531956, 0.22733189242285307, 0.22762035339322442, 0.22969474310061586, 0.23172648684120348, 0.2328228903875993, 0.2325315098078435, 0.2334629809871213, 0.23501150812589333, 0.23464818869591653, 0.23662253536829483, 0.23741457140158354, 0.23830132977583807, 0.24070411888554993, 0.24276218998259358, 0.244717279334677, 0.24703894558159112, 0.24936846492057393, 0.2515068561473081, 0.25404514923040367, 0.25652819219977174, 0.25896000483916515, 0.26157440872160553, 0.2641240670071663, 0.26658934529610284, 0.2691272687215657, 0.2718429375294879, 2.2788093658137135e-12, 2.0721572630004862e-12, 2.0930881444449358e-12, 1.8845600696986385e-12, 1.9035960299986248e-12, 1.9228242727258835e-12, 1.942246740127155e-12, 1.9618653940678332e-12, 1.9816822162301344e-12, 1.772028829077679e-12, 1.3305873517081934e-12, 1.1143572487525385e-12, 8.959430033427337e-13, 6.753225534338923e-13, 4.524736141319801e-13, 2.2737367544323206e-13], [0.038754442749260945, 0.03407472396853778, 0.02472648533824775, 0.02452773426220012, 0.02437112001995823, 0.02342463720610255, 0.02155363362084524, 0.013528369035159953, 0.013181730716794085, 0.013290312968708761, 0.012807490475087162, 0.012886074906231906, 0.012927308264382912, 0.011202837063276882, 0.01131063047920446, 0.008454474031645018, 0.00820435096627491, 0.008159842095942378, 0.0069161657515219455, 0.005403786266402584, 0.005388577879200477, 0.0053861443503617525, 0.005319426871779483, 0.0053402408501157135, 0.0053334124030051545, 0.00518297103931777, 0.005211255853390594, 0.005198557758572323, 0.005247068635955601, 0.004708512761838483, 0.0046378395944138805, 0.00382126471053481, 0.003785908450146489, 0.0036909095226303644, 0.00366327992248609, 0.0035597287498713493, 0.003513675789416152, 0.0034413583714104075, 0.003037115487052121, 0.002403153232215952, 0.0024110310187083124, 0.0024125339799613067, 0.002298154177707519, 0.0022439712353489894, 0.0013348994258782563, 0.0013435064978372588, 0.0013549110845906256, 0.001311922703990166, 0.0012547136344471807, 0.0011077562106978518]]\n",
            "DEBUGGING: traj_returns = [0.2678488751739718, 0.2419407719611569, 0.038754442749260945]\n",
            "DEBUGGING: actions = [[41], [41], [13], [26], [1], [52], [36], [50], [44], [67], [14], [69], [9], [11], [23], [26], [64], [19], [0], [74], [21], [44], [25], [26], [0], [26], [39], [29], [36], [46], [42], [25], [19], [4], [87], [62], [39], [75], [53], [87], [12], [70], [45], [84], [4], [16], [12], [87], [0], [26], [10], [21], [6], [5], [17], [28], [6], [25], [45], [4], [14], [55], [23], [22], [11], [54], [66], [43], [56], [40], [37], [30], [78], [13], [48], [42], [18], [13], [65], [16], [23], [3], [66], [0], [11], [88], [42], [61], [38], [39], [76], [0], [76], [1], [61], [19], [22], [58], [12], [51], [27], [40], [39], [52], [56], [53], [10], [8], [31], [11], [16], [9], [8], [50], [50], [1], [6], [10], [46], [52], [27], [80], [75], [56], [1], [50], [71], [8], [22], [17], [4], [23], [85], [40], [13], [47], [79], [25], [20], [65], [70], [94], [2], [5], [101], [97], [69], [6], [83], [34]]\n",
            "DEBUGGING: actions length = 150\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        ...,\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        ...,\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[0.18284803 0.1754516  0.17366394 0.17241636 0.1702236  0.16821547\n",
            "  0.16902393 0.16740862 0.16884842 0.16986953 0.17129748 0.17207914\n",
            "  0.17253983 0.17269255 0.1738589  0.17410276 0.17477156 0.17568701\n",
            "  0.17615271 0.08123504 0.0820309  0.08271611 0.08334557 0.0841264\n",
            "  0.08490063 0.08556328 0.0864188  0.08724225 0.08806902 0.08876097\n",
            "  0.0895873  0.09013687 0.09097106 0.09184462 0.00122109 0.00118658\n",
            "  0.00117123 0.00114712 0.00101237 0.00080105 0.00080368 0.00080418\n",
            "  0.00076605 0.00074799 0.00044497 0.00044784 0.00045164 0.00043731\n",
            "  0.00041824 0.00036925]]\n",
            "DEBUGGING: baseline2 looks like: 0.18284802996146324\n",
            "DEBUGGING: ADS looks like: [2.67848875e-01 2.70511341e-01 2.73159146e-01 2.71289718e-01\n",
            " 2.65377744e-01 2.58232902e-01 2.60798354e-01 2.63409081e-01\n",
            " 2.66031632e-01 2.68697938e-01 2.71390196e-01 2.71624847e-01\n",
            " 2.71869304e-01 2.74343303e-01 2.76803076e-01 2.78842284e-01\n",
            " 2.81462155e-01 2.82278659e-01 2.84127390e-01 7.00316587e-12\n",
            " 7.07390492e-12 7.14535851e-12 6.98786347e-12 6.82877757e-12\n",
            " 6.66808474e-12 6.50576875e-12 6.34181321e-12 6.17620155e-12\n",
            " 6.23858742e-12 6.07193308e-12 6.13326574e-12 5.73587716e-12\n",
            " 5.10480417e-12 4.23768633e-12 3.82115049e-12 3.63007759e-12\n",
            " 3.66674504e-12 3.47411249e-12 3.27953415e-12 3.31266076e-12\n",
            " 3.11645160e-12 1.99957901e-12 1.33076564e-12 8.84866963e-13\n",
            " 6.64134634e-13 6.70843065e-13 4.47948878e-13 4.52473614e-13\n",
            " 2.27373675e-13 0.00000000e+00 2.41940772e-01 2.21768740e-01\n",
            " 2.23106183e-01 2.21431628e-01 2.20921946e-01 2.22988872e-01\n",
            " 2.24719800e-01 2.25288397e-01 2.27331892e-01 2.27620353e-01\n",
            " 2.29694743e-01 2.31726487e-01 2.32822890e-01 2.32531510e-01\n",
            " 2.33462981e-01 2.35011508e-01 2.34648189e-01 2.36622535e-01\n",
            " 2.37414571e-01 2.38301330e-01 2.40704119e-01 2.42762190e-01\n",
            " 2.44717279e-01 2.47038946e-01 2.49368465e-01 2.51506856e-01\n",
            " 2.54045149e-01 2.56528192e-01 2.58960005e-01 2.61574409e-01\n",
            " 2.64124067e-01 2.66589345e-01 2.69127269e-01 2.71842938e-01\n",
            " 2.27880937e-12 2.07215726e-12 2.09308814e-12 1.88456007e-12\n",
            " 1.90359603e-12 1.92282427e-12 1.94224674e-12 1.96186539e-12\n",
            " 1.98168222e-12 1.77202883e-12 1.33058735e-12 1.11435725e-12\n",
            " 8.95943003e-13 6.75322553e-13 4.52473614e-13 2.27373675e-13\n",
            " 3.87544427e-02 3.40747240e-02 2.47264853e-02 2.45277343e-02\n",
            " 2.43711200e-02 2.34246372e-02 2.15536336e-02 1.35283690e-02\n",
            " 1.31817307e-02 1.32903130e-02 1.28074905e-02 1.28860749e-02\n",
            " 1.29273083e-02 1.12028371e-02 1.13106305e-02 8.45447403e-03\n",
            " 8.20435097e-03 8.15984210e-03 6.91616575e-03 5.40378627e-03\n",
            " 5.38857788e-03 5.38614435e-03 5.31942687e-03 5.34024085e-03\n",
            " 5.33341240e-03 5.18297104e-03 5.21125585e-03 5.19855776e-03\n",
            " 5.24706864e-03 4.70851276e-03 4.63783959e-03 3.82126471e-03\n",
            " 3.78590845e-03 3.69090952e-03 3.66327992e-03 3.55972875e-03\n",
            " 3.51367579e-03 3.44135837e-03 3.03711549e-03 2.40315323e-03\n",
            " 2.41103102e-03 2.41253398e-03 2.29815418e-03 2.24397124e-03\n",
            " 1.33489943e-03 1.34350650e-03 1.35491108e-03 1.31192270e-03\n",
            " 1.25471363e-03 1.10775621e-03]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(0.3907, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-4.5338e-36, -4.8954e-36, -4.5207e-36,  ..., -4.9497e-36,\n",
            "         -4.3329e-36, -1.3623e-33],\n",
            "        [ 1.0616e-39,  1.2065e-39,  1.0690e-39,  ...,  2.2559e-39,\n",
            "          1.4826e-39,  4.0040e-37],\n",
            "        ...,\n",
            "        [-2.6891e-30, -3.0498e-30, -2.5319e-30,  ..., -4.6981e-30,\n",
            "         -2.8639e-30, -8.6254e-28],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]])\n",
            "   Last layer:\n",
            "tensor([[-1.2097e-13, -8.2623e-14, -1.3830e-14, -3.8261e-14, -8.2796e-14,\n",
            "         -1.0093e-13,  4.2727e-14, -7.3476e-14,  4.3518e-14,  1.2327e-13,\n",
            "          4.2637e-14, -7.1841e-15, -3.7011e-14, -9.9649e-14, -9.6028e-15,\n",
            "          1.1452e-14, -5.3601e-14,  5.0579e-16, -6.1162e-14, -6.1730e-14],\n",
            "        [ 4.5396e-14,  6.5415e-16,  1.3847e-13,  1.6011e-13,  2.7982e-13,\n",
            "          5.7417e-14,  7.3268e-14,  4.3289e-14,  1.6735e-13,  1.8078e-13,\n",
            "          3.4207e-14,  2.4656e-13,  7.7681e-14, -2.9447e-14,  9.2808e-14,\n",
            "          1.0912e-14,  1.7694e-13,  7.3670e-14,  1.6643e-13, -5.7688e-15],\n",
            "        [ 1.2592e-14,  3.5645e-14,  5.4574e-14, -3.5486e-14,  3.0110e-14,\n",
            "         -1.3376e-14, -2.8559e-14,  9.0746e-15, -1.3529e-14, -1.6797e-14,\n",
            "          5.9886e-15, -3.6158e-15,  4.9027e-14,  2.0340e-15, -7.6925e-14,\n",
            "         -6.5736e-14,  5.5331e-14, -2.3767e-14, -3.7745e-15, -2.0171e-14],\n",
            "        [ 5.6511e-14,  8.8288e-14,  1.3370e-14,  1.2692e-13,  9.1061e-14,\n",
            "          6.2144e-14,  7.2918e-14,  1.5159e-13,  6.3988e-14,  5.3194e-14,\n",
            "          3.9190e-15,  5.6580e-14,  1.0595e-13,  8.0399e-14,  1.7508e-14,\n",
            "          1.7693e-13,  9.0375e-14,  2.1245e-14,  1.2422e-13,  8.1315e-14],\n",
            "        [-9.6390e-14,  4.6853e-14, -6.8311e-14, -3.8921e-14, -1.9625e-14,\n",
            "         -9.8246e-14, -1.5068e-13, -2.5594e-13, -8.3428e-14, -2.2948e-13,\n",
            "         -1.3903e-13, -3.2985e-14, -1.3364e-13, -5.3840e-14, -1.1450e-13,\n",
            "         -1.4465e-13, -5.9538e-14, -1.1393e-13, -1.4212e-13, -4.8266e-14],\n",
            "        [ 7.4135e-14,  5.6254e-14,  6.0083e-14,  1.1571e-13,  6.1954e-14,\n",
            "          1.4788e-13,  7.3321e-14,  2.2140e-13,  8.8502e-14,  4.4871e-14,\n",
            "          1.7922e-13,  1.0705e-13,  8.7512e-14,  8.3891e-14,  1.8962e-14,\n",
            "          5.0909e-14,  2.1121e-13,  1.4773e-13, -2.0249e-14,  1.0011e-13],\n",
            "        [-1.5298e-14, -1.0707e-13, -3.9099e-15,  1.6767e-13, -1.8431e-13,\n",
            "         -1.0366e-13, -8.1899e-14, -1.5114e-13,  1.9117e-13, -2.9246e-13,\n",
            "         -7.9340e-14, -5.9208e-14, -1.6609e-13, -5.2510e-14, -1.9669e-14,\n",
            "         -1.0144e-14, -1.7508e-13,  1.2108e-14, -7.9434e-15,  1.0487e-14],\n",
            "        [ 5.9431e-14,  2.7698e-15,  4.0709e-14,  1.2322e-14,  4.3796e-14,\n",
            "          6.6857e-15, -1.0383e-14, -9.8864e-15, -1.1297e-14,  3.3919e-14,\n",
            "          6.9173e-15, -5.8140e-14, -8.9345e-15,  6.5332e-14, -5.0264e-14,\n",
            "          4.8253e-14,  3.5334e-14,  7.8112e-14,  1.4386e-14, -1.5189e-14],\n",
            "        [-5.3431e-15, -2.1140e-14,  1.2404e-14,  7.5289e-15, -1.7262e-14,\n",
            "         -1.1561e-14, -2.1849e-14, -1.0575e-14, -3.6633e-16,  1.5757e-14,\n",
            "          3.5453e-15,  1.6859e-14,  1.9294e-14, -2.6286e-14, -1.8319e-15,\n",
            "         -1.9892e-15, -7.6519e-15,  7.2397e-15, -1.0039e-14,  8.2309e-15],\n",
            "        [-2.6281e-14, -5.4648e-15,  1.4636e-14, -6.0249e-15,  8.1168e-15,\n",
            "         -1.6261e-14, -6.0536e-17, -1.2274e-14,  2.2685e-14,  3.3953e-16,\n",
            "         -1.5255e-14, -1.2915e-14, -3.3749e-14, -9.1337e-16,  5.9085e-15,\n",
            "          8.9087e-16, -1.2016e-14, -2.3173e-14, -1.2326e-14, -5.0776e-15]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 1.4564e-36,  1.5718e-36,  1.4459e-36,  ...,  1.5874e-36,\n",
            "          1.3920e-36,  4.3692e-34],\n",
            "        [-3.7873e-39, -4.3025e-39, -3.8146e-39,  ..., -8.0522e-39,\n",
            "         -5.2911e-39, -1.4288e-36],\n",
            "        ...,\n",
            "        [ 1.6648e-30,  1.9158e-30,  1.5609e-30,  ...,  2.8890e-30,\n",
            "          1.7640e-30,  5.3343e-28],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]])\n",
            "   Last layer:\n",
            "tensor([[-1.8141e-13, -1.2258e-13, -1.3308e-13, -2.7164e-14, -4.2125e-13,\n",
            "         -1.4147e-13, -2.0157e-13, -2.6047e-13, -1.5158e-13, -2.4468e-14,\n",
            "         -3.0011e-14, -1.7817e-13, -8.3140e-14, -2.2890e-13, -1.8309e-13,\n",
            "         -3.0540e-13, -1.5210e-13, -2.4699e-14, -6.1307e-14, -2.1628e-13],\n",
            "        [-2.4401e-13, -3.1628e-13, -4.6680e-14,  3.8799e-14, -1.0650e-13,\n",
            "         -5.8948e-14, -1.8329e-14, -1.5115e-13, -1.6826e-13, -8.8616e-15,\n",
            "         -1.8756e-13, -4.8737e-13, -1.5601e-13, -1.4464e-13, -4.0463e-14,\n",
            "         -1.7656e-13, -2.9823e-14, -1.7739e-13, -1.2569e-13, -2.7943e-13],\n",
            "        [-1.7682e-13, -2.4551e-13, -1.0897e-13, -3.1013e-13, -2.4586e-13,\n",
            "         -8.2309e-14, -1.1768e-13, -2.4362e-13, -1.6712e-13, -1.0900e-13,\n",
            "         -1.7904e-13, -2.2144e-13, -9.0262e-14, -1.5819e-13, -2.6006e-13,\n",
            "         -2.0097e-13, -1.9743e-13, -2.5185e-13, -1.1151e-13, -1.0184e-13],\n",
            "        [ 7.3664e-14,  8.5799e-15,  4.4353e-14,  2.4793e-13,  9.6508e-14,\n",
            "          1.1106e-13,  3.8822e-14,  1.1876e-13, -2.9925e-14,  4.5299e-14,\n",
            "         -4.4147e-14,  3.1501e-14,  7.2485e-14,  1.2627e-13,  1.3697e-14,\n",
            "          5.2307e-15,  3.0627e-16,  6.7596e-14,  2.0529e-14, -3.3277e-14],\n",
            "        [ 4.9041e-14,  9.3820e-14,  1.4839e-13,  2.0532e-13,  1.6603e-13,\n",
            "          9.2416e-14,  1.3283e-13,  1.3967e-13,  1.2412e-13,  3.8617e-13,\n",
            "          1.6450e-13,  1.8269e-13,  1.0998e-13,  8.8774e-14,  1.9164e-13,\n",
            "          1.7741e-13,  1.0137e-13,  2.1152e-13,  8.2324e-14,  1.6421e-13],\n",
            "        [ 6.0395e-14, -3.9287e-14,  7.3974e-15,  2.6094e-13,  6.7006e-14,\n",
            "         -2.4680e-14,  1.0292e-14, -7.8610e-15, -7.2019e-14,  4.1935e-14,\n",
            "         -5.4415e-14,  2.2835e-14,  1.1640e-13, -3.8566e-14, -9.2185e-14,\n",
            "         -3.9019e-14,  1.1994e-13, -6.7379e-15,  9.7192e-15,  2.8085e-14],\n",
            "        [ 9.0557e-14,  3.0404e-14,  1.8376e-15, -5.9026e-14,  2.2949e-13,\n",
            "         -1.0320e-13,  7.3386e-14,  6.0264e-14, -2.2011e-14,  1.1070e-13,\n",
            "          1.3343e-13,  1.3785e-13,  1.1445e-13,  1.5442e-13, -8.1935e-14,\n",
            "          1.2149e-13, -3.0153e-13,  3.8846e-14, -1.0814e-13, -7.6949e-14],\n",
            "        [ 5.0985e-14,  1.4782e-13,  1.2551e-13,  3.0699e-14,  1.5492e-13,\n",
            "          1.3277e-13,  1.3398e-13,  1.5410e-13,  1.2391e-13,  1.2867e-13,\n",
            "          4.2996e-14,  1.5037e-13,  1.5698e-13,  1.3491e-13,  6.9208e-14,\n",
            "          1.2086e-13,  1.2266e-13,  8.9436e-14,  6.7009e-14,  8.3324e-14],\n",
            "        [-1.9151e-15, -2.0546e-14, -8.2967e-15, -1.5807e-14, -2.5668e-15,\n",
            "          2.4692e-15, -3.2505e-15,  2.4603e-14, -9.2829e-15,  2.1521e-14,\n",
            "         -5.3827e-15,  2.7185e-15,  1.7798e-14, -2.3153e-14,  1.4206e-14,\n",
            "          4.3966e-15,  4.1523e-14,  2.2029e-14, -1.5471e-14,  3.7861e-14],\n",
            "        [ 6.3793e-14,  5.6976e-14,  5.7795e-14,  7.9667e-14,  4.5459e-14,\n",
            "          2.6248e-14,  2.5656e-14,  4.0344e-14,  5.4896e-14,  8.8540e-14,\n",
            "          3.8210e-14,  6.9817e-14,  6.7955e-14,  4.2921e-14,  5.2851e-14,\n",
            "          4.5454e-14,  8.9818e-14,  7.3268e-14,  7.0035e-14,  7.0612e-14]])\n",
            "DEBUGGING: training for one iteration takes 0.004580 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 8\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [17]\n",
            "DEBUGGING: rel_reward looks like: 0.005991038410894446\n",
            "DEBUGGING: rel_reward looks like: 0.0019092662221108696\n",
            "DEBUGGING: rel_reward looks like: 0.012311439942263835\n",
            "DEBUGGING: rel_reward looks like: 0.0011593903903119138\n",
            "DEBUGGING: rel_reward looks like: 0.0006537701403707748\n",
            "DEBUGGING: rel_reward looks like: 0.0034595415539121696\n",
            "DEBUGGING: rel_reward looks like: 0.0020125489803150005\n",
            "DEBUGGING: rel_reward looks like: 4.1647921829688175e-05\n",
            "DEBUGGING: rel_reward looks like: 7.45588384597675e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0032765857867063284\n",
            "DEBUGGING: the action_prob is: tensor([0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [42]\n",
            "DEBUGGING: rel_reward looks like: 0.0008765289521183046\n",
            "DEBUGGING: rel_reward looks like: 0.0036384017611040603\n",
            "DEBUGGING: rel_reward looks like: 0.0014920124367645718\n",
            "DEBUGGING: rel_reward looks like: 0.00011557203753787224\n",
            "DEBUGGING: rel_reward looks like: 0.0002194920312362387\n",
            "DEBUGGING: rel_reward looks like: 5.908684225380152e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0020319295775539374\n",
            "DEBUGGING: rel_reward looks like: 0.0002470247308209503\n",
            "DEBUGGING: rel_reward looks like: 0.0003544786571621198\n",
            "DEBUGGING: rel_reward looks like: 0.0011229837817292856\n",
            "DEBUGGING: the action_prob is: tensor([0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [79]\n",
            "DEBUGGING: rel_reward looks like: 5.423757633747356e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0015419499346187764\n",
            "DEBUGGING: rel_reward looks like: 0.0007298277507557585\n",
            "DEBUGGING: rel_reward looks like: 0.0006895888825452747\n",
            "DEBUGGING: rel_reward looks like: 0.00081414508635642\n",
            "DEBUGGING: rel_reward looks like: 0.2752877008222607\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118,\n",
            "        0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118,\n",
            "        0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118,\n",
            "        0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118,\n",
            "        0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118,\n",
            "        0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118,\n",
            "        0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118,\n",
            "        0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118,\n",
            "        0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0118,\n",
            "        0.0118, 0.0118, 0.0118, 0.0118], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [74]\n",
            "DEBUGGING: rel_reward looks like: 4.547473508864641e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508862573e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508865675e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 4.547473508864641e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508866709e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: the action_prob is: tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [73]\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508863607e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.88674132898268 and immediate abs rewards look like: [0.017294578184191778, 0.005478537757426238, 0.03525957265128454, 0.0032795776146485878, 0.0018471811549716222, 0.009768298850303836, 0.005662937656779832, 0.00011695363855324103, 0.00020936373039148748, 0.009200077283821884, 0.002453075181620079, 0.010173594128445984, 0.004156743039857247, 0.000321503013765323, 0.0006105213287810329, 0.0001643151204007154, 0.005650276994401793, 0.0006855169031041441, 0.0009834686720751051, 0.003114511559942912, 0.00015025493939901935, 0.004271448754934681, 0.0020186225629004184, 0.0019059342994296458, 0.002248639797016949, 0.7597158241542274, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 9.094947017729282e-13, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 9.094947017729282e-13, 9.094947017729282e-13, 0.0, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.3201647490533326 and immediate relative rewards look like: [0.005991038410894446, 0.0019092662221108696, 0.012311439942263835, 0.0011593903903119138, 0.0006537701403707748, 0.0034595415539121696, 0.0020125489803150005, 4.1647921829688175e-05, 7.45588384597675e-05, 0.0032765857867063284, 0.0008765289521183046, 0.0036384017611040603, 0.0014920124367645718, 0.00011557203753787224, 0.0002194920312362387, 5.908684225380152e-05, 0.0020319295775539374, 0.0002470247308209503, 0.0003544786571621198, 0.0011229837817292856, 5.423757633747356e-05, 0.0015419499346187764, 0.0007298277507557585, 0.0006895888825452747, 0.00081414508635642, 0.2752877008222607, 0.0, 0.0, 2.2737367544323206e-13, 2.2737367544328376e-13, 4.547473508864641e-13, 4.547473508862573e-13, 2.2737367544323206e-13, 4.547473508865675e-13, 2.2737367544318036e-13, 0.0, 4.547473508864641e-13, 4.547473508866709e-13, 0.0, 0.0, 2.2737367544323206e-13, 4.547473508863607e-13, 2.2737367544328376e-13, 2.2737367544323206e-13, 2.2737367544328376e-13, 0.0, 2.2737367544323206e-13, 0.0, 2.2737367544318036e-13, 0.0]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [59]\n",
            "DEBUGGING: rel_reward looks like: 0.0009447095463842318\n",
            "DEBUGGING: rel_reward looks like: 0.003821404924421501\n",
            "DEBUGGING: rel_reward looks like: 0.006897184805368098\n",
            "DEBUGGING: rel_reward looks like: 0.012886451866361254\n",
            "DEBUGGING: rel_reward looks like: 0.0002455506738721794\n",
            "DEBUGGING: rel_reward looks like: 0.0003361910200431306\n",
            "DEBUGGING: rel_reward looks like: 0.0003704041912232779\n",
            "DEBUGGING: rel_reward looks like: 0.0006861903667383426\n",
            "DEBUGGING: rel_reward looks like: 0.001711947785442172\n",
            "DEBUGGING: rel_reward looks like: 0.003210412742934467\n",
            "DEBUGGING: the action_prob is: tensor([0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [55]\n",
            "DEBUGGING: rel_reward looks like: 0.0016340280876454489\n",
            "DEBUGGING: rel_reward looks like: 0.00017734236872830395\n",
            "DEBUGGING: rel_reward looks like: 3.0002827625057708e-05\n",
            "DEBUGGING: rel_reward looks like: 1.361543057211447e-05\n",
            "DEBUGGING: rel_reward looks like: 5.602047543249308e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0002797502362172338\n",
            "DEBUGGING: rel_reward looks like: 4.0363720581663694e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0005593558952463956\n",
            "DEBUGGING: rel_reward looks like: 4.588745396713061e-06\n",
            "DEBUGGING: rel_reward looks like: 0.0006616991010373311\n",
            "DEBUGGING: the action_prob is: tensor([0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [55]\n",
            "DEBUGGING: rel_reward looks like: 0.00317957406221396\n",
            "DEBUGGING: rel_reward looks like: 0.00011815484047359409\n",
            "DEBUGGING: rel_reward looks like: 0.000947580667134493\n",
            "DEBUGGING: rel_reward looks like: 0.0034967713432995586\n",
            "DEBUGGING: rel_reward looks like: 0.0002978686733805924\n",
            "DEBUGGING: rel_reward looks like: 0.00039991222330394065\n",
            "DEBUGGING: rel_reward looks like: 0.0002110814820765887\n",
            "DEBUGGING: rel_reward looks like: 6.756927986444515e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00015564080121721347\n",
            "DEBUGGING: rel_reward looks like: 0.001085573181109098\n",
            "DEBUGGING: the action_prob is: tensor([0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [53]\n",
            "DEBUGGING: rel_reward looks like: 2.4323661494540644e-06\n",
            "DEBUGGING: rel_reward looks like: 0.0010152466096373625\n",
            "DEBUGGING: rel_reward looks like: 0.0011483785652606254\n",
            "DEBUGGING: rel_reward looks like: 0.0001768008047900378\n",
            "DEBUGGING: rel_reward looks like: 0.0007496824633016157\n",
            "DEBUGGING: rel_reward looks like: 9.821719362502436e-05\n",
            "DEBUGGING: rel_reward looks like: 8.1588943622284e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00012317736068235153\n",
            "DEBUGGING: rel_reward looks like: 0.00039621116391635764\n",
            "DEBUGGING: rel_reward looks like: 0.00010472032602008657\n",
            "DEBUGGING: the action_prob is: tensor([0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [53]\n",
            "DEBUGGING: rel_reward looks like: 4.258149642635786e-05\n",
            "DEBUGGING: rel_reward looks like: 4.396592400652748e-05\n",
            "DEBUGGING: rel_reward looks like: 8.161374728994891e-05\n",
            "DEBUGGING: rel_reward looks like: 2.1515109260562898e-05\n",
            "DEBUGGING: rel_reward looks like: 4.884699563703029e-05\n",
            "DEBUGGING: rel_reward looks like: 1.469248387326734e-05\n",
            "DEBUGGING: rel_reward looks like: 3.408844041650007e-05\n",
            "DEBUGGING: rel_reward looks like: 4.672793847838517e-06\n",
            "DEBUGGING: rel_reward looks like: 6.553761767520728e-07\n",
            "DEBUGGING: rel_reward looks like: 1.318359702233314e-05\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.13767620221551624 and immediate abs rewards look like: [0.0027271320914223907, 0.011020986054063542, 0.019815565119642997, 0.036767336397588224, 0.0006915714493516134, 0.0009466193546359136, 0.0010426033900330367, 0.0019307540878799045, 0.004813652662051027, 0.009011580761125515, 0.004571967332594795, 0.0004953884495080274, 8.379508244615863e-05, 3.8025479170755716e-05, 0.00015645310713807703, 0.0007812383141754253, 0.00011268931848462671, 0.0015615728543707519, 1.2803390745830256e-05, 0.001846246095283277, 0.008865648799201153, 0.00032840519952515024, 0.0026334397148275457, 0.009708736170978227, 0.0008241362297667365, 0.0011061384066124447, 0.0005836079694745422, 0.00018677927619137336, 0.00043020307430197136, 0.0030001402546986355, 6.7149021560908295e-06, 0.002802729789436853, 0.0031670405164732074, 0.0004870278071393841, 0.0020647623960030614, 0.00027030529054172803, 0.00022452033317676978, 0.00033893766021719784, 0.0010900894653786963, 0.00028800121071981266, 0.00011709511181834387, 0.00012089700931028347, 0.00022441070859713363, 5.915457904848154e-05, 0.00013429916907625739, 4.0393313156528166e-05, 9.3716272203892e-05, 1.284605241380632e-05, 1.801696726033697e-06, 3.624304463301087e-05]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.048729213126307215 and immediate relative rewards look like: [0.0009447095463842318, 0.003821404924421501, 0.006897184805368098, 0.012886451866361254, 0.0002455506738721794, 0.0003361910200431306, 0.0003704041912232779, 0.0006861903667383426, 0.001711947785442172, 0.003210412742934467, 0.0016340280876454489, 0.00017734236872830395, 3.0002827625057708e-05, 1.361543057211447e-05, 5.602047543249308e-05, 0.0002797502362172338, 4.0363720581663694e-05, 0.0005593558952463956, 4.588745396713061e-06, 0.0006616991010373311, 0.00317957406221396, 0.00011815484047359409, 0.000947580667134493, 0.0034967713432995586, 0.0002978686733805924, 0.00039991222330394065, 0.0002110814820765887, 6.756927986444515e-05, 0.00015564080121721347, 0.001085573181109098, 2.4323661494540644e-06, 0.0010152466096373625, 0.0011483785652606254, 0.0001768008047900378, 0.0007496824633016157, 9.821719362502436e-05, 8.1588943622284e-05, 0.00012317736068235153, 0.00039621116391635764, 0.00010472032602008657, 4.258149642635786e-05, 4.396592400652748e-05, 8.161374728994891e-05, 2.1515109260562898e-05, 4.884699563703029e-05, 1.469248387326734e-05, 3.408844041650007e-05, 4.672793847838517e-06, 6.553761767520728e-07, 1.318359702233314e-05]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [42]\n",
            "DEBUGGING: rel_reward looks like: 0.014567795605050084\n",
            "DEBUGGING: rel_reward looks like: 0.008428071697120341\n",
            "DEBUGGING: rel_reward looks like: 0.0024331731288250416\n",
            "DEBUGGING: rel_reward looks like: 0.0003573623557440551\n",
            "DEBUGGING: rel_reward looks like: 0.0002598266138917612\n",
            "DEBUGGING: rel_reward looks like: 0.00042669234157090627\n",
            "DEBUGGING: rel_reward looks like: 0.00013619797303159\n",
            "DEBUGGING: rel_reward looks like: 0.0014710146586314737\n",
            "DEBUGGING: rel_reward looks like: 0.00014296792049565808\n",
            "DEBUGGING: rel_reward looks like: 0.001096001751128679\n",
            "DEBUGGING: the action_prob is: tensor([0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [59]\n",
            "DEBUGGING: rel_reward looks like: 0.003105872485761317\n",
            "DEBUGGING: rel_reward looks like: 0.0002907738297607033\n",
            "DEBUGGING: rel_reward looks like: 5.08190102053364e-05\n",
            "DEBUGGING: rel_reward looks like: 0.002918264038308315\n",
            "DEBUGGING: rel_reward looks like: 0.0006551678870190068\n",
            "DEBUGGING: rel_reward looks like: 0.00017458486295005729\n",
            "DEBUGGING: rel_reward looks like: 0.007303232563178879\n",
            "DEBUGGING: rel_reward looks like: 0.0022317336968406595\n",
            "DEBUGGING: rel_reward looks like: 0.000540934701125701\n",
            "DEBUGGING: rel_reward looks like: 1.6453430248331812e-06\n",
            "DEBUGGING: the action_prob is: tensor([0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [59]\n",
            "DEBUGGING: rel_reward looks like: 0.0009414898547426535\n",
            "DEBUGGING: rel_reward looks like: 0.0036895890797050386\n",
            "DEBUGGING: rel_reward looks like: 0.0010393352337083055\n",
            "DEBUGGING: rel_reward looks like: 6.637592187889629e-05\n",
            "DEBUGGING: rel_reward looks like: 9.92407672989537e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00016453525017134135\n",
            "DEBUGGING: rel_reward looks like: 0.0006863619905011681\n",
            "DEBUGGING: rel_reward looks like: 7.7510988317506e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0011008486924919615\n",
            "DEBUGGING: rel_reward looks like: 0.0004941765400294971\n",
            "DEBUGGING: the action_prob is: tensor([0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [76]\n",
            "DEBUGGING: rel_reward looks like: 4.86943696290346e-06\n",
            "DEBUGGING: rel_reward looks like: 4.647805498017048e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0004498688233191723\n",
            "DEBUGGING: rel_reward looks like: 0.0005708148269217076\n",
            "DEBUGGING: rel_reward looks like: 8.542636141004076e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0001955555250126918\n",
            "DEBUGGING: rel_reward looks like: 7.260800901896675e-07\n",
            "DEBUGGING: rel_reward looks like: 0.00010913143884010658\n",
            "DEBUGGING: rel_reward looks like: 8.683190549846847e-05\n",
            "DEBUGGING: rel_reward looks like: 9.497317448706641e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [72]\n",
            "DEBUGGING: rel_reward looks like: 8.726628713565162e-07\n",
            "DEBUGGING: rel_reward looks like: 8.290425562228615e-07\n",
            "DEBUGGING: rel_reward looks like: 8.830559649209126e-07\n",
            "DEBUGGING: rel_reward looks like: 4.354113503498623e-06\n",
            "DEBUGGING: rel_reward looks like: 3.4689636605203934e-07\n",
            "DEBUGGING: rel_reward looks like: 5.116709126715883e-06\n",
            "DEBUGGING: rel_reward looks like: 7.428389071790863e-06\n",
            "DEBUGGING: rel_reward looks like: 7.515142326079183e-06\n",
            "DEBUGGING: rel_reward looks like: 0.00012743791944395459\n",
            "DEBUGGING: rel_reward looks like: 9.296397144510764e-05\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.16005075845441752 and immediate abs rewards look like: [0.04205345764512458, 0.023975233335477242, 0.0068632821948995115, 0.00100556383449657, 0.0007308516778721241, 0.0011999070552519697, 0.0003828406574939436, 0.004134331111799838, 0.0004012245804005943, 0.0030753750356780074, 0.008705509815172263, 0.0008124842338474991, 0.00014195788753568195, 0.008151468193773326, 0.001824713219775731, 0.0004859191035393451, 0.020323415196344286, 0.00616510585314245, 0.001490983016537939, 4.532620096142637e-06, 0.0025936286538126296, 0.010154559174679889, 0.0028499247518993798, 0.00018181792484028847, 0.0002718237606131879, 0.000450622795142408, 0.0018794724483086611, 0.0002121034963238344, 0.0030121633503767953, 0.0013506868062904687, 1.3302602383191697e-05, 0.00012697075226242305, 0.001228913905833906, 0.001558602568366041, 0.0002331224391127762, 0.0005336114727469976, 1.9808639990515076e-06, 0.000297728011446452, 0.00023686546728640678, 0.0002590512308415782, 2.3800712369848043e-06, 2.2611006897932384e-06, 2.4084129108814523e-06, 1.1875231393787544e-05, 9.461068657401484e-07, 1.3955037957202876e-05, 2.0259686152712675e-05, 2.0496138859016355e-05, 0.000347560372119915, 0.0002535075514060736]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.05684402031270794 and immediate relative rewards look like: [0.014567795605050084, 0.008428071697120341, 0.0024331731288250416, 0.0003573623557440551, 0.0002598266138917612, 0.00042669234157090627, 0.00013619797303159, 0.0014710146586314737, 0.00014296792049565808, 0.001096001751128679, 0.003105872485761317, 0.0002907738297607033, 5.08190102053364e-05, 0.002918264038308315, 0.0006551678870190068, 0.00017458486295005729, 0.007303232563178879, 0.0022317336968406595, 0.000540934701125701, 1.6453430248331812e-06, 0.0009414898547426535, 0.0036895890797050386, 0.0010393352337083055, 6.637592187889629e-05, 9.92407672989537e-05, 0.00016453525017134135, 0.0006863619905011681, 7.7510988317506e-05, 0.0011008486924919615, 0.0004941765400294971, 4.86943696290346e-06, 4.647805498017048e-05, 0.0004498688233191723, 0.0005708148269217076, 8.542636141004076e-05, 0.0001955555250126918, 7.260800901896675e-07, 0.00010913143884010658, 8.683190549846847e-05, 9.497317448706641e-05, 8.726628713565162e-07, 8.290425562228615e-07, 8.830559649209126e-07, 4.354113503498623e-06, 3.4689636605203934e-07, 5.116709126715883e-06, 7.428389071790863e-06, 7.515142326079183e-06, 0.00012743791944395459, 9.296397144510764e-05]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 150 number of matrices\n",
            "DEBUGGING: ACT_MAT has 150 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.2560534224617399, 0.2525882667180257, 0.2532111116120352, 0.24333300168663774, 0.24461980939022812, 0.2464303426766236, 0.24542505163910244, 0.245871214806856, 0.2483126938232589, 0.2507455908937365, 0.24996869202730318, 0.25160824553048977, 0.2504745896660462, 0.2514975527568501, 0.25392119264576996, 0.2562643440550846, 0.25879318910386945, 0.2593548076023389, 0.2617250332035535, 0.2640106611579711, 0.2655431084608504, 0.2681705766510231, 0.2693218451680852, 0.2713050680983126, 0.2733489689048155, 0.2752877008267264, 4.510794063456809e-12, 4.5563576398553625e-12, 4.602381454399356e-12, 4.4191997767233576e-12, 4.234167779070782e-12, 3.817596392105372e-12, 3.3968172133526415e-12, 3.2014581191004135e-12, 2.774455321428127e-12, 2.572809743419138e-12, 2.598797720625392e-12, 2.1657074441807353e-12, 1.7282425184788531e-12, 1.7456995136150032e-12, 1.7633328420353567e-12, 1.5514739056486107e-12, 1.1078046007699495e-12, 8.89324166996632e-13, 6.686368601549495e-13, 4.457203885976422e-13, 4.502226147450931e-13, 2.2509993868874854e-13, 2.2737367544318036e-13, 0.0], [0.043975430337406564, 0.043465374536386193, 0.04004441374945929, 0.03348204943847595, 0.020803633911226964, 0.02076574064379271, 0.020635908710858162, 0.020470206585489783, 0.019983854766415596, 0.018456471697952954, 0.015400059550523723, 0.01390508228573563, 0.013866403956573058, 0.013976162756513132, 0.01410358315751618, 0.014189457254629988, 0.014050209109507833, 0.014151358978713302, 0.013729296043905968, 0.013863340705564903, 0.01333499151972482, 0.01025799743182915, 0.010242265243793491, 0.009388570279453534, 0.005951312056721187, 0.005710548872061206, 0.005364279443189157, 0.005205250465770271, 0.005189576955460431, 0.005084783994185068, 0.004039606881894919, 0.0040779540563085515, 0.0030936438855264535, 0.001964914464914978, 0.0018061754142676164, 0.0010671645969353544, 0.0009787347508185152, 0.0009062078860567993, 0.0007909399246206544, 0.00039871591990333, 0.00029696524634671055, 0.00025695328274783104, 0.00021513874620333694, 0.00013487373627614954, 0.00011450366365210773, 6.631986668189639e-05, 5.214887152386773e-05, 1.8242859704411778e-05, 1.3707137228861881e-05, 1.318359702233314e-05], [0.05183862423599156, 0.03764730164741563, 0.029514373687166955, 0.027354748038729206, 0.02727008654846985, 0.02728309084300817, 0.027127675253977034, 0.027264118465601457, 0.026053640209060588, 0.026172396251075685, 0.025329691414087883, 0.022448301947804613, 0.02238134153337769, 0.02255608335673975, 0.019836181129728722, 0.019374760851221935, 0.019394117159870584, 0.012213014744133034, 0.010082102067972096, 0.009637542794794339, 0.009733229749262126, 0.008880545347999466, 0.0052433901699943716, 0.004246520137662693, 0.004222367894731107, 0.004164774876194094, 0.004040646086891669, 0.003388165753929799, 0.0033440957228406997, 0.0022659060912613516, 0.0017896258093251054, 0.0018027842145072748, 0.0017740466257849538, 0.0013375533358240218, 0.0007744833423255699, 0.0006960171524399284, 0.0005055167953810471, 0.0005098896114049065, 0.0004048062349139394, 0.0003211861913287585, 0.00022849799680979, 0.00022992457973579139, 0.0002314096335147157, 0.00023285512883817656, 0.00023080910639866457, 0.00023279011114405305, 0.00022997313335084563, 0.0002247926709889442, 0.00021947225117461113, 9.296397144510764e-05]]\n",
            "DEBUGGING: traj_returns = [0.2560534224617399, 0.043975430337406564, 0.05183862423599156]\n",
            "DEBUGGING: actions = [[17], [39], [3], [4], [24], [33], [30], [2], [6], [58], [42], [1], [34], [42], [61], [65], [50], [51], [48], [9], [79], [57], [67], [47], [31], [0], [22], [34], [80], [37], [74], [16], [29], [53], [72], [40], [14], [15], [93], [0], [73], [67], [3], [81], [5], [66], [76], [90], [52], [88], [59], [30], [1], [50], [40], [13], [19], [19], [15], [41], [55], [33], [16], [33], [66], [16], [51], [25], [34], [9], [55], [23], [5], [13], [57], [55], [72], [16], [14], [70], [53], [1], [54], [6], [90], [73], [23], [61], [92], [37], [53], [5], [11], [33], [4], [66], [42], [22], [93], [23], [42], [47], [20], [46], [56], [53], [41], [48], [16], [58], [59], [53], [13], [68], [50], [67], [50], [5], [9], [37], [59], [31], [73], [20], [42], [66], [74], [57], [16], [1], [76], [20], [15], [57], [44], [66], [39], [68], [22], [55], [72], [32], [10], [63], [87], [81], [69], [47], [10], [87]]\n",
            "DEBUGGING: actions length = 150\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        ...,\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        ...,\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[1.17289159e-01 1.11233648e-01 1.07589966e-01 1.01389933e-01\n",
            "  9.75645099e-02 9.81597247e-02 9.77295452e-02 9.78685133e-02\n",
            "  9.81167296e-02 9.84581529e-02 9.68994810e-02 9.59872099e-02\n",
            "  9.55741117e-02 9.60099330e-02 9.59536523e-02 9.66095207e-02\n",
            "  9.74125051e-02 9.52397271e-02 9.51788104e-02 9.58371816e-02\n",
            "  9.62037766e-02 9.57697065e-02 9.49358335e-02 9.49800528e-02\n",
            "  9.45075496e-02 9.50543415e-02 3.13497518e-03 2.86447207e-03\n",
            "  2.84455756e-03 2.45023003e-03 1.94307757e-03 1.96024609e-03\n",
            "  1.62256350e-03 1.10082260e-03 8.60219586e-04 5.87727251e-04\n",
            "  4.94750516e-04 4.72032500e-04 3.98582054e-04 2.39967371e-04\n",
            "  1.75154415e-04 1.62292621e-04 1.48849460e-04 1.22576289e-04\n",
            "  1.15104257e-04 9.97033261e-05 9.40406684e-05 8.10118436e-05\n",
            "  7.77264629e-05 3.53825228e-05]]\n",
            "DEBUGGING: baseline2 looks like: 0.11728915901171268\n",
            "DEBUGGING: ADS looks like: [2.56053422e-01 2.52588267e-01 2.53211112e-01 2.43333002e-01\n",
            " 2.44619809e-01 2.46430343e-01 2.45425052e-01 2.45871215e-01\n",
            " 2.48312694e-01 2.50745591e-01 2.49968692e-01 2.51608246e-01\n",
            " 2.50474590e-01 2.51497553e-01 2.53921193e-01 2.56264344e-01\n",
            " 2.58793189e-01 2.59354808e-01 2.61725033e-01 2.64010661e-01\n",
            " 2.65543108e-01 2.68170577e-01 2.69321845e-01 2.71305068e-01\n",
            " 2.73348969e-01 2.75287701e-01 4.51079406e-12 4.55635764e-12\n",
            " 4.60238145e-12 4.41919978e-12 4.23416778e-12 3.81759639e-12\n",
            " 3.39681721e-12 3.20145812e-12 2.77445532e-12 2.57280974e-12\n",
            " 2.59879772e-12 2.16570744e-12 1.72824252e-12 1.74569951e-12\n",
            " 1.76333284e-12 1.55147391e-12 1.10780460e-12 8.89324167e-13\n",
            " 6.68636860e-13 4.45720389e-13 4.50222615e-13 2.25099939e-13\n",
            " 2.27373675e-13 0.00000000e+00 4.39754303e-02 4.34653745e-02\n",
            " 4.00444137e-02 3.34820494e-02 2.08036339e-02 2.07657406e-02\n",
            " 2.06359087e-02 2.04702066e-02 1.99838548e-02 1.84564717e-02\n",
            " 1.54000596e-02 1.39050823e-02 1.38664040e-02 1.39761628e-02\n",
            " 1.41035832e-02 1.41894573e-02 1.40502091e-02 1.41513590e-02\n",
            " 1.37292960e-02 1.38633407e-02 1.33349915e-02 1.02579974e-02\n",
            " 1.02422652e-02 9.38857028e-03 5.95131206e-03 5.71054887e-03\n",
            " 5.36427944e-03 5.20525047e-03 5.18957696e-03 5.08478399e-03\n",
            " 4.03960688e-03 4.07795406e-03 3.09364389e-03 1.96491446e-03\n",
            " 1.80617541e-03 1.06716460e-03 9.78734751e-04 9.06207886e-04\n",
            " 7.90939925e-04 3.98715920e-04 2.96965246e-04 2.56953283e-04\n",
            " 2.15138746e-04 1.34873736e-04 1.14503664e-04 6.63198667e-05\n",
            " 5.21488715e-05 1.82428597e-05 1.37071372e-05 1.31835970e-05\n",
            " 5.18386242e-02 3.76473016e-02 2.95143737e-02 2.73547480e-02\n",
            " 2.72700865e-02 2.72830908e-02 2.71276753e-02 2.72641185e-02\n",
            " 2.60536402e-02 2.61723963e-02 2.53296914e-02 2.24483019e-02\n",
            " 2.23813415e-02 2.25560834e-02 1.98361811e-02 1.93747609e-02\n",
            " 1.93941172e-02 1.22130147e-02 1.00821021e-02 9.63754279e-03\n",
            " 9.73322975e-03 8.88054535e-03 5.24339017e-03 4.24652014e-03\n",
            " 4.22236789e-03 4.16477488e-03 4.04064609e-03 3.38816575e-03\n",
            " 3.34409572e-03 2.26590609e-03 1.78962581e-03 1.80278421e-03\n",
            " 1.77404663e-03 1.33755334e-03 7.74483342e-04 6.96017152e-04\n",
            " 5.05516795e-04 5.09889611e-04 4.04806235e-04 3.21186191e-04\n",
            " 2.28497997e-04 2.29924580e-04 2.31409634e-04 2.32855129e-04\n",
            " 2.30809106e-04 2.32790111e-04 2.29973133e-04 2.24792671e-04\n",
            " 2.19472251e-04 9.29639714e-05]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(0.2187, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 1.4564e-36,  1.5718e-36,  1.4459e-36,  ...,  1.5874e-36,\n",
            "          1.3920e-36,  4.3692e-34],\n",
            "        [-3.7873e-39, -4.3025e-39, -3.8146e-39,  ..., -8.0522e-39,\n",
            "         -5.2911e-39, -1.4288e-36],\n",
            "        ...,\n",
            "        [ 1.6648e-30,  1.9158e-30,  1.5609e-30,  ...,  2.8890e-30,\n",
            "          1.7640e-30,  5.3343e-28],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]])\n",
            "   Last layer:\n",
            "tensor([[-1.8141e-13, -1.2258e-13, -1.3308e-13, -2.7164e-14, -4.2125e-13,\n",
            "         -1.4147e-13, -2.0157e-13, -2.6047e-13, -1.5158e-13, -2.4468e-14,\n",
            "         -3.0011e-14, -1.7817e-13, -8.3140e-14, -2.2890e-13, -1.8309e-13,\n",
            "         -3.0540e-13, -1.5210e-13, -2.4699e-14, -6.1307e-14, -2.1628e-13],\n",
            "        [-2.4401e-13, -3.1628e-13, -4.6680e-14,  3.8799e-14, -1.0650e-13,\n",
            "         -5.8948e-14, -1.8329e-14, -1.5115e-13, -1.6826e-13, -8.8616e-15,\n",
            "         -1.8756e-13, -4.8737e-13, -1.5601e-13, -1.4464e-13, -4.0463e-14,\n",
            "         -1.7656e-13, -2.9823e-14, -1.7739e-13, -1.2569e-13, -2.7943e-13],\n",
            "        [-1.7682e-13, -2.4551e-13, -1.0897e-13, -3.1013e-13, -2.4586e-13,\n",
            "         -8.2309e-14, -1.1768e-13, -2.4362e-13, -1.6712e-13, -1.0900e-13,\n",
            "         -1.7904e-13, -2.2144e-13, -9.0262e-14, -1.5819e-13, -2.6006e-13,\n",
            "         -2.0097e-13, -1.9743e-13, -2.5185e-13, -1.1151e-13, -1.0184e-13],\n",
            "        [ 7.3664e-14,  8.5799e-15,  4.4353e-14,  2.4793e-13,  9.6508e-14,\n",
            "          1.1106e-13,  3.8822e-14,  1.1876e-13, -2.9925e-14,  4.5299e-14,\n",
            "         -4.4147e-14,  3.1501e-14,  7.2485e-14,  1.2627e-13,  1.3697e-14,\n",
            "          5.2307e-15,  3.0627e-16,  6.7596e-14,  2.0529e-14, -3.3277e-14],\n",
            "        [ 4.9041e-14,  9.3820e-14,  1.4839e-13,  2.0532e-13,  1.6603e-13,\n",
            "          9.2416e-14,  1.3283e-13,  1.3967e-13,  1.2412e-13,  3.8617e-13,\n",
            "          1.6450e-13,  1.8269e-13,  1.0998e-13,  8.8774e-14,  1.9164e-13,\n",
            "          1.7741e-13,  1.0137e-13,  2.1152e-13,  8.2324e-14,  1.6421e-13],\n",
            "        [ 6.0395e-14, -3.9287e-14,  7.3974e-15,  2.6094e-13,  6.7006e-14,\n",
            "         -2.4680e-14,  1.0292e-14, -7.8610e-15, -7.2019e-14,  4.1935e-14,\n",
            "         -5.4415e-14,  2.2835e-14,  1.1640e-13, -3.8566e-14, -9.2185e-14,\n",
            "         -3.9019e-14,  1.1994e-13, -6.7379e-15,  9.7192e-15,  2.8085e-14],\n",
            "        [ 9.0557e-14,  3.0404e-14,  1.8376e-15, -5.9026e-14,  2.2949e-13,\n",
            "         -1.0320e-13,  7.3386e-14,  6.0264e-14, -2.2011e-14,  1.1070e-13,\n",
            "          1.3343e-13,  1.3785e-13,  1.1445e-13,  1.5442e-13, -8.1935e-14,\n",
            "          1.2149e-13, -3.0153e-13,  3.8846e-14, -1.0814e-13, -7.6949e-14],\n",
            "        [ 5.0985e-14,  1.4782e-13,  1.2551e-13,  3.0699e-14,  1.5492e-13,\n",
            "          1.3277e-13,  1.3398e-13,  1.5410e-13,  1.2391e-13,  1.2867e-13,\n",
            "          4.2996e-14,  1.5037e-13,  1.5698e-13,  1.3491e-13,  6.9208e-14,\n",
            "          1.2086e-13,  1.2266e-13,  8.9436e-14,  6.7009e-14,  8.3324e-14],\n",
            "        [-1.9151e-15, -2.0546e-14, -8.2967e-15, -1.5807e-14, -2.5668e-15,\n",
            "          2.4692e-15, -3.2505e-15,  2.4603e-14, -9.2829e-15,  2.1521e-14,\n",
            "         -5.3827e-15,  2.7185e-15,  1.7798e-14, -2.3153e-14,  1.4206e-14,\n",
            "          4.3966e-15,  4.1523e-14,  2.2029e-14, -1.5471e-14,  3.7861e-14],\n",
            "        [ 6.3793e-14,  5.6976e-14,  5.7795e-14,  7.9667e-14,  4.5459e-14,\n",
            "          2.6248e-14,  2.5656e-14,  4.0344e-14,  5.4896e-14,  8.8540e-14,\n",
            "          3.8210e-14,  6.9817e-14,  6.7955e-14,  4.2921e-14,  5.2851e-14,\n",
            "          4.5454e-14,  8.9818e-14,  7.3268e-14,  7.0035e-14,  7.0612e-14]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-7.8017e-37, -8.3278e-37, -7.9029e-37,  ..., -8.6723e-37,\n",
            "         -7.5213e-37, -2.3619e-34],\n",
            "        [ 3.6175e-39,  4.1081e-39,  3.6471e-39,  ...,  7.7022e-39,\n",
            "          5.0576e-39,  1.3657e-36],\n",
            "        ...,\n",
            "        [-5.7651e-30, -6.5138e-30, -5.4308e-30,  ..., -1.0105e-29,\n",
            "         -6.1539e-30, -1.8504e-27],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]])\n",
            "   Last layer:\n",
            "tensor([[ 1.3448e-13,  6.1979e-15,  6.4115e-14,  2.2380e-14, -1.2788e-13,\n",
            "          3.0684e-15,  5.8434e-14, -1.2153e-15,  1.1599e-14,  1.6240e-13,\n",
            "          8.8153e-15,  5.6048e-14, -7.6761e-14,  1.3969e-14,  3.6636e-14,\n",
            "         -6.8810e-14,  2.1280e-14,  6.7714e-14,  1.1925e-13,  1.0271e-14],\n",
            "        [-6.7389e-14, -1.5723e-13, -1.0395e-13, -1.4865e-14, -1.4848e-13,\n",
            "         -9.0504e-14, -1.2296e-13, -3.3414e-13, -9.5114e-14, -1.1711e-13,\n",
            "         -1.1306e-13, -1.4106e-13, -1.5442e-13, -6.2953e-14, -1.3147e-13,\n",
            "         -1.8429e-13, -1.0646e-13, -6.4866e-14, -6.5163e-14, -1.0644e-13],\n",
            "        [ 1.1887e-13,  8.1100e-14,  1.2327e-13,  9.7430e-14,  1.5481e-13,\n",
            "          8.6516e-14,  1.1851e-13,  1.8491e-13,  8.2724e-14,  1.2054e-13,\n",
            "          7.2234e-14,  1.7517e-13,  8.0111e-14,  1.6182e-13,  1.7738e-13,\n",
            "          2.2437e-15,  1.6361e-13,  9.2145e-14,  9.5606e-14,  1.1534e-13],\n",
            "        [-1.6721e-14, -2.4327e-14,  3.1141e-14,  4.4109e-14, -5.5299e-14,\n",
            "          2.9043e-14,  6.5347e-14,  2.4546e-14,  4.3736e-14,  6.1478e-14,\n",
            "          1.3077e-14,  1.7588e-15,  3.0893e-14,  1.4111e-14,  4.8967e-14,\n",
            "          1.8600e-14,  5.4600e-14,  1.1338e-15,  1.6054e-14,  4.3686e-14],\n",
            "        [-8.9691e-14, -1.0227e-13, -5.6977e-14, -1.0462e-13, -1.0336e-13,\n",
            "         -9.1785e-14, -8.2250e-14, -2.3465e-13, -3.7042e-14, -6.9758e-14,\n",
            "         -5.2110e-14, -1.7105e-13, -1.5767e-13, -5.1320e-14, -4.3413e-16,\n",
            "         -1.0245e-13, -1.3444e-13, -7.2863e-14, -4.7813e-14, -1.0431e-13],\n",
            "        [-8.1777e-15,  5.9704e-14,  1.7368e-14,  2.9253e-14, -1.4534e-14,\n",
            "          1.8771e-14, -3.7234e-14, -8.7427e-14, -6.1994e-14, -2.6462e-14,\n",
            "         -1.0202e-13, -2.5214e-14,  9.3095e-14, -2.6257e-14,  4.0322e-14,\n",
            "         -3.9013e-14,  8.4807e-15,  3.5310e-14,  7.4024e-15, -1.8044e-14],\n",
            "        [ 2.2319e-13,  1.2263e-13,  1.2996e-13,  1.0432e-13,  7.5729e-14,\n",
            "          1.8889e-13,  7.8896e-14,  3.9127e-14,  2.2083e-13,  2.3497e-13,\n",
            "          1.1037e-13,  2.0144e-13,  3.1110e-13,  1.0378e-13,  2.6572e-13,\n",
            "          1.5506e-13,  2.1869e-13, -3.8918e-15,  1.1975e-13,  1.2369e-13],\n",
            "        [ 2.1242e-14,  5.9094e-14,  2.8855e-14,  1.2123e-13,  6.4426e-14,\n",
            "          2.7560e-14, -1.6302e-14,  9.4337e-14,  3.1565e-14,  7.0857e-14,\n",
            "          5.8668e-14,  4.3156e-14,  3.8465e-14,  7.8942e-14,  6.1126e-14,\n",
            "          6.6295e-14,  3.5663e-14,  7.4444e-14,  7.8527e-14,  7.5660e-14],\n",
            "        [ 7.3388e-15, -2.2728e-14, -2.3888e-14, -1.2534e-14, -5.0969e-17,\n",
            "         -2.2212e-14, -1.1408e-14, -3.6838e-14, -2.7886e-14, -3.2806e-14,\n",
            "         -4.5531e-15, -2.2682e-14, -8.5197e-15, -1.8291e-14, -1.6395e-14,\n",
            "         -2.8080e-14, -2.0582e-14, -2.5150e-14, -1.7218e-14, -1.1986e-14],\n",
            "        [-1.5110e-14, -1.6271e-14, -2.8133e-14, -3.1159e-14, -2.8867e-14,\n",
            "         -1.8943e-14, -2.4874e-14, -2.6049e-14, -3.1640e-14, -2.6832e-14,\n",
            "         -1.1023e-14, -3.8927e-14, -1.8514e-14, -3.2620e-14, -2.0764e-14,\n",
            "         -2.1999e-15, -4.5287e-14, -4.2905e-14, -1.1209e-14, -1.0288e-14]])\n",
            "DEBUGGING: training for one iteration takes 0.004864 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 9\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [9]\n",
            "DEBUGGING: rel_reward looks like: 0.010343512707207616\n",
            "DEBUGGING: rel_reward looks like: 0.001435505768568684\n",
            "DEBUGGING: rel_reward looks like: 0.0004289440883701652\n",
            "DEBUGGING: rel_reward looks like: 0.0032493932306996267\n",
            "DEBUGGING: rel_reward looks like: 0.014120906349290308\n",
            "DEBUGGING: rel_reward looks like: 0.00638439071207135\n",
            "DEBUGGING: rel_reward looks like: 0.002508481953813667\n",
            "DEBUGGING: rel_reward looks like: 0.005892796533489365\n",
            "DEBUGGING: rel_reward looks like: 0.0004549530213394005\n",
            "DEBUGGING: rel_reward looks like: 0.0015773736924059375\n",
            "DEBUGGING: the action_prob is: tensor([0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [65]\n",
            "DEBUGGING: rel_reward looks like: 0.00015621607759095117\n",
            "DEBUGGING: rel_reward looks like: 0.2740145486285247\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 4.547473508864641e-13\n",
            "DEBUGGING: rel_reward looks like: 6.821210263300064e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508863607e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [30]\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 4.547473508864641e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544312866e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: the action_prob is: tensor([0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [36]\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: the action_prob is: tensor([0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106,\n",
            "        0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106,\n",
            "        0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106,\n",
            "        0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106,\n",
            "        0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106,\n",
            "        0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106,\n",
            "        0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106,\n",
            "        0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106,\n",
            "        0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106,\n",
            "        0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106,\n",
            "        0.0106, 0.0106, 0.0106, 0.0106], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [84]\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544318036e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544328376e-13\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 0.0\n",
            "DEBUGGING: rel_reward looks like: 2.2737367544323206e-13\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.8867413289831347 and immediate abs rewards look like: [0.02985904561865027, 0.004101070997876377, 0.001223683636453643, 0.009265831730772334, 0.040135734870546, 0.017890058498323924, 0.006984280773849605, 0.016365955519631825, 0.001256086924513511, 0.0043530144221222145, 0.0004304231842979789, 0.7548761427956379, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 9.094947017729282e-13, 1.3642420526593924e-12, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.3205670227686014 and immediate relative rewards look like: [0.010343512707207616, 0.001435505768568684, 0.0004289440883701652, 0.0032493932306996267, 0.014120906349290308, 0.00638439071207135, 0.002508481953813667, 0.005892796533489365, 0.0004549530213394005, 0.0015773736924059375, 0.00015621607759095117, 0.2740145486285247, 0.0, 2.2737367544323206e-13, 2.2737367544328376e-13, 0.0, 4.547473508864641e-13, 6.821210263300064e-13, 4.547473508863607e-13, 2.2737367544328376e-13, 0.0, 0.0, 2.2737367544323206e-13, 2.2737367544318036e-13, 4.547473508864641e-13, 2.2737367544312866e-13, 0.0, 2.2737367544318036e-13, 0.0, 2.2737367544323206e-13, 0.0, 0.0, 2.2737367544318036e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.2737367544323206e-13, 0.0, 2.2737367544318036e-13, 2.2737367544323206e-13, 2.2737367544328376e-13, 0.0, 0.0, 2.2737367544323206e-13]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [56]\n",
            "DEBUGGING: rel_reward looks like: 0.0003597018465518302\n",
            "DEBUGGING: rel_reward looks like: 0.005868706334765855\n",
            "DEBUGGING: rel_reward looks like: 0.0008418317791623762\n",
            "DEBUGGING: rel_reward looks like: 0.00018707897736389115\n",
            "DEBUGGING: rel_reward looks like: 0.015116918625225743\n",
            "DEBUGGING: rel_reward looks like: 0.004710933859543597\n",
            "DEBUGGING: rel_reward looks like: 0.00015305390966542832\n",
            "DEBUGGING: rel_reward looks like: 0.00010423172032507248\n",
            "DEBUGGING: rel_reward looks like: 0.00040053332120663373\n",
            "DEBUGGING: rel_reward looks like: 0.0008977131716388797\n",
            "DEBUGGING: the action_prob is: tensor([0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [31]\n",
            "DEBUGGING: rel_reward looks like: 0.0020838114765676717\n",
            "DEBUGGING: rel_reward looks like: 4.873285217628998e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00017170400097261745\n",
            "DEBUGGING: rel_reward looks like: 2.1740474715665047e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0026245228468097445\n",
            "DEBUGGING: rel_reward looks like: 0.004479649626489773\n",
            "DEBUGGING: rel_reward looks like: 0.0002883837183189811\n",
            "DEBUGGING: rel_reward looks like: 4.5746733636472324e-05\n",
            "DEBUGGING: rel_reward looks like: 8.292604456468228e-05\n",
            "DEBUGGING: rel_reward looks like: 4.545447478886905e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [58]\n",
            "DEBUGGING: rel_reward looks like: 0.00035979104866245003\n",
            "DEBUGGING: rel_reward looks like: 0.00019141790208997841\n",
            "DEBUGGING: rel_reward looks like: 7.411468119164965e-06\n",
            "DEBUGGING: rel_reward looks like: 2.584382811950529e-05\n",
            "DEBUGGING: rel_reward looks like: 3.5873724195612854e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0001391745664513345\n",
            "DEBUGGING: rel_reward looks like: 0.0003500695442686471\n",
            "DEBUGGING: rel_reward looks like: 0.0006271467550078074\n",
            "DEBUGGING: rel_reward looks like: 4.763793499961636e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00012047774225038597\n",
            "DEBUGGING: the action_prob is: tensor([0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [49]\n",
            "DEBUGGING: rel_reward looks like: 8.386767204183144e-06\n",
            "DEBUGGING: rel_reward looks like: 4.7864892343123406e-05\n",
            "DEBUGGING: rel_reward looks like: 4.2772331333112805e-06\n",
            "DEBUGGING: rel_reward looks like: 3.0656317097197545e-05\n",
            "DEBUGGING: rel_reward looks like: 3.048998953306806e-05\n",
            "DEBUGGING: rel_reward looks like: 3.976434155360105e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0005989745396073697\n",
            "DEBUGGING: rel_reward looks like: 1.6654691936854842e-05\n",
            "DEBUGGING: rel_reward looks like: 2.969072123322182e-05\n",
            "DEBUGGING: rel_reward looks like: 7.060497153848992e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [8]\n",
            "DEBUGGING: rel_reward looks like: 8.361133506230102e-05\n",
            "DEBUGGING: rel_reward looks like: 7.2039948284097965e-06\n",
            "DEBUGGING: rel_reward looks like: 9.002180173637057e-05\n",
            "DEBUGGING: rel_reward looks like: 8.450283611987153e-05\n",
            "DEBUGGING: rel_reward looks like: 1.4969797543451686e-05\n",
            "DEBUGGING: rel_reward looks like: 1.832189581625773e-06\n",
            "DEBUGGING: rel_reward looks like: 5.11779442412562e-08\n",
            "DEBUGGING: rel_reward looks like: 1.3920711644738067e-06\n",
            "DEBUGGING: rel_reward looks like: 2.6028876784717542e-06\n",
            "DEBUGGING: rel_reward looks like: 1.5632225731178506e-05\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.11811911111908557 and immediate abs rewards look like: [0.0010383661865489557, 0.016935343257955537, 0.002415019749150815, 0.0005362343131309899, 0.04332231279158805, 0.013296582801103796, 0.0004299586557863222, 0.0002927626696873631, 0.0011248877185607853, 0.0025201949474649155, 0.00584473624758175, 0.0001364025188195228, 0.0004805734902220138, 6.0837847740913276e-05, 0.007344220957747893, 0.012502535889325372, 0.0008012627295101993, 0.0001270688308068202, 0.00023032974468151224, 0.0001262407899957907, 0.0009992028699343791, 0.0005314098343660589, 2.0571603272401262e-05, 7.173275662353262e-05, 9.956940630218014e-05, 0.0003862724638565851, 0.0009714663342492713, 0.001739764822559664, 0.0001320692895205866, 0.00033399121593902237, 2.3247191165864933e-05, 0.00013267507529235445, 1.1855350294354139e-05, 8.497077715219348e-05, 8.45071726871538e-05, 0.00011020894635294098, 0.0016600231610937044, 4.612986458596424e-05, 8.223545319197001e-05, 0.00019555131029846962, 0.0002315580809408857, 1.9949492980231298e-05, 0.00024928895800258033, 0.00023398473695124267, 4.1447226067248266e-05, 5.0727499001368415e-06, 1.4169518181006424e-07, 3.854194801533595e-06, 7.206544069049414e-06, 4.328040404288913e-05]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.04161740509122539 and immediate relative rewards look like: [0.0003597018465518302, 0.005868706334765855, 0.0008418317791623762, 0.00018707897736389115, 0.015116918625225743, 0.004710933859543597, 0.00015305390966542832, 0.00010423172032507248, 0.00040053332120663373, 0.0008977131716388797, 0.0020838114765676717, 4.873285217628998e-05, 0.00017170400097261745, 2.1740474715665047e-05, 0.0026245228468097445, 0.004479649626489773, 0.0002883837183189811, 4.5746733636472324e-05, 8.292604456468228e-05, 4.545447478886905e-05, 0.00035979104866245003, 0.00019141790208997841, 7.411468119164965e-06, 2.584382811950529e-05, 3.5873724195612854e-05, 0.0001391745664513345, 0.0003500695442686471, 0.0006271467550078074, 4.763793499961636e-05, 0.00012047774225038597, 8.386767204183144e-06, 4.7864892343123406e-05, 4.2772331333112805e-06, 3.0656317097197545e-05, 3.048998953306806e-05, 3.976434155360105e-05, 0.0005989745396073697, 1.6654691936854842e-05, 2.969072123322182e-05, 7.060497153848992e-05, 8.361133506230102e-05, 7.2039948284097965e-06, 9.002180173637057e-05, 8.450283611987153e-05, 1.4969797543451686e-05, 1.832189581625773e-06, 5.11779442412562e-08, 1.3920711644738067e-06, 2.6028876784717542e-06, 1.5632225731178506e-05]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [17]\n",
            "DEBUGGING: rel_reward looks like: 0.005991038410894446\n",
            "DEBUGGING: rel_reward looks like: 0.002090787409720726\n",
            "DEBUGGING: rel_reward looks like: 0.0016991505351479706\n",
            "DEBUGGING: rel_reward looks like: 0.011873197684639793\n",
            "DEBUGGING: rel_reward looks like: 0.001442467200679482\n",
            "DEBUGGING: rel_reward looks like: 0.005847275421023622\n",
            "DEBUGGING: rel_reward looks like: 0.0095889156250421\n",
            "DEBUGGING: rel_reward looks like: 0.0023611276328808196\n",
            "DEBUGGING: rel_reward looks like: 0.000117644402469773\n",
            "DEBUGGING: rel_reward looks like: 8.225415018602512e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [26]\n",
            "DEBUGGING: rel_reward looks like: 6.966130570208907e-05\n",
            "DEBUGGING: rel_reward looks like: 5.515852021650384e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0019861535927034853\n",
            "DEBUGGING: rel_reward looks like: 0.0006647216828347999\n",
            "DEBUGGING: rel_reward looks like: 2.6959139680109197e-06\n",
            "DEBUGGING: rel_reward looks like: 3.62692644013287e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0013596150649535469\n",
            "DEBUGGING: rel_reward looks like: 0.000475582465264819\n",
            "DEBUGGING: rel_reward looks like: 0.00016619625941231565\n",
            "DEBUGGING: rel_reward looks like: 1.5686103975917078e-05\n",
            "DEBUGGING: the action_prob is: tensor([0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [36]\n",
            "DEBUGGING: rel_reward looks like: 0.00018099362036357947\n",
            "DEBUGGING: rel_reward looks like: 2.417160514147834e-05\n",
            "DEBUGGING: rel_reward looks like: 0.0011285311199799842\n",
            "DEBUGGING: rel_reward looks like: 0.00016203753930230412\n",
            "DEBUGGING: rel_reward looks like: 7.373389330464477e-06\n",
            "DEBUGGING: rel_reward looks like: 8.46116881716985e-05\n",
            "DEBUGGING: rel_reward looks like: 7.8010621284009e-06\n",
            "DEBUGGING: rel_reward looks like: 8.855565188390353e-05\n",
            "DEBUGGING: rel_reward looks like: 2.197070560040472e-05\n",
            "DEBUGGING: rel_reward looks like: 7.28824207864597e-06\n",
            "DEBUGGING: the action_prob is: tensor([0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [85]\n",
            "DEBUGGING: rel_reward looks like: 3.0288483643512977e-05\n",
            "DEBUGGING: rel_reward looks like: 0.00016345085976048263\n",
            "DEBUGGING: rel_reward looks like: 1.5537604556730745e-06\n",
            "DEBUGGING: rel_reward looks like: 3.285958509200725e-05\n",
            "DEBUGGING: rel_reward looks like: 2.2139255131874659e-07\n",
            "DEBUGGING: rel_reward looks like: 3.0336105803304056e-08\n",
            "DEBUGGING: rel_reward looks like: 8.690208765062844e-08\n",
            "DEBUGGING: rel_reward looks like: 4.064332575213502e-07\n",
            "DEBUGGING: rel_reward looks like: 7.0000015291437955e-06\n",
            "DEBUGGING: rel_reward looks like: 6.812404015743098e-06\n",
            "DEBUGGING: the action_prob is: tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [20]\n",
            "DEBUGGING: rel_reward looks like: 1.256097539447769e-06\n",
            "DEBUGGING: rel_reward looks like: 7.4206372823683146e-06\n",
            "DEBUGGING: rel_reward looks like: 4.6333019847628054e-06\n",
            "DEBUGGING: rel_reward looks like: 5.5032492650969537e-05\n",
            "DEBUGGING: rel_reward looks like: 1.119959552285409e-05\n",
            "DEBUGGING: rel_reward looks like: 1.726869211241512e-05\n",
            "DEBUGGING: rel_reward looks like: 4.296362157740621e-05\n",
            "DEBUGGING: rel_reward looks like: 6.959880469990928e-06\n",
            "DEBUGGING: rel_reward looks like: 7.248640539512283e-06\n",
            "DEBUGGING: rel_reward looks like: 3.972070262506507e-06\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.1358505530192815 and immediate abs rewards look like: [0.017294578184191778, 0.005999403139412607, 0.004865428093125956, 0.03394050822862482, 0.004074452589520661, 0.016492631851178885, 0.026888032126862527, 0.006557291302215162, 0.0003259489935771853, 0.00022786892350268317, 0.00019296705431770533, 0.00015278260434570257, 0.005501108069893235, 0.0018374425058027555, 7.447168172802776e-06, 0.00010018961529567605, 0.0037556419183601975, 0.001311907359649922, 0.00045823893560736906, 4.324278415879235e-05, 0.0004989476865375764, 6.662214718744508e-05, 0.003110399520210194, 0.00044609554333874257, 2.0295933609304484e-05, 0.00023289975160878384, 2.1471167201525532e-05, 0.00024373326914428617, 6.046500220691087e-05, 2.005733904297813e-05, 8.335370193890412e-05, 0.00044980203529121354, 4.275109859008808e-06, 9.041169596457621e-05, 6.091318027756643e-07, 8.346569302375428e-08, 2.3909933588583954e-07, 1.1182460184500087e-06, 1.925954757098225e-05, 1.8743267446552636e-05, 3.455932983342791e-06, 2.041656171059003e-05, 1.2747610526275821e-05, 0.0001514102759756497, 3.08116250380408e-05, 4.750801099362434e-05, 0.00011819545716207358, 1.9146219074173132e-05, 1.9940442143706605e-05, 1.0926774848485366e-05]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.04803959845853952 and immediate relative rewards look like: [0.005991038410894446, 0.002090787409720726, 0.0016991505351479706, 0.011873197684639793, 0.001442467200679482, 0.005847275421023622, 0.0095889156250421, 0.0023611276328808196, 0.000117644402469773, 8.225415018602512e-05, 6.966130570208907e-05, 5.515852021650384e-05, 0.0019861535927034853, 0.0006647216828347999, 2.6959139680109197e-06, 3.62692644013287e-05, 0.0013596150649535469, 0.000475582465264819, 0.00016619625941231565, 1.5686103975917078e-05, 0.00018099362036357947, 2.417160514147834e-05, 0.0011285311199799842, 0.00016203753930230412, 7.373389330464477e-06, 8.46116881716985e-05, 7.8010621284009e-06, 8.855565188390353e-05, 2.197070560040472e-05, 7.28824207864597e-06, 3.0288483643512977e-05, 0.00016345085976048263, 1.5537604556730745e-06, 3.285958509200725e-05, 2.2139255131874659e-07, 3.0336105803304056e-08, 8.690208765062844e-08, 4.064332575213502e-07, 7.0000015291437955e-06, 6.812404015743098e-06, 1.256097539447769e-06, 7.4206372823683146e-06, 4.6333019847628054e-06, 5.5032492650969537e-05, 1.119959552285409e-05, 1.726869211241512e-05, 4.296362157740621e-05, 6.959880469990928e-06, 7.248640539512283e-06, 3.972070262506507e-06]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 150 number of matrices\n",
            "DEBUGGING: ACT_MAT has 150 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.2901658404712037, 0.2826488159232284, 0.2840538486410704, 0.2864898025784851, 0.28610142358362173, 0.27472779518619334, 0.27105394391325455, 0.2712580423832736, 0.26804570287857, 0.27029368672447535, 0.2714306192243125, 0.274014548633052, 4.5730805488383965e-12, 4.619273281654946e-12, 4.43626222849668e-12, 4.251402578841815e-12, 4.294346039234157e-12, 3.878382513482518e-12, 3.228546956719709e-12, 2.801817783670049e-12, 2.6004485941684497e-12, 2.626715751685303e-12, 2.6532482340255583e-12, 2.45037834200235e-12, 2.2454592591506762e-12, 1.8087999073375879e-12, 1.5974002342368276e-12, 1.6135355901382097e-12, 1.4001635501969992e-12, 1.4143066163606053e-12, 1.1989221625428013e-12, 1.2110324874169711e-12, 1.2232651388050213e-12, 1.0059509730927686e-12, 1.0161120940330995e-12, 1.0263758525586865e-12, 1.0367432854128147e-12, 1.047215439810924e-12, 1.0577933735463878e-12, 1.0684781550973614e-12, 1.0792708637347086e-12, 1.0901725896310189e-12, 1.1011844339707262e-12, 8.826371298257516e-13, 8.915526563896481e-13, 6.708878595418866e-13, 4.4799412535217634e-13, 2.2284893930191173e-13, 2.2509993868879974e-13, 2.2737367544323206e-13], [0.038527490180997034, 0.03855332154994465, 0.033014762843614945, 0.032497910166113704, 0.03263720322095941, 0.01769725716740774, 0.013117498290771861, 0.013095398364753973, 0.013122390549928182, 0.012850360837092473, 0.012073381480256155, 0.010090474751200488, 0.010143173635377976, 0.010072191549904403, 0.010151970783018927, 0.007603482763847659, 0.0031553870074322076, 0.0028959629182961884, 0.002879006247131026, 0.002824323436935701, 0.0028069383456028603, 0.0024718659565054653, 0.0023034828832479664, 0.002319264055685658, 0.0023165860884506594, 0.0023037498628838853, 0.002186439693366213, 0.0018549193425227938, 0.0012401743308232186, 0.0012045822180036386, 0.0010950550260133865, 0.0010976447058678822, 0.0010603836500250089, 0.0010667741584764621, 0.0010465836781608734, 0.0010263572612402075, 0.0009965585047339459, 0.0004015999647743193, 0.0003888336089267318, 0.00036277059362980807, 0.0002951167899912304, 0.00021364187366558523, 0.00020852310993654084, 0.00011969829111128309, 3.5550964637789464e-05, 2.0789057671048262e-05, 1.914835160547726e-05, 1.9290074405288896e-05, 1.8078791152338475e-05, 1.5632225731178506e-05], [0.04545098205059644, 0.03985852892899191, 0.03814923385784968, 0.03681826598252698, 0.02519703868473453, 0.023994516650560654, 0.018330546696502054, 0.008829930375212076, 0.006534144184172986, 0.006481312910811327, 0.006463695717803335, 0.006458620618284087, 0.006468143533401599, 0.004527262566361731, 0.003901556448007001, 0.0039382429636757475, 0.003941387575024665, 0.0026078510202738573, 0.002153806621221251, 0.0020076872341504397, 0.002012122353711639, 0.0018496249831798576, 0.001843892301048868, 0.0007225870515847313, 0.0005662116285681082, 0.0005644830699370139, 0.0004847185674397126, 0.0004817348538498098, 0.000397150709056471, 0.000378969700460673, 0.00037543581654750206, 0.00034863366959998895, 0.00018705334327222863, 0.00018737331597631874, 0.00015607447564071868, 0.00015742735665595954, 0.00015898688944460227, 0.0001605050377342946, 0.00016171576209775076, 0.00015627854602889594, 0.0001509759010233867, 0.00015123212473125143, 0.00014526412873624555, 0.00014205134015301288, 8.789782575963973e-05, 7.747295983513701e-05, 6.081239163911302e-05, 1.8029060668390714e-05, 1.1180990099393725e-05, 3.972070262506507e-06]]\n",
            "DEBUGGING: traj_returns = [0.2901658404712037, 0.038527490180997034, 0.04545098205059644]\n",
            "DEBUGGING: actions = [[9], [57], [61], [29], [18], [41], [25], [24], [55], [67], [65], [0], [23], [46], [66], [39], [4], [7], [21], [25], [30], [41], [60], [32], [36], [48], [30], [44], [12], [24], [36], [7], [50], [28], [30], [53], [33], [68], [13], [53], [84], [33], [66], [98], [43], [95], [59], [94], [73], [79], [56], [48], [15], [29], [10], [24], [43], [22], [55], [26], [31], [47], [63], [69], [41], [9], [7], [68], [40], [16], [58], [19], [43], [16], [12], [25], [30], [17], [79], [17], [49], [42], [55], [52], [39], [64], [38], [48], [29], [40], [8], [91], [98], [75], [48], [74], [49], [93], [99], [58], [17], [37], [31], [3], [5], [37], [52], [54], [18], [58], [26], [59], [33], [1], [21], [49], [20], [64], [47], [59], [36], [49], [3], [11], [69], [49], [6], [8], [5], [67], [85], [30], [70], [77], [73], [64], [19], [78], [52], [29], [20], [97], [56], [74], [76], [97], [69], [12], [105], [76]]\n",
            "DEBUGGING: actions length = 150\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        ...,\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        ...,\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664],\n",
            "        [-0.4778, -0.4365, -0.2364,  ..., -0.1699,  0.0788, -0.0664]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155,\n",
            "        1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155, 1.1155],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[1.24714771e-01 1.20353555e-01 1.18405948e-01 1.18601993e-01\n",
            "  1.14645222e-01 1.05473190e-01 1.00833996e-01 9.77277904e-02\n",
            "  9.59007459e-02 9.65417868e-02 9.66558988e-02 9.68545480e-02\n",
            "  5.53710572e-03 4.86648471e-03 4.68450908e-03 3.84724191e-03\n",
            "  2.36559153e-03 1.83460465e-03 1.67760429e-03 1.61067022e-03\n",
            "  1.60635357e-03 1.44049698e-03 1.38245840e-03 1.01395037e-03\n",
            "  9.60932573e-04 9.56077645e-04 8.90386087e-04 7.78884733e-04\n",
            "  5.45775014e-04 5.27850640e-04 4.90163615e-04 4.82092792e-04\n",
            "  4.15812332e-04 4.18049158e-04 4.00886052e-04 3.94594873e-04\n",
            "  3.85181798e-04 1.87368335e-04 1.83516457e-04 1.73016380e-04\n",
            "  1.48697564e-04 1.21624666e-04 1.17929080e-04 8.72498774e-05\n",
            "  4.11495971e-05 3.27540061e-05 2.66535812e-05 1.24397118e-05\n",
            "  9.75326049e-06 6.53476541e-06]]\n",
            "DEBUGGING: baseline2 looks like: 0.1247147709009324\n",
            "DEBUGGING: ADS looks like: [2.90165840e-01 2.82648816e-01 2.84053849e-01 2.86489803e-01\n",
            " 2.86101424e-01 2.74727795e-01 2.71053944e-01 2.71258042e-01\n",
            " 2.68045703e-01 2.70293687e-01 2.71430619e-01 2.74014549e-01\n",
            " 4.57308055e-12 4.61927328e-12 4.43626223e-12 4.25140258e-12\n",
            " 4.29434604e-12 3.87838251e-12 3.22854696e-12 2.80181778e-12\n",
            " 2.60044859e-12 2.62671575e-12 2.65324823e-12 2.45037834e-12\n",
            " 2.24545926e-12 1.80879991e-12 1.59740023e-12 1.61353559e-12\n",
            " 1.40016355e-12 1.41430662e-12 1.19892216e-12 1.21103249e-12\n",
            " 1.22326514e-12 1.00595097e-12 1.01611209e-12 1.02637585e-12\n",
            " 1.03674329e-12 1.04721544e-12 1.05779337e-12 1.06847816e-12\n",
            " 1.07927086e-12 1.09017259e-12 1.10118443e-12 8.82637130e-13\n",
            " 8.91552656e-13 6.70887860e-13 4.47994125e-13 2.22848939e-13\n",
            " 2.25099939e-13 2.27373675e-13 3.85274902e-02 3.85533215e-02\n",
            " 3.30147628e-02 3.24979102e-02 3.26372032e-02 1.76972572e-02\n",
            " 1.31174983e-02 1.30953984e-02 1.31223905e-02 1.28503608e-02\n",
            " 1.20733815e-02 1.00904748e-02 1.01431736e-02 1.00721915e-02\n",
            " 1.01519708e-02 7.60348276e-03 3.15538701e-03 2.89596292e-03\n",
            " 2.87900625e-03 2.82432344e-03 2.80693835e-03 2.47186596e-03\n",
            " 2.30348288e-03 2.31926406e-03 2.31658609e-03 2.30374986e-03\n",
            " 2.18643969e-03 1.85491934e-03 1.24017433e-03 1.20458222e-03\n",
            " 1.09505503e-03 1.09764471e-03 1.06038365e-03 1.06677416e-03\n",
            " 1.04658368e-03 1.02635726e-03 9.96558505e-04 4.01599965e-04\n",
            " 3.88833609e-04 3.62770594e-04 2.95116790e-04 2.13641874e-04\n",
            " 2.08523110e-04 1.19698291e-04 3.55509646e-05 2.07890577e-05\n",
            " 1.91483516e-05 1.92900744e-05 1.80787912e-05 1.56322257e-05\n",
            " 4.54509821e-02 3.98585289e-02 3.81492339e-02 3.68182660e-02\n",
            " 2.51970387e-02 2.39945167e-02 1.83305467e-02 8.82993038e-03\n",
            " 6.53414418e-03 6.48131291e-03 6.46369572e-03 6.45862062e-03\n",
            " 6.46814353e-03 4.52726257e-03 3.90155645e-03 3.93824296e-03\n",
            " 3.94138758e-03 2.60785102e-03 2.15380662e-03 2.00768723e-03\n",
            " 2.01212235e-03 1.84962498e-03 1.84389230e-03 7.22587052e-04\n",
            " 5.66211629e-04 5.64483070e-04 4.84718567e-04 4.81734854e-04\n",
            " 3.97150709e-04 3.78969700e-04 3.75435817e-04 3.48633670e-04\n",
            " 1.87053343e-04 1.87373316e-04 1.56074476e-04 1.57427357e-04\n",
            " 1.58986889e-04 1.60505038e-04 1.61715762e-04 1.56278546e-04\n",
            " 1.50975901e-04 1.51232125e-04 1.45264129e-04 1.42051340e-04\n",
            " 8.78978258e-05 7.74729598e-05 6.08123916e-05 1.80290607e-05\n",
            " 1.11809901e-05 3.97207026e-06]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(0.1101, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-7.8017e-37, -8.3278e-37, -7.9029e-37,  ..., -8.6723e-37,\n",
            "         -7.5213e-37, -2.3619e-34],\n",
            "        [ 3.6175e-39,  4.1081e-39,  3.6471e-39,  ...,  7.7022e-39,\n",
            "          5.0576e-39,  1.3657e-36],\n",
            "        ...,\n",
            "        [-5.7651e-30, -6.5138e-30, -5.4308e-30,  ..., -1.0105e-29,\n",
            "         -6.1539e-30, -1.8504e-27],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]])\n",
            "   Last layer:\n",
            "tensor([[ 1.3448e-13,  6.1979e-15,  6.4115e-14,  2.2380e-14, -1.2788e-13,\n",
            "          3.0684e-15,  5.8434e-14, -1.2153e-15,  1.1599e-14,  1.6240e-13,\n",
            "          8.8153e-15,  5.6048e-14, -7.6761e-14,  1.3969e-14,  3.6636e-14,\n",
            "         -6.8810e-14,  2.1280e-14,  6.7714e-14,  1.1925e-13,  1.0271e-14],\n",
            "        [-6.7389e-14, -1.5723e-13, -1.0395e-13, -1.4865e-14, -1.4848e-13,\n",
            "         -9.0504e-14, -1.2296e-13, -3.3414e-13, -9.5114e-14, -1.1711e-13,\n",
            "         -1.1306e-13, -1.4106e-13, -1.5442e-13, -6.2953e-14, -1.3147e-13,\n",
            "         -1.8429e-13, -1.0646e-13, -6.4866e-14, -6.5163e-14, -1.0644e-13],\n",
            "        [ 1.1887e-13,  8.1100e-14,  1.2327e-13,  9.7430e-14,  1.5481e-13,\n",
            "          8.6516e-14,  1.1851e-13,  1.8491e-13,  8.2724e-14,  1.2054e-13,\n",
            "          7.2234e-14,  1.7517e-13,  8.0111e-14,  1.6182e-13,  1.7738e-13,\n",
            "          2.2437e-15,  1.6361e-13,  9.2145e-14,  9.5606e-14,  1.1534e-13],\n",
            "        [-1.6721e-14, -2.4327e-14,  3.1141e-14,  4.4109e-14, -5.5299e-14,\n",
            "          2.9043e-14,  6.5347e-14,  2.4546e-14,  4.3736e-14,  6.1478e-14,\n",
            "          1.3077e-14,  1.7588e-15,  3.0893e-14,  1.4111e-14,  4.8967e-14,\n",
            "          1.8600e-14,  5.4600e-14,  1.1338e-15,  1.6054e-14,  4.3686e-14],\n",
            "        [-8.9691e-14, -1.0227e-13, -5.6977e-14, -1.0462e-13, -1.0336e-13,\n",
            "         -9.1785e-14, -8.2250e-14, -2.3465e-13, -3.7042e-14, -6.9758e-14,\n",
            "         -5.2110e-14, -1.7105e-13, -1.5767e-13, -5.1320e-14, -4.3413e-16,\n",
            "         -1.0245e-13, -1.3444e-13, -7.2863e-14, -4.7813e-14, -1.0431e-13],\n",
            "        [-8.1777e-15,  5.9704e-14,  1.7368e-14,  2.9253e-14, -1.4534e-14,\n",
            "          1.8771e-14, -3.7234e-14, -8.7427e-14, -6.1994e-14, -2.6462e-14,\n",
            "         -1.0202e-13, -2.5214e-14,  9.3095e-14, -2.6257e-14,  4.0322e-14,\n",
            "         -3.9013e-14,  8.4807e-15,  3.5310e-14,  7.4024e-15, -1.8044e-14],\n",
            "        [ 2.2319e-13,  1.2263e-13,  1.2996e-13,  1.0432e-13,  7.5729e-14,\n",
            "          1.8889e-13,  7.8896e-14,  3.9127e-14,  2.2083e-13,  2.3497e-13,\n",
            "          1.1037e-13,  2.0144e-13,  3.1110e-13,  1.0378e-13,  2.6572e-13,\n",
            "          1.5506e-13,  2.1869e-13, -3.8918e-15,  1.1975e-13,  1.2369e-13],\n",
            "        [ 2.1242e-14,  5.9094e-14,  2.8855e-14,  1.2123e-13,  6.4426e-14,\n",
            "          2.7560e-14, -1.6302e-14,  9.4337e-14,  3.1565e-14,  7.0857e-14,\n",
            "          5.8668e-14,  4.3156e-14,  3.8465e-14,  7.8942e-14,  6.1126e-14,\n",
            "          6.6295e-14,  3.5663e-14,  7.4444e-14,  7.8527e-14,  7.5660e-14],\n",
            "        [ 7.3388e-15, -2.2728e-14, -2.3888e-14, -1.2534e-14, -5.0969e-17,\n",
            "         -2.2212e-14, -1.1408e-14, -3.6838e-14, -2.7886e-14, -3.2806e-14,\n",
            "         -4.5531e-15, -2.2682e-14, -8.5197e-15, -1.8291e-14, -1.6395e-14,\n",
            "         -2.8080e-14, -2.0582e-14, -2.5150e-14, -1.7218e-14, -1.1986e-14],\n",
            "        [-1.5110e-14, -1.6271e-14, -2.8133e-14, -3.1159e-14, -2.8867e-14,\n",
            "         -1.8943e-14, -2.4874e-14, -2.6049e-14, -3.1640e-14, -2.6832e-14,\n",
            "         -1.1023e-14, -3.8927e-14, -1.8514e-14, -3.2620e-14, -2.0764e-14,\n",
            "         -2.1999e-15, -4.5287e-14, -4.2905e-14, -1.1209e-14, -1.0288e-14]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-1.2677e-36, -1.3693e-36, -1.2638e-36,  ..., -1.3827e-36,\n",
            "         -1.2108e-36, -3.8080e-34],\n",
            "        [ 5.1010e-40,  5.7980e-40,  5.1347e-40,  ...,  1.0828e-39,\n",
            "          7.1178e-40,  1.9228e-37],\n",
            "        ...,\n",
            "        [-1.1550e-30, -1.3117e-30, -1.0878e-30,  ..., -2.0149e-30,\n",
            "         -1.2288e-30, -3.7039e-28],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]])\n",
            "   Last layer:\n",
            "tensor([[-9.3104e-14, -2.5895e-14, -2.9281e-15, -1.3959e-13, -4.8495e-14,\n",
            "         -5.4543e-14, -5.4206e-14, -1.3630e-13,  2.0628e-14, -8.1336e-15,\n",
            "          1.4935e-14, -2.2866e-13, -1.0571e-14, -5.2752e-14, -2.4792e-14,\n",
            "          2.0894e-14, -1.3327e-13, -1.3209e-13, -7.7526e-15, -8.6139e-14],\n",
            "        [-2.4774e-14, -7.2552e-14, -1.2879e-13, -2.3855e-13, -1.7844e-13,\n",
            "         -1.0339e-13, -3.8921e-14, -1.8222e-13, -2.7271e-14,  4.1633e-14,\n",
            "         -5.1388e-14, -6.1358e-14, -8.1217e-14, -1.2588e-13, -1.1980e-13,\n",
            "         -7.4870e-14,  2.0962e-14, -2.8234e-14, -3.7185e-14, -1.3263e-13],\n",
            "        [ 2.6311e-14,  3.9359e-14,  4.5606e-14,  5.2001e-14,  1.8649e-14,\n",
            "          6.2398e-14,  5.2880e-15,  6.8044e-14,  1.0768e-14,  4.6295e-14,\n",
            "          4.2090e-14, -7.0595e-15,  3.1647e-14,  2.5819e-14,  2.4124e-14,\n",
            "          7.4939e-16,  2.5092e-14,  2.0874e-15,  2.1841e-14,  7.2977e-14],\n",
            "        [ 4.1745e-14,  6.3660e-14,  1.0175e-14,  5.0363e-14,  4.0118e-14,\n",
            "          7.2742e-14,  2.4080e-15, -1.0301e-14,  4.2190e-15,  2.7902e-14,\n",
            "          5.6878e-14,  5.1720e-14,  7.5833e-15,  2.2197e-14,  7.2253e-14,\n",
            "          4.0122e-14,  5.3999e-14,  6.1827e-14,  2.9454e-14,  1.7424e-14],\n",
            "        [-1.5271e-14, -3.0903e-14,  1.2900e-14,  1.7039e-14,  6.4904e-14,\n",
            "         -2.1374e-14,  1.0428e-14, -7.4364e-14, -3.8801e-15,  2.2629e-14,\n",
            "         -2.8300e-14,  5.3583e-14, -4.2175e-15, -2.2421e-14, -1.8654e-14,\n",
            "          2.0802e-15,  1.8971e-14, -1.0515e-14,  4.8671e-14,  2.8926e-14],\n",
            "        [ 8.7133e-14,  8.3377e-14,  4.1402e-14,  1.3531e-13,  7.4696e-14,\n",
            "          1.1589e-13,  1.0333e-13,  1.9212e-13,  2.6362e-14,  1.4330e-13,\n",
            "          7.9298e-14,  1.2827e-13,  1.3648e-13,  1.1107e-13,  1.0278e-13,\n",
            "          7.1271e-15,  7.3908e-14,  8.5842e-14,  1.3552e-13,  1.0090e-13],\n",
            "        [ 2.8165e-14,  1.1267e-14,  4.2066e-14, -5.8434e-14, -1.1437e-14,\n",
            "         -7.0999e-14,  1.1625e-14,  1.1872e-13, -5.7653e-14,  1.0019e-13,\n",
            "         -2.3898e-14,  1.8498e-13,  4.4772e-15,  1.3619e-14,  1.0364e-13,\n",
            "         -1.7298e-14,  1.6201e-14,  4.4332e-14,  1.4874e-14,  8.0705e-14],\n",
            "        [-1.0212e-15,  4.9692e-15, -3.1842e-14,  1.8931e-14,  4.5748e-15,\n",
            "         -1.6809e-14, -5.5130e-14, -3.6778e-14, -7.9020e-15, -2.9348e-14,\n",
            "          5.9018e-15, -4.3818e-16,  1.0223e-14, -9.7290e-15, -2.3862e-14,\n",
            "         -2.0356e-14, -9.9598e-15, -2.4073e-14, -1.5697e-14,  2.6237e-15],\n",
            "        [ 3.6652e-14,  1.8336e-14,  1.5150e-14,  1.5128e-14,  1.4082e-14,\n",
            "          2.2468e-14,  1.6280e-14,  1.3474e-14,  2.0172e-14,  2.1529e-14,\n",
            "          2.4823e-15,  1.5865e-14,  2.7893e-14,  2.1719e-14,  1.4893e-14,\n",
            "          3.2621e-14,  3.9057e-14,  1.8858e-14,  1.9983e-14,  2.0661e-14],\n",
            "        [ 1.1093e-14, -9.6364e-16, -5.0597e-16,  1.8678e-14, -1.3111e-14,\n",
            "          1.8760e-15,  6.4875e-15,  1.6346e-15,  2.1191e-14, -9.0655e-15,\n",
            "          1.6374e-15,  1.9694e-14,  4.3205e-15, -6.8330e-15,  1.0969e-14,\n",
            "          1.4510e-15,  9.5249e-15, -5.7476e-15,  2.9758e-15,  7.4850e-15]])\n",
            "DEBUGGING: training for one iteration takes 0.006839 min:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256,
          "referenced_widgets": [
            "ced572e143cf4e6f9d7c42c31b2d0af1",
            "37b10496ae1d45b6b68f11b3eeb250fb",
            "77a8e198db354f3d83bfc2f3011e97fe",
            "0c6df2472cd84d3985d6407326d71fd9",
            "98e43e3d6868406286236c8cd5d17b2c",
            "4822cb1580fc4498bffa4c46cd7d387e",
            "6a760fbe60df403e94c9d84e72aac997",
            "2bd0dcdf9dde4376b34b6e44ee3fdbec"
          ]
        },
        "id": "Oe4fCU1V-kAs",
        "outputId": "44963547-0233-4d83-b5ca-8e3a1c5ba596"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ced572e143cf4e6f9d7c42c31b2d0af1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training reward (easy config)</td><td>â–â–‚â–ˆâ–ˆâ–‚â–â–ˆâ–â–‚â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training reward (easy config)</td><td>0.13585</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">toasty-sea-2318</strong>: <a href=\"https://wandb.ai/ieor4575-spring2022/finalproject/runs/vg5kvpbc\" target=\"_blank\">https://wandb.ai/ieor4575-spring2022/finalproject/runs/vg5kvpbc</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220508_033729-vg5kvpbc/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "I-DsTgK4t4-g"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = [   54246.9141, 27220988.0000,  3948314.5000,  3902082.5000,\n",
        "        27219886.0000,  3816622.7500, 27175632.0000,  3976903.5000,\n",
        "         3977309.0000,  3961905.7500,  3904790.7500, 23420360.0000,\n",
        "        27218672.0000, 27206276.0000, 27190054.0000,  3976984.7500,\n",
        "         3931598.0000,  3963138.5000, 19419398.0000,  3931559.5000,\n",
        "         7862683.0000, 19299310.0000,  3915878.7500, 15562425.0000,\n",
        "        27141808.0000, 27323768.0000, 11622510.0000, 15451909.0000,\n",
        "        23389744.0000,  7781165.5000, 11871555.0000, 11657742.0000,\n",
        "        27473456.0000, 11592561.0000,  4054315.2500,  7718911.0000,\n",
        "        27136936.0000,  3914171.2500, 23255876.0000, 15599427.0000,\n",
        "        19661078.0000,  3915166.7500,  7554840.5000, 27196042.0000,\n",
        "        11791202.0000, 23477022.0000, 15708443.0000, 23297932.0000,\n",
        "        23511102.0000,  7689591.5000, 27405178.0000,  4021836.7500,\n",
        "        15558291.0000,  3948567.5000,  7915618.5000,  3864714.5000,\n",
        "         3995934.2500, 19536616.0000,  3890406.7500, 27284138.0000,\n",
        "        11731948.0000, 23232848.0000, 19200360.0000, 22949674.0000,\n",
        "        26724900.0000,  3268551.2500, 22647966.0000, 25925040.0000,\n",
        "        25380838.0000,  5669201.5000,  7810961.0000, 13092715.0000,\n",
        "        14364957.0000, 15622157.0000, 27473452.0000]"
      ],
      "metadata": {
        "id": "R5xFgG-ZGZkL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_t = torch.FloatTensor(a)\n",
        "print(a_t)\n",
        "b_t = a_t /1000000\n",
        "print(b_t)\n",
        "print(torch.nn.functional.softmax(b_t, dim=0))\n",
        "b_t = a_t / a_t.max()\n",
        "print(b_t)\n",
        "torch.nn.functional.softmax(b_t, dim=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5GfEt5DGbcN",
        "outputId": "03befb32-950c-4bc2-9f7d-69536315fbed"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([   54246.9141, 27220988.0000,  3948314.5000,  3902082.5000,\n",
            "        27219886.0000,  3816622.7500, 27175632.0000,  3976903.5000,\n",
            "         3977309.0000,  3961905.7500,  3904790.7500, 23420360.0000,\n",
            "        27218672.0000, 27206276.0000, 27190054.0000,  3976984.7500,\n",
            "         3931598.0000,  3963138.5000, 19419398.0000,  3931559.5000,\n",
            "         7862683.0000, 19299310.0000,  3915878.7500, 15562425.0000,\n",
            "        27141808.0000, 27323768.0000, 11622510.0000, 15451909.0000,\n",
            "        23389744.0000,  7781165.5000, 11871555.0000, 11657742.0000,\n",
            "        27473456.0000, 11592561.0000,  4054315.2500,  7718911.0000,\n",
            "        27136936.0000,  3914171.2500, 23255876.0000, 15599427.0000,\n",
            "        19661078.0000,  3915166.7500,  7554840.5000, 27196042.0000,\n",
            "        11791202.0000, 23477022.0000, 15708443.0000, 23297932.0000,\n",
            "        23511102.0000,  7689591.5000, 27405178.0000,  4021836.7500,\n",
            "        15558291.0000,  3948567.5000,  7915618.5000,  3864714.5000,\n",
            "         3995934.2500, 19536616.0000,  3890406.7500, 27284138.0000,\n",
            "        11731948.0000, 23232848.0000, 19200360.0000, 22949674.0000,\n",
            "        26724900.0000,  3268551.2500, 22647966.0000, 25925040.0000,\n",
            "        25380838.0000,  5669201.5000,  7810961.0000, 13092715.0000,\n",
            "        14364957.0000, 15622157.0000, 27473452.0000])\n",
            "tensor([ 0.0542, 27.2210,  3.9483,  3.9021, 27.2199,  3.8166, 27.1756,  3.9769,\n",
            "         3.9773,  3.9619,  3.9048, 23.4204, 27.2187, 27.2063, 27.1901,  3.9770,\n",
            "         3.9316,  3.9631, 19.4194,  3.9316,  7.8627, 19.2993,  3.9159, 15.5624,\n",
            "        27.1418, 27.3238, 11.6225, 15.4519, 23.3897,  7.7812, 11.8716, 11.6577,\n",
            "        27.4735, 11.5926,  4.0543,  7.7189, 27.1369,  3.9142, 23.2559, 15.5994,\n",
            "        19.6611,  3.9152,  7.5548, 27.1960, 11.7912, 23.4770, 15.7084, 23.2979,\n",
            "        23.5111,  7.6896, 27.4052,  4.0218, 15.5583,  3.9486,  7.9156,  3.8647,\n",
            "         3.9959, 19.5366,  3.8904, 27.2841, 11.7319, 23.2328, 19.2004, 22.9497,\n",
            "        26.7249,  3.2686, 22.6480, 25.9250, 25.3808,  5.6692,  7.8110, 13.0927,\n",
            "        14.3650, 15.6222, 27.4735])\n",
            "tensor([1.0010e-13, 6.2920e-02, 4.9158e-12, 4.6937e-12, 6.2851e-02, 4.3093e-12,\n",
            "        6.0130e-02, 5.0584e-12, 5.0605e-12, 4.9831e-12, 4.7065e-12, 1.4067e-03,\n",
            "        6.2775e-02, 6.2001e-02, 6.1004e-02, 5.0588e-12, 4.8344e-12, 4.9893e-12,\n",
            "        2.5740e-05, 4.8342e-12, 2.4637e-10, 2.2827e-05, 4.7589e-12, 5.4393e-07,\n",
            "        5.8130e-02, 6.9731e-02, 1.0579e-08, 4.8702e-07, 1.3643e-03, 2.2708e-10,\n",
            "        1.3571e-08, 1.0959e-08, 8.0991e-02, 1.0267e-08, 5.4655e-12, 2.1338e-10,\n",
            "        5.7848e-02, 4.7508e-12, 1.1933e-03, 5.6443e-07, 3.2777e-05, 4.7556e-12,\n",
            "        1.8109e-10, 6.1370e-02, 1.2523e-08, 1.4887e-03, 6.2944e-07, 1.2446e-03,\n",
            "        1.5403e-03, 2.0721e-10, 7.5646e-02, 5.2909e-12, 5.4168e-07, 4.9171e-12,\n",
            "        2.5976e-10, 4.5216e-12, 5.1556e-12, 2.8941e-05, 4.6393e-12, 6.7022e-02,\n",
            "        1.1803e-08, 1.1662e-03, 2.0676e-05, 8.7858e-04, 3.8313e-02, 2.4910e-12,\n",
            "        6.4976e-04, 1.7217e-02, 9.9914e-03, 2.7477e-11, 2.3395e-10, 4.6021e-08,\n",
            "        1.6424e-07, 5.7741e-07, 8.0991e-02])\n",
            "tensor([0.0020, 0.9908, 0.1437, 0.1420, 0.9908, 0.1389, 0.9892, 0.1448, 0.1448,\n",
            "        0.1442, 0.1421, 0.8525, 0.9907, 0.9903, 0.9897, 0.1448, 0.1431, 0.1443,\n",
            "        0.7068, 0.1431, 0.2862, 0.7025, 0.1425, 0.5665, 0.9879, 0.9946, 0.4230,\n",
            "        0.5624, 0.8514, 0.2832, 0.4321, 0.4243, 1.0000, 0.4220, 0.1476, 0.2810,\n",
            "        0.9878, 0.1425, 0.8465, 0.5678, 0.7156, 0.1425, 0.2750, 0.9899, 0.4292,\n",
            "        0.8545, 0.5718, 0.8480, 0.8558, 0.2799, 0.9975, 0.1464, 0.5663, 0.1437,\n",
            "        0.2881, 0.1407, 0.1454, 0.7111, 0.1416, 0.9931, 0.4270, 0.8456, 0.6989,\n",
            "        0.8353, 0.9728, 0.1190, 0.8244, 0.9436, 0.9238, 0.2064, 0.2843, 0.4766,\n",
            "        0.5229, 0.5686, 1.0000])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0074, 0.0199, 0.0085, 0.0085, 0.0199, 0.0085, 0.0199, 0.0085, 0.0085,\n",
              "        0.0085, 0.0085, 0.0173, 0.0199, 0.0199, 0.0199, 0.0085, 0.0085, 0.0085,\n",
              "        0.0150, 0.0085, 0.0098, 0.0149, 0.0085, 0.0130, 0.0198, 0.0200, 0.0113,\n",
              "        0.0130, 0.0173, 0.0098, 0.0114, 0.0113, 0.0201, 0.0113, 0.0086, 0.0098,\n",
              "        0.0198, 0.0085, 0.0172, 0.0130, 0.0151, 0.0085, 0.0097, 0.0199, 0.0113,\n",
              "        0.0174, 0.0131, 0.0172, 0.0174, 0.0098, 0.0200, 0.0085, 0.0130, 0.0085,\n",
              "        0.0099, 0.0085, 0.0085, 0.0150, 0.0085, 0.0199, 0.0113, 0.0172, 0.0149,\n",
              "        0.0170, 0.0195, 0.0083, 0.0168, 0.0190, 0.0186, 0.0091, 0.0098, 0.0119,\n",
              "        0.0125, 0.0130, 0.0201])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYdGyAom2SMj",
        "outputId": "a7d50f8f-5162-4eef-fb22-fe6fe7cdacec"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2., 0., 1., 3., 0., 0., 0., 3., 2., 3., 1., 1., 2., 0., 4., 4., 0.,\n",
              "       2., 1., 2., 2., 2., 4., 1., 3., 2., 0., 1., 2., 0., 3., 0., 3., 1.,\n",
              "       3., 0., 4., 1., 4., 4., 0., 0., 1., 2., 4., 0., 0., 1., 1., 1., 2.,\n",
              "       3., 4., 4., 3., 3., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s[1][-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WUaedhX198a",
        "outputId": "f07be305-c569-4ed2-e2f7-01473cd6f1d9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4239171.0"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s[-2] / np.max(s[-2], axis=1, keepdims=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmmYGIMyEQNl",
        "outputId": "a9a38830-44f8-4cc8-d4f4-0365b43ca7a5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.54545455, 0.63636364, 0.81818182, ..., 0.63636364, 0.81818182,\n",
              "        0.63636364],\n",
              "       [0.6211078 , 0.73286546, 0.83098654, ..., 0.70226092, 0.70350464,\n",
              "        0.54639542],\n",
              "       [0.62111973, 0.73290835, 0.83097746, ..., 0.70228197, 0.70376016,\n",
              "        0.54633222],\n",
              "       ...,\n",
              "       [0.62088109, 0.73298711, 0.83121418, ..., 0.70254298, 0.70406519,\n",
              "        0.54647206],\n",
              "       [0.62110977, 0.7329845 , 0.83091882, ..., 0.70244836, 0.70354172,\n",
              "        0.54633137],\n",
              "       [0.62107404, 0.7329943 , 0.83087876, ..., 0.70235964, 0.70362083,\n",
              "        0.54641985]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(s[-2]), len(s[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GEP-MOCEZKu",
        "outputId": "39b77742-9617-493b-f979-efe39e38a731"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(108, 110)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([0.0057, 0.0235, 0.0144, 0.0181, 0.0181, 0.0123, 0.0293, 0.0095, 0.0122,\n",
        "        0.0146, 0.0152, 0.0095, 0.0184, 0.0151, 0.0143, 0.0095, 0.0220, 0.0120,\n",
        "        0.0209, 0.0226, 0.0142, 0.0124, 0.0145, 0.0133, 0.0218, 0.0146, 0.0098,\n",
        "        0.0186, 0.0146, 0.0126, 0.0182, 0.0204, 0.0155, 0.0180, 0.0098, 0.0184,\n",
        "        0.0248, 0.0150, 0.0215, 0.0114, 0.0186, 0.0175, 0.0141, 0.0199, 0.0250,\n",
        "        0.0178, 0.0310, 0.0170, 0.0153, 0.0204, 0.0121, 0.0247, 0.0187, 0.0197,\n",
        "        0.0130, 0.0127, 0.0240, 0.0189, 0.0104, 0.0125])\n",
        "a.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlcbwh-Ts6Jt",
        "outputId": "a06a39b1-0dd8-43f2-cb64-bf78170d7369"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.env_now.env.remain_gap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "aIRTZjAPgkW_",
        "outputId": "e0800ef3-a75e-4e4b-d411-7f50e41c9982"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-c243c5b8551f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_now\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremain_gap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'GurobiOriginalEnv' object has no attribute 'remain_gap'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.env_now.env.oldobj"
      ],
      "metadata": {
        "id": "gg-stUzCuAdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.env_now.env.newobj\n"
      ],
      "metadata": {
        "id": "kyF1UWObuLoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.env_now.env.ip_obj"
      ],
      "metadata": {
        "id": "aPL2qJZ0nNrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OH notes:\n",
        "\n",
        "baseline is what you use to interprete the reward\n",
        "\n",
        "don't use nn, use \"mean\" of rewards in this episode (I think so)\n",
        "\n",
        "Maybe look in to the instances, and look at how your agent is solving them\n",
        "\n",
        "Cutting off early, but only after solved some LP's\n",
        "- counterfactual exploration\n",
        "  - more than to do it more random\n",
        "- all prob entirely the same\n",
        "- activation function ?\n",
        "\n",
        "not learning? \n",
        "\n",
        "reward shaping?\n",
        "- someone: amplify the reward (shouldn't, since every step wil GIVE a reward), compare it to other POSSIBLE states\n",
        "- pre-trained on the instances and pre-trained the baseline?\n",
        "- get the max reward from the LP solver (isn't this cheating lol)\n",
        "- recalcluate the max-gap-to-go (lol remaining max gap) every step\n",
        "- go in the environment to make if return the shaped new-gap reward\n",
        "- the original reward doesn't help you across LPs?\n",
        "- moving average of *returns* from all *previous* episodes\n",
        "  - return is the discounted sum of the reward\n",
        "- advantage? Q - running averaged RETURN (but different states are mixed in, how do you deal with that)\n",
        "- Think about what's wrong with this base line, and write it down\n",
        "  - something about the states\n",
        "\n",
        "\n",
        "differences between LPs\n",
        "\n",
        "mode: \n",
        "- standardize the constraints, and the b vector by itself\n",
        "  - do you normalize by row or columns?\n",
        "  - He thinks it's more sense to normalize by rows\n",
        "  - Normalize it twice? LOL\n",
        "- or a normalization layer\\\n",
        "\n",
        "\n",
        "When doing softmax you can use a lamda (parameter) to mitigate large score difference\n",
        "- softmax comes with a scalar parameter\n",
        "\n",
        "Professor didn't think normalizing the input is necessary & and it shouldn't make a difference anyways (but come numerical value can be lost after normalization)\n",
        "\n",
        "iteration: \n",
        "\n",
        "plot random policy together, (as a bseline to see if your model is really learning)\n",
        "\n",
        "For baseline, Prof is suggesting Q network can work too.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ha4GPGL8Dduc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sKnhC54KYNNL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}