{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_policy_gradient.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c3bbdea7eb114147a5e441be84d0f0ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f96f4aa52a54181a1abe40c14059aac",
              "IPY_MODEL_d5b3791c0ac54fb390106d7f94b4d23d"
            ],
            "layout": "IPY_MODEL_359e3fd0e6f440e0be84078e0bf2d836"
          }
        },
        "2f96f4aa52a54181a1abe40c14059aac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eaccc9080697496e8a1877fb228f1896",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1212079cfed84271b7ec828d244d1597",
            "value": "3.073 MB of 3.073 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "d5b3791c0ac54fb390106d7f94b4d23d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1197b0edc0249a5bcf2c1f284af8813",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7df16b4afe44f4191845b91a33b369b",
            "value": 1
          }
        },
        "359e3fd0e6f440e0be84078e0bf2d836": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaccc9080697496e8a1877fb228f1896": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1212079cfed84271b7ec828d244d1597": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1197b0edc0249a5bcf2c1f284af8813": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7df16b4afe44f4191845b91a33b369b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ClaireZixiWang/learn2cut/blob/main/Project_policy_gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## See README.md file for further details about the project and the environment.\n",
        "\n",
        "### State-Action Description\n",
        "\n",
        "### State\n",
        "State s is an array with give components\n",
        "\n",
        "* s[0]:  constraint matrix $A$of the current LP ($\\max  -c^Tx \\text{ s.t. }Ax \\le  b$) . Dimension is $m \\times n$. See by printing s[0].shape. Here $n$ is the (fixed) number of variables. For instances of size 60 by 60 used in the above command, $n$ will remain fixed as 60. And $m$ is the current number of constraints. Initially, $m$ is to the number of constraints in the IP instance. (For instances generated with --num-c=60, $m$ is 60 at the first step).  But $m$ will increase by one in every step of the episode as one new constraint (cut) is added on taking an action.\n",
        "* s[1]: rhs $b$ for the current LP ($Ax\\le b$). Dimension same as the number $m$ in matrix A.\n",
        "* s[2]: coefficient vector $c$ from the LP objective ($-c^Tx$). Dimension same as the number of variables, i.e., $n$.\n",
        "* s[3],  s[4]: Gomory cuts available in the current round of Gomory's cutting plane algorithm. Each cut $i$ is of the form $D_i x\\le d_i$.   s[3] gives the matrix $D$ (of dimension $k \\times n$) of cuts and s[4] gives the rhs $d$ (of dimension $k$). The number of cuts $k$ available in each round changes, you can find it out by printing the size of last component of state, i.e., s[4].size or s[-1].size.\n",
        "\n",
        "### Actions\n",
        "There are k=s[4].size actions available in each state $s$, with $i^{th}$ action corresponding to the $i^{th}$ cut with inequality $D_i x\\le d_i$ in $s[3], s[4]$."
      ],
      "metadata": {
        "id": "5TN-sMTvcG_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***QUESTIONS***:\n",
        "1. By \"current\" LP, you mean the LP that the agent was running in the last state? As in, the LP with all the added constraints? \n",
        "  * ==> I think so.\n",
        "1. What do you mean Gomory cuts *available*? As in, after doing Simplex methods, the *variables* that you can choose to cut?\n",
        "  * Yes I think so.\n",
        "2. Isn't the number of variables (n) changing? in the C-G cutting plane method?\n",
        "  * No, as the spec says, **$n$ is the fixed number of variables**.\n",
        "  * If you look that cuttng plane lecture notes, you can see that after each step, the dummy variable is not added in the constraint. They are merely there for the sake of the LP solver (simplex method), but not really relevant for us.\n",
        "    * This is not correct, I think they are still very much relevant, it's just that I think among the 60 variables a lot of them are space holders for dummy variables so that our $n$ is fixed, so that we don't have to worry about using LSTM. Since each time the sequence [a, b] will be of size n+1. And we can just use a fixed-input-size network to do that.\n",
        "    * But still need to verify with the TA about the place holder understanding.\n",
        "3. dimension of s[3] and s[4]? Where is the \"available all\" stored? In which dimension?\n",
        "  * Each row of D is an \"available cut\". Therefore each $D_i x\\le d_i$ is an \"available\" cut in CG method solved from the simplex method.\n",
        "4. pointing towards the slides: why does the number of constraints m increase 1 in each step, if you can choose *multiple* cuts in one step? (OR in the algorithm we just choose one cut each time? or is that a more vanilla version to start, but to expand on multiple cuts a time later?)\n",
        "5. What do you mean by each \"instance\"?"
      ],
      "metadata": {
        "id": "XJE0bz30UL1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYROdPaaZOVa",
        "outputId": "c5f86cb7-f980-4685-c3e6-bbfa770eb7ce"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -i https://pypi.gurobi.com gurobipy"
      ],
      "metadata": {
        "id": "xSXTKB2zurrt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a31f49a-5694-42b8-fb10-145090f6caa2"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.gurobi.com\n",
            "Requirement already satisfied: gurobipy in /usr/local/lib/python3.7/dist-packages (9.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qqq"
      ],
      "metadata": {
        "id": "YULy9ymNvDxN"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/IEOR_RL/Project_learn2cut\n",
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "lTnvB0_iZUrX",
        "outputId": "dfb2463c-bf54-455d-c88d-ee4300d2e64f"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive/IEOR_RL/Project_learn2cut'\n",
            "/content/drive/MyDrive/IEOR_RL/Project_learn2cut\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/IEOR_RL/Project_learn2cut'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import time"
      ],
      "metadata": {
        "id": "Q8PiSPj5us0O"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(4, 40),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(40, 20), \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(20, 10)\n",
        "        )\n",
        "\n",
        "datapoint = torch.FloatTensor([\n",
        "                               [[1,2,3,4],\n",
        "                                [2,3,4,5]],\n",
        "                               [[3,4,5,6],\n",
        "                                [4,5,6,7]]\n",
        "                              ])\n"
      ],
      "metadata": {
        "id": "ojB8UvBAPAWk"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(datapoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r_nraniPaKl",
        "outputId": "a15b00b9-a2b9-4705-8b96-d139af6ea462"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.3423, -0.2811, -0.0353, -0.1702,  0.4645, -0.1405, -0.4045,\n",
              "           0.5938, -0.5780,  0.2394],\n",
              "         [-0.3725, -0.4098,  0.1293, -0.2190,  0.6075, -0.1502, -0.5207,\n",
              "           0.7261, -0.8427,  0.4220]],\n",
              "\n",
              "        [[-0.4002, -0.5510,  0.2580, -0.2877,  0.7529, -0.1680, -0.6420,\n",
              "           0.8261, -1.0986,  0.6124],\n",
              "         [-0.4273, -0.6856,  0.3553, -0.3688,  0.8959, -0.1910, -0.7691,\n",
              "           0.9207, -1.3426,  0.7895]]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Code for Policy Model\n",
        "\n",
        "class Policy(object):\n",
        "\n",
        "    # inputsize = n+1 = 61\n",
        "    def __init__(self, lr, input_size, attention_size=10, temperature=1) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_size, 40),\n",
        "            # torch.nn.Sigmoid(),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(40, 30), \n",
        "            # torch.nn.Sigmoid(),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(30, 20), \n",
        "            # torch.nn.Sigmoid(),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(20, attention_size)\n",
        "        )\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "        # RECORD HYPER-PARAMS\n",
        "        self.input_size = input_size\n",
        "        self.attention_size = attention_size\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def compute_logits_big_batch(self, obs_matrix, act_matrix, batchsize):\n",
        "        '''\n",
        "        Function that takes in a batch of observations and computes the action logits for each of observations in this batch\n",
        "        Args: obs_matrix: np.array(m * batchsize, n+1) # TODO maybe change this to tensor\n",
        "              act_matrix: np.array(k * batchsize, n+1)\n",
        "        Return: batch_logit: tensor(batchsize, k)\n",
        "        '''\n",
        "\n",
        "        # Get the batch result\n",
        "\n",
        "        # transform to tensor\n",
        "        obs_attention = self.model(torch.FloatTensor(obs_matrix)) # tensor(m * batchsize, u)\n",
        "        act_attention = self.model(torch.FloatTensor(act_matrix)) # tensor(k * batchsize, u)\n",
        "\n",
        "        assert obs_attention.shape == (obs_matrix.shape[0], self.attention_size)\n",
        "        assert act_attention.shape == (act_matrix.shape[0], self.attention_size)\n",
        "\n",
        "        # split a batch of output of size tensor(m * batchsize, u) into (batchsize, m, u)\n",
        "        batch_obs_output = torch.reshape(obs_attention, (batchsize, obs_attention.shape[0]/batchsize, obs_attention.shape[1]))\n",
        "        \n",
        "        # split a batch of output of size tensor(k * batchsize, u) into (batchsize, k, u)\n",
        "        batch_act_output = torch.reshape(act_attention, (batchsize, act_attention.shape[0]/batchsize, act_attention.shape[1]))\n",
        "\n",
        "        # To do batch matrix multiplication, transpose (batchsize, k, u) into (batchsize, u, k)\n",
        "        # (batchsize, m, u) @ (batchsize, u, k) =  (batchsize, m, k)\n",
        "        # (batchsize, m, k) == mean across all observation ==> (batchsize, k)\n",
        "        batch_logit = torch.bmm(batch_obs_output, batch_act_output.transpose(0, 2, 1)).mean(dim=1)\n",
        "\n",
        "        assert batch_logit.shape == (batchsize, act_matrix.shape[0]/batchsize)\n",
        "\n",
        "        return batch_logit\n",
        "\n",
        "    def compute_batch_selected_prob(self, batch_probs):\n",
        "        '''\n",
        "        Function that takes in a batch of probabilities and return the max probability for each \"datapoint\" in the batch\n",
        "        Args:    batch_logit: tensor(batchsize, k)\n",
        "        Return:  batch_selected_prob: tensor(batchsize,)\n",
        "        '''\n",
        "        # TODO\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def compute_one_step_logits(self, obs, act):\n",
        "        '''\n",
        "        Function that takes in ONE observation and action space, computes the action logits\n",
        "        Args:   obs_matrix: np.array(m, n+1)\n",
        "                act_matrix: np.array(k, n+1)\n",
        "        Return: logit: tensor(k)\n",
        "        '''\n",
        "\n",
        "        obs_attention = self.model(torch.FloatTensor(obs)) #-> (m, 10)\n",
        "        act_attention = self.model(torch.FloatTensor(act)) #-> (k, 10)\n",
        "        # print(\"DEBUGGING: obs_attention looks like:\", obs_attention)\n",
        "        # print(\"DEBUGGING: act_attention looks like:\", act_attention)\n",
        "\n",
        "        # attention matrix multiplication & mean to get the score\n",
        "        logits = torch.mm(obs_attention, act_attention.transpose(1, 0)).mean(dim=0)\n",
        "        # print(\"DEBUGGING: logits looks like:\", logits)\n",
        "\n",
        "        # print(\"DEBUGGING: act.shape =\", act_attention.shape)\n",
        "        # print(\"DEBUGGING: logits.shape =\", logits.shape)\n",
        "        assert logits.shape[0] == act_attention.shape[0]\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def predict_prob(self, obs, act):\n",
        "        # Function that uses softmax to transform logits to probabilities\n",
        "        # TODO: What shape should obs and act take?\n",
        "        # Args:   obs_matrix: np.array(m, n+1)\n",
        "        #         act_matrix: np.array(k, n+1)\n",
        "        # Return: probs: tensor(k)\n",
        "\n",
        "        logits = self.compute_one_step_logits(obs, act)\n",
        "\n",
        "        # TODO: make temperature a argument in the function, so that it's depended on the steps\n",
        "        probs = torch.nn.functional.softmax(logits/self.temperature, dim=0)\n",
        "        return probs\n",
        "\n",
        "    def selected_prob(self, probs, action):\n",
        "\n",
        "\n",
        "        # TODO: do I need this function?\n",
        "        one_hot = torch.zeros()\n",
        "        return probs.max()\n",
        "\n",
        "    def choose_action(self, obs, act):\n",
        "\n",
        "        # TODO: is this returning a number?\n",
        "        return torch.argmax(self.predict_prob(obs, act)).item()\n",
        "\n",
        "\n",
        "    def train(self, obs_matrix, act_matrix, actions, Qs):\n",
        "        \"\"\"\n",
        "        Args: obs_matrix: np.array(batchsize * m, n+1) => changed to [np.array(m, n+1)] * batchsize, note that m are varied!\n",
        "              act_matrix: np.array(batchsize * k, n+1)  \n",
        "              actions: => [[action number]] * batchsize      \n",
        "              Qs: np.array(batchsize, )\n",
        "        \"\"\"\n",
        "        # Convert numpy array to tensor\n",
        "\n",
        "        # use compute_batch_prob to compute the batch logits for every datapoint in batch.\n",
        "        # use compute_batch_selected_prob the compute the max probabilty for every datapoint in batch.\n",
        "        start = time.time()\n",
        "        print(\"DEBUGGING: I'm inside the training now!\")\n",
        "        \n",
        "        Qs = torch.FloatTensor(Qs)\n",
        "\n",
        "\n",
        "        # Try using a for loop first, see whether it really cost too much time & whether it works at all\n",
        "        prob_selected = torch.zeros(len(actions))\n",
        "        for i in range(len(obs_matrix)):\n",
        "            #TODO: adjust the temperatures based on trajectory steps\n",
        "            probs = self.predict_prob(obs_matrix[i], act_matrix[i])\n",
        "            one_hot = torch.zeros_like(probs)\n",
        "            one_hot[actions[i][0]] = 1\n",
        "            prob_selected[i] = torch.sum(probs * one_hot)\n",
        "\n",
        "\n",
        "        # For robustness add in noise for prob_selected # TODO: why do this?\n",
        "\n",
        "        # define loss function as in lab 4\n",
        "        # TODO define loss function as described in the text above\n",
        "        loss = - (torch.sum(Qs * torch.log(prob_selected)) / (len(obs_matrix) + 1))\n",
        "        print(\"DEBUGGING: the loss =\", loss)\n",
        "\n",
        "        print(\"DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\")\n",
        "        print(\"   First layer:\")\n",
        "        print(policy.model[0].weight.grad)\n",
        "        print(\"   Last layer:\")\n",
        "        print(policy.model[6].weight.grad)\n",
        "\n",
        "        # backward pass\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "\n",
        "        print(\"DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\")\n",
        "        print(\"   First layer:\")\n",
        "        print(policy.model[0].weight.grad)\n",
        "        print(\"   Last layer:\")\n",
        "        print(policy.model[6].weight.grad)\n",
        "\n",
        "        # step\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "        print(\"DEBUGGING: training for one iteration takes %f min:\" % ((time.time() - start)/60))\n",
        "\n",
        "\n",
        "        # return detached loss (why?)\n",
        "        return loss.detach().cpu().data.numpy()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HViRnY1ssGfc"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchsummary import summary\n",
        "# summary(policy.model, (61,))\n",
        "# print(policy.model)"
      ],
      "metadata": {
        "id": "OGm4an0pWPHr"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(policy.model[6].weight.grad)"
      ],
      "metadata": {
        "id": "dfkXKSJWW6OR"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP model for policy model:\n",
        "#   model.forward\n",
        "#   model.train --> What is in this function? what are the function arguments?\n",
        "# Baseline function b(s) ==> Okay maybe we stil need the V model as a proper baseline\n",
        "\n",
        "\n",
        "# Q value model ==> Discard this right now, just do vanilla policy gradient\n",
        "#   Can I just use the one in Lab4? What does it mean? what does the states and actions mean? --> Print out the s, r to check\n",
        "#   What is a Q-value in our set-up?\n",
        "#   How do I used this? \n",
        "#   (What's the baseline function??)"
      ],
      "metadata": {
        "id": "ACiQz-gESgOO"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "2xI0riE6md5V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "1692c1ab-41fd-460f-e2af-f082fb012e91"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/My Drive/IEOR_RL/Project_learn2cut/wandb/run-20220509_023104-3rbp8ihl</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/ieor4575-spring2022/finalproject/runs/3rbp8ihl\" target=\"_blank\">desert-flower-2814</a></strong> to <a href=\"https://wandb.ai/ieor4575-spring2022/finalproject\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# import gymenv_v2\n",
        "from gymenv_v2 import make_multiple_env\n",
        "\n",
        "\n",
        "import wandb\n",
        "wandb.login()\n",
        "run=wandb.init(project=\"finalproject\", entity=\"ieor4575-spring2022\", tags=[\"training-easy\"])\n",
        "#run=wandb.init(project=\"finalproject\", entity=\"ieor-4575\", tags=[\"training-hard\"])\n",
        "#run=wandb.init(project=\"finalproject\", entity=\"ieor-4575\", tags=[\"test\"])\n",
        "\n",
        "### TRAINING\n",
        "\n",
        "# Setup: You may generate your own instances on which you train the cutting agent.\n",
        "custom_config = {\n",
        "    \"load_dir\"        : 'instances/randomip_n60_m60',   # this is the location of the randomly generated instances (you may specify a different directory)\n",
        "    \"idx_list\"        : list(range(20)),                # take the first 20 instances from the directory\n",
        "    \"timelimit\"       : 50,                             # the maximum horizon length is 50\n",
        "    \"reward_type\"     : 'obj'                           # DO NOT CHANGE reward_type\n",
        "}\n",
        "\n",
        "# Easy Setup: Use the following environment settings. We will evaluate your agent with the same easy config below:\n",
        "easy_config = {\n",
        "    \"load_dir\"        : 'instances/train_10_n60_m60',\n",
        "    \"idx_list\"        : list(range(5)),\n",
        "    \"timelimit\"       : 50,\n",
        "    \"reward_type\"     : 'obj'\n",
        "}\n",
        "\n",
        "# Hard Setup: Use the following environment settings. We will evaluate your agent with the same hard config below:\n",
        "hard_config = {\n",
        "    \"load_dir\"        : 'instances/train_100_n60_m60',\n",
        "    \"idx_list\"        : list(range(99)),\n",
        "    \"timelimit\"       : 50,\n",
        "    \"reward_type\"     : 'obj'\n",
        "}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._C import dtype\n",
        "def discounted_rewards(r, gamma):\n",
        "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "    discounted_r = np.zeros_like(r, dtype=float)\n",
        "    running_sum = 0\n",
        "    for i in reversed(range(0,len(r))):\n",
        "        discounted_r[i] = running_sum * gamma + r[i]\n",
        "        running_sum = discounted_r[i]\n",
        "    return list(discounted_r)"
      ],
      "metadata": {
        "id": "COyIO0TEDRjt"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = make_multiple_env(**easy_config) \n",
        "s = env.reset()   # samples a RANDOM INSTANCE every time env.reset() is called\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFhvgi_q53V4",
        "outputId": "7e4bb5c3-0443-43ae-928b-74b117d12d58"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading training instances, dir instances/train_10_n60_m60 idx 0\n",
            "loading training instances, dir instances/train_10_n60_m60 idx 1\n",
            "loading training instances, dir instances/train_10_n60_m60 idx 2\n",
            "loading training instances, dir instances/train_10_n60_m60 idx 3\n",
            "loading training instances, dir instances/train_10_n60_m60 idx 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "A, b, c0, cuts_a, cuts_b = s\n",
        "concat = np.hstack((s[0], np.expand_dims(s[1], axis=1)))\n",
        "concat\n",
        "# np.linalg.norm(concat, axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJXouQZx9DQ7",
        "outputId": "c479f169-d7ff-4638-ef10-b83cf33017c4"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   1,   3, ...,   3,   3, 577],\n",
              "       [  1,   4,   4, ...,   0,   2, 588],\n",
              "       [  0,   1,   1, ...,   0,   1, 541],\n",
              "       ...,\n",
              "       [  0,   1,   1, ...,   1,   4, 578],\n",
              "       [  3,   1,   1, ...,   4,   0, 599],\n",
              "       [  2,   3,   4, ...,   2,   2, 562]])"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.expand_dims(s[1], axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSUhT5go6czx",
        "outputId": "5dcb0ec5-9589-4bd5-87a3-4ee519632f53"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[577],\n",
              "       [588],\n",
              "       [541],\n",
              "       [566],\n",
              "       [563],\n",
              "       [586],\n",
              "       [589],\n",
              "       [555],\n",
              "       [585],\n",
              "       [589],\n",
              "       [543],\n",
              "       [549],\n",
              "       [574],\n",
              "       [584],\n",
              "       [554],\n",
              "       [543],\n",
              "       [555],\n",
              "       [553],\n",
              "       [552],\n",
              "       [559],\n",
              "       [547],\n",
              "       [588],\n",
              "       [582],\n",
              "       [576],\n",
              "       [555],\n",
              "       [590],\n",
              "       [548],\n",
              "       [587],\n",
              "       [563],\n",
              "       [551],\n",
              "       [547],\n",
              "       [576],\n",
              "       [576],\n",
              "       [581],\n",
              "       [563],\n",
              "       [589],\n",
              "       [565],\n",
              "       [553],\n",
              "       [562],\n",
              "       [587],\n",
              "       [554],\n",
              "       [564],\n",
              "       [573],\n",
              "       [546],\n",
              "       [561],\n",
              "       [551],\n",
              "       [570],\n",
              "       [547],\n",
              "       [540],\n",
              "       [594],\n",
              "       [578],\n",
              "       [570],\n",
              "       [562],\n",
              "       [544],\n",
              "       [557],\n",
              "       [598],\n",
              "       [579],\n",
              "       [578],\n",
              "       [599],\n",
              "       [562]])"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.hstack((s[0], np.expand_dims(s[1], axis=1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjIckPCE7n2z",
        "outputId": "bb3de51b-1823-4acb-dcd1-d4ff880b68f8"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   1,   3, ...,   3,   3, 577],\n",
              "       [  1,   4,   4, ...,   0,   2, 588],\n",
              "       [  0,   1,   1, ...,   0,   1, 541],\n",
              "       ...,\n",
              "       [  0,   1,   1, ...,   1,   4, 578],\n",
              "       [  3,   1,   1, ...,   4,   0, 599],\n",
              "       [  2,   3,   4, ...,   2,   2, 562]])"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([\n",
        "              [1,2,3,4],\n",
        "              [3,3,3,3],\n",
        "              [2,2,2,2],\n",
        "              [1,1,1,1]\n",
        "])\n",
        "print(a.mean(axis=0, keepdims=True))\n",
        "a = a - a.mean(axis=0, keepdims=True)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMUMT8x_A8I0",
        "outputId": "a8143bb2-ec5d-448a-ea86-5058438f5f21"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.75 2.   2.25 2.5 ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.75,  0.  ,  0.75,  1.5 ],\n",
              "       [ 1.25,  1.  ,  0.75,  0.5 ],\n",
              "       [ 0.25,  0.  , -0.25, -0.5 ],\n",
              "       [-0.75, -1.  , -1.25, -1.5 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "empty = [np.array([1,2]), np.array([3,4])]\n",
        "a = []\n",
        "b = a + empty + empty\n",
        "\n",
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBFhGXfSJz3l",
        "outputId": "adc422f9-1097-42d9-b9fc-b0272186a80e"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([1, 2]), array([3, 4]), array([1, 2]), array([3, 4])]"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "empty = [np.array([10,20])]\n",
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJY9NttjZiM7",
        "outputId": "5b53b2e8-3db0-469e-fcac-9febaf636786"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([1, 2]), array([3, 4]), array([1, 2]), array([3, 4])]"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%wandb\n",
        "# create env\n",
        "env = make_multiple_env(**easy_config) \n",
        "# Parameter initialization\n",
        "numtrajs = 4  # num of trajecories from the current policy to collect in each iteration\n",
        "lr_pg = 1e-2  # learning rate for PG\n",
        "attention_size = 10\n",
        "iterations = 50\n",
        "discount_gamma = .99\n",
        "\n",
        "# Network initialize\n",
        "policy = Policy(lr_pg, 61, attention_size, 0.2)\n",
        "\n",
        "#To record training reward for logging and plotting purposes\n",
        "rrecord = []\n",
        "\n",
        "VAL_ALL = []  # Monte carlo value predictions of ALL trajectories (to compute baseline, and policy gradient) => 2 d list (numtrajs, #steps per traj)\n",
        "\n",
        "# For every training iteration, we roll out 5 random instances using the policy network\n",
        "# collect observation and action matrices from these 5 roll-outs, consider this a BATCH\n",
        "# train the network using this BATCH\n",
        "for ite in range(iterations):\n",
        "\n",
        "    print(\"==========================================================================================================\")\n",
        "    print(\"Outer iteration no\", ite)\n",
        "\n",
        "    # To record traectories generated from current policy\n",
        "    OBS_MAT = []  # observations: [obs_matrices_batch_traj1, obs_matrices_batch_traj2, ....]\n",
        "    ACT_MAT = []  # actions\n",
        "    ADS = []  # advantages (to compute policy gradient)\n",
        "    VAL = []  # Monte carlo value predictions (to compute baseline, and policy gradient) => 2 d list (numtrajs, #steps per traj)\n",
        "    traj_returns = []  # a list of 5 numbers, each number represents the RETURN of that roll-out\n",
        "    actions = []\n",
        "    \n",
        "    # collect some trajectories\n",
        "    for num in range(numtrajs):\n",
        "        print(\"-------------------------------------------------------------------------------------------\")\n",
        "        print(\"Running trajectories:\", num)\n",
        "        # Initialize a list of obs_matrices and act_matrices, to store all the obs_matrix and act_matrix in the trajectory\n",
        "        obs_matrices = []  # states: [obs_matrix_state1, obs_matrix_state2, ...]\n",
        "        act_matrices = []  # actions matrices\n",
        "        \n",
        "        # this is used to collect all the immedaite rewards in this trajectory\n",
        "        rews = []  # instant rewards\n",
        "        rel_rews = []\n",
        "\n",
        "        # gym loop\n",
        "        s = env.reset()   # samples a RANDOM INSTANCE every time env.reset() is called\n",
        "        done = False\n",
        "\n",
        "        # TODO: wandb logging -> what reward average should we log??\n",
        "        t = 0 # TODO: how is this used?\n",
        "        repisode = 0  # TODO: how is this used?\n",
        "\n",
        "        inner_step = 1\n",
        "        \n",
        "\n",
        "        # roll out ONE policy\n",
        "        while not done:\n",
        "\n",
        "            # TODO compute the running average RETURN as basline\n",
        "            A, b, c0, cuts_a, cuts_b = s\n",
        "\n",
        "            # choose the action according to the model output probabilities\n",
        "\n",
        "            # Concat [A,b] and [cuts_a, cuts_b]\n",
        "            assert A.shape[0] == b.shape[0]\n",
        "            assert cuts_a.shape[0] == cuts_b.shape[0]\n",
        "\n",
        "            obs_matrix = np.hstack((A, np.expand_dims(b, axis=1)))\n",
        "            act_matrix = np.hstack((cuts_a, np.expand_dims(cuts_b, axis=1)))\n",
        "\n",
        "            assert obs_matrix.shape == (A.shape[0], A.shape[1]+1)\n",
        "            assert act_matrix.shape == (cuts_a.shape[0], cuts_a.shape[1]+1)\n",
        "\n",
        "            # Normalize on a row (MIGHT NOT NEED THIS)\n",
        "            \n",
        "            # The reason we want to normalize a row: we want the numeric space of the model input to be just between 0, 1\n",
        "            # Right now I'm normalizing such that the largest number has value 1 --> each row divided by the largest num in that row\n",
        "            #   => This would result in b vector always be 1 (does it make sense?)\n",
        "            # another option is to normalize such that the SUM of the row is 1\n",
        "            #   => I think this makes more sense, consider this differentiates the max among datapoints\n",
        "            #   => According to prof in OH this might cause some information loss\n",
        "            \n",
        "            obs_matrix = obs_matrix / obs_matrix.max(axis=1, keepdims=True) * 100\n",
        "            act_matrix = act_matrix / act_matrix.max(axis=1, keepdims=True) * 100\n",
        "            \n",
        "            # print(\"DEBUGGING: obs_matrix.shape = \", obs_matrix.shape)\n",
        "            # print(\"DEBUGGING: act_matrix.shape = \", act_matrix.shape)\n",
        "\n",
        "            action_prob = policy.predict_prob(obs_matrix, act_matrix)\n",
        "            action = random.choices(range(0, len(cuts_b)), action_prob) # this returns a list\n",
        "            actions.append(action)\n",
        "\n",
        "            if  inner_step %10 == 0:\n",
        "                # TODO: print the logits as well as well.\n",
        "                print(\"DEBUGGING: the action_prob is:\", action_prob)\n",
        "                print(\"DEBUGGING: the actual action to take is:\", action)\n",
        "                logits_dbg = policy.compute_one_step_logits(obs_matrix, act_matrix)\n",
        "                print(\"DEBUGGING: logits looks like:\", logits_dbg)\n",
        "\n",
        "\n",
        "\n",
        "            # take the action in the environment\n",
        "            # TODO: why does the environment.step function takes in a list?\n",
        "            # TODO: remember to go in the environment to change the returned r to the NORMALIZED r!!\n",
        "            s, r, done, _ = env.step(action)\n",
        "            \n",
        "            # Record the observed immediate reward & observed matrices along the trajectory\n",
        "            abs_reward, rel_reward = r\n",
        "            # print(\"DEBUGGING: rel_reward looks like:\", rel_reward)\n",
        "            rews.append(abs_reward) # rews = list(len of trajectory)\n",
        "            rel_rews.append(rel_reward * inner_step)\n",
        "            obs_matrices.append(obs_matrix)\n",
        "            act_matrices.append(act_matrix)\n",
        "\n",
        "            inner_step += 1\n",
        "\n",
        "            # TODO: do we need to also record the one step of observation and action where the environment terminates?\n",
        "            #   ==> RN I'm thinking maybe don't need to\n",
        "        print(\"-------------------------------------------------------------------------------------------\")\n",
        "        #Below is for logging training performance\n",
        "        print(\"DEBUGGING: the total abs reward of the trajectory =\", np.sum(rews), \"and immediate abs rewards look like:\", rews)\n",
        "        print(\"DEBUGGING: the total relative reward of the trajectory =\", np.sum(rel_rews), \"and immediate relative rewards look like:\", rel_rews)\n",
        "\n",
        "        rrecord.append(np.sum(rews))\n",
        "\n",
        "        # After the policy roll out for this trajectory,\n",
        "        # compute the monte-carlo RETURN of this trajectory (i.e. discounted sum of rewards), add to big list\n",
        "        # TODO: one of the next steps could be: to make the basline state-dependent\n",
        "        v_hat = discounted_rewards(rel_rews, discount_gamma) # This is a list\n",
        "        traj_returns.append(v_hat[0])\n",
        "        # OBS_MAT.append(np.concatenate(obs_matrices, axis=1))\n",
        "        # ACT_MAT.append(np.concatenate(act_matrices, aixs=1))\n",
        "        VAL.append(v_hat) # VAL -> 2d list\n",
        "        VAL_ALL.append(v_hat) # VAL_ALL -> 2d list\n",
        "        OBS_MAT += obs_matrices\n",
        "        ACT_MAT += act_matrices\n",
        "\n",
        "        # TODO: do I need to specify batchsize somewhere?\n",
        "\n",
        "    print(\"+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\")\n",
        "    \n",
        "    # After collecting 5 (or however many) trajectories,\n",
        "    print(\"DEBUGGING: OBS_MAT has %d number of matrices\" % len(OBS_MAT))\n",
        "    print(\"DEBUGGING: ACT_MAT has %d number of matrices\" % len(ACT_MAT))\n",
        "    print(\"DEBUGGING: VAL looks like:\", VAL)\n",
        "    # print(\"DEBUGGING: OBS_MAT looks like:\", OBS_MAT)\n",
        "    # print(\"DEBUGGING: ACT_MAT looks like:\", ACT_MAT)\n",
        "    print(\"DEBUGGING: traj_returns =\", traj_returns)\n",
        "    print(\"DEBUGGING: actions =\", actions)\n",
        "    print(\"DEBUGGING: actions length =\", len(actions))\n",
        "\n",
        "    ## For debugging purposes, let's look at what does the model output\n",
        "    print(\"DEBUGGING: what does the model output in this round of roll-out?\")\n",
        "    obs_attention_dbg = policy.model(torch.FloatTensor(obs_matrix))\n",
        "    act_attention_dbg = policy.model(torch.FloatTensor(act_matrix))\n",
        "    logits_dbg = policy.compute_one_step_logits(obs_matrix, act_matrix)\n",
        "    print(\"DEBUGGING: obs_attention looks like:\", obs_attention_dbg)\n",
        "    print(\"DEBUGGING: act_attention looks like:\", act_attention_dbg)\n",
        "    print(\"DEBUGGING: logits looks like:\", logits_dbg)\n",
        "\n",
        "\n",
        "    assert len(traj_returns) == numtrajs\n",
        "    VAL = np.array(VAL)\n",
        "    VAL_ALL_np = np.array(VAL_ALL)\n",
        "    # 1. calculate the baseline: average return of the trajectories\n",
        "    # TODO: potentially can make this into *running average*, take into account of all the previous trajectories as well\n",
        "    # TODO: potentially make the baseline the average *VALUE* of every *state*, \n",
        "    #       but I'm not sure whether that \"state\" is useful in this concept, since the first *step*\n",
        "    #       doesn't really mean the same thing among instances\n",
        "    #       I think prof says it makes sense in the OH, also it would make more sense if you engineered the reward\n",
        "    baseline = np.mean(traj_returns)\n",
        "    baseline_2 = VAL.mean(axis=0, keepdims=True) # This is NOT a good baseline, this is only the mean of that episode, NOT running mean\n",
        "    baseline_3 = VAL_ALL_np.mean(axis=0, keepdims=True)\n",
        "    # assert baseline_2.shape == VAL.shape\n",
        "\n",
        "    # 2. Update the policy\n",
        "    ADS = (VAL - baseline_3).flatten()\n",
        "    print(\"DEBUGGING: baseline2 looks like:\", baseline_2)\n",
        "    print(\"DEBUGGING: baseline2 looks like:\", baseline)\n",
        "    print(\"DEBUGGING: ADS looks like:\", ADS)\n",
        "\n",
        "\n",
        "    # Train the agent using the batch\n",
        "    # obs_batch = np.concatenate(OBS_MAT)\n",
        "    # act_batch = np.concatenate(ACT_MAT)\n",
        "\n",
        "    assert ADS.shape[0] == len(actions)\n",
        "\n",
        "    # scaling up the rewards to artificially make bigger loss, improve learning\n",
        "\n",
        "    loss = policy.train(OBS_MAT, ACT_MAT, actions, ADS)\n",
        "\n",
        "    fixedWindow=50\n",
        "    movingAverage=0\n",
        "    if len(rrecord) >= fixedWindow:\n",
        "        movingAverage=np.mean(rrecord[len(rrecord)-fixedWindow:len(rrecord)-1])\n",
        "\n",
        "    # TODO: wandb logging\n",
        "    wandb.log({ \"training reward\" : rrecord[-1], \"training reward moving average\" : movingAverage, \"training loss\": loss})\n",
        "    #make sure to use the correct tag in wandb.init in the initialization on top\n"
      ],
      "metadata": {
        "id": "aeQHQnp1-8fR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ac507a57-cfb3-4159-fd2e-da16168b4f5a"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<wandb.jupyter.IFrame at 0x7f788b9f3090>"
            ],
            "text/html": [
              "<iframe src=\"https://wandb.ai/ieor4575-spring2022/finalproject/runs/3rbp8ihl?jupyter=true\" style=\"border:none;width:100%;height:420px;\"></iframe>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "        0.0148, 0.0151, 0.0148, 0.0152, 0.0148, 0.0153, 0.0150, 0.0148, 0.0149,\n",
            "        0.0148, 0.0150, 0.0152, 0.0150, 0.0150, 0.0150, 0.0150, 0.0151, 0.0148,\n",
            "        0.0149, 0.0149, 0.0149, 0.0150, 0.0152, 0.0149, 0.0149, 0.0149, 0.0150,\n",
            "        0.0149, 0.0151, 0.0145, 0.0148, 0.0148, 0.0150, 0.0148, 0.0149, 0.0154,\n",
            "        0.0147, 0.0149, 0.0148, 0.0149, 0.0147, 0.0150, 0.0150, 0.0144, 0.0152,\n",
            "        0.0147, 0.0148, 0.0148, 0.0144], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [58]\n",
            "DEBUGGING: logits looks like: tensor([6.3929, 6.3776, 6.3772, 6.3756, 6.3763, 6.3758, 6.3818, 6.3774, 6.3760,\n",
            "        6.3728, 6.3753, 6.3770, 6.3782, 6.3699, 6.3768, 6.3770, 6.3665, 6.3766,\n",
            "        6.3748, 6.3781, 6.3741, 6.3796, 6.3749, 6.3816, 6.3774, 6.3749, 6.3754,\n",
            "        6.3746, 6.3779, 6.3793, 6.3775, 6.3773, 6.3779, 6.3777, 6.3786, 6.3744,\n",
            "        6.3763, 6.3755, 6.3760, 6.3779, 6.3795, 6.3760, 6.3760, 6.3766, 6.3775,\n",
            "        6.3758, 6.3784, 6.3708, 6.3740, 6.3742, 6.3777, 6.3741, 6.3758, 6.3822,\n",
            "        6.3732, 6.3760, 6.3746, 6.3763, 6.3733, 6.3772, 6.3770, 6.3688, 6.3801,\n",
            "        6.3739, 6.3746, 6.3753, 6.3688], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0132, 0.0125, 0.0125, 0.0125, 0.0126, 0.0125, 0.0124, 0.0125, 0.0126,\n",
            "        0.0128, 0.0125, 0.0124, 0.0125, 0.0123, 0.0124, 0.0126, 0.0125, 0.0126,\n",
            "        0.0126, 0.0126, 0.0125, 0.0125, 0.0126, 0.0125, 0.0124, 0.0126, 0.0125,\n",
            "        0.0124, 0.0126, 0.0125, 0.0126, 0.0125, 0.0125, 0.0126, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0123, 0.0125, 0.0124, 0.0125, 0.0126, 0.0124, 0.0125,\n",
            "        0.0125, 0.0125, 0.0124, 0.0125, 0.0124, 0.0125, 0.0125, 0.0125, 0.0124,\n",
            "        0.0125, 0.0125, 0.0125, 0.0126, 0.0125, 0.0124, 0.0124, 0.0125, 0.0124,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0124, 0.0125, 0.0125, 0.0123,\n",
            "        0.0125, 0.0124, 0.0125, 0.0125, 0.0124, 0.0126, 0.0123, 0.0124],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [54]\n",
            "DEBUGGING: logits looks like: tensor([6.3892, 6.3777, 6.3785, 6.3773, 6.3794, 6.3783, 6.3771, 6.3786, 6.3795,\n",
            "        6.3835, 6.3775, 6.3759, 6.3777, 6.3748, 6.3763, 6.3794, 6.3784, 6.3796,\n",
            "        6.3789, 6.3795, 6.3784, 6.3773, 6.3798, 6.3778, 6.3767, 6.3796, 6.3775,\n",
            "        6.3768, 6.3794, 6.3787, 6.3801, 6.3777, 6.3785, 6.3795, 6.3782, 6.3788,\n",
            "        6.3782, 6.3780, 6.3754, 6.3784, 6.3770, 6.3785, 6.3790, 6.3770, 6.3773,\n",
            "        6.3772, 6.3773, 6.3759, 6.3775, 6.3766, 6.3776, 6.3779, 6.3785, 6.3768,\n",
            "        6.3782, 6.3780, 6.3778, 6.3800, 6.3782, 6.3764, 6.3769, 6.3777, 6.3761,\n",
            "        6.3777, 6.3788, 6.3773, 6.3784, 6.3787, 6.3765, 6.3772, 6.3778, 6.3755,\n",
            "        6.3774, 6.3771, 6.3782, 6.3774, 6.3771, 6.3793, 6.3743, 6.3770],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0122, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0114, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0114, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [43]\n",
            "DEBUGGING: logits looks like: tensor([6.3892, 6.3779, 6.3773, 6.3773, 6.3772, 6.3781, 6.3776, 6.3779, 6.3775,\n",
            "        6.3776, 6.3773, 6.3770, 6.3769, 6.3774, 6.3774, 6.3778, 6.3773, 6.3773,\n",
            "        6.3778, 6.3774, 6.3767, 6.3783, 6.3776, 6.3774, 6.3774, 6.3776, 6.3777,\n",
            "        6.3777, 6.3772, 6.3777, 6.3775, 6.3775, 6.3777, 6.3772, 6.3775, 6.3774,\n",
            "        6.3773, 6.3771, 6.3781, 6.3775, 6.3775, 6.3770, 6.3778, 6.3777, 6.3768,\n",
            "        6.3775, 6.3775, 6.3775, 6.3775, 6.3774, 6.3774, 6.3778, 6.3777, 6.3776,\n",
            "        6.3777, 6.3773, 6.3779, 6.3778, 6.3771, 6.3772, 6.3769, 6.3775, 6.3771,\n",
            "        6.3779, 6.3777, 6.3771, 6.3772, 6.3758, 6.3773, 6.3774, 6.3776, 6.3775,\n",
            "        6.3772, 6.3774, 6.3777, 6.3780, 6.3777, 6.3768, 6.3775, 6.3773, 6.3778,\n",
            "        6.3775, 6.3777, 6.3770, 6.3769, 6.3783, 6.3779],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0107, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0100, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [48]\n",
            "DEBUGGING: logits looks like: tensor([6.3892, 6.3770, 6.3773, 6.3771, 6.3770, 6.3772, 6.3771, 6.3769, 6.3776,\n",
            "        6.3768, 6.3768, 6.3767, 6.3773, 6.3760, 6.3769, 6.3771, 6.3772, 6.3774,\n",
            "        6.3771, 6.3774, 6.3773, 6.3772, 6.3771, 6.3775, 6.3772, 6.3773, 6.3774,\n",
            "        6.3772, 6.3769, 6.3773, 6.3772, 6.3769, 6.3773, 6.3772, 6.3773, 6.3776,\n",
            "        6.3771, 6.3769, 6.3771, 6.3770, 6.3773, 6.3770, 6.3765, 6.3769, 6.3774,\n",
            "        6.3773, 6.3767, 6.3772, 6.3772, 6.3769, 6.3770, 6.3775, 6.3771, 6.3771,\n",
            "        6.3771, 6.3771, 6.3776, 6.3774, 6.3772, 6.3772, 6.3773, 6.3773, 6.3771,\n",
            "        6.3769, 6.3775, 6.3773, 6.3770, 6.3767, 6.3775, 6.3774, 6.3774, 6.3772,\n",
            "        6.3774, 6.3773, 6.3770, 6.3772, 6.3773, 6.3769, 6.3770, 6.3768, 6.3769,\n",
            "        6.3773, 6.3767, 6.3770, 6.3773, 6.3772, 6.3774, 6.3771, 6.3769, 6.3769,\n",
            "        6.3768, 6.3768, 6.3767, 6.3768, 6.3771, 6.3771, 6.3772, 6.3768, 6.3767],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0100, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [88]\n",
            "DEBUGGING: logits looks like: tensor([6.3892, 6.3775, 6.3776, 6.3775, 6.3775, 6.3776, 6.3776, 6.3776, 6.3775,\n",
            "        6.3775, 6.3776, 6.3775, 6.3775, 6.3775, 6.3776, 6.3775, 6.3776, 6.3776,\n",
            "        6.3775, 6.3775, 6.3776, 6.3776, 6.3775, 6.3775, 6.3775, 6.3775, 6.3776,\n",
            "        6.3775, 6.3775, 6.3776, 6.3776, 6.3776, 6.3775, 6.3775, 6.3775, 6.3775,\n",
            "        6.3775, 6.3775, 6.3775, 6.3775, 6.3775, 6.3775, 6.3775, 6.3775, 6.3775,\n",
            "        6.3775, 6.3775, 6.3774, 6.3776, 6.3776, 6.3776, 6.3776, 6.3775, 6.3775,\n",
            "        6.3775, 6.3775, 6.3775, 6.3775, 6.3774, 6.3776, 6.3776, 6.3775, 6.3775,\n",
            "        6.3775, 6.3775, 6.3775, 6.3776, 6.3774, 6.3775, 6.3775, 6.3775, 6.3776,\n",
            "        6.3775, 6.3775, 6.3775, 6.3775, 6.3776, 6.3775, 6.3775, 6.3775, 6.3775,\n",
            "        6.3775, 6.3775, 6.3776, 6.3776, 6.3775, 6.3776, 6.3776, 6.3775, 6.3774,\n",
            "        6.3775, 6.3775, 6.3775, 6.3775, 6.3775, 6.3776, 6.3774, 6.3776, 6.3775,\n",
            "        6.3775, 6.3774, 6.3775, 6.3775, 6.3775, 6.3775, 6.3776],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.08934524324558879 and immediate abs rewards look like: [0.00017956798865270684, 0.014739069999450294, 0.004162974672908604, 0.0016511604417246417, 0.01409658443844819, 0.00616107686118994, 0.011725885566193028, 0.00047186820665956475, 0.0016480535914524808, 0.0030348754230544728, 0.01152079390385552, 7.897693149061524e-05, 0.00026722436314230436, 0.002909223273945827, 2.8401727831806056e-06, 0.003429601084008027, 8.722773191038868e-05, 2.0117945496167522e-05, 0.0002145545090570522, 0.0002491484228812624, 0.0019410313834669068, 0.0015553576367892674, 0.00038819165638415143, 0.001621837750008126, 0.0002450223187224765, 0.00018055086638923967, 0.0017229470972779382, 9.590955914973165e-05, 0.0001504872238911048, 0.00042446155930520035, 0.0008166947600329877, 0.0010290316226928553, 0.0012877969570581627, 0.0001114849378609506, 3.3834658097475767e-07, 8.311844794661738e-06, 0.0002483875286998227, 0.00010626988387230085, 2.4423459308309248e-05, 2.558445385147934e-05, 0.00020753115131810773, 5.0266977268620394e-05, 0.00012120897918066476, 1.665405534367892e-05, 7.295430259546265e-05, 5.351062054614886e-05, 0.0001399090565428196, 1.8587344584375387e-05, 1.0818703231052496e-06, 2.8592513444891665e-05]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.288230209295924 and immediate relative rewards look like: [5.7900669561215545e-05, 0.009505607803382578, 0.004046447174565135, 0.0021428131210104837, 0.02287976961861955, 0.012055007786556859, 0.026821110657819627, 0.0012382576030997212, 0.004866103762472552, 0.00996192508991191, 0.041639964664087425, 0.00031258193963442044, 0.0011458112858687187, 0.013434981156791762, 1.4066464684579202e-05, 0.01811811189557349, 0.0004901679537233455, 0.00011970438718439826, 0.0013475598737271704, 0.0016473116304569868, 0.013476442139963516, 0.011320227042620763, 0.0029552875857394873, 0.012885467200905922, 0.0020288989165763104, 0.001554973270003931, 0.015410320974673855, 0.0008901103464719266, 0.0014465563562502278, 0.004221035580628034, 0.008393478187179929, 0.010919853373166968, 0.014097680644323096, 0.0012579605400016673, 3.930228588649759e-06, 9.930884901521166e-05, 0.003050146724681923, 0.0013403518754896906, 0.0003161637882042701, 0.000339687843764833, 0.002824325216924209, 0.0007008249102574186, 0.0017301667651307177, 0.00024326233495935206, 0.0010898532309510536, 0.0008171709690555956, 0.0021830642186007254, 0.0002962112751776693, 1.7600174187777118e-05, 0.0004746441936973376]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 200 number of matrices\n",
            "DEBUGGING: ACT_MAT has 200 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.15267713965184582, 0.1534519226056838, 0.1363055945077078, 0.13763948263838702, 0.138975658435874, 0.1401834237567458, 0.12305214788231286, 0.12428569181271694, 0.12461601972183702, 0.12057081619093614, 0.11554750463302271, 0.10150169701645721, 0.09955787666520752, 0.09751508372262166, 0.08011628435135867, 0.07849382675042868, 0.06983322769547706, 0.06551707565615128, 0.04194540025362967, 0.02547097077104819, 0.02302554274466444, 0.0223419919795284, 0.020241096139479553, 0.019878445676694924, 0.017464313207945333, 0.017609191198985977, 0.017121640765587795, 0.0168166633093783, 0.01694819043955867, 0.017093413193313412, 0.015862956124621016, 0.015034495638028108, 0.014940745704532337, 0.014294515879280515, 0.014131473077604977, 0.01388060850243913, 0.014014288973421784, 0.013816986230145198, 0.013592525303344058, 0.013058488396664014, 0.012504714593932735, 0.010365782009792001, 0.009671134697180799, 0.006953985430836729, 0.006711426922356848, 0.006319566768655664, 0.005654057247948273, 0.0032431932749713453, 0.0024543857392314225, 0.001887382206781264], [0.2680689854969024, 0.2637537347277666, 0.2386171622871697, 0.21680622231889765, 0.21882472175758536, 0.1958176288056514, 0.19761973013111098, 0.1995826878260129, 0.20145757478416648, 0.2026198365119945, 0.1736122976356635, 0.17427697168118783, 0.1406634727206221, 0.140686769822586, 0.14161506470311547, 0.14267819263849546, 0.14147614342080464, 0.1420893994336325, 0.13995303251509314, 0.13417194712125927, 0.13479084873101266, 0.1356461225159277, 0.13477530369759566, 0.1321060847474853, 0.11246979983879729, 0.10675471567181063, 0.10595357622990863, 0.0917927545221428, 0.092711149137451, 0.09363795190408018, 0.09217774035154676, 0.09219564763459635, 0.09190278063279081, 0.09173602233476999, 0.06303607162387564, 0.05792785140499038, 0.035667621308770894, 0.03507459245117805, 0.033050591024280825, 0.0331333780666959, 0.03071582437007456, 0.028798356718391926, 0.014894757700614571, 0.013042327286294026, 0.012988651574819382, 0.012460170853262275, 0.012508075712938015, 0.008093181638834298, 0.006671650143931288, 0.0027240584748669324], [0.17684848420703034, 0.17186016794970657, 0.1614823352968116, 0.13487668200451214, 0.13473746032235748, 0.11061258526315332, 0.1115531493834553, 0.11052410704045784, 0.10754893960810556, 0.10649714797347419, 0.10730743887224031, 0.0965036077828682, 0.07827581612965885, 0.07164135314287613, 0.05648698838535049, 0.05700566263528239, 0.05669204410525173, 0.056262405696048706, 0.04792614689316231, 0.04167168549841041, 0.031370767013487844, 0.02826042905942766, 0.022868415343684324, 0.023060196381203664, 0.023274226727342435, 0.022249718537733925, 0.014022127361307611, 0.007976350342181245, 0.006551883279334729, 0.005919038367166329, 0.0056964940660552705, 0.004067031007522489, 0.004048501910210564, 0.0027942998944338725, 0.002648558593138624, 0.0022485751644270957, 0.002255135746423267, 0.00040060816562057324, 0.0003991378987406501, 0.00040024591638335123, 0.00035597991040223627, 0.0003537303598904805, 0.00018296415373161935, 0.00017979044330540002, 0.00018071848654101355, 0.00017885154267435627, 0.0001746800338999735, 0.00010726272953081052, 9.632651801943717e-05, 5.589905328177928e-05], [0.2462940821461773, 0.24872341563294553, 0.24163414932279087, 0.23998757792750075, 0.240247237178273, 0.21956309854510453, 0.2096041320793411, 0.1846293145671934, 0.18524349188292288, 0.18219938193984883, 0.17397722914135041, 0.13367400452248787, 0.13470850765944792, 0.13491181451876688, 0.12270387208280314, 0.12392909658395815, 0.10687978251351986, 0.10746425713110759, 0.10842884115547798, 0.10816291038560687, 0.10759151389409079, 0.095065729044573, 0.08459141616358812, 0.08246073593722084, 0.07027804922860093, 0.06893853566871172, 0.06806420444313918, 0.05318574087723771, 0.05282386922299574, 0.05189627562297527, 0.04815680812358306, 0.040164979733740544, 0.02954053167734705, 0.01559883942729692, 0.014485736249793185, 0.014628086890105592, 0.014675533374838769, 0.011742814798138228, 0.010507538305705593, 0.010294317694445781, 0.010055181667354494, 0.007303895404475036, 0.006669768175977392, 0.0049894963745926, 0.004794175797609342, 0.0037417399663215033, 0.0029541100982483914, 0.0007788342218663292, 0.00048749792594814133, 0.0004746441936973376]]\n",
            "DEBUGGING: traj_returns = [0.15267713965184582, 0.2680689854969024, 0.17684848420703034, 0.2462940821461773]\n",
            "DEBUGGING: actions = [[45], [25], [16], [29], [33], [57], [17], [42], [56], [7], [12], [33], [61], [48], [18], [16], [51], [47], [22], [27], [2], [68], [63], [30], [63], [25], [56], [4], [5], [41], [40], [75], [76], [41], [68], [46], [3], [13], [27], [47], [67], [65], [95], [27], [82], [89], [36], [20], [46], [61], [50], [45], [53], [16], [18], [56], [12], [53], [28], [8], [2], [15], [14], [58], [20], [47], [66], [64], [74], [64], [31], [80], [69], [54], [72], [84], [22], [38], [22], [35], [14], [22], [41], [1], [30], [91], [6], [26], [23], [57], [63], [15], [22], [76], [60], [94], [39], [32], [1], [71], [10], [19], [20], [48], [49], [4], [56], [14], [52], [17], [42], [58], [35], [50], [55], [26], [62], [19], [70], [18], [4], [64], [60], [64], [68], [58], [22], [76], [73], [83], [60], [84], [46], [47], [82], [75], [50], [53], [22], [81], [27], [12], [1], [60], [1], [34], [8], [88], [13], [70], [36], [41], [1], [49], [54], [42], [30], [34], [62], [58], [11], [58], [23], [24], [70], [67], [11], [56], [43], [54], [52], [60], [59], [52], [38], [49], [4], [5], [63], [43], [80], [16], [86], [65], [60], [70], [55], [80], [11], [48], [56], [83], [57], [91], [32], [31], [90], [2], [22], [88]]\n",
            "DEBUGGING: actions length = 200\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-1.0027, -0.4266,  0.0826,  ...,  0.1519,  0.4410, -0.6484],\n",
            "        [-0.9922, -0.4173,  0.0802,  ...,  0.1486,  0.4377, -0.6407],\n",
            "        [-0.9679, -0.3869,  0.0764,  ...,  0.1548,  0.4285, -0.6107],\n",
            "        ...,\n",
            "        [-0.9964, -0.4209,  0.0805,  ...,  0.1509,  0.4389, -0.6430],\n",
            "        [-0.9964, -0.4209,  0.0805,  ...,  0.1509,  0.4389, -0.6430],\n",
            "        [-0.9964, -0.4209,  0.0805,  ...,  0.1509,  0.4389, -0.6431]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-0.9983, -0.4218,  0.0804,  ...,  0.1509,  0.4392, -0.6437],\n",
            "        [-0.9964, -0.4209,  0.0805,  ...,  0.1509,  0.4389, -0.6430],\n",
            "        [-0.9964, -0.4209,  0.0805,  ...,  0.1509,  0.4389, -0.6431],\n",
            "        ...,\n",
            "        [-0.9964, -0.4209,  0.0805,  ...,  0.1509,  0.4389, -0.6430],\n",
            "        [-0.9964, -0.4209,  0.0805,  ...,  0.1509,  0.4389, -0.6431],\n",
            "        [-0.9964, -0.4209,  0.0805,  ...,  0.1509,  0.4389, -0.6431]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([6.3892, 6.3775, 6.3776, 6.3775, 6.3775, 6.3776, 6.3776, 6.3776, 6.3775,\n",
            "        6.3775, 6.3776, 6.3775, 6.3775, 6.3775, 6.3776, 6.3775, 6.3776, 6.3776,\n",
            "        6.3775, 6.3775, 6.3776, 6.3776, 6.3775, 6.3775, 6.3775, 6.3775, 6.3776,\n",
            "        6.3775, 6.3775, 6.3776, 6.3776, 6.3776, 6.3775, 6.3775, 6.3775, 6.3775,\n",
            "        6.3775, 6.3775, 6.3775, 6.3775, 6.3775, 6.3775, 6.3775, 6.3775, 6.3775,\n",
            "        6.3775, 6.3775, 6.3774, 6.3776, 6.3776, 6.3776, 6.3776, 6.3775, 6.3775,\n",
            "        6.3775, 6.3775, 6.3775, 6.3775, 6.3774, 6.3776, 6.3776, 6.3775, 6.3775,\n",
            "        6.3775, 6.3775, 6.3775, 6.3776, 6.3774, 6.3775, 6.3775, 6.3775, 6.3776,\n",
            "        6.3775, 6.3775, 6.3775, 6.3775, 6.3776, 6.3775, 6.3775, 6.3775, 6.3775,\n",
            "        6.3775, 6.3775, 6.3776, 6.3776, 6.3775, 6.3776, 6.3776, 6.3775, 6.3774,\n",
            "        6.3775, 6.3775, 6.3775, 6.3775, 6.3775, 6.3776, 6.3774, 6.3776, 6.3775,\n",
            "        6.3775, 6.3774, 6.3775, 6.3775, 6.3775, 6.3775, 6.3776],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[0.21097217 0.20944731 0.19450981 0.18232749 0.18319627 0.16654418\n",
            "  0.16045729 0.15475545 0.15471651 0.1529718  0.14261112 0.12648907\n",
            "  0.11330142 0.11118876 0.10023055 0.10052669 0.0937203  0.09283328\n",
            "  0.08456336 0.07736938 0.07419467 0.07032857 0.06561906 0.06437637\n",
            "  0.0558716  0.05388804 0.05129039 0.04244288 0.04225877 0.04213667\n",
            "  0.0404735  0.03786554 0.03510814 0.03110592 0.02357546 0.02217128\n",
            "  0.01665314 0.01525875 0.01438745 0.01422161 0.01340793 0.01170544\n",
            "  0.00785466 0.0062914  0.00616874 0.00567508 0.00532273 0.00305562\n",
            "  0.00242747 0.0012855 ]]\n",
            "DEBUGGING: baseline2 looks like: 0.21097217287548897\n",
            "DEBUGGING: ADS looks like: [-1.49035593 -1.49308201 -1.51448533 -1.50290548 -1.46615132 -1.44919402\n",
            " -1.46848928 -1.47258678 -1.45353957 -1.44004667 -1.43641677 -1.43124395\n",
            " -1.40370731 -1.39546658 -1.41992898 -1.41387605 -1.40490555 -1.35416555\n",
            " -1.33722735 -1.27420244 -1.23384357 -1.18842427 -1.16818468 -1.11912809\n",
            " -1.08797579 -1.01825487 -1.02414023 -1.03109544 -0.96999461 -0.93182188\n",
            " -0.86730759 -0.74822381 -0.65377289 -0.56255397 -0.53021301 -0.4756762\n",
            " -0.47850369 -0.38831262 -0.2702048  -0.24188553 -0.24314048 -0.18483953\n",
            " -0.18472184 -0.1881558  -0.14886848 -0.14861581 -0.14920118 -0.15156106\n",
            " -0.14427099 -0.03370867 -1.37496409 -1.3827802  -1.41217376 -1.42373875\n",
            " -1.38630226 -1.39355982 -1.39392169 -1.39728978 -1.37669802 -1.35799765\n",
            " -1.37835198 -1.35846868 -1.36260171 -1.3522949  -1.3584302  -1.34969168\n",
            " -1.33326263 -1.27759323 -1.23921972 -1.16550147 -1.12207826 -1.07512014\n",
            " -1.05365048 -1.00690045 -0.9929703  -0.92910934 -0.9353083  -0.95611935\n",
            " -0.89423165 -0.85527734 -0.79099281 -0.67106266 -0.57681085 -0.48511246\n",
            " -0.48130841 -0.43162896 -0.45685036 -0.36705501 -0.25074673 -0.22181064\n",
            " -0.22492937 -0.16640696 -0.17949822 -0.18206746 -0.14259125 -0.14247521\n",
            " -0.14234716 -0.14671107 -0.14005373 -0.03287199 -1.46618459 -1.47467376\n",
            " -1.48930859 -1.50566829 -1.47038952 -1.47876486 -1.47998827 -1.48634836\n",
            " -1.47060665 -1.45412034 -1.44465684 -1.43624204 -1.42498937 -1.42134031\n",
            " -1.44355827 -1.43536421 -1.41804673 -1.36342022 -1.33124661 -1.25800173\n",
            " -1.22549835 -1.18250583 -1.16555736 -1.11594633 -1.08216587 -1.01361434\n",
            " -1.02723975 -1.03993575 -0.98039092 -0.94299625 -0.87747406 -0.75919128\n",
            " -0.66466513 -0.57405418 -0.54169592 -0.48730824 -0.49026285 -0.40172899\n",
            " -0.28339819 -0.25454377 -0.25528921 -0.19485158 -0.19421001 -0.19493\n",
            " -0.15539919 -0.15475653 -0.15468056 -0.15469699 -0.14662905 -0.03554015\n",
            " -1.39673899 -1.39781052 -1.40915677 -1.40055739 -1.36487974 -1.36981435\n",
            " -1.38193729 -1.41224316 -1.3929121  -1.37841811 -1.37798705 -1.39907165\n",
            " -1.36855668 -1.35806985 -1.37734139 -1.36844078 -1.367859   -1.31221837\n",
            " -1.27074391 -1.1915105  -1.1492776  -1.11570053 -1.10383436 -1.0565458\n",
            " -1.03516205 -0.96692552 -0.97319767 -0.99472636 -0.93411893 -0.89701902\n",
            " -0.83501374 -0.72309333 -0.6391731  -0.56124964 -0.52985874 -0.47492872\n",
            " -0.47784245 -0.39038679 -0.27328979 -0.2446497  -0.24559001 -0.18790142\n",
            " -0.18772321 -0.19012029 -0.15078573 -0.15119364 -0.15190113 -0.15402542\n",
            " -0.14623788 -0.03512141]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(-3.8731, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-1.3671e-03, -4.7077e-04,  2.8282e-04,  ...,  5.7981e-04,\n",
            "         -6.7232e-04, -5.4203e-05],\n",
            "        [-7.0729e-04, -2.2297e-04,  2.2606e-04,  ...,  3.4302e-04,\n",
            "         -3.1706e-04,  1.4941e-02]])\n",
            "   Last layer:\n",
            "tensor([[ 1.5723e-02,  1.0786e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  3.9262e-03,  1.2018e-02,  6.2751e-03,  9.7894e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  8.5866e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 9.5180e-03,  4.4827e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  2.3917e-03,  7.4189e-03,  3.5970e-03,  5.5651e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.1076e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-3.4365e-04, -1.1726e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -8.6510e-05, -2.7149e-04, -1.2548e-04, -1.9334e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.8260e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.1379e-02, -9.4715e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -2.8293e-03, -8.5794e-03, -4.7055e-03, -7.3787e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.2880e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 2.2186e-02,  1.5401e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  5.5388e-03,  1.6945e-02,  8.8727e-03,  1.3845e-02,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.2125e-02,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 2.1690e-02,  1.4460e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  5.4195e-03,  1.6608e-02,  8.6149e-03,  1.3430e-02,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.1827e-02,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.1005e-02, -6.8169e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -2.7535e-03, -8.4628e-03, -4.3198e-03, -6.7223e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.9775e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 2.7218e-04, -1.0545e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  7.7226e-05,  2.9481e-04, -1.3555e-05, -4.9219e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  9.3749e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-4.8238e-03, -4.7170e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -1.1943e-03, -3.5878e-03, -2.0639e-03, -3.2517e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.6966e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 9.9311e-03,  7.1832e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  2.4770e-03,  7.5639e-03,  4.0000e-03,  6.2488e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.4400e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-3.1972e-04,  3.4204e-04,  1.9522e-04,  ..., -1.8807e-04,\n",
            "          4.3838e-04, -2.8235e-03],\n",
            "        [-1.6469e-04,  1.7623e-04,  1.0057e-04,  ..., -9.6909e-05,\n",
            "          2.2583e-04, -1.4540e-03]])\n",
            "   Last layer:\n",
            "tensor([[ 2.1167e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  5.6317e-04,  1.4688e-03,  9.3533e-04,  1.4332e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.2226e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.5638e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  4.0575e-04,  1.1352e-03,  6.0837e-04,  9.1071e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  8.4361e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.1016e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -5.5384e-05, -1.4885e-04, -8.7622e-05, -1.3278e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1769e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.9267e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -5.1008e-04, -1.3496e-03, -8.3100e-04, -1.2680e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0982e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 3.3740e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  8.9176e-04,  2.3689e-03,  1.4449e-03,  2.2019e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.9156e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 3.2858e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  8.6482e-04,  2.3251e-03,  1.3777e-03,  2.0916e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.8441e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.5567e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -4.0794e-04, -1.1102e-03, -6.3859e-04, -9.6574e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -8.6344e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.5321e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -4.3424e-05, -9.3671e-05, -8.7979e-05, -1.3998e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0316e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-8.5834e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -2.2951e-04, -5.9014e-04, -3.8822e-04, -5.9721e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.0231e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.6726e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  4.4025e-04,  1.1840e-03,  7.0121e-04,  1.0646e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  9.3889e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "DEBUGGING: training for one iteration takes 0.007430 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 43\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0175, 0.0153, 0.0153, 0.0182, 0.0141, 0.0150, 0.0144, 0.0143, 0.0161,\n",
            "        0.0142, 0.0150, 0.0147, 0.0141, 0.0142, 0.0139, 0.0141, 0.0149, 0.0140,\n",
            "        0.0145, 0.0148, 0.0150, 0.0141, 0.0145, 0.0155, 0.0148, 0.0144, 0.0145,\n",
            "        0.0149, 0.0145, 0.0148, 0.0143, 0.0154, 0.0141, 0.0147, 0.0144, 0.0147,\n",
            "        0.0140, 0.0151, 0.0153, 0.0142, 0.0143, 0.0158, 0.0144, 0.0145, 0.0146,\n",
            "        0.0146, 0.0146, 0.0143, 0.0153, 0.0144, 0.0144, 0.0142, 0.0145, 0.0142,\n",
            "        0.0141, 0.0150, 0.0142, 0.0147, 0.0150, 0.0146, 0.0143, 0.0146, 0.0143,\n",
            "        0.0148, 0.0148, 0.0144, 0.0148, 0.0146], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [49]\n",
            "DEBUGGING: logits looks like: tensor([7.1753, 7.1482, 7.1481, 7.1835, 7.1323, 7.1441, 7.1365, 7.1346, 7.1581,\n",
            "        7.1330, 7.1440, 7.1402, 7.1325, 7.1328, 7.1291, 7.1314, 7.1429, 7.1312,\n",
            "        7.1376, 7.1420, 7.1445, 7.1324, 7.1377, 7.1516, 7.1420, 7.1367, 7.1373,\n",
            "        7.1426, 7.1376, 7.1420, 7.1344, 7.1503, 7.1320, 7.1410, 7.1365, 7.1410,\n",
            "        7.1305, 7.1454, 7.1477, 7.1338, 7.1342, 7.1554, 7.1360, 7.1376, 7.1396,\n",
            "        7.1384, 7.1387, 7.1345, 7.1479, 7.1363, 7.1361, 7.1331, 7.1372, 7.1334,\n",
            "        7.1316, 7.1446, 7.1339, 7.1403, 7.1446, 7.1396, 7.1342, 7.1389, 7.1343,\n",
            "        7.1423, 7.1418, 7.1369, 7.1417, 7.1386], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0152, 0.0127, 0.0130, 0.0125, 0.0128, 0.0127, 0.0129, 0.0137, 0.0124,\n",
            "        0.0127, 0.0130, 0.0127, 0.0129, 0.0128, 0.0128, 0.0126, 0.0126, 0.0131,\n",
            "        0.0125, 0.0127, 0.0125, 0.0133, 0.0127, 0.0129, 0.0127, 0.0129, 0.0127,\n",
            "        0.0126, 0.0128, 0.0123, 0.0124, 0.0137, 0.0130, 0.0128, 0.0128, 0.0129,\n",
            "        0.0125, 0.0127, 0.0127, 0.0125, 0.0124, 0.0127, 0.0130, 0.0127, 0.0125,\n",
            "        0.0128, 0.0126, 0.0127, 0.0127, 0.0129, 0.0130, 0.0126, 0.0126, 0.0127,\n",
            "        0.0126, 0.0128, 0.0133, 0.0129, 0.0135, 0.0132, 0.0124, 0.0125, 0.0133,\n",
            "        0.0126, 0.0127, 0.0128, 0.0130, 0.0128, 0.0130, 0.0127, 0.0129, 0.0127,\n",
            "        0.0127, 0.0128, 0.0127, 0.0128, 0.0130, 0.0126],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [11]\n",
            "DEBUGGING: logits looks like: tensor([7.1805, 7.1440, 7.1490, 7.1423, 7.1462, 7.1444, 7.1480, 7.1604, 7.1408,\n",
            "        7.1440, 7.1493, 7.1453, 7.1477, 7.1468, 7.1461, 7.1435, 7.1432, 7.1512,\n",
            "        7.1416, 7.1454, 7.1422, 7.1534, 7.1453, 7.1476, 7.1443, 7.1471, 7.1447,\n",
            "        7.1435, 7.1459, 7.1384, 7.1393, 7.1601, 7.1493, 7.1457, 7.1459, 7.1476,\n",
            "        7.1420, 7.1442, 7.1446, 7.1420, 7.1401, 7.1451, 7.1489, 7.1450, 7.1421,\n",
            "        7.1461, 7.1430, 7.1454, 7.1454, 7.1484, 7.1494, 7.1438, 7.1432, 7.1442,\n",
            "        7.1433, 7.1470, 7.1540, 7.1478, 7.1567, 7.1525, 7.1393, 7.1420, 7.1534,\n",
            "        7.1436, 7.1443, 7.1457, 7.1502, 7.1461, 7.1501, 7.1444, 7.1484, 7.1443,\n",
            "        7.1452, 7.1458, 7.1446, 7.1462, 7.1497, 7.1432],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0134, 0.0114, 0.0113, 0.0114, 0.0113, 0.0114, 0.0113, 0.0114, 0.0113,\n",
            "        0.0114, 0.0113, 0.0113, 0.0113, 0.0113, 0.0114, 0.0113, 0.0113, 0.0114,\n",
            "        0.0114, 0.0114, 0.0113, 0.0114, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113,\n",
            "        0.0114, 0.0113, 0.0113, 0.0114, 0.0113, 0.0113, 0.0111, 0.0113, 0.0113,\n",
            "        0.0114, 0.0114, 0.0113, 0.0114, 0.0113, 0.0113, 0.0114, 0.0113, 0.0114,\n",
            "        0.0114, 0.0114, 0.0113, 0.0114, 0.0113, 0.0114, 0.0113, 0.0114, 0.0113,\n",
            "        0.0113, 0.0113, 0.0112, 0.0114, 0.0114, 0.0114, 0.0113, 0.0113, 0.0113,\n",
            "        0.0114, 0.0114, 0.0114, 0.0113, 0.0113, 0.0113, 0.0114, 0.0114, 0.0114,\n",
            "        0.0112, 0.0113, 0.0113, 0.0114, 0.0113, 0.0112, 0.0113, 0.0113, 0.0114,\n",
            "        0.0113, 0.0113, 0.0114, 0.0114, 0.0113, 0.0113, 0.0114],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [74]\n",
            "DEBUGGING: logits looks like: tensor([7.1819, 7.1495, 7.1476, 7.1499, 7.1489, 7.1494, 7.1490, 7.1498, 7.1483,\n",
            "        7.1495, 7.1492, 7.1486, 7.1491, 7.1490, 7.1493, 7.1488, 7.1492, 7.1496,\n",
            "        7.1497, 7.1506, 7.1491, 7.1498, 7.1490, 7.1476, 7.1488, 7.1491, 7.1488,\n",
            "        7.1500, 7.1487, 7.1489, 7.1495, 7.1481, 7.1486, 7.1457, 7.1488, 7.1490,\n",
            "        7.1500, 7.1503, 7.1492, 7.1494, 7.1491, 7.1490, 7.1496, 7.1483, 7.1494,\n",
            "        7.1498, 7.1494, 7.1485, 7.1500, 7.1492, 7.1495, 7.1489, 7.1497, 7.1489,\n",
            "        7.1485, 7.1492, 7.1466, 7.1500, 7.1493, 7.1497, 7.1487, 7.1491, 7.1482,\n",
            "        7.1493, 7.1501, 7.1501, 7.1486, 7.1489, 7.1488, 7.1494, 7.1495, 7.1508,\n",
            "        7.1468, 7.1483, 7.1492, 7.1498, 7.1483, 7.1473, 7.1480, 7.1485, 7.1503,\n",
            "        7.1491, 7.1486, 7.1495, 7.1501, 7.1492, 7.1490, 7.1500],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0119, 0.0102, 0.0101, 0.0100, 0.0100, 0.0100, 0.0100, 0.0102, 0.0103,\n",
            "        0.0101, 0.0100, 0.0099, 0.0101, 0.0100, 0.0099, 0.0100, 0.0102, 0.0104,\n",
            "        0.0104, 0.0102, 0.0102, 0.0100, 0.0100, 0.0100, 0.0101, 0.0106, 0.0101,\n",
            "        0.0108, 0.0101, 0.0100, 0.0101, 0.0103, 0.0101, 0.0100, 0.0100, 0.0100,\n",
            "        0.0101, 0.0100, 0.0100, 0.0099, 0.0100, 0.0101, 0.0101, 0.0102, 0.0101,\n",
            "        0.0100, 0.0101, 0.0099, 0.0108, 0.0100, 0.0100, 0.0101, 0.0100, 0.0100,\n",
            "        0.0102, 0.0101, 0.0099, 0.0101, 0.0100, 0.0100, 0.0101, 0.0101, 0.0099,\n",
            "        0.0100, 0.0101, 0.0101, 0.0101, 0.0100, 0.0102, 0.0100, 0.0099, 0.0104,\n",
            "        0.0101, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0099, 0.0101, 0.0100,\n",
            "        0.0100, 0.0101, 0.0100, 0.0099, 0.0099, 0.0100, 0.0101, 0.0104, 0.0101,\n",
            "        0.0100, 0.0103, 0.0099, 0.0101, 0.0101, 0.0099, 0.0099, 0.0101, 0.0100],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [63]\n",
            "DEBUGGING: logits looks like: tensor([7.1826, 7.1512, 7.1490, 7.1477, 7.1475, 7.1472, 7.1474, 7.1502, 7.1521,\n",
            "        7.1490, 7.1472, 7.1458, 7.1492, 7.1464, 7.1448, 7.1476, 7.1512, 7.1553,\n",
            "        7.1555, 7.1507, 7.1506, 7.1467, 7.1475, 7.1477, 7.1492, 7.1578, 7.1490,\n",
            "        7.1623, 7.1481, 7.1480, 7.1491, 7.1525, 7.1486, 7.1469, 7.1480, 7.1478,\n",
            "        7.1499, 7.1470, 7.1478, 7.1458, 7.1461, 7.1500, 7.1487, 7.1503, 7.1496,\n",
            "        7.1470, 7.1498, 7.1459, 7.1622, 7.1479, 7.1474, 7.1487, 7.1465, 7.1469,\n",
            "        7.1506, 7.1495, 7.1460, 7.1487, 7.1462, 7.1466, 7.1499, 7.1481, 7.1453,\n",
            "        7.1467, 7.1483, 7.1490, 7.1487, 7.1467, 7.1509, 7.1470, 7.1460, 7.1549,\n",
            "        7.1484, 7.1471, 7.1479, 7.1465, 7.1476, 7.1477, 7.1460, 7.1487, 7.1469,\n",
            "        7.1474, 7.1483, 7.1480, 7.1443, 7.1450, 7.1464, 7.1492, 7.1550, 7.1499,\n",
            "        7.1478, 7.1520, 7.1445, 7.1492, 7.1483, 7.1458, 7.1447, 7.1493, 7.1480],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0094, 0.0096, 0.0091, 0.0100, 0.0092, 0.0091, 0.0093, 0.0096, 0.0091,\n",
            "        0.0090, 0.0089, 0.0096, 0.0103, 0.0095, 0.0095, 0.0090, 0.0094, 0.0092,\n",
            "        0.0091, 0.0093, 0.0095, 0.0097, 0.0093, 0.0087, 0.0092, 0.0095, 0.0092,\n",
            "        0.0096, 0.0094, 0.0085, 0.0092, 0.0093, 0.0092, 0.0089, 0.0092, 0.0097,\n",
            "        0.0093, 0.0093, 0.0089, 0.0093, 0.0092, 0.0093, 0.0094, 0.0092, 0.0096,\n",
            "        0.0092, 0.0092, 0.0094, 0.0096, 0.0099, 0.0094, 0.0093, 0.0092, 0.0092,\n",
            "        0.0086, 0.0089, 0.0090, 0.0091, 0.0092, 0.0093, 0.0082, 0.0095, 0.0097,\n",
            "        0.0093, 0.0094, 0.0095, 0.0091, 0.0097, 0.0094, 0.0093, 0.0094, 0.0093,\n",
            "        0.0092, 0.0092, 0.0094, 0.0091, 0.0092, 0.0091, 0.0095, 0.0094, 0.0092,\n",
            "        0.0095, 0.0090, 0.0088, 0.0092, 0.0092, 0.0090, 0.0094, 0.0091, 0.0090,\n",
            "        0.0094, 0.0091, 0.0089, 0.0094, 0.0093, 0.0095, 0.0092, 0.0091, 0.0093,\n",
            "        0.0094, 0.0090, 0.0089, 0.0093, 0.0089, 0.0094, 0.0091, 0.0092, 0.0091],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [29]\n",
            "DEBUGGING: logits looks like: tensor([7.1688, 7.1729, 7.1615, 7.1801, 7.1648, 7.1607, 7.1658, 7.1716, 7.1614,\n",
            "        7.1591, 7.1577, 7.1718, 7.1854, 7.1691, 7.1704, 7.1596, 7.1676, 7.1648,\n",
            "        7.1624, 7.1654, 7.1707, 7.1745, 7.1662, 7.1534, 7.1638, 7.1692, 7.1640,\n",
            "        7.1722, 7.1687, 7.1472, 7.1631, 7.1666, 7.1631, 7.1570, 7.1627, 7.1748,\n",
            "        7.1661, 7.1649, 7.1563, 7.1660, 7.1633, 7.1664, 7.1682, 7.1637, 7.1728,\n",
            "        7.1641, 7.1637, 7.1682, 7.1723, 7.1776, 7.1690, 7.1665, 7.1637, 7.1631,\n",
            "        7.1510, 7.1581, 7.1582, 7.1623, 7.1634, 7.1649, 7.1404, 7.1702, 7.1752,\n",
            "        7.1655, 7.1682, 7.1701, 7.1625, 7.1738, 7.1676, 7.1650, 7.1674, 7.1652,\n",
            "        7.1646, 7.1638, 7.1677, 7.1624, 7.1643, 7.1617, 7.1695, 7.1676, 7.1644,\n",
            "        7.1698, 7.1595, 7.1545, 7.1633, 7.1644, 7.1597, 7.1687, 7.1617, 7.1587,\n",
            "        7.1681, 7.1610, 7.1570, 7.1674, 7.1660, 7.1707, 7.1638, 7.1611, 7.1661,\n",
            "        7.1691, 7.1586, 7.1568, 7.1666, 7.1570, 7.1673, 7.1604, 7.1631, 7.1611],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.6072478073047023 and immediate abs rewards look like: [0.010392810452685808, 0.004855416455711747, 0.040487226016011846, 0.004487613183755457, 0.00014433028763960465, 1.0427358574816026e-05, 0.0002349525777844974, 0.03616503858347642, 0.0008410193661347876, 0.0021744948317063972, 0.005416378371592145, 0.0047027743200942496, 0.019016144922943568, 0.00995247029300117, 0.0017670146298769396, 1.577014745635097e-05, 0.01400760132855794, 0.016954647078136986, 0.00021310142983566038, 0.0033058864848953817, 0.006593378494926583, 0.005463083100266886, 0.0014078281742513354, 0.006328487743985534, 0.0022582679628158076, 6.841266826995707e-05, 0.00208767361664286, 0.00044769080295736785, 0.0004159363329563348, 0.0021231418556908466, 0.0011710400385709363, 7.087285143825284e-05, 0.0002975405700453848, 0.00012614612273864623, 7.609126214447315e-06, 3.508854661049554e-05, 3.646185837169469e-05, 0.0001924490411511215, 0.0028560023401951184, 0.00012994303324376233, 0.0006785999394196551, 0.0066374989726227795, 0.0006152552055027627, 5.472400880535133e-05, 0.0012451163797777554, 0.390754440396222, 4.547473508864641e-13, 0.0, 2.2737367544323206e-13, 4.547473508864641e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 6.12200607746954 and immediate relative rewards look like: [0.00288109134937857, 0.0026998121747823464, 0.03381450229309005, 0.005054312344680571, 0.0002034526091168195, 1.7639205217210014e-05, 0.0004636956110936862, 0.08157600264513414, 0.0021561711997615716, 0.006195796901672028, 0.016986724263422476, 0.016114414016244773, 0.07068521059331627, 0.04005802138002271, 0.007641988769405259, 7.278670208679798e-05, 0.06869276143395783, 0.08839301980653307, 0.0011785136861453352, 0.01924595815653419, 0.0403428920789106, 0.035086119261407374, 0.00946772232099811, 0.044428128427299256, 0.016545009255260815, 0.0005216136178762576, 0.016530048340497875, 0.0036783235494132114, 0.0035399378443614475, 0.018694939458749227, 0.01066173676048247, 0.0006663061063078595, 0.0028847825508595258, 0.0012602123810201303, 7.825459502229406e-05, 0.0003711725645749223, 0.0003964176554543718, 0.0021489012873738625, 0.032731419772420646, 0.0015286896020478883, 0.008183151490582302, 0.08200934895933402, 0.007797981370118897, 0.0007098519622978103, 0.016518350357115515, 5.301092890739741, 7.1243751638890184e-12, 0.0, 3.713770032239457e-12, 7.579122514774976e-12]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0149, 0.0148, 0.0129, 0.0139, 0.0146, 0.0146, 0.0145, 0.0147, 0.0146,\n",
            "        0.0148, 0.0141, 0.0137, 0.0145, 0.0145, 0.0147, 0.0143, 0.0145, 0.0149,\n",
            "        0.0146, 0.0142, 0.0132, 0.0137, 0.0147, 0.0137, 0.0145, 0.0139, 0.0147,\n",
            "        0.0147, 0.0146, 0.0148, 0.0147, 0.0147, 0.0146, 0.0139, 0.0144, 0.0141,\n",
            "        0.0146, 0.0147, 0.0145, 0.0146, 0.0146, 0.0145, 0.0145, 0.0147, 0.0142,\n",
            "        0.0146, 0.0149, 0.0146, 0.0146, 0.0145, 0.0148, 0.0149, 0.0147, 0.0145,\n",
            "        0.0144, 0.0148, 0.0147, 0.0146, 0.0147, 0.0148, 0.0146, 0.0145, 0.0149,\n",
            "        0.0148, 0.0147, 0.0144, 0.0147, 0.0148, 0.0147],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [58]\n",
            "DEBUGGING: logits looks like: tensor([7.1157, 7.1147, 7.0871, 7.1012, 7.1115, 7.1111, 7.1100, 7.1126, 7.1110,\n",
            "        7.1141, 7.1045, 7.0985, 7.1104, 7.1106, 7.1127, 7.1080, 7.1101, 7.1152,\n",
            "        7.1115, 7.1057, 7.0914, 7.0990, 7.1126, 7.0987, 7.1103, 7.1011, 7.1132,\n",
            "        7.1127, 7.1114, 7.1137, 7.1131, 7.1135, 7.1120, 7.1016, 7.1089, 7.1041,\n",
            "        7.1117, 7.1130, 7.1100, 7.1117, 7.1111, 7.1101, 7.1100, 7.1126, 7.1062,\n",
            "        7.1118, 7.1150, 7.1114, 7.1119, 7.1104, 7.1138, 7.1160, 7.1128, 7.1100,\n",
            "        7.1088, 7.1137, 7.1135, 7.1119, 7.1125, 7.1148, 7.1109, 7.1099, 7.1150,\n",
            "        7.1147, 7.1130, 7.1089, 7.1130, 7.1148, 7.1135],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0132, 0.0129, 0.0129, 0.0127, 0.0129, 0.0127, 0.0128, 0.0127, 0.0129,\n",
            "        0.0128, 0.0128, 0.0126, 0.0129, 0.0128, 0.0130, 0.0128, 0.0126, 0.0127,\n",
            "        0.0128, 0.0129, 0.0129, 0.0127, 0.0127, 0.0127, 0.0127, 0.0129, 0.0129,\n",
            "        0.0131, 0.0128, 0.0128, 0.0127, 0.0127, 0.0128, 0.0127, 0.0130, 0.0125,\n",
            "        0.0127, 0.0129, 0.0124, 0.0129, 0.0128, 0.0127, 0.0128, 0.0129, 0.0129,\n",
            "        0.0127, 0.0128, 0.0129, 0.0133, 0.0128, 0.0130, 0.0130, 0.0128, 0.0129,\n",
            "        0.0129, 0.0129, 0.0128, 0.0128, 0.0128, 0.0129, 0.0127, 0.0130, 0.0129,\n",
            "        0.0129, 0.0125, 0.0126, 0.0128, 0.0128, 0.0130, 0.0128, 0.0128, 0.0130,\n",
            "        0.0129, 0.0129, 0.0128, 0.0127, 0.0126, 0.0128],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [28]\n",
            "DEBUGGING: logits looks like: tensor([7.1127, 7.1069, 7.1077, 7.1048, 7.1074, 7.1051, 7.1063, 7.1050, 7.1082,\n",
            "        7.1053, 7.1064, 7.1027, 7.1081, 7.1063, 7.1085, 7.1054, 7.1028, 7.1052,\n",
            "        7.1067, 7.1080, 7.1070, 7.1051, 7.1040, 7.1046, 7.1051, 7.1079, 7.1078,\n",
            "        7.1100, 7.1062, 7.1061, 7.1041, 7.1046, 7.1060, 7.1037, 7.1095, 7.1015,\n",
            "        7.1047, 7.1071, 7.1002, 7.1083, 7.1068, 7.1045, 7.1064, 7.1070, 7.1075,\n",
            "        7.1049, 7.1065, 7.1072, 7.1145, 7.1054, 7.1087, 7.1092, 7.1067, 7.1071,\n",
            "        7.1080, 7.1072, 7.1064, 7.1057, 7.1059, 7.1082, 7.1047, 7.1086, 7.1076,\n",
            "        7.1077, 7.1019, 7.1036, 7.1061, 7.1067, 7.1095, 7.1060, 7.1061, 7.1092,\n",
            "        7.1079, 7.1076, 7.1055, 7.1040, 7.1036, 7.1059],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0115, 0.0111, 0.0111, 0.0112, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0109, 0.0112, 0.0111, 0.0111, 0.0112, 0.0110, 0.0109, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0112, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0112, 0.0111, 0.0111, 0.0111, 0.0111, 0.0112, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0112, 0.0111, 0.0111, 0.0110, 0.0111, 0.0112,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0112, 0.0111, 0.0111, 0.0111, 0.0112, 0.0111, 0.0109, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0110, 0.0111, 0.0111, 0.0111, 0.0112, 0.0112,\n",
            "        0.0110, 0.0110, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0112, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [12]\n",
            "DEBUGGING: logits looks like: tensor([7.1103, 7.1040, 7.1044, 7.1051, 7.1039, 7.1047, 7.1041, 7.1043, 7.1041,\n",
            "        7.1036, 7.1001, 7.1050, 7.1031, 7.1043, 7.1053, 7.1028, 7.1005, 7.1035,\n",
            "        7.1044, 7.1036, 7.1039, 7.1045, 7.1035, 7.1050, 7.1041, 7.1038, 7.1044,\n",
            "        7.1032, 7.1057, 7.1040, 7.1043, 7.1030, 7.1042, 7.1048, 7.1037, 7.1041,\n",
            "        7.1036, 7.1042, 7.1040, 7.1050, 7.1041, 7.1038, 7.1027, 7.1044, 7.1047,\n",
            "        7.1036, 7.1042, 7.1034, 7.1042, 7.1032, 7.1032, 7.1037, 7.1038, 7.1042,\n",
            "        7.1052, 7.1042, 7.1043, 7.1034, 7.1049, 7.1042, 7.1006, 7.1042, 7.1034,\n",
            "        7.1045, 7.1032, 7.1044, 7.1024, 7.1043, 7.1041, 7.1041, 7.1048, 7.1048,\n",
            "        7.1024, 7.1028, 7.1042, 7.1041, 7.1037, 7.1039, 7.1043, 7.1038, 7.1031,\n",
            "        7.1042, 7.1045, 7.1043, 7.1047, 7.1044, 7.1040, 7.1045, 7.1043, 7.1036],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0104, 0.0101, 0.0101, 0.0101, 0.0100, 0.0101, 0.0101, 0.0101, 0.0100,\n",
            "        0.0103, 0.0101, 0.0101, 0.0101, 0.0102, 0.0100, 0.0101, 0.0115, 0.0103,\n",
            "        0.0102, 0.0101, 0.0103, 0.0100, 0.0101, 0.0102, 0.0101, 0.0100, 0.0102,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0102, 0.0100, 0.0103, 0.0100, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0100, 0.0101,\n",
            "        0.0101, 0.0100, 0.0100, 0.0101, 0.0096, 0.0101, 0.0100, 0.0101, 0.0101,\n",
            "        0.0102, 0.0100, 0.0100, 0.0106, 0.0101, 0.0100, 0.0100, 0.0101, 0.0101,\n",
            "        0.0101, 0.0100, 0.0101, 0.0100, 0.0101, 0.0101, 0.0100, 0.0101, 0.0101,\n",
            "        0.0101, 0.0100, 0.0100, 0.0100, 0.0101, 0.0101, 0.0100, 0.0101, 0.0101,\n",
            "        0.0100, 0.0101, 0.0100, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0100, 0.0101, 0.0101, 0.0100, 0.0101, 0.0101, 0.0101, 0.0101, 0.0104],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [41]\n",
            "DEBUGGING: logits looks like: tensor([7.1082, 7.1019, 7.1021, 7.1021, 7.1013, 7.1027, 7.1023, 7.1019, 7.1013,\n",
            "        7.1067, 7.1025, 7.1017, 7.1022, 7.1047, 7.1010, 7.1017, 7.1281, 7.1071,\n",
            "        7.1045, 7.1017, 7.1062, 7.1012, 7.1018, 7.1040, 7.1017, 7.1007, 7.1052,\n",
            "        7.1022, 7.1024, 7.1018, 7.1022, 7.1052, 7.1010, 7.1063, 7.1012, 7.1028,\n",
            "        7.1022, 7.1027, 7.1017, 7.1021, 7.1028, 7.1033, 7.1017, 7.0999, 7.1021,\n",
            "        7.1027, 7.1015, 7.1012, 7.1017, 7.0924, 7.1023, 7.1013, 7.1022, 7.1024,\n",
            "        7.1056, 7.1005, 7.1015, 7.1126, 7.1017, 7.1014, 7.1014, 7.1024, 7.1026,\n",
            "        7.1022, 7.1015, 7.1025, 7.1014, 7.1017, 7.1021, 7.1011, 7.1020, 7.1019,\n",
            "        7.1018, 7.1016, 7.1014, 7.1013, 7.1022, 7.1024, 7.1013, 7.1016, 7.1020,\n",
            "        7.1012, 7.1028, 7.1013, 7.1020, 7.1027, 7.1018, 7.1016, 7.1030, 7.1019,\n",
            "        7.1004, 7.1028, 7.1019, 7.1008, 7.1023, 7.1018, 7.1022, 7.1018, 7.1083],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0096, 0.0093, 0.0093, 0.0092, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0092, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0092, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0092, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0092, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0092, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0092, 0.0093,\n",
            "        0.0092, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [97]\n",
            "DEBUGGING: logits looks like: tensor([7.1066, 7.0999, 7.1000, 7.0998, 7.1000, 7.0999, 7.1000, 7.0999, 7.0999,\n",
            "        7.0999, 7.0998, 7.1000, 7.1001, 7.1000, 7.1001, 7.0998, 7.0998, 7.0999,\n",
            "        7.1001, 7.1000, 7.1000, 7.0998, 7.0999, 7.0999, 7.1000, 7.1000, 7.1001,\n",
            "        7.0999, 7.1000, 7.0999, 7.1000, 7.0999, 7.0999, 7.0999, 7.0999, 7.1002,\n",
            "        7.0996, 7.0998, 7.1000, 7.1000, 7.0999, 7.0999, 7.0999, 7.0997, 7.0999,\n",
            "        7.1000, 7.1000, 7.0999, 7.0999, 7.0997, 7.0999, 7.1000, 7.0999, 7.1000,\n",
            "        7.1003, 7.0999, 7.0998, 7.1000, 7.1001, 7.1000, 7.1000, 7.0998, 7.0999,\n",
            "        7.0999, 7.0999, 7.0999, 7.1004, 7.1000, 7.1001, 7.1001, 7.1000, 7.0999,\n",
            "        7.1000, 7.0999, 7.1000, 7.0999, 7.1000, 7.1000, 7.1000, 7.0997, 7.1000,\n",
            "        7.0996, 7.1000, 7.1000, 7.0999, 7.1000, 7.0998, 7.0999, 7.1002, 7.0998,\n",
            "        7.1002, 7.1000, 7.0999, 7.1001, 7.1001, 7.0999, 7.1000, 7.1000, 7.1000,\n",
            "        7.1000, 7.1000, 7.1000, 7.1000, 7.1000, 7.1000, 7.0999, 7.1000, 7.0999],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.025365797846461646 and immediate abs rewards look like: [0.0030965808728069533, 0.006897282049067144, 0.003051834177767887, 5.423912944024778e-05, 0.00288014846319129, 0.0023049614082992775, 0.0005618868767669483, 0.0014037659316272766, 0.0005658513764501549, 0.00027353579571354203, 6.047261604180676e-05, 0.0011755090195038065, 6.870463221275713e-05, 1.403617443429539e-05, 2.952013119283947e-05, 0.0005911787598051887, 0.00021700695197068853, 1.388918735756306e-05, 2.6951708150591003e-05, 3.0199920274753822e-05, 5.5551694003952434e-05, 2.0137283172516618e-05, 0.00010097516133100726, 0.00010119321996171493, 6.062236934667453e-08, 5.884021447855048e-08, 6.002359441481531e-05, 0.0005814357396047853, 9.60009497248393e-05, 1.7666014628048288e-05, 7.127563549147453e-06, 0.00019604519638960483, 1.0141857273993082e-06, 1.836288811318809e-05, 0.0003762640731110878, 7.960647963045631e-07, 1.9104118109680712e-07, 5.8625367273634765e-05, 2.4911782929848414e-05, 4.273181684766314e-05, 0.00011332754911563825, 4.055578301631613e-05, 2.2210870156413876e-06, 1.913587175295106e-05, 5.978786248306278e-06, 1.5470620837731985e-05, 5.3402799039758975e-05, 8.320685537910322e-06, 8.41409018903505e-06, 2.224229228886543e-05]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.06469834316675303 and immediate relative rewards look like: [0.001144403894416729, 0.005103900209994791, 0.003396143985358188, 8.056910323714348e-05, 0.005347976068959766, 0.005141433851828697, 0.0014634860321179085, 0.004179430860278001, 0.0018962879849365487, 0.0010187440602098124, 0.00024776879552349706, 0.00525426729008001, 0.00033283157609136034, 7.322899294887537e-05, 0.00016501297082527748, 0.003524943200356487, 0.0013750914332629036, 9.319518612817809e-05, 0.0001908913649316541, 0.0002251575646626471, 0.0004348828101724248, 0.00016515365579490332, 0.0008657854550805373, 0.0009054133614867484, 5.65032646642774e-07, 5.703589113157709e-07, 0.0006042079132238616, 0.006069740434299126, 0.0010381928820872531, 0.00019764230398605466, 8.239971710998216e-05, 0.00233953887931024, 1.2482088863002187e-05, 0.00023284981923715398, 0.004911563589346455, 1.068983109066408e-05, 2.6366274596235133e-06, 0.0008309773654643493, 0.00036240896846325804, 0.0006375950017426911, 0.0017332444977394977, 0.0006354200551953874, 3.562865567964274e-05, 0.00031409910239383503, 0.00010036780076469661, 0.0002654822013145247, 0.0009363413893845148, 0.00014899830954353578, 0.00015381035893523488, 0.00041489027387741115]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0204, 0.0143, 0.0147, 0.0141, 0.0145, 0.0149, 0.0150, 0.0145, 0.0149,\n",
            "        0.0148, 0.0148, 0.0141, 0.0142, 0.0144, 0.0146, 0.0140, 0.0146, 0.0150,\n",
            "        0.0144, 0.0145, 0.0148, 0.0147, 0.0146, 0.0142, 0.0151, 0.0152, 0.0150,\n",
            "        0.0143, 0.0142, 0.0149, 0.0146, 0.0148, 0.0149, 0.0151, 0.0149, 0.0151,\n",
            "        0.0145, 0.0141, 0.0147, 0.0140, 0.0147, 0.0144, 0.0150, 0.0148, 0.0143,\n",
            "        0.0149, 0.0147, 0.0151, 0.0150, 0.0149, 0.0142, 0.0146, 0.0146, 0.0145,\n",
            "        0.0147, 0.0145, 0.0147, 0.0147, 0.0139, 0.0143, 0.0143, 0.0147, 0.0149,\n",
            "        0.0144, 0.0146, 0.0145, 0.0151, 0.0144], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [29]\n",
            "DEBUGGING: logits looks like: tensor([7.2095, 7.1380, 7.1445, 7.1355, 7.1415, 7.1467, 7.1480, 7.1405, 7.1459,\n",
            "        7.1448, 7.1454, 7.1356, 7.1372, 7.1403, 7.1428, 7.1347, 7.1425, 7.1478,\n",
            "        7.1397, 7.1405, 7.1448, 7.1444, 7.1419, 7.1365, 7.1487, 7.1499, 7.1478,\n",
            "        7.1387, 7.1376, 7.1470, 7.1428, 7.1452, 7.1467, 7.1489, 7.1470, 7.1494,\n",
            "        7.1414, 7.1352, 7.1435, 7.1339, 7.1440, 7.1404, 7.1484, 7.1449, 7.1384,\n",
            "        7.1462, 7.1443, 7.1497, 7.1473, 7.1472, 7.1369, 7.1430, 7.1420, 7.1406,\n",
            "        7.1434, 7.1409, 7.1435, 7.1435, 7.1330, 7.1388, 7.1379, 7.1443, 7.1460,\n",
            "        7.1401, 7.1430, 7.1417, 7.1494, 7.1397], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0184, 0.0130, 0.0127, 0.0128, 0.0130, 0.0129, 0.0130, 0.0127, 0.0129,\n",
            "        0.0129, 0.0130, 0.0129, 0.0124, 0.0128, 0.0129, 0.0129, 0.0130, 0.0128,\n",
            "        0.0130, 0.0129, 0.0127, 0.0124, 0.0129, 0.0142, 0.0129, 0.0131, 0.0131,\n",
            "        0.0131, 0.0130, 0.0129, 0.0129, 0.0128, 0.0130, 0.0128, 0.0130, 0.0129,\n",
            "        0.0130, 0.0130, 0.0131, 0.0128, 0.0127, 0.0130, 0.0130, 0.0129, 0.0129,\n",
            "        0.0129, 0.0128, 0.0128, 0.0131, 0.0129, 0.0129, 0.0129, 0.0127, 0.0127,\n",
            "        0.0131, 0.0129, 0.0129, 0.0130, 0.0129, 0.0131, 0.0130, 0.0133, 0.0131,\n",
            "        0.0128, 0.0128, 0.0127, 0.0128, 0.0129, 0.0131, 0.0130, 0.0129, 0.0134,\n",
            "        0.0126, 0.0129, 0.0126, 0.0129, 0.0129], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [67]\n",
            "DEBUGGING: logits looks like: tensor([7.2142, 7.1455, 7.1399, 7.1426, 7.1450, 7.1433, 7.1454, 7.1403, 7.1439,\n",
            "        7.1439, 7.1447, 7.1439, 7.1357, 7.1420, 7.1435, 7.1433, 7.1445, 7.1414,\n",
            "        7.1455, 7.1432, 7.1407, 7.1353, 7.1434, 7.1627, 7.1435, 7.1460, 7.1468,\n",
            "        7.1466, 7.1445, 7.1433, 7.1432, 7.1421, 7.1445, 7.1426, 7.1447, 7.1441,\n",
            "        7.1445, 7.1453, 7.1466, 7.1413, 7.1403, 7.1451, 7.1458, 7.1433, 7.1438,\n",
            "        7.1440, 7.1415, 7.1418, 7.1469, 7.1432, 7.1436, 7.1435, 7.1400, 7.1410,\n",
            "        7.1459, 7.1429, 7.1438, 7.1456, 7.1442, 7.1461, 7.1454, 7.1497, 7.1459,\n",
            "        7.1425, 7.1425, 7.1402, 7.1412, 7.1437, 7.1463, 7.1448, 7.1435, 7.1516,\n",
            "        7.1395, 7.1429, 7.1394, 7.1439, 7.1435], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0159, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113,\n",
            "        0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113,\n",
            "        0.0113, 0.0113, 0.0116, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113,\n",
            "        0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113,\n",
            "        0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113,\n",
            "        0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113,\n",
            "        0.0113, 0.0113, 0.0114, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113,\n",
            "        0.0113, 0.0114, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113,\n",
            "        0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113,\n",
            "        0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [33]\n",
            "DEBUGGING: logits looks like: tensor([7.2165, 7.1482, 7.1472, 7.1475, 7.1483, 7.1478, 7.1479, 7.1483, 7.1486,\n",
            "        7.1479, 7.1477, 7.1471, 7.1484, 7.1475, 7.1478, 7.1475, 7.1479, 7.1480,\n",
            "        7.1474, 7.1476, 7.1536, 7.1476, 7.1480, 7.1483, 7.1474, 7.1478, 7.1481,\n",
            "        7.1476, 7.1483, 7.1478, 7.1484, 7.1478, 7.1475, 7.1474, 7.1484, 7.1480,\n",
            "        7.1480, 7.1477, 7.1480, 7.1481, 7.1479, 7.1481, 7.1479, 7.1486, 7.1480,\n",
            "        7.1479, 7.1478, 7.1478, 7.1484, 7.1482, 7.1476, 7.1478, 7.1481, 7.1477,\n",
            "        7.1480, 7.1477, 7.1494, 7.1480, 7.1479, 7.1477, 7.1483, 7.1479, 7.1477,\n",
            "        7.1481, 7.1487, 7.1485, 7.1478, 7.1479, 7.1475, 7.1477, 7.1479, 7.1480,\n",
            "        7.1476, 7.1484, 7.1479, 7.1470, 7.1480, 7.1482, 7.1476, 7.1480, 7.1482,\n",
            "        7.1479, 7.1481, 7.1478, 7.1477, 7.1485, 7.1474, 7.1476],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0142, 0.0100, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0100, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0100, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0100, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0100, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [47]\n",
            "DEBUGGING: logits looks like: tensor([7.2194, 7.1506, 7.1507, 7.1508, 7.1507, 7.1508, 7.1511, 7.1508, 7.1508,\n",
            "        7.1510, 7.1506, 7.1511, 7.1509, 7.1508, 7.1510, 7.1511, 7.1508, 7.1509,\n",
            "        7.1507, 7.1508, 7.1510, 7.1509, 7.1509, 7.1509, 7.1507, 7.1508, 7.1510,\n",
            "        7.1508, 7.1508, 7.1510, 7.1508, 7.1508, 7.1509, 7.1508, 7.1508, 7.1508,\n",
            "        7.1509, 7.1511, 7.1510, 7.1510, 7.1509, 7.1511, 7.1506, 7.1507, 7.1508,\n",
            "        7.1510, 7.1511, 7.1508, 7.1509, 7.1508, 7.1507, 7.1509, 7.1507, 7.1508,\n",
            "        7.1509, 7.1508, 7.1507, 7.1510, 7.1509, 7.1508, 7.1510, 7.1508, 7.1509,\n",
            "        7.1511, 7.1509, 7.1509, 7.1509, 7.1508, 7.1510, 7.1509, 7.1510, 7.1508,\n",
            "        7.1508, 7.1509, 7.1509, 7.1506, 7.1509, 7.1509, 7.1508, 7.1510, 7.1508,\n",
            "        7.1509, 7.1509, 7.1511, 7.1509, 7.1509, 7.1509, 7.1508, 7.1511, 7.1508,\n",
            "        7.1509, 7.1509, 7.1507, 7.1510, 7.1509, 7.1507, 7.1508, 7.1510, 7.1508],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0131, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [40]\n",
            "DEBUGGING: logits looks like: tensor([7.2218, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533,\n",
            "        7.1533, 7.1533, 7.1533, 7.1534, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533,\n",
            "        7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533,\n",
            "        7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1534, 7.1533, 7.1533,\n",
            "        7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533,\n",
            "        7.1533, 7.1533, 7.1533, 7.1533, 7.1534, 7.1533, 7.1533, 7.1534, 7.1533,\n",
            "        7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533,\n",
            "        7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533,\n",
            "        7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1534, 7.1534,\n",
            "        7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533,\n",
            "        7.1533, 7.1534, 7.1533, 7.1534, 7.1533, 7.1533, 7.1533, 7.1533, 7.1533,\n",
            "        7.1533, 7.1533, 7.1533, 7.1533, 7.1533, 7.1534, 7.1533, 7.1534],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.16697318136084505 and immediate abs rewards look like: [0.017973709444504493, 0.02978440661672721, 0.026170163930146373, 0.0002080220110656228, 0.05755637961192406, 0.000671487116960634, 0.003205262206847692, 8.032464938878547e-05, 0.0003247301269766467, 0.0028349649264782784, 0.0006448123335758282, 0.0012400582404552551, 0.0008776006584412244, 0.0010311225200894114, 0.00368604119421434, 0.00030541150363205816, 2.554848379077157e-06, 0.0014783256988266658, 0.004476018273180671, 0.0006819536110924673, 0.0041695141076161235, 0.001428942275197187, 0.00010952971115330001, 0.0031242403711075895, 1.7681470126262866e-05, 0.0006222670108400052, 0.0016437558660982177, 2.8262594241823535e-05, 0.0001101549360100762, 8.998486009659246e-06, 4.543529394140933e-05, 5.3414208196045365e-05, 7.00685764059017e-05, 0.00028768738911821856, 5.896197217225563e-05, 5.247176886769012e-06, 0.00013376268498177524, 0.0005262931986180774, 0.0005316589522408321, 0.00016174241090993746, 3.5573143577494193e-06, 0.00010006546244767378, 3.6625548545998754e-05, 0.00013248080085759284, 4.770415853272425e-05, 0.00012216882987559075, 0.00011153816603837186, 2.975370989588555e-06, 4.495500752454973e-05, 1.4248689694795758e-07]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.39504743569838 and immediate relative rewards look like: [0.0062262971968122, 0.020764600390760043, 0.02765444030573353, 0.0002958206001510994, 0.10231863176665869, 0.0014623780620450955, 0.008145883494752716, 0.00023357217176046689, 0.0010623314654676184, 0.010306089494376836, 0.002581190503761842, 0.005416506731180258, 0.004154629737767, 0.005258588138101815, 0.02014862670147681, 0.001783130528878419, 1.585039967164575e-05, 0.009711117331272112, 0.031053202070521407, 0.0049883411744482305, 0.032032025118713434, 0.011518076799022864, 0.0009234849379071829, 0.02748800098585595, 0.0001622346265137607, 0.005937971612286262, 0.01629252020523181, 0.0002906828350336523, 0.0011734262557668686, 9.916584450654705e-05, 0.0005174017409403021, 0.0006278947785862056, 0.0008494265941720425, 0.003593350169417393, 0.000758203432978951, 6.940381037195017e-05, 0.001818413503819083, 0.007348329888403392, 0.007620071027380085, 0.0023780996478085146, 5.361398240903209e-05, 0.0015449201252868935, 0.0005789500586427026, 0.0021428904897025074, 0.0007891948836721979, 0.002066052581431021, 0.001927365087967767, 5.2510094772208775e-05, 0.0008099068466066252, 2.619467574989363e-06]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 3\n",
            "DEBUGGING: the action_prob is: tensor([0.0151, 0.0150, 0.0150, 0.0146, 0.0146, 0.0147, 0.0145, 0.0145, 0.0149,\n",
            "        0.0145, 0.0148, 0.0150, 0.0147, 0.0146, 0.0145, 0.0144, 0.0146, 0.0150,\n",
            "        0.0146, 0.0147, 0.0143, 0.0148, 0.0146, 0.0147, 0.0148, 0.0153, 0.0147,\n",
            "        0.0150, 0.0147, 0.0150, 0.0148, 0.0149, 0.0148, 0.0149, 0.0148, 0.0147,\n",
            "        0.0150, 0.0140, 0.0147, 0.0146, 0.0147, 0.0148, 0.0142, 0.0147, 0.0148,\n",
            "        0.0148, 0.0145, 0.0149, 0.0150, 0.0145, 0.0148, 0.0148, 0.0146, 0.0144,\n",
            "        0.0149, 0.0143, 0.0149, 0.0147, 0.0146, 0.0148, 0.0145, 0.0147, 0.0148,\n",
            "        0.0147, 0.0147, 0.0149, 0.0147, 0.0146], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [46]\n",
            "DEBUGGING: logits looks like: tensor([7.1158, 7.1151, 7.1147, 7.1095, 7.1091, 7.1105, 7.1079, 7.1082, 7.1131,\n",
            "        7.1079, 7.1122, 7.1152, 7.1104, 7.1092, 7.1079, 7.1075, 7.1091, 7.1155,\n",
            "        7.1096, 7.1113, 7.1057, 7.1117, 7.1091, 7.1112, 7.1119, 7.1195, 7.1105,\n",
            "        7.1150, 7.1116, 7.1147, 7.1125, 7.1139, 7.1121, 7.1140, 7.1125, 7.1113,\n",
            "        7.1148, 7.1018, 7.1110, 7.1093, 7.1114, 7.1121, 7.1035, 7.1117, 7.1119,\n",
            "        7.1124, 7.1076, 7.1134, 7.1150, 7.1083, 7.1119, 7.1119, 7.1096, 7.1076,\n",
            "        7.1137, 7.1052, 7.1133, 7.1112, 7.1091, 7.1129, 7.1083, 7.1105, 7.1125,\n",
            "        7.1106, 7.1107, 7.1137, 7.1106, 7.1091], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0131, 0.0126, 0.0126, 0.0128, 0.0126, 0.0124, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0126, 0.0126, 0.0127, 0.0127, 0.0126, 0.0127, 0.0126, 0.0127,\n",
            "        0.0126, 0.0126, 0.0127, 0.0127, 0.0126, 0.0126, 0.0126, 0.0126, 0.0127,\n",
            "        0.0127, 0.0128, 0.0127, 0.0127, 0.0127, 0.0126, 0.0126, 0.0127, 0.0126,\n",
            "        0.0126, 0.0126, 0.0127, 0.0127, 0.0127, 0.0126, 0.0126, 0.0127, 0.0122,\n",
            "        0.0127, 0.0127, 0.0126, 0.0127, 0.0127, 0.0127, 0.0126, 0.0127, 0.0126,\n",
            "        0.0127, 0.0126, 0.0127, 0.0126, 0.0131, 0.0126, 0.0127, 0.0127, 0.0126,\n",
            "        0.0127, 0.0127, 0.0126, 0.0125, 0.0127, 0.0126, 0.0127, 0.0127, 0.0126,\n",
            "        0.0127, 0.0127, 0.0127, 0.0126, 0.0126, 0.0128, 0.0128],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [55]\n",
            "DEBUGGING: logits looks like: tensor([7.1126, 7.1055, 7.1048, 7.1078, 7.1058, 7.1022, 7.1063, 7.1064, 7.1063,\n",
            "        7.1063, 7.1047, 7.1055, 7.1068, 7.1066, 7.1052, 7.1066, 7.1058, 7.1069,\n",
            "        7.1056, 7.1050, 7.1060, 7.1060, 7.1059, 7.1059, 7.1053, 7.1048, 7.1071,\n",
            "        7.1059, 7.1076, 7.1066, 7.1074, 7.1059, 7.1044, 7.1052, 7.1060, 7.1056,\n",
            "        7.1055, 7.1057, 7.1062, 7.1064, 7.1064, 7.1050, 7.1055, 7.1069, 7.0994,\n",
            "        7.1070, 7.1060, 7.1058, 7.1062, 7.1069, 7.1060, 7.1052, 7.1064, 7.1051,\n",
            "        7.1061, 7.1051, 7.1071, 7.1058, 7.1127, 7.1050, 7.1061, 7.1066, 7.1052,\n",
            "        7.1062, 7.1060, 7.1046, 7.1031, 7.1062, 7.1055, 7.1074, 7.1063, 7.1045,\n",
            "        7.1068, 7.1071, 7.1065, 7.1054, 7.1053, 7.1081, 7.1078],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0119, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0114, 0.0115, 0.0116, 0.0115, 0.0116, 0.0115, 0.0115, 0.0115, 0.0116,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0114, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0116, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0114, 0.0114,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0114, 0.0115, 0.0115, 0.0114, 0.0115, 0.0113, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0114, 0.0115, 0.0115,\n",
            "        0.0115, 0.0111, 0.0115, 0.0115, 0.0115, 0.0115],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [42]\n",
            "DEBUGGING: logits looks like: tensor([7.1101, 7.1025, 7.1022, 7.1030, 7.1026, 7.1033, 7.1036, 7.1028, 7.1025,\n",
            "        7.1029, 7.1029, 7.1027, 7.1032, 7.1037, 7.1027, 7.1024, 7.1033, 7.1028,\n",
            "        7.1020, 7.1026, 7.1045, 7.1024, 7.1041, 7.1027, 7.1026, 7.1024, 7.1054,\n",
            "        7.1027, 7.1029, 7.1035, 7.1023, 7.1027, 7.1030, 7.1035, 7.1027, 7.1031,\n",
            "        7.1030, 7.1028, 7.1019, 7.1031, 7.1033, 7.1034, 7.1036, 7.1026, 7.1027,\n",
            "        7.1035, 7.1042, 7.1029, 7.1035, 7.1035, 7.1024, 7.1024, 7.1017, 7.1016,\n",
            "        7.1038, 7.1028, 7.1033, 7.1025, 7.1032, 7.1037, 7.1027, 7.1024, 7.1029,\n",
            "        7.1013, 7.1032, 7.1026, 7.1015, 7.1030, 7.1001, 7.1027, 7.1036, 7.1028,\n",
            "        7.1034, 7.1023, 7.1030, 7.1034, 7.1032, 7.1038, 7.1017, 7.1028, 7.1032,\n",
            "        7.1025, 7.0967, 7.1032, 7.1032, 7.1032, 7.1034],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0104, 0.0101, 0.0101, 0.0100, 0.0101, 0.0101, 0.0104, 0.0101, 0.0101,\n",
            "        0.0101, 0.0100, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0102, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0100, 0.0102,\n",
            "        0.0101, 0.0101, 0.0101, 0.0102, 0.0101, 0.0102, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0099, 0.0102, 0.0101, 0.0101, 0.0101, 0.0100,\n",
            "        0.0100, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0102, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0102, 0.0101, 0.0101, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [98]\n",
            "DEBUGGING: logits looks like: tensor([7.1082, 7.1018, 7.1013, 7.0996, 7.1020, 7.1016, 7.1087, 7.1021, 7.1020,\n",
            "        7.1017, 7.1009, 7.1016, 7.1013, 7.1013, 7.1022, 7.1011, 7.1010, 7.1016,\n",
            "        7.1020, 7.1016, 7.1028, 7.1027, 7.1025, 7.1018, 7.1022, 7.1019, 7.1023,\n",
            "        7.1017, 7.1015, 7.1024, 7.1020, 7.1014, 7.1019, 7.1019, 7.1017, 7.1025,\n",
            "        7.1013, 7.1018, 7.1014, 7.1021, 7.1018, 7.1022, 7.1012, 7.1013, 7.1021,\n",
            "        7.1037, 7.1010, 7.1014, 7.1019, 7.1022, 7.1027, 7.1026, 7.1004, 7.1033,\n",
            "        7.1018, 7.1018, 7.1019, 7.1037, 7.1015, 7.1033, 7.1022, 7.1025, 7.1016,\n",
            "        7.1019, 7.1018, 7.1020, 7.0986, 7.1032, 7.1020, 7.1024, 7.1026, 7.1010,\n",
            "        7.0996, 7.1014, 7.1012, 7.1017, 7.1016, 7.1015, 7.1020, 7.1019, 7.1017,\n",
            "        7.1025, 7.1037, 7.1015, 7.1021, 7.1029, 7.1021, 7.1012, 7.1022, 7.1013,\n",
            "        7.1017, 7.1020, 7.1026, 7.1022, 7.1019, 7.1033, 7.1028, 7.1017, 7.1022],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0098, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0095,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0095,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0095, 0.0094, 0.0094, 0.0094, 0.0094, 0.0095, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [25]\n",
            "DEBUGGING: logits looks like: tensor([7.1066, 7.0998, 7.1002, 7.0994, 7.0998, 7.0999, 7.0997, 7.1002, 7.0997,\n",
            "        7.1000, 7.0997, 7.0999, 7.1001, 7.0997, 7.0998, 7.0996, 7.0999, 7.0997,\n",
            "        7.0995, 7.0997, 7.1001, 7.0996, 7.1000, 7.0999, 7.0996, 7.1000, 7.1001,\n",
            "        7.0999, 7.0998, 7.0996, 7.0998, 7.0997, 7.1001, 7.0998, 7.0997, 7.1005,\n",
            "        7.0996, 7.0998, 7.0999, 7.1000, 7.0999, 7.0992, 7.0998, 7.0998, 7.1004,\n",
            "        7.0999, 7.1000, 7.1000, 7.1001, 7.0999, 7.0999, 7.1001, 7.1000, 7.0999,\n",
            "        7.0994, 7.0999, 7.0996, 7.0999, 7.0999, 7.1000, 7.0997, 7.0997, 7.1001,\n",
            "        7.1001, 7.0999, 7.1001, 7.0999, 7.0990, 7.0999, 7.0999, 7.1000, 7.1001,\n",
            "        7.1000, 7.1001, 7.1010, 7.1000, 7.0998, 7.1001, 7.0991, 7.1004, 7.0998,\n",
            "        7.0998, 7.1000, 7.0997, 7.1001, 7.0998, 7.1000, 7.0998, 7.1000, 7.0997,\n",
            "        7.0998, 7.1001, 7.0999, 7.1002, 7.0997, 7.0998, 7.0999, 7.0999, 7.0999,\n",
            "        7.0997, 7.1001, 7.0998, 7.0998, 7.0996, 7.0997, 7.0999],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.020505750444954174 and immediate abs rewards look like: [0.0033830484758254897, 0.004920135279917304, 0.0011260425794716866, 0.00036119514516030904, 0.003040101339138346, 2.0648973531933734e-05, 0.002192045321862679, 5.522815854419605e-05, 0.00023423366883434937, 0.00015628320625182823, 0.00038477777798107127, 0.00020100534993616748, 0.00033865256409626454, 0.00047627042749809334, 4.0454915506416e-07, 0.00022295913277048385, 0.00047489599501204793, 6.487792052212171e-05, 0.0003676223045658844, 0.0005844866850566177, 4.977863227395574e-06, 9.357732778880745e-06, 4.697790245700162e-07, 2.0115647657803493e-05, 5.73824081584462e-06, 3.546113430275e-05, 5.197573864279548e-05, 6.006279818393523e-06, 0.00011934106714761583, 2.374117229919648e-06, 0.00019897833499271655, 0.0004200123426016944, 2.0361449514894048e-05, 1.4246159935282776e-05, 0.0002077917306451127, 3.159549214615254e-05, 8.054412319324911e-07, 2.70725413429318e-05, 0.0002477539783285465, 9.930195574270329e-06, 8.765909251451376e-05, 1.026709242069046e-05, 1.2193372185720364e-05, 2.3256960048456676e-05, 0.00014632530610469985, 6.465131855293293e-05, 3.687087428261293e-06, 2.1664508949470473e-05, 8.91481968210428e-05, 7.617387836944545e-06]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.06333718121645314 and immediate relative rewards look like: [0.0012502737728357178, 0.0036412229091785813, 0.001252298028069787, 0.0005358149800454062, 0.005638053594256868, 4.6005615377333256e-05, 0.005697863297912769, 0.00016419830036700262, 0.0007834636503021786, 0.0005808675848661282, 0.0015732318278980548, 0.0008966873375262326, 0.0016367499944464035, 0.0024792536557514335, 2.25672574662664e-06, 0.0013266658214971128, 0.0030026155657625346, 0.0004344086291328132, 0.002598333858536731, 0.004349141241630244, 3.890053723957131e-05, 7.661035404197879e-05, 4.020843034586908e-06, 0.00017965569937591794, 5.3384818126177125e-05, 0.0003431040837928402, 0.0005222399379825185, 6.25860639866514e-05, 0.0012879616536091787, 2.6506825633594163e-05, 0.002295631506737616, 0.005002405387530844, 0.00025012516052948757, 0.00018030790740964048, 0.0027073016512515246, 0.0004234494406122857, 1.1094676123527441e-05, 0.00038299384894152817, 0.0035972348977702513, 0.00014789088474823775, 0.0013381538457474724, 0.00016055955506791758, 0.0001952240599404421, 0.000381020769250586, 0.002451764441481948, 0.001107402929224086, 6.453007926524493e-05, 0.00038723231119273726, 0.0016266482416486083, 0.00014183241401617016]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 200 number of matrices\n",
            "DEBUGGING: ACT_MAT has 200 number of matrices\n",
            "DEBUGGING: VAL looks like: [[4.047219494893538, 4.085190306610262, 4.123727772157051, 4.131225525115112, 4.167849709869123, 4.209743694202026, 4.252248540400817, 4.294732166454266, 4.255713296776901, 4.296522349067818, 4.333663184006208, 4.36027925226544, 4.388045291160804, 4.360969778350998, 4.364557330273713, 4.400924587378088, 4.4453048491676785, 4.420820290640122, 4.376189162458171, 4.419202675527298, 4.4444007246169335, 4.448543265189922, 4.458037521139914, 4.493504847291835, 4.494016887741956, 4.522698867158277, 4.56785581165697, 4.597298750824719, 4.640020633611421, 4.683313834108141, 4.711736257221608, 4.7485601216779045, 4.795852338961208, 4.841381370111463, 4.889011270434792, 4.938316177615929, 4.98782323742561, 5.037804868454703, 5.0865211789568985, 5.104838140590382, 5.154858031301347, 5.1986614947583485, 5.1683355008070855, 5.212664161047441, 5.264600312207216, 5.301092890757678, 1.8118256169450552e-11, 1.1104930308648015e-11, 1.1217101321866683e-11, 7.579122514774976e-12], [0.05534136501370217, 0.054744405170995394, 0.05014192420303091, 0.04721795981583103, 0.04761352597231706, 0.04269247464985585, 0.037930344240431464, 0.036835210311427835, 0.032985635809242256, 0.03140338164071284, 0.030691553111619217, 0.030751297288985575, 0.025754575756470267, 0.025678529475130208, 0.025863939880991248, 0.025958512030470678, 0.022660170535468878, 0.021500079901218157, 0.02162311587382826, 0.02164871162514809, 0.021639953596449945, 0.021419263420482344, 0.02146879774210853, 0.020811123522250497, 0.020106777940165403, 0.020309305967190668, 0.020513874351797325, 0.02011077418037724, 0.014182862369775872, 0.013277443926958203, 0.013211920831284998, 0.013262142539570723, 0.011032932990162104, 0.01113176858717081, 0.011009008856498644, 0.006159035623386049, 0.006210450295247864, 0.006270518856351758, 0.005494486354431726, 0.005183916551483301, 0.004592243989636979, 0.0028878782746439213, 0.002275210322675287, 0.002262203704036004, 0.0019677824259011806, 0.0018862773991277617, 0.0016371668664780172, 0.0007079045223166691, 0.000564551730073872, 0.00041489027387741115], [0.34560691538271937, 0.34280870523829005, 0.32529707560356563, 0.30064912656346676, 0.3033871777407229, 0.20309954138794364, 0.20367390234939248, 0.197503049348121, 0.19926209815793994, 0.20020178453785084, 0.19181383337724647, 0.19114408371059052, 0.1876036131105154, 0.18530200340681657, 0.1818620356249644, 0.1633468777004925, 0.1631957042137516, 0.16482813516573733, 0.1566838563984497, 0.1268996508362912, 0.12314273703216462, 0.09203102213479918, 0.08132620740987506, 0.08121487118380594, 0.05426956585651514, 0.05465386992929432, 0.04920797809798794, 0.033247937265410234, 0.033290155990279374, 0.03244114114597223, 0.03266866192067241, 0.03247602038356778, 0.03216982384341573, 0.03163676489822595, 0.028326681544251067, 0.027846947587143553, 0.028058125027042025, 0.02650475911436661, 0.019349928511073956, 0.01184834089262007, 0.0095659002472844, 0.009608369964520575, 0.00814489882750877, 0.0076423724938041076, 0.005555032327375354, 0.0048139772158617734, 0.002775681448919952, 0.0008568852130830148, 0.0008125001195058647, 2.619467574989363e-06], [0.05225293623456547, 0.051517840870434095, 0.04836022016288436, 0.04758375973213593, 0.047523176517263156, 0.042308204972733625, 0.04268909025995585, 0.037364875719235434, 0.03757644183724084, 0.037164624431251174, 0.03695328974382328, 0.03573743223830831, 0.035192671616951596, 0.03389487032576283, 0.031732946131324644, 0.03205120141977577, 0.031034884442705715, 0.028315423108023416, 0.028162640887768285, 0.025822532352759147, 0.021690294051645357, 0.021870094458995744, 0.022013620308034106, 0.022231918651514665, 0.02227501308296843, 0.022446089156406317, 0.02232624754809442, 0.022024250111224143, 0.022183499037613626, 0.021106603418186314, 0.02129302686116436, 0.019189288236794694, 0.01433018469622611, 0.01422228235928952, 0.014183812577656444, 0.01159243527919689, 0.011281803877358186, 0.011384554748721878, 0.011112687777555908, 0.007591366545238037, 0.007518662283323029, 0.006242937815732885, 0.006143816424914109, 0.006008679156539057, 0.005684503421503506, 0.0032653929091126846, 0.002179787858473332, 0.0021366240194021077, 0.0017670623315246168, 0.00014183241401617016]]\n",
            "DEBUGGING: traj_returns = [4.047219494893538, 0.05534136501370217, 0.34560691538271937, 0.05225293623456547]\n",
            "DEBUGGING: actions = [[51], [38], [13], [4], [5], [46], [41], [27], [51], [49], [39], [22], [13], [69], [6], [37], [39], [76], [53], [11], [32], [48], [45], [43], [50], [39], [58], [8], [55], [74], [9], [47], [57], [43], [31], [55], [2], [9], [22], [63], [33], [87], [46], [20], [50], [0], [35], [74], [53], [29], [48], [47], [26], [33], [12], [60], [42], [47], [56], [58], [19], [10], [66], [9], [52], [3], [34], [53], [58], [28], [54], [11], [24], [71], [47], [62], [41], [27], [59], [12], [16], [42], [40], [75], [49], [43], [39], [70], [44], [41], [52], [26], [41], [75], [84], [62], [2], [53], [40], [97], [49], [53], [3], [41], [47], [25], [34], [25], [18], [29], [1], [43], [26], [50], [65], [63], [18], [11], [40], [67], [78], [2], [58], [14], [18], [54], [83], [79], [14], [33], [24], [72], [69], [54], [56], [43], [81], [84], [94], [47], [23], [53], [39], [10], [87], [7], [67], [54], [79], [40], [21], [58], [55], [3], [13], [35], [44], [44], [4], [46], [58], [3], [26], [37], [41], [67], [39], [1], [60], [55], [38], [57], [29], [28], [35], [71], [10], [73], [21], [42], [71], [88], [43], [9], [13], [38], [28], [36], [58], [98], [62], [87], [35], [82], [58], [16], [58], [97], [21], [25]]\n",
            "DEBUGGING: actions length = 200\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-1.0729, -0.4709,  0.1485,  ...,  0.2000,  0.4705, -0.6769],\n",
            "        [-1.0723, -0.4739,  0.1548,  ...,  0.2084,  0.4729, -0.6788],\n",
            "        [-1.0592, -0.4636,  0.1499,  ...,  0.2062,  0.4694, -0.6680],\n",
            "        ...,\n",
            "        [-1.0750, -0.4723,  0.1522,  ...,  0.2076,  0.4723, -0.6763],\n",
            "        [-1.0751, -0.4723,  0.1520,  ...,  0.2076,  0.4723, -0.6761],\n",
            "        [-1.0750, -0.4723,  0.1520,  ...,  0.2077,  0.4723, -0.6760]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-1.0761, -0.4731,  0.1530,  ...,  0.2081,  0.4727, -0.6771],\n",
            "        [-1.0750, -0.4723,  0.1520,  ...,  0.2077,  0.4723, -0.6760],\n",
            "        [-1.0751, -0.4723,  0.1520,  ...,  0.2077,  0.4723, -0.6761],\n",
            "        ...,\n",
            "        [-1.0750, -0.4722,  0.1520,  ...,  0.2077,  0.4723, -0.6760],\n",
            "        [-1.0750, -0.4722,  0.1520,  ...,  0.2077,  0.4723, -0.6760],\n",
            "        [-1.0750, -0.4723,  0.1520,  ...,  0.2077,  0.4723, -0.6761]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([7.1066, 7.0998, 7.1002, 7.0994, 7.0998, 7.0999, 7.0997, 7.1002, 7.0997,\n",
            "        7.1000, 7.0997, 7.0999, 7.1001, 7.0997, 7.0998, 7.0996, 7.0999, 7.0997,\n",
            "        7.0995, 7.0997, 7.1001, 7.0996, 7.1000, 7.0999, 7.0996, 7.1000, 7.1001,\n",
            "        7.0999, 7.0998, 7.0996, 7.0998, 7.0997, 7.1001, 7.0998, 7.0997, 7.1005,\n",
            "        7.0996, 7.0998, 7.0999, 7.1000, 7.0999, 7.0992, 7.0998, 7.0998, 7.1004,\n",
            "        7.0999, 7.1000, 7.1000, 7.1001, 7.0999, 7.0999, 7.1001, 7.1000, 7.0999,\n",
            "        7.0994, 7.0999, 7.0996, 7.0999, 7.0999, 7.1000, 7.0997, 7.0997, 7.1001,\n",
            "        7.1001, 7.0999, 7.1001, 7.0999, 7.0990, 7.0999, 7.0999, 7.1000, 7.1001,\n",
            "        7.1000, 7.1001, 7.1010, 7.1000, 7.0998, 7.1001, 7.0991, 7.1004, 7.0998,\n",
            "        7.0998, 7.1000, 7.0997, 7.1001, 7.0998, 7.1000, 7.0998, 7.1000, 7.0997,\n",
            "        7.0998, 7.1001, 7.0999, 7.1002, 7.0997, 7.0998, 7.0999, 7.0999, 7.0999,\n",
            "        7.0997, 7.1001, 7.0998, 7.0998, 7.0996, 7.0997, 7.0999],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[1.12510518e+00 1.13356531e+00 1.13688175e+00 1.13166909e+00\n",
            "  1.14159340e+00 1.12446098e+00 1.13413547e+00 1.14160883e+00\n",
            "  1.13138437e+00 1.14132303e+00 1.14828047e+00 1.15447802e+00\n",
            "  1.15914904e+00 1.15146130e+00 1.15100406e+00 1.15557029e+00\n",
            "  1.16554890e+00 1.15886598e+00 1.14566469e+00 1.14839339e+00\n",
            "  1.15271843e+00 1.14596591e+00 1.14571154e+00 1.15444069e+00\n",
            "  1.14766706e+00 1.15502703e+00 1.16497598e+00 1.16817043e+00\n",
            "  1.17741929e+00 1.18753476e+00 1.19472747e+00 1.20337189e+00\n",
            "  1.21334632e+00 1.22459305e+00 1.23563269e+00 1.24597865e+00\n",
            "  1.25834340e+00 1.27049118e+00 1.28061957e+00 1.28236544e+00\n",
            "  1.29413371e+00 1.30435017e+00 1.29622486e+00 1.30714435e+00\n",
            "  1.31945191e+00 1.32776463e+00 1.64815905e-03 9.25353441e-04\n",
            "  7.86028548e-04 1.39835541e-04]]\n",
            "DEBUGGING: baseline2 looks like: 1.1251051778811312\n",
            "DEBUGGING: ADS looks like: [ 2.41595751  2.45031475  2.4846166   2.50224592  2.57325758  2.63093253\n",
            "  2.67110271  2.7082066   2.6877116   2.74543428  2.79087354  2.83613059\n",
            "  2.89260093  2.87574994  2.87244482  2.91620925  2.97759311  3.00706532\n",
            "  3.00232341  3.12296745  3.18989867  3.23924974  3.27058252  3.35414754\n",
            "  3.38761709  3.48412656  3.52378225  3.54665351  3.64874882  3.72897537\n",
            "  3.82148487  3.97529923  4.11476069  4.24981142  4.32895569  4.43156796\n",
            "  4.47790013  4.61593978  4.7800688   4.82654364  4.87561083  4.97824834\n",
            "  4.94890089  4.99228086  5.08256877  5.1195023  -0.15137326 -0.151307\n",
            " -0.14340858 -0.03479023 -1.57592062 -1.58013115 -1.58896924 -1.58176165\n",
            " -1.5469786  -1.53611869 -1.54321549 -1.54969036 -1.53501606 -1.51968469\n",
            " -1.51209809 -1.49339736 -1.46968979 -1.45954131 -1.46624857 -1.45875683\n",
            " -1.44505156 -1.39225489 -1.35224264 -1.27458652 -1.2328621  -1.18787426\n",
            " -1.1659862  -1.11854618 -1.08629302 -1.018263   -1.02355968 -1.03053447\n",
            " -0.97708895 -0.94106102 -0.87703947 -0.75999875 -0.67005871 -0.58043818\n",
            " -0.54904657 -0.50058918 -0.50371266 -0.41559457 -0.30095789 -0.27311058\n",
            " -0.27465496 -0.21752527 -0.2171594  -0.2181211  -0.18006376 -0.17970431\n",
            " -0.14973609 -0.1505991  -0.14284403 -0.03437534 -1.28565507 -1.29206685\n",
            " -1.31381409 -1.32833048 -1.29120495 -1.37571162 -1.37747193 -1.38902252\n",
            " -1.3687396  -1.35088629 -1.35097581 -1.33300458 -1.30784075 -1.29991784\n",
            " -1.31025047 -1.32136846 -1.30451603 -1.24892684 -1.2171819  -1.16933558\n",
            " -1.13135931 -1.1172625  -1.10612879 -1.05814244 -1.05213024 -0.98391844\n",
            " -0.99486558 -1.01739731 -0.95798166 -0.92189732 -0.85758273 -0.74078487\n",
            " -0.64892182 -0.55993318 -0.53172889 -0.47890127 -0.48186498 -0.39536033\n",
            " -0.28710245 -0.26644616 -0.2696813  -0.21080478 -0.21128971 -0.21274093\n",
            " -0.17647651 -0.17677661 -0.14859758 -0.15045012 -0.14259608 -0.03478761\n",
            " -1.57900905 -1.58335771 -1.59075095 -1.58139585 -1.54706895 -1.53650296\n",
            " -1.53845674 -1.54916069 -1.53042526 -1.51392345 -1.50583636 -1.48841123\n",
            " -1.46025169 -1.45132497 -1.46037956 -1.45266414 -1.43667685 -1.38543955\n",
            " -1.34570311 -1.2704127  -1.23281176 -1.18742343 -1.16544138 -1.11712539\n",
            " -1.08412479 -1.01612622 -1.02174731 -1.02862099 -0.96908832 -0.93323186\n",
            " -0.86895836 -0.7540716  -0.66676146 -0.57734767 -0.54587176 -0.49515578\n",
            " -0.4986413  -0.41048054 -0.29533969 -0.27070313 -0.27172854 -0.21417021\n",
            " -0.21329079 -0.21437462 -0.17634704 -0.17832519 -0.14919347 -0.14917038\n",
            " -0.14164151 -0.0346484 ]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(0.5938, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-3.1972e-04,  3.4204e-04,  1.9522e-04,  ..., -1.8807e-04,\n",
            "          4.3838e-04, -2.8235e-03],\n",
            "        [-1.6469e-04,  1.7623e-04,  1.0057e-04,  ..., -9.6909e-05,\n",
            "          2.2583e-04, -1.4540e-03]])\n",
            "   Last layer:\n",
            "tensor([[ 2.1167e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  5.6317e-04,  1.4688e-03,  9.3533e-04,  1.4332e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.2226e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.5638e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  4.0575e-04,  1.1352e-03,  6.0837e-04,  9.1071e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  8.4361e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.1016e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -5.5384e-05, -1.4885e-04, -8.7622e-05, -1.3278e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1769e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.9267e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -5.1008e-04, -1.3496e-03, -8.3100e-04, -1.2680e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0982e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 3.3740e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  8.9176e-04,  2.3689e-03,  1.4449e-03,  2.2019e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.9156e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 3.2858e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  8.6482e-04,  2.3251e-03,  1.3777e-03,  2.0916e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.8441e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.5567e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -4.0794e-04, -1.1102e-03, -6.3859e-04, -9.6574e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -8.6344e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.5321e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -4.3424e-05, -9.3671e-05, -8.7979e-05, -1.3998e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0316e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-8.5834e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -2.2951e-04, -5.9014e-04, -3.8822e-04, -5.9721e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.0231e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.6726e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  4.4025e-04,  1.1840e-03,  7.0121e-04,  1.0646e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  9.3889e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-1.3322e-04,  3.6199e-04, -7.0712e-04,  ...,  8.6095e-04,\n",
            "         -4.6923e-04, -1.7117e-04],\n",
            "        [-6.9497e-05,  1.8905e-04, -3.6898e-04,  ...,  4.4945e-04,\n",
            "         -2.4492e-04, -7.3690e-05]])\n",
            "   Last layer:\n",
            "tensor([[-1.4268e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  5.8046e-05, -3.7572e-04,  1.9166e-04,  9.8258e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.6846e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 2.9691e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  1.1487e-04,  9.5693e-05,  1.9492e-04,  1.9231e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  9.2710e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.3199e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -4.5673e-05, -5.8743e-05, -7.3779e-05, -7.6699e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.7383e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 3.7092e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -6.4422e-05,  2.4206e-04, -1.7412e-04, -1.0856e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  9.7702e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.4112e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  1.6673e-04, -2.8293e-04,  3.7519e-04,  2.8014e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -7.9040e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.3546e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  1.1644e-04, -3.2042e-04,  2.8807e-04,  1.9532e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1780e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.9685e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  4.1553e-06,  2.7388e-04, -5.1142e-05,  6.9392e-06,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.4055e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.9551e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -6.6323e-05, -8.9985e-05, -1.0537e-04, -1.1120e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -7.1235e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-8.2763e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -6.1480e-05,  5.9415e-05, -1.2886e-04, -1.0344e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.3074e-06,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 2.5998e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  1.2310e-04,  1.7648e-05,  2.2803e-04,  2.0676e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.6585e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "DEBUGGING: training for one iteration takes 0.004379 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 44\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0154, 0.0147, 0.0148, 0.0145, 0.0148, 0.0145, 0.0147, 0.0147, 0.0148,\n",
            "        0.0146, 0.0147, 0.0148, 0.0148, 0.0147, 0.0147, 0.0146, 0.0145, 0.0146,\n",
            "        0.0148, 0.0146, 0.0148, 0.0147, 0.0146, 0.0148, 0.0147, 0.0148, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0148, 0.0148, 0.0145, 0.0146, 0.0147, 0.0147,\n",
            "        0.0146, 0.0146, 0.0147, 0.0145, 0.0146, 0.0148, 0.0146, 0.0148, 0.0148,\n",
            "        0.0147, 0.0147, 0.0148, 0.0145, 0.0147, 0.0147, 0.0147, 0.0147, 0.0149,\n",
            "        0.0147, 0.0147, 0.0148, 0.0147, 0.0148, 0.0146, 0.0147, 0.0146, 0.0148,\n",
            "        0.0147, 0.0147, 0.0147, 0.0146, 0.0147], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [29]\n",
            "DEBUGGING: logits looks like: tensor([7.8849, 7.8754, 7.8768, 7.8737, 7.8768, 7.8736, 7.8759, 7.8765, 7.8775,\n",
            "        7.8748, 7.8759, 7.8773, 7.8768, 7.8758, 7.8766, 7.8752, 7.8739, 7.8750,\n",
            "        7.8770, 7.8751, 7.8779, 7.8757, 7.8746, 7.8770, 7.8755, 7.8767, 7.8759,\n",
            "        7.8764, 7.8765, 7.8758, 7.8769, 7.8779, 7.8726, 7.8751, 7.8754, 7.8765,\n",
            "        7.8752, 7.8750, 7.8754, 7.8734, 7.8744, 7.8768, 7.8751, 7.8769, 7.8769,\n",
            "        7.8763, 7.8754, 7.8769, 7.8738, 7.8765, 7.8753, 7.8756, 7.8766, 7.8782,\n",
            "        7.8762, 7.8761, 7.8767, 7.8760, 7.8769, 7.8752, 7.8764, 7.8745, 7.8771,\n",
            "        7.8763, 7.8757, 7.8756, 7.8743, 7.8765], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0132, 0.0127, 0.0127, 0.0126, 0.0127, 0.0126, 0.0127, 0.0127, 0.0126,\n",
            "        0.0127, 0.0127, 0.0127, 0.0126, 0.0127, 0.0127, 0.0127, 0.0127, 0.0126,\n",
            "        0.0127, 0.0127, 0.0126, 0.0126, 0.0127, 0.0127, 0.0126, 0.0126, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0126, 0.0127, 0.0126, 0.0126, 0.0126, 0.0126,\n",
            "        0.0127, 0.0126, 0.0127, 0.0127, 0.0126, 0.0127, 0.0126, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0126, 0.0126, 0.0126, 0.0127, 0.0126, 0.0127,\n",
            "        0.0126, 0.0127, 0.0127, 0.0127, 0.0126, 0.0127, 0.0126, 0.0126, 0.0126,\n",
            "        0.0127, 0.0127, 0.0126, 0.0126, 0.0126, 0.0127, 0.0126, 0.0126, 0.0127,\n",
            "        0.0127, 0.0126, 0.0126, 0.0126, 0.0127, 0.0127, 0.0126],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [0]\n",
            "DEBUGGING: logits looks like: tensor([7.8871, 7.8779, 7.8779, 7.8778, 7.8779, 7.8778, 7.8780, 7.8779, 7.8778,\n",
            "        7.8779, 7.8778, 7.8779, 7.8778, 7.8782, 7.8781, 7.8780, 7.8780, 7.8778,\n",
            "        7.8779, 7.8779, 7.8778, 7.8778, 7.8780, 7.8779, 7.8778, 7.8778, 7.8779,\n",
            "        7.8779, 7.8779, 7.8779, 7.8778, 7.8779, 7.8777, 7.8777, 7.8778, 7.8778,\n",
            "        7.8779, 7.8778, 7.8779, 7.8779, 7.8778, 7.8779, 7.8778, 7.8779, 7.8779,\n",
            "        7.8778, 7.8779, 7.8779, 7.8778, 7.8778, 7.8778, 7.8779, 7.8776, 7.8780,\n",
            "        7.8778, 7.8780, 7.8779, 7.8780, 7.8778, 7.8778, 7.8777, 7.8775, 7.8776,\n",
            "        7.8779, 7.8779, 7.8777, 7.8778, 7.8778, 7.8779, 7.8778, 7.8778, 7.8778,\n",
            "        7.8779, 7.8778, 7.8777, 7.8778, 7.8779, 7.8779, 7.8778],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0112, 0.0110, 0.0113, 0.0113, 0.0113, 0.0113, 0.0111, 0.0111, 0.0113,\n",
            "        0.0113, 0.0113, 0.0113, 0.0111, 0.0115, 0.0112, 0.0106, 0.0113, 0.0113,\n",
            "        0.0111, 0.0110, 0.0113, 0.0112, 0.0113, 0.0113, 0.0113, 0.0111, 0.0113,\n",
            "        0.0112, 0.0112, 0.0114, 0.0112, 0.0112, 0.0110, 0.0114, 0.0112, 0.0112,\n",
            "        0.0112, 0.0113, 0.0112, 0.0111, 0.0112, 0.0112, 0.0113, 0.0112, 0.0113,\n",
            "        0.0113, 0.0112, 0.0112, 0.0112, 0.0110, 0.0114, 0.0113, 0.0113, 0.0112,\n",
            "        0.0113, 0.0114, 0.0113, 0.0111, 0.0112, 0.0111, 0.0112, 0.0112, 0.0113,\n",
            "        0.0113, 0.0113, 0.0113, 0.0112, 0.0113, 0.0112, 0.0113, 0.0111, 0.0113,\n",
            "        0.0113, 0.0113, 0.0113, 0.0113, 0.0111, 0.0112, 0.0113, 0.0116, 0.0113,\n",
            "        0.0112, 0.0110, 0.0112, 0.0114, 0.0112, 0.0113, 0.0113, 0.0113],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [31]\n",
            "DEBUGGING: logits looks like: tensor([7.8710, 7.8677, 7.8720, 7.8727, 7.8723, 7.8726, 7.8687, 7.8689, 7.8714,\n",
            "        7.8728, 7.8724, 7.8730, 7.8694, 7.8750, 7.8706, 7.8592, 7.8720, 7.8732,\n",
            "        7.8691, 7.8676, 7.8722, 7.8707, 7.8721, 7.8719, 7.8724, 7.8696, 7.8726,\n",
            "        7.8711, 7.8710, 7.8735, 7.8714, 7.8710, 7.8663, 7.8749, 7.8703, 7.8713,\n",
            "        7.8711, 7.8728, 7.8712, 7.8682, 7.8706, 7.8704, 7.8715, 7.8700, 7.8726,\n",
            "        7.8725, 7.8713, 7.8710, 7.8706, 7.8670, 7.8740, 7.8731, 7.8727, 7.8709,\n",
            "        7.8715, 7.8734, 7.8718, 7.8696, 7.8701, 7.8687, 7.8713, 7.8709, 7.8721,\n",
            "        7.8719, 7.8724, 7.8719, 7.8702, 7.8720, 7.8701, 7.8717, 7.8690, 7.8717,\n",
            "        7.8720, 7.8722, 7.8724, 7.8723, 7.8685, 7.8702, 7.8715, 7.8768, 7.8715,\n",
            "        7.8711, 7.8673, 7.8707, 7.8733, 7.8709, 7.8728, 7.8729, 7.8722],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0105, 0.0104, 0.0105, 0.0104,\n",
            "        0.0104, 0.0106, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0105, 0.0104, 0.0106, 0.0104,\n",
            "        0.0105, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0105, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0101, 0.0104, 0.0105, 0.0104, 0.0104, 0.0105,\n",
            "        0.0105, 0.0105, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0105, 0.0104, 0.0104, 0.0104, 0.0104, 0.0105,\n",
            "        0.0104, 0.0104, 0.0105, 0.0105, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [56]\n",
            "DEBUGGING: logits looks like: tensor([7.8737, 7.8735, 7.8733, 7.8731, 7.8729, 7.8742, 7.8734, 7.8748, 7.8726,\n",
            "        7.8731, 7.8774, 7.8727, 7.8735, 7.8733, 7.8735, 7.8737, 7.8737, 7.8731,\n",
            "        7.8735, 7.8730, 7.8733, 7.8734, 7.8725, 7.8741, 7.8740, 7.8770, 7.8736,\n",
            "        7.8740, 7.8732, 7.8727, 7.8727, 7.8730, 7.8730, 7.8746, 7.8732, 7.8730,\n",
            "        7.8733, 7.8730, 7.8728, 7.8739, 7.8738, 7.8729, 7.8727, 7.8731, 7.8735,\n",
            "        7.8725, 7.8721, 7.8735, 7.8730, 7.8729, 7.8734, 7.8726, 7.8734, 7.8734,\n",
            "        7.8730, 7.8722, 7.8733, 7.8675, 7.8731, 7.8744, 7.8737, 7.8726, 7.8743,\n",
            "        7.8740, 7.8741, 7.8736, 7.8731, 7.8730, 7.8736, 7.8733, 7.8730, 7.8728,\n",
            "        7.8735, 7.8727, 7.8731, 7.8746, 7.8732, 7.8724, 7.8724, 7.8728, 7.8742,\n",
            "        7.8737, 7.8734, 7.8742, 7.8744, 7.8737, 7.8739, 7.8731, 7.8737, 7.8733,\n",
            "        7.8728, 7.8732, 7.8733, 7.8734, 7.8731, 7.8727],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0092, 0.0092, 0.0091, 0.0094, 0.0092, 0.0100, 0.0092, 0.0095, 0.0094,\n",
            "        0.0093, 0.0092, 0.0091, 0.0094, 0.0088, 0.0091, 0.0093, 0.0093, 0.0091,\n",
            "        0.0095, 0.0092, 0.0094, 0.0090, 0.0092, 0.0094, 0.0092, 0.0093, 0.0093,\n",
            "        0.0090, 0.0093, 0.0095, 0.0095, 0.0093, 0.0096, 0.0093, 0.0091, 0.0090,\n",
            "        0.0091, 0.0092, 0.0093, 0.0096, 0.0093, 0.0094, 0.0098, 0.0092, 0.0092,\n",
            "        0.0099, 0.0092, 0.0093, 0.0093, 0.0093, 0.0092, 0.0098, 0.0092, 0.0100,\n",
            "        0.0092, 0.0093, 0.0095, 0.0095, 0.0094, 0.0092, 0.0094, 0.0093, 0.0095,\n",
            "        0.0095, 0.0091, 0.0096, 0.0095, 0.0092, 0.0093, 0.0093, 0.0101, 0.0093,\n",
            "        0.0099, 0.0086, 0.0091, 0.0093, 0.0092, 0.0093, 0.0093, 0.0094, 0.0093,\n",
            "        0.0094, 0.0093, 0.0093, 0.0093, 0.0094, 0.0092, 0.0097, 0.0095, 0.0093,\n",
            "        0.0093, 0.0095, 0.0094, 0.0094, 0.0096, 0.0095, 0.0093, 0.0095, 0.0097,\n",
            "        0.0091, 0.0090, 0.0094, 0.0097, 0.0092, 0.0095, 0.0093, 0.0095],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [88]\n",
            "DEBUGGING: logits looks like: tensor([7.8688, 7.8689, 7.8653, 7.8718, 7.8691, 7.8844, 7.8689, 7.8754, 7.8735,\n",
            "        7.8700, 7.8679, 7.8655, 7.8715, 7.8589, 7.8670, 7.8694, 7.8696, 7.8668,\n",
            "        7.8748, 7.8673, 7.8729, 7.8630, 7.8683, 7.8715, 7.8677, 7.8701, 7.8700,\n",
            "        7.8646, 7.8708, 7.8738, 7.8743, 7.8707, 7.8775, 7.8714, 7.8669, 7.8628,\n",
            "        7.8658, 7.8674, 7.8697, 7.8774, 7.8705, 7.8724, 7.8813, 7.8685, 7.8687,\n",
            "        7.8819, 7.8681, 7.8703, 7.8693, 7.8708, 7.8672, 7.8809, 7.8691, 7.8847,\n",
            "        7.8677, 7.8693, 7.8736, 7.8744, 7.8724, 7.8671, 7.8726, 7.8708, 7.8752,\n",
            "        7.8756, 7.8655, 7.8762, 7.8755, 7.8672, 7.8707, 7.8698, 7.8861, 7.8707,\n",
            "        7.8830, 7.8558, 7.8663, 7.8706, 7.8689, 7.8698, 7.8711, 7.8727, 7.8703,\n",
            "        7.8716, 7.8695, 7.8710, 7.8694, 7.8719, 7.8686, 7.8798, 7.8748, 7.8699,\n",
            "        7.8700, 7.8737, 7.8720, 7.8730, 7.8777, 7.8751, 7.8714, 7.8736, 7.8790,\n",
            "        7.8650, 7.8648, 7.8733, 7.8791, 7.8689, 7.8748, 7.8706, 7.8747],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.7082653196289357 and immediate abs rewards look like: [0.0515991779216165, 0.03714726554790104, 0.0007120273648979492, 0.0014973191637182026, 0.009248168783415167, 0.003750296275029541, 0.009508447761163552, 0.00474690176315562, 0.00033359401822963264, 0.0019189744252798846, 0.00017581195606908295, 2.1896191356063355e-05, 0.0011135510640087887, 4.798443978870637e-05, 0.0002913873945544765, 0.00016488396113345516, 0.00019080996707998565, 9.977531135518802e-06, 8.076164704107214e-06, 0.5857787679251487, 0.0, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "DEBUGGING: the total relative reward of the trajectory = 3.3657292399498293 and immediate relative rewards look like: [0.013914640262824097, 0.020317559278544995, 0.0005901563627057966, 0.0016550418149238837, 0.012783214579348324, 0.006236528539788806, 0.018466534418335474, 0.010563921904292853, 0.000836295144805051, 0.005345746441048304, 0.0005390294388350359, 7.323901614189615e-05, 0.004035049026566482, 0.00018730898553123162, 0.0012187032717085099, 0.0007356472895941318, 0.0009045679387697244, 5.008525841632267e-05, 4.279315420851952e-05, 3.2672331777123302, 0.0, 3.3348139065002315e-12, 0.0, 0.0, 3.7895612573872005e-12, 3.941143707682092e-12, 0.0, 4.2443086082736646e-12, 0.0, 9.094947017730662e-12, 4.699055959159417e-12, 0.0, 5.002220859751105e-12, 5.153803310047375e-12, 5.305385760342081e-12, 1.0913936421273484e-11, 5.608550660933908e-12, 5.760133111228545e-12, 5.911715561523138e-12, 6.063298011819521e-12, 1.2429760924231904e-11, 6.366462912409532e-12, 0.0, 6.6696278130014735e-12, 6.8212102632979966e-12, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0156, 0.0154, 0.0135, 0.0147, 0.0147, 0.0148, 0.0145, 0.0148, 0.0141,\n",
            "        0.0145, 0.0148, 0.0139, 0.0157, 0.0150, 0.0152, 0.0137, 0.0149, 0.0145,\n",
            "        0.0136, 0.0143, 0.0148, 0.0150, 0.0144, 0.0145, 0.0131, 0.0143, 0.0151,\n",
            "        0.0145, 0.0141, 0.0145, 0.0148, 0.0149, 0.0146, 0.0139, 0.0137, 0.0149,\n",
            "        0.0151, 0.0149, 0.0145, 0.0145, 0.0141, 0.0148, 0.0151, 0.0141, 0.0145,\n",
            "        0.0135, 0.0141, 0.0148, 0.0142, 0.0136, 0.0140, 0.0148, 0.0138, 0.0144,\n",
            "        0.0150, 0.0146, 0.0135, 0.0145, 0.0145, 0.0150, 0.0147, 0.0146, 0.0149,\n",
            "        0.0150, 0.0132, 0.0148, 0.0147, 0.0143, 0.0146],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [6]\n",
            "DEBUGGING: logits looks like: tensor([7.8428, 7.8403, 7.8148, 7.8313, 7.8315, 7.8322, 7.8290, 7.8333, 7.8226,\n",
            "        7.8290, 7.8325, 7.8198, 7.8441, 7.8356, 7.8380, 7.8179, 7.8342, 7.8286,\n",
            "        7.8161, 7.8264, 7.8321, 7.8355, 7.8271, 7.8280, 7.8088, 7.8255, 7.8367,\n",
            "        7.8284, 7.8231, 7.8287, 7.8323, 7.8343, 7.8294, 7.8203, 7.8178, 7.8343,\n",
            "        7.8371, 7.8341, 7.8287, 7.8292, 7.8228, 7.8330, 7.8373, 7.8232, 7.8280,\n",
            "        7.8147, 7.8232, 7.8331, 7.8245, 7.8166, 7.8212, 7.8324, 7.8183, 7.8266,\n",
            "        7.8360, 7.8296, 7.8139, 7.8292, 7.8291, 7.8352, 7.8314, 7.8298, 7.8346,\n",
            "        7.8358, 7.8104, 7.8321, 7.8311, 7.8261, 7.8306],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0135, 0.0127, 0.0126, 0.0127, 0.0127, 0.0126, 0.0126, 0.0126, 0.0126,\n",
            "        0.0128, 0.0127, 0.0127, 0.0127, 0.0126, 0.0127, 0.0127, 0.0126, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0126, 0.0126, 0.0126, 0.0127, 0.0127, 0.0126,\n",
            "        0.0126, 0.0126, 0.0126, 0.0126, 0.0126, 0.0128, 0.0126, 0.0126, 0.0127,\n",
            "        0.0127, 0.0126, 0.0127, 0.0126, 0.0127, 0.0126, 0.0127, 0.0127, 0.0125,\n",
            "        0.0128, 0.0127, 0.0127, 0.0127, 0.0126, 0.0127, 0.0126, 0.0126, 0.0126,\n",
            "        0.0126, 0.0127, 0.0127, 0.0126, 0.0126, 0.0126, 0.0127, 0.0126, 0.0127,\n",
            "        0.0127, 0.0126, 0.0127, 0.0126, 0.0128, 0.0126, 0.0127, 0.0126, 0.0126,\n",
            "        0.0126, 0.0126, 0.0126, 0.0126, 0.0127, 0.0127, 0.0126],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [9]\n",
            "DEBUGGING: logits looks like: tensor([7.8390, 7.8262, 7.8255, 7.8268, 7.8262, 7.8251, 7.8254, 7.8247, 7.8249,\n",
            "        7.8282, 7.8264, 7.8262, 7.8265, 7.8246, 7.8259, 7.8263, 7.8249, 7.8261,\n",
            "        7.8262, 7.8260, 7.8261, 7.8250, 7.8251, 7.8257, 7.8261, 7.8266, 7.8255,\n",
            "        7.8253, 7.8258, 7.8255, 7.8254, 7.8251, 7.8277, 7.8258, 7.8244, 7.8262,\n",
            "        7.8265, 7.8257, 7.8265, 7.8252, 7.8265, 7.8249, 7.8269, 7.8259, 7.8243,\n",
            "        7.8277, 7.8259, 7.8264, 7.8261, 7.8259, 7.8260, 7.8252, 7.8256, 7.8257,\n",
            "        7.8247, 7.8264, 7.8264, 7.8250, 7.8253, 7.8258, 7.8262, 7.8252, 7.8261,\n",
            "        7.8264, 7.8254, 7.8260, 7.8258, 7.8281, 7.8248, 7.8265, 7.8257, 7.8257,\n",
            "        7.8255, 7.8258, 7.8252, 7.8255, 7.8260, 7.8260, 7.8259],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0120, 0.0114, 0.0113, 0.0114, 0.0114, 0.0113, 0.0114, 0.0113, 0.0114,\n",
            "        0.0112, 0.0114, 0.0113, 0.0114, 0.0114, 0.0113, 0.0113, 0.0113, 0.0114,\n",
            "        0.0114, 0.0113, 0.0114, 0.0114, 0.0113, 0.0114, 0.0112, 0.0113, 0.0114,\n",
            "        0.0114, 0.0113, 0.0113, 0.0113, 0.0113, 0.0114, 0.0114, 0.0113, 0.0114,\n",
            "        0.0113, 0.0113, 0.0114, 0.0114, 0.0113, 0.0114, 0.0113, 0.0114, 0.0113,\n",
            "        0.0113, 0.0114, 0.0114, 0.0114, 0.0113, 0.0114, 0.0114, 0.0114, 0.0113,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0113, 0.0114,\n",
            "        0.0115, 0.0114, 0.0114, 0.0113, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0113, 0.0113, 0.0113, 0.0114, 0.0114, 0.0113,\n",
            "        0.0113, 0.0114, 0.0114, 0.0113, 0.0114, 0.0113, 0.0114],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [8]\n",
            "DEBUGGING: logits looks like: tensor([7.8393, 7.8278, 7.8271, 7.8277, 7.8278, 7.8270, 7.8280, 7.8267, 7.8283,\n",
            "        7.8241, 7.8279, 7.8273, 7.8277, 7.8285, 7.8271, 7.8267, 7.8271, 7.8288,\n",
            "        7.8277, 7.8272, 7.8274, 7.8282, 7.8273, 7.8276, 7.8255, 7.8272, 7.8286,\n",
            "        7.8279, 7.8268, 7.8274, 7.8274, 7.8273, 7.8276, 7.8281, 7.8269, 7.8280,\n",
            "        7.8272, 7.8268, 7.8287, 7.8275, 7.8272, 7.8284, 7.8266, 7.8276, 7.8273,\n",
            "        7.8264, 7.8276, 7.8277, 7.8281, 7.8267, 7.8279, 7.8278, 7.8274, 7.8266,\n",
            "        7.8275, 7.8276, 7.8275, 7.8275, 7.8275, 7.8279, 7.8275, 7.8272, 7.8279,\n",
            "        7.8292, 7.8276, 7.8275, 7.8272, 7.8283, 7.8284, 7.8278, 7.8282, 7.8279,\n",
            "        7.8287, 7.8275, 7.8278, 7.8273, 7.8274, 7.8270, 7.8280, 7.8280, 7.8262,\n",
            "        7.8273, 7.8275, 7.8275, 7.8273, 7.8280, 7.8272, 7.8278],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0107, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0100,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [27]\n",
            "DEBUGGING: logits looks like: tensor([7.8393, 7.8275, 7.8274, 7.8274, 7.8273, 7.8274, 7.8283, 7.8276, 7.8273,\n",
            "        7.8280, 7.8274, 7.8281, 7.8274, 7.8272, 7.8281, 7.8273, 7.8272, 7.8271,\n",
            "        7.8276, 7.8271, 7.8275, 7.8274, 7.8276, 7.8276, 7.8271, 7.8278, 7.8274,\n",
            "        7.8275, 7.8269, 7.8274, 7.8275, 7.8279, 7.8274, 7.8276, 7.8275, 7.8271,\n",
            "        7.8279, 7.8275, 7.8273, 7.8275, 7.8274, 7.8276, 7.8280, 7.8275, 7.8277,\n",
            "        7.8275, 7.8272, 7.8278, 7.8275, 7.8272, 7.8285, 7.8280, 7.8275, 7.8276,\n",
            "        7.8276, 7.8272, 7.8274, 7.8274, 7.8274, 7.8272, 7.8276, 7.8278, 7.8272,\n",
            "        7.8274, 7.8275, 7.8276, 7.8275, 7.8275, 7.8273, 7.8274, 7.8268, 7.8275,\n",
            "        7.8276, 7.8272, 7.8277, 7.8274, 7.8277, 7.8274, 7.8274, 7.8276, 7.8281,\n",
            "        7.8280, 7.8278, 7.8275, 7.8274, 7.8270, 7.8274, 7.8274, 7.8277, 7.8260,\n",
            "        7.8275, 7.8272, 7.8273, 7.8273, 7.8273, 7.8271, 7.8273, 7.8277, 7.8276],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0099, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0102, 0.0097, 0.0097,\n",
            "        0.0102, 0.0095, 0.0103, 0.0094, 0.0103, 0.0093, 0.0093, 0.0093, 0.0094,\n",
            "        0.0094, 0.0094, 0.0093, 0.0093, 0.0103, 0.0093, 0.0095, 0.0093, 0.0094,\n",
            "        0.0093, 0.0094, 0.0093, 0.0095, 0.0094, 0.0093, 0.0094, 0.0094, 0.0105,\n",
            "        0.0094, 0.0094, 0.0094, 0.0093, 0.0098, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0094, 0.0095, 0.0094, 0.0094, 0.0093, 0.0094, 0.0093,\n",
            "        0.0094, 0.0095, 0.0096, 0.0093, 0.0094, 0.0093, 0.0094, 0.0093, 0.0094,\n",
            "        0.0093, 0.0093, 0.0094, 0.0094, 0.0094, 0.0094, 0.0093, 0.0094, 0.0094,\n",
            "        0.0093, 0.0094, 0.0093, 0.0093, 0.0094, 0.0094, 0.0094, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0094, 0.0094, 0.0093, 0.0093, 0.0094,\n",
            "        0.0094, 0.0094, 0.0093, 0.0093, 0.0094, 0.0094, 0.0093, 0.0093, 0.0093,\n",
            "        0.0094, 0.0093, 0.0093, 0.0094, 0.0098, 0.0093, 0.0094],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [7]\n",
            "DEBUGGING: logits looks like: tensor([7.8393, 7.8274, 7.8270, 7.8256, 7.8271, 7.8273, 7.8454, 7.8353, 7.8351,\n",
            "        7.8454, 7.8310, 7.8473, 7.8276, 7.8477, 7.8272, 7.8272, 7.8274, 7.8279,\n",
            "        7.8277, 7.8277, 7.8275, 7.8274, 7.8477, 7.8258, 7.8315, 7.8272, 7.8276,\n",
            "        7.8275, 7.8280, 7.8274, 7.8306, 7.8295, 7.8271, 7.8276, 7.8286, 7.8510,\n",
            "        7.8294, 7.8293, 7.8295, 7.8275, 7.8361, 7.8260, 7.8258, 7.8275, 7.8271,\n",
            "        7.8271, 7.8272, 7.8277, 7.8299, 7.8282, 7.8291, 7.8275, 7.8278, 7.8255,\n",
            "        7.8277, 7.8300, 7.8320, 7.8275, 7.8278, 7.8272, 7.8277, 7.8274, 7.8279,\n",
            "        7.8272, 7.8274, 7.8280, 7.8277, 7.8276, 7.8278, 7.8269, 7.8277, 7.8277,\n",
            "        7.8274, 7.8279, 7.8275, 7.8273, 7.8278, 7.8278, 7.8282, 7.8276, 7.8270,\n",
            "        7.8275, 7.8271, 7.8275, 7.8271, 7.8277, 7.8283, 7.8271, 7.8275, 7.8285,\n",
            "        7.8288, 7.8290, 7.8273, 7.8267, 7.8280, 7.8279, 7.8271, 7.8275, 7.8275,\n",
            "        7.8278, 7.8268, 7.8266, 7.8282, 7.8362, 7.8275, 7.8296],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.08811586424326379 and immediate abs rewards look like: [0.01472099603006427, 0.008120340202367515, 0.001615783796296455, 0.004697656700500374, 0.004917403131003084, 0.0020860390372945403, 0.0022653108617305406, 0.0009002476181194652, 0.008415288926244102, 0.005621533499834186, 0.005687365669018618, 0.0013323868229235813, 0.004426489566867531, 0.003822281072189071, 0.006938255435670726, 0.0016759269733483961, 0.0005614432507172751, 4.8859430535230786e-05, 0.000464584063138318, 0.001511425465650973, 1.900593542814022e-05, 1.7445618141209707e-06, 1.6448831502202665e-05, 2.9338661988731474e-07, 0.00020941998900525505, 0.0011752042055377387, 0.0008582867817494844, 0.0005479814676618844, 0.0014680099188808526, 0.0014979836605562014, 4.3677763642335776e-05, 0.00015929111623336212, 6.357974598358851e-06, 2.7196774681215174e-06, 4.881921449850779e-05, 2.269348851768882e-05, 1.7808109987527132e-05, 0.0007191800896180212, 6.17765317656449e-06, 1.9445041289145593e-05, 2.805432814056985e-06, 1.2218194569868501e-05, 2.8345940336294007e-05, 0.00010587651604510029, 7.044832318570116e-05, 0.0001678868243288889, 0.00029483692242138204, 6.551463229698129e-07, 0.0006712535005135578, 9.137102142631193e-05]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.29169267618613237 and immediate relative rewards look like: [0.004746700863243581, 0.005261690055402813, 0.0015745976981914505, 0.006107090873616181, 0.008003178596570156, 0.004080619276409602, 0.0051733719970248745, 0.0023513696805153366, 0.024734785808448862, 0.018409696759873757, 0.020525603205133757, 0.005255509210887223, 0.01892327360433182, 0.017622875066331707, 0.03431742353831105, 0.008862228306654493, 0.0031561909125542072, 0.00029087742845753246, 0.0029195374008192695, 0.00999953037591912, 0.00013209567808894406, 1.2702577089993215e-05, 0.0001252118503049366, 2.3304321075214808e-06, 0.0017327786692896226, 0.010113506067900152, 0.007673265192152557, 0.005081961423375875, 0.0141030576231687, 0.014894498245677063, 0.00044898859333718705, 0.0016902893074778119, 6.957851882600266e-05, 3.066476825882996e-05, 0.000566633947694736, 0.00027092841258919613, 0.00021851115431234926, 0.009063123016212886, 7.991868795719192e-05, 0.0002580060955919202, 3.815466423695377e-05, 0.0001702239399291204, 0.00040432015912459637, 0.0015453344036853436, 0.0010516435979146684, 0.0025619461886679344, 0.004597264309800017, 1.0433782361917486e-05, 0.010913020379733124, 0.0015161338405683994]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0212, 0.0150, 0.0143, 0.0144, 0.0143, 0.0148, 0.0148, 0.0150, 0.0144,\n",
            "        0.0143, 0.0148, 0.0149, 0.0145, 0.0152, 0.0148, 0.0137, 0.0148, 0.0151,\n",
            "        0.0151, 0.0154, 0.0147, 0.0149, 0.0150, 0.0147, 0.0147, 0.0149, 0.0148,\n",
            "        0.0149, 0.0154, 0.0147, 0.0144, 0.0148, 0.0151, 0.0148, 0.0150, 0.0150,\n",
            "        0.0147, 0.0140, 0.0150, 0.0152, 0.0145, 0.0147, 0.0149, 0.0145, 0.0151,\n",
            "        0.0151, 0.0147, 0.0147, 0.0148, 0.0152, 0.0151, 0.0149, 0.0148, 0.0149,\n",
            "        0.0160, 0.0147, 0.0145, 0.0150, 0.0146, 0.0150, 0.0147, 0.0158, 0.0150,\n",
            "        0.0151, 0.0147, 0.0147, 0.0151], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [27]\n",
            "DEBUGGING: logits looks like: tensor([7.9331, 7.8643, 7.8549, 7.8558, 7.8546, 7.8619, 7.8617, 7.8637, 7.8557,\n",
            "        7.8550, 7.8617, 7.8625, 7.8574, 7.8667, 7.8617, 7.8456, 7.8616, 7.8656,\n",
            "        7.8651, 7.8691, 7.8608, 7.8632, 7.8639, 7.8601, 7.8602, 7.8630, 7.8615,\n",
            "        7.8624, 7.8695, 7.8597, 7.8558, 7.8621, 7.8654, 7.8611, 7.8638, 7.8643,\n",
            "        7.8600, 7.8499, 7.8642, 7.8672, 7.8578, 7.8604, 7.8626, 7.8577, 7.8653,\n",
            "        7.8656, 7.8598, 7.8605, 7.8612, 7.8672, 7.8651, 7.8633, 7.8611, 7.8625,\n",
            "        7.8768, 7.8607, 7.8577, 7.8646, 7.8581, 7.8638, 7.8604, 7.8748, 7.8644,\n",
            "        7.8649, 7.8603, 7.8598, 7.8652], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0183, 0.0129, 0.0129, 0.0129, 0.0128, 0.0129, 0.0130, 0.0129, 0.0129,\n",
            "        0.0129, 0.0129, 0.0130, 0.0130, 0.0130, 0.0130, 0.0129, 0.0131, 0.0129,\n",
            "        0.0129, 0.0129, 0.0129, 0.0129, 0.0129, 0.0130, 0.0129, 0.0129, 0.0130,\n",
            "        0.0129, 0.0129, 0.0130, 0.0129, 0.0130, 0.0128, 0.0129, 0.0129, 0.0128,\n",
            "        0.0129, 0.0129, 0.0129, 0.0129, 0.0130, 0.0129, 0.0129, 0.0129, 0.0131,\n",
            "        0.0128, 0.0128, 0.0130, 0.0129, 0.0128, 0.0130, 0.0129, 0.0130, 0.0129,\n",
            "        0.0128, 0.0129, 0.0130, 0.0128, 0.0129, 0.0129, 0.0130, 0.0130, 0.0128,\n",
            "        0.0129, 0.0130, 0.0130, 0.0128, 0.0128, 0.0129, 0.0129, 0.0129, 0.0128,\n",
            "        0.0129, 0.0129, 0.0129, 0.0130, 0.0130], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [15]\n",
            "DEBUGGING: logits looks like: tensor([7.9388, 7.8699, 7.8696, 7.8693, 7.8683, 7.8695, 7.8711, 7.8687, 7.8692,\n",
            "        7.8686, 7.8691, 7.8703, 7.8700, 7.8707, 7.8704, 7.8684, 7.8716, 7.8687,\n",
            "        7.8692, 7.8694, 7.8688, 7.8692, 7.8685, 7.8703, 7.8688, 7.8688, 7.8710,\n",
            "        7.8688, 7.8697, 7.8700, 7.8693, 7.8707, 7.8682, 7.8691, 7.8696, 7.8679,\n",
            "        7.8696, 7.8696, 7.8694, 7.8693, 7.8705, 7.8693, 7.8687, 7.8692, 7.8721,\n",
            "        7.8680, 7.8679, 7.8703, 7.8696, 7.8681, 7.8704, 7.8688, 7.8712, 7.8699,\n",
            "        7.8675, 7.8686, 7.8704, 7.8682, 7.8688, 7.8691, 7.8700, 7.8704, 7.8680,\n",
            "        7.8687, 7.8705, 7.8704, 7.8682, 7.8683, 7.8685, 7.8695, 7.8686, 7.8677,\n",
            "        7.8688, 7.8689, 7.8697, 7.8703, 7.8711], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0161, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [47]\n",
            "DEBUGGING: logits looks like: tensor([7.9431, 7.8700, 7.8697, 7.8698, 7.8695, 7.8700, 7.8701, 7.8700, 7.8700,\n",
            "        7.8697, 7.8697, 7.8699, 7.8697, 7.8700, 7.8697, 7.8699, 7.8697, 7.8697,\n",
            "        7.8700, 7.8699, 7.8698, 7.8697, 7.8697, 7.8697, 7.8702, 7.8700, 7.8700,\n",
            "        7.8700, 7.8697, 7.8698, 7.8699, 7.8698, 7.8700, 7.8699, 7.8699, 7.8701,\n",
            "        7.8696, 7.8701, 7.8703, 7.8706, 7.8700, 7.8701, 7.8694, 7.8699, 7.8699,\n",
            "        7.8701, 7.8697, 7.8702, 7.8695, 7.8701, 7.8699, 7.8696, 7.8698, 7.8699,\n",
            "        7.8700, 7.8699, 7.8695, 7.8698, 7.8700, 7.8697, 7.8698, 7.8701, 7.8699,\n",
            "        7.8696, 7.8697, 7.8698, 7.8698, 7.8698, 7.8696, 7.8702, 7.8698, 7.8696,\n",
            "        7.8698, 7.8698, 7.8700, 7.8696, 7.8696, 7.8697, 7.8697, 7.8700, 7.8698,\n",
            "        7.8699, 7.8702, 7.8699, 7.8700, 7.8698, 7.8699, 7.8702, 7.8698],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0110, 0.0093, 0.0122, 0.0119, 0.0099, 0.0092, 0.0098, 0.0099, 0.0135,\n",
            "        0.0095, 0.0112, 0.0100, 0.0097, 0.0122, 0.0101, 0.0097, 0.0109, 0.0094,\n",
            "        0.0110, 0.0107, 0.0110, 0.0098, 0.0110, 0.0100, 0.0107, 0.0108, 0.0104,\n",
            "        0.0099, 0.0094, 0.0104, 0.0086, 0.0124, 0.0113, 0.0112, 0.0104, 0.0101,\n",
            "        0.0099, 0.0112, 0.0097, 0.0102, 0.0089, 0.0102, 0.0117, 0.0096, 0.0104,\n",
            "        0.0107, 0.0119, 0.0101, 0.0088, 0.0107, 0.0110, 0.0099, 0.0084, 0.0112,\n",
            "        0.0110, 0.0092, 0.0101, 0.0105, 0.0112, 0.0109, 0.0116, 0.0107, 0.0085,\n",
            "        0.0113, 0.0092, 0.0105, 0.0115, 0.0115, 0.0109, 0.0088, 0.0112, 0.0106,\n",
            "        0.0092, 0.0106, 0.0093, 0.0111, 0.0091, 0.0106, 0.0115, 0.0111, 0.0100,\n",
            "        0.0109, 0.0108, 0.0106, 0.0113, 0.0112, 0.0096, 0.0109, 0.0108, 0.0106,\n",
            "        0.0111, 0.0118, 0.0112, 0.0127, 0.0116], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [72]\n",
            "DEBUGGING: logits looks like: tensor([7.8872, 7.8537, 7.9067, 7.9021, 7.8651, 7.8513, 7.8640, 7.8660, 7.9283,\n",
            "        7.8572, 7.8901, 7.8676, 7.8611, 7.9069, 7.8695, 7.8621, 7.8856, 7.8547,\n",
            "        7.8868, 7.8806, 7.8874, 7.8640, 7.8864, 7.8671, 7.8809, 7.8830, 7.8749,\n",
            "        7.8664, 7.8557, 7.8755, 7.8382, 7.9109, 7.8922, 7.8910, 7.8750, 7.8699,\n",
            "        7.8658, 7.8904, 7.8613, 7.8722, 7.8453, 7.8716, 7.8991, 7.8597, 7.8763,\n",
            "        7.8818, 7.9034, 7.8702, 7.8422, 7.8816, 7.8877, 7.8667, 7.8319, 7.8906,\n",
            "        7.8877, 7.8509, 7.8697, 7.8772, 7.8908, 7.8844, 7.8973, 7.8814, 7.8356,\n",
            "        7.8919, 7.8513, 7.8777, 7.8950, 7.8964, 7.8843, 7.8413, 7.8901, 7.8786,\n",
            "        7.8522, 7.8802, 7.8532, 7.8894, 7.8493, 7.8797, 7.8965, 7.8881, 7.8687,\n",
            "        7.8844, 7.8832, 7.8793, 7.8925, 7.8905, 7.8603, 7.8855, 7.8837, 7.8793,\n",
            "        7.8887, 7.9005, 7.8908, 7.9158, 7.8977], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0098, 0.0094, 0.0093, 0.0104, 0.0099, 0.0087, 0.0087, 0.0097, 0.0097,\n",
            "        0.0097, 0.0098, 0.0096, 0.0099, 0.0094, 0.0096, 0.0092, 0.0096, 0.0100,\n",
            "        0.0098, 0.0100, 0.0100, 0.0097, 0.0099, 0.0098, 0.0093, 0.0090, 0.0098,\n",
            "        0.0098, 0.0095, 0.0094, 0.0092, 0.0096, 0.0098, 0.0099, 0.0094, 0.0098,\n",
            "        0.0097, 0.0098, 0.0096, 0.0100, 0.0096, 0.0100, 0.0091, 0.0093, 0.0098,\n",
            "        0.0101, 0.0096, 0.0096, 0.0094, 0.0097, 0.0098, 0.0098, 0.0098, 0.0098,\n",
            "        0.0098, 0.0098, 0.0097, 0.0100, 0.0097, 0.0096, 0.0101, 0.0100, 0.0097,\n",
            "        0.0096, 0.0102, 0.0096, 0.0100, 0.0091, 0.0097, 0.0098, 0.0101, 0.0092,\n",
            "        0.0095, 0.0098, 0.0098, 0.0099, 0.0096, 0.0097, 0.0095, 0.0096, 0.0097,\n",
            "        0.0096, 0.0099, 0.0100, 0.0097, 0.0100, 0.0100, 0.0099, 0.0098, 0.0099,\n",
            "        0.0097, 0.0100, 0.0100, 0.0099, 0.0100, 0.0098, 0.0096, 0.0096, 0.0101,\n",
            "        0.0103, 0.0099, 0.0094, 0.0095], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [44]\n",
            "DEBUGGING: logits looks like: tensor([7.8851, 7.8765, 7.8764, 7.8986, 7.8888, 7.8620, 7.8629, 7.8846, 7.8841,\n",
            "        7.8839, 7.8860, 7.8823, 7.8881, 7.8783, 7.8821, 7.8729, 7.8825, 7.8898,\n",
            "        7.8866, 7.8891, 7.8904, 7.8847, 7.8880, 7.8860, 7.8745, 7.8686, 7.8851,\n",
            "        7.8861, 7.8793, 7.8784, 7.8740, 7.8808, 7.8849, 7.8877, 7.8779, 7.8851,\n",
            "        7.8839, 7.8853, 7.8812, 7.8890, 7.8822, 7.8898, 7.8710, 7.8764, 7.8851,\n",
            "        7.8921, 7.8809, 7.8811, 7.8778, 7.8832, 7.8859, 7.8851, 7.8851, 7.8868,\n",
            "        7.8856, 7.8866, 7.8841, 7.8899, 7.8828, 7.8825, 7.8921, 7.8893, 7.8833,\n",
            "        7.8808, 7.8929, 7.8813, 7.8904, 7.8721, 7.8832, 7.8856, 7.8916, 7.8734,\n",
            "        7.8787, 7.8863, 7.8862, 7.8884, 7.8826, 7.8847, 7.8800, 7.8823, 7.8846,\n",
            "        7.8821, 7.8877, 7.8892, 7.8828, 7.8897, 7.8909, 7.8876, 7.8862, 7.8889,\n",
            "        7.8829, 7.8892, 7.8901, 7.8880, 7.8890, 7.8858, 7.8827, 7.8827, 7.8914,\n",
            "        7.8957, 7.8880, 7.8774, 7.8788], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.8867413289794968 and immediate abs rewards look like: [0.0044687274585157866, 0.002890378038500785, 0.02953879744745791, 0.0045400276471809775, 0.010108153379405849, 0.01965652136141216, 0.0017605467924113327, 0.004982755506716785, 0.0037969544355291873, 0.0005376354520194582, 0.0010077799993268854, 0.006058414168364834, 0.0002611609797895653, 0.00014464524838331272, 7.562837117802701e-05, 0.0007044543831398187, 0.00012130449886171846, 0.0006894235684740124, 7.18205701559782e-05, 0.002192467312397639, 1.4161191757011693e-05, 0.0011815735397249227, 0.0023680951635469683, 4.6161184855009196e-05, 0.00012409483724695747, 0.0003667984415187675, 0.00013690671403310262, 8.379152313864324e-05, 1.3944581951363944e-06, 1.0104170996783068e-05, 0.00013000460876355646, 8.587900538259419e-06, 0.0001248400189979293, 6.527877849293873e-05, 0.00010095180414282368, 0.00017419335381418932, 0.7881967946636905, 4.547473508864641e-13, 9.094947017729282e-13, 9.094947017729282e-13, 0.0, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 10.693320855295157 and immediate relative rewards look like: [0.0015480179722601275, 0.002005624337533075, 0.0307761823421297, 0.006372318711569877, 0.017762874400595374, 0.04159823856095872, 0.0043770762033582995, 0.014166732964853252, 0.012166279416479114, 0.0019167049763580582, 0.003952838231243968, 0.02593265115770952, 0.0012136624171691524, 0.0007239674096785263, 0.0004055881650558578, 0.004029896286881857, 0.0007374901755459, 0.0044382103499764425, 0.0004881561849458481, 0.015686665210376684, 0.00010647002807369101, 0.0093066613196, 0.019508380489814514, 0.000397146683990197, 0.0011121507536471207, 0.003418929048647409, 0.0013253631206069816, 0.0008412514117699782, 1.4500541982531846e-05, 0.0001086933307781744, 0.0014451168713115162, 9.854617201471028e-05, 0.0014773108179955046, 0.0007959292972514479, 0.001267114470309076, 0.0022489692957856325, 10.459549146018652, 8.640199666842818e-12, 1.7735146684568068e-11, 1.81898940354627e-11, 0.0, 1.909938873722715e-11, 9.777068044061202e-12, 1.000444171950221e-11, 1.0231815394947769e-11, 1.0459189070388675e-11, 1.0686562745829477e-11, 1.0913936421275139e-11, 1.1141310096715838e-11, 1.1368683772161603e-11]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 3\n",
            "DEBUGGING: the action_prob is: tensor([0.0160, 0.0143, 0.0142, 0.0140, 0.0144, 0.0143, 0.0146, 0.0152, 0.0136,\n",
            "        0.0142, 0.0143, 0.0143, 0.0142, 0.0142, 0.0144, 0.0142, 0.0151, 0.0153,\n",
            "        0.0139, 0.0150, 0.0144, 0.0143, 0.0139, 0.0140, 0.0143, 0.0142, 0.0140,\n",
            "        0.0143, 0.0142, 0.0143, 0.0143, 0.0143, 0.0147, 0.0139, 0.0140, 0.0149,\n",
            "        0.0142, 0.0143, 0.0141, 0.0143, 0.0142, 0.0140, 0.0146, 0.0139, 0.0140,\n",
            "        0.0142, 0.0143, 0.0140, 0.0141, 0.0141, 0.0142, 0.0143, 0.0139, 0.0141,\n",
            "        0.0142, 0.0140, 0.0142, 0.0143, 0.0141, 0.0142, 0.0145, 0.0140, 0.0143,\n",
            "        0.0143, 0.0139, 0.0142, 0.0141, 0.0146, 0.0141, 0.0144],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [12]\n",
            "DEBUGGING: logits looks like: tensor([7.9045, 7.8824, 7.8802, 7.8783, 7.8835, 7.8816, 7.8856, 7.8938, 7.8722,\n",
            "        7.8805, 7.8815, 7.8814, 7.8807, 7.8801, 7.8831, 7.8812, 7.8930, 7.8962,\n",
            "        7.8763, 7.8920, 7.8838, 7.8813, 7.8764, 7.8785, 7.8826, 7.8808, 7.8782,\n",
            "        7.8827, 7.8799, 7.8818, 7.8820, 7.8817, 7.8878, 7.8764, 7.8784, 7.8902,\n",
            "        7.8812, 7.8825, 7.8796, 7.8819, 7.8801, 7.8774, 7.8860, 7.8764, 7.8777,\n",
            "        7.8808, 7.8822, 7.8776, 7.8793, 7.8798, 7.8812, 7.8815, 7.8766, 7.8795,\n",
            "        7.8811, 7.8774, 7.8812, 7.8823, 7.8787, 7.8807, 7.8845, 7.8775, 7.8816,\n",
            "        7.8824, 7.8764, 7.8802, 7.8789, 7.8857, 7.8794, 7.8832],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0125, 0.0129, 0.0125, 0.0132, 0.0153, 0.0129, 0.0124, 0.0124, 0.0130,\n",
            "        0.0125, 0.0127, 0.0125, 0.0127, 0.0121, 0.0134, 0.0113, 0.0126, 0.0129,\n",
            "        0.0141, 0.0134, 0.0121, 0.0134, 0.0136, 0.0124, 0.0126, 0.0127, 0.0123,\n",
            "        0.0133, 0.0136, 0.0125, 0.0128, 0.0130, 0.0128, 0.0122, 0.0126, 0.0143,\n",
            "        0.0133, 0.0136, 0.0139, 0.0130, 0.0128, 0.0125, 0.0132, 0.0129, 0.0120,\n",
            "        0.0125, 0.0113, 0.0124, 0.0130, 0.0133, 0.0119, 0.0136, 0.0121, 0.0144,\n",
            "        0.0129, 0.0120, 0.0137, 0.0134, 0.0130, 0.0127, 0.0132, 0.0126, 0.0124,\n",
            "        0.0120, 0.0131, 0.0128, 0.0124, 0.0117, 0.0125, 0.0121, 0.0126, 0.0133,\n",
            "        0.0128, 0.0117, 0.0133, 0.0127, 0.0133, 0.0126],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [53]\n",
            "DEBUGGING: logits looks like: tensor([7.8571, 7.8635, 7.8578, 7.8676, 7.8978, 7.8629, 7.8562, 7.8554, 7.8650,\n",
            "        7.8564, 7.8605, 7.8576, 7.8599, 7.8506, 7.8705, 7.8368, 7.8588, 7.8631,\n",
            "        7.8809, 7.8716, 7.8502, 7.8713, 7.8746, 7.8552, 7.8593, 7.8603, 7.8534,\n",
            "        7.8701, 7.8734, 7.8565, 7.8612, 7.8652, 7.8619, 7.8518, 7.8593, 7.8846,\n",
            "        7.8702, 7.8741, 7.8782, 7.8656, 7.8614, 7.8566, 7.8677, 7.8634, 7.8490,\n",
            "        7.8575, 7.8376, 7.8555, 7.8652, 7.8696, 7.8473, 7.8743, 7.8498, 7.8850,\n",
            "        7.8629, 7.8485, 7.8756, 7.8713, 7.8648, 7.8597, 7.8677, 7.8590, 7.8559,\n",
            "        7.8492, 7.8659, 7.8625, 7.8552, 7.8436, 7.8578, 7.8499, 7.8586, 7.8689,\n",
            "        7.8620, 7.8434, 7.8696, 7.8600, 7.8692, 7.8593],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0115, 0.0121, 0.0115, 0.0122, 0.0110, 0.0121, 0.0109, 0.0119, 0.0118,\n",
            "        0.0117, 0.0123, 0.0126, 0.0117, 0.0131, 0.0124, 0.0117, 0.0121, 0.0132,\n",
            "        0.0115, 0.0115, 0.0120, 0.0123, 0.0112, 0.0115, 0.0118, 0.0127, 0.0116,\n",
            "        0.0113, 0.0122, 0.0122, 0.0115, 0.0120, 0.0123, 0.0110, 0.0125, 0.0122,\n",
            "        0.0127, 0.0117, 0.0120, 0.0125, 0.0124, 0.0115, 0.0121, 0.0128, 0.0110,\n",
            "        0.0126, 0.0133, 0.0123, 0.0130, 0.0117, 0.0123, 0.0112, 0.0118, 0.0115,\n",
            "        0.0116, 0.0109, 0.0115, 0.0122, 0.0123, 0.0114, 0.0112, 0.0112, 0.0119,\n",
            "        0.0126, 0.0123, 0.0123, 0.0118, 0.0116, 0.0116, 0.0130, 0.0123, 0.0118,\n",
            "        0.0123, 0.0115, 0.0121, 0.0126, 0.0121, 0.0117, 0.0112, 0.0109, 0.0108,\n",
            "        0.0112, 0.0112, 0.0115], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [50]\n",
            "DEBUGGING: logits looks like: tensor([7.8542, 7.8634, 7.8537, 7.8663, 7.8449, 7.8638, 7.8435, 7.8610, 7.8586,\n",
            "        7.8583, 7.8676, 7.8718, 7.8583, 7.8797, 7.8697, 7.8579, 7.8648, 7.8817,\n",
            "        7.8543, 7.8539, 7.8620, 7.8668, 7.8489, 7.8544, 7.8592, 7.8732, 7.8565,\n",
            "        7.8508, 7.8657, 7.8651, 7.8548, 7.8632, 7.8682, 7.8458, 7.8715, 7.8666,\n",
            "        7.8746, 7.8567, 7.8620, 7.8706, 7.8696, 7.8543, 7.8647, 7.8750, 7.8460,\n",
            "        7.8723, 7.8831, 7.8668, 7.8790, 7.8571, 7.8669, 7.8489, 7.8596, 7.8535,\n",
            "        7.8565, 7.8430, 7.8533, 7.8658, 7.8683, 7.8530, 7.8495, 7.8482, 7.8607,\n",
            "        7.8717, 7.8680, 7.8668, 7.8589, 7.8560, 7.8566, 7.8783, 7.8670, 7.8590,\n",
            "        7.8671, 7.8541, 7.8634, 7.8721, 7.8643, 7.8582, 7.8490, 7.8426, 7.8412,\n",
            "        7.8484, 7.8485, 7.8536], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0103, 0.0103, 0.0103, 0.0103, 0.0104, 0.0102, 0.0103, 0.0102, 0.0103,\n",
            "        0.0103, 0.0104, 0.0104, 0.0103, 0.0103, 0.0103, 0.0102, 0.0102, 0.0103,\n",
            "        0.0103, 0.0104, 0.0104, 0.0103, 0.0104, 0.0104, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0104, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0102, 0.0104, 0.0104, 0.0103, 0.0104,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0102, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0102, 0.0104, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0104, 0.0103, 0.0106,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0104, 0.0103, 0.0103, 0.0103, 0.0104,\n",
            "        0.0103, 0.0104, 0.0102, 0.0103, 0.0103, 0.0103, 0.0103, 0.0102, 0.0103,\n",
            "        0.0104, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [45]\n",
            "DEBUGGING: logits looks like: tensor([7.8753, 7.8752, 7.8751, 7.8748, 7.8762, 7.8740, 7.8752, 7.8738, 7.8742,\n",
            "        7.8751, 7.8761, 7.8761, 7.8745, 7.8741, 7.8750, 7.8726, 7.8734, 7.8746,\n",
            "        7.8748, 7.8774, 7.8766, 7.8741, 7.8765, 7.8764, 7.8747, 7.8756, 7.8749,\n",
            "        7.8751, 7.8751, 7.8772, 7.8755, 7.8754, 7.8741, 7.8743, 7.8756, 7.8747,\n",
            "        7.8747, 7.8749, 7.8747, 7.8748, 7.8739, 7.8777, 7.8764, 7.8748, 7.8760,\n",
            "        7.8752, 7.8748, 7.8753, 7.8749, 7.8739, 7.8757, 7.8743, 7.8751, 7.8750,\n",
            "        7.8754, 7.8748, 7.8752, 7.8745, 7.8748, 7.8739, 7.8762, 7.8751, 7.8746,\n",
            "        7.8758, 7.8741, 7.8743, 7.8749, 7.8744, 7.8751, 7.8763, 7.8751, 7.8803,\n",
            "        7.8740, 7.8749, 7.8759, 7.8741, 7.8766, 7.8752, 7.8757, 7.8754, 7.8763,\n",
            "        7.8748, 7.8760, 7.8738, 7.8752, 7.8741, 7.8753, 7.8757, 7.8732, 7.8756,\n",
            "        7.8765, 7.8758, 7.8749, 7.8751, 7.8754, 7.8745, 7.8753],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0094, 0.0094, 0.0091, 0.0093, 0.0094, 0.0094, 0.0093, 0.0093, 0.0095,\n",
            "        0.0088, 0.0092, 0.0094, 0.0096, 0.0092, 0.0092, 0.0093, 0.0094, 0.0094,\n",
            "        0.0092, 0.0095, 0.0088, 0.0092, 0.0094, 0.0092, 0.0093, 0.0092, 0.0094,\n",
            "        0.0091, 0.0092, 0.0092, 0.0093, 0.0093, 0.0094, 0.0094, 0.0093, 0.0094,\n",
            "        0.0094, 0.0094, 0.0092, 0.0093, 0.0092, 0.0096, 0.0092, 0.0091, 0.0093,\n",
            "        0.0088, 0.0094, 0.0095, 0.0092, 0.0093, 0.0091, 0.0092, 0.0093, 0.0091,\n",
            "        0.0094, 0.0094, 0.0092, 0.0094, 0.0090, 0.0092, 0.0097, 0.0094, 0.0092,\n",
            "        0.0096, 0.0092, 0.0092, 0.0094, 0.0094, 0.0093, 0.0094, 0.0093, 0.0092,\n",
            "        0.0091, 0.0091, 0.0092, 0.0094, 0.0095, 0.0094, 0.0089, 0.0083, 0.0092,\n",
            "        0.0093, 0.0092, 0.0094, 0.0093, 0.0092, 0.0090, 0.0093, 0.0091, 0.0092,\n",
            "        0.0092, 0.0091, 0.0095, 0.0084, 0.0093, 0.0089, 0.0092, 0.0095, 0.0092,\n",
            "        0.0091, 0.0093, 0.0094, 0.0092, 0.0093, 0.0090, 0.0095, 0.0094, 0.0094],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [55]\n",
            "DEBUGGING: logits looks like: tensor([7.8810, 7.8812, 7.8753, 7.8796, 7.8818, 7.8810, 7.8787, 7.8781, 7.8838,\n",
            "        7.8687, 7.8767, 7.8806, 7.8842, 7.8757, 7.8761, 7.8796, 7.8812, 7.8813,\n",
            "        7.8770, 7.8837, 7.8673, 7.8756, 7.8811, 7.8765, 7.8790, 7.8762, 7.8802,\n",
            "        7.8751, 7.8757, 7.8768, 7.8790, 7.8785, 7.8817, 7.8805, 7.8779, 7.8801,\n",
            "        7.8816, 7.8805, 7.8776, 7.8785, 7.8762, 7.8847, 7.8771, 7.8747, 7.8795,\n",
            "        7.8687, 7.8817, 7.8830, 7.8758, 7.8785, 7.8739, 7.8765, 7.8782, 7.8747,\n",
            "        7.8804, 7.8811, 7.8760, 7.8806, 7.8724, 7.8773, 7.8879, 7.8808, 7.8770,\n",
            "        7.8849, 7.8772, 7.8773, 7.8815, 7.8819, 7.8783, 7.8811, 7.8787, 7.8768,\n",
            "        7.8741, 7.8746, 7.8769, 7.8801, 7.8838, 7.8810, 7.8711, 7.8569, 7.8775,\n",
            "        7.8783, 7.8775, 7.8800, 7.8792, 7.8759, 7.8726, 7.8795, 7.8743, 7.8763,\n",
            "        7.8766, 7.8754, 7.8828, 7.8586, 7.8785, 7.8694, 7.8775, 7.8822, 7.8766,\n",
            "        7.8746, 7.8783, 7.8821, 7.8760, 7.8796, 7.8730, 7.8840, 7.8819, 7.8818],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.6072478073233469 and immediate abs rewards look like: [0.03254296463705941, 0.06218411305712834, 0.005505190992607822, 0.0004946845888298412, 0.002327373590787829, 0.010227391287116916, 0.0006317569957445812, 0.0009393294506025995, 0.00919603050601836, 0.014855053319024591, 0.002499528055977862, 0.009189114091213924, 0.4566552767309986, 4.547473508864641e-13, 6.821210263296962e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 3.637978807091713e-12, 3.410605131648481e-12, 6.821210263296962e-13, 0.0, 2.2737367544323206e-13, 2.2737367544323206e-13, 4.547473508864641e-13, 6.821210263296962e-13, 2.2737367544323206e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 6.821210263296962e-13, 6.821210263296962e-13, 6.821210263296962e-13, 1.3642420526593924e-12, 6.821210263296962e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 2.2737367544323206e-13, 0.0, 2.2737367544323206e-13, 4.547473508864641e-13, 0.0, 2.2737367544323206e-13, 2.2737367544323206e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 1.8968325878459926 and immediate relative rewards look like: [0.00902154949576074, 0.034791187409336316, 0.004701914735649767, 0.0005642228651487731, 0.003318636460003763, 0.017511689370261083, 0.0012656960191873073, 0.0021511355313614095, 0.023698428869043407, 0.04264773111612963, 0.007927359356176382, 0.03181601845327151, 1.7174170179669233, 2.1221543041368323e-12, 3.410605131647964e-12, 1.2126596023639961e-12, 1.2884508275116482e-12, 0.0, 0.0, 3.0316490059099903e-12, 3.1832314562050073e-12, 3.334813906500989e-12, 2.7891170854367684e-11, 2.7284841053152692e-11, 5.68434188607994e-12, 0.0, 2.0463630789892436e-12, 2.1221543041368323e-12, 4.39589105856882e-12, 6.821210263297478e-12, 2.3495279795797085e-12, 0.0, 5.002220859750726e-12, 5.153803310046984e-12, 5.305385760341679e-12, 8.185452315956975e-12, 8.412825991398312e-12, 8.640199666843473e-12, 1.7735146684569412e-11, 9.094947017732039e-12, 6.21488046211548e-12, 6.366462912410015e-12, 0.0, 3.334813906500989e-12, 0.0, 3.4863963567962246e-12, 7.124375163887398e-12, 0.0, 3.713770032239738e-12, 3.7895612573872005e-12]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 200 number of matrices\n",
            "DEBUGGING: ACT_MAT has 200 number of matrices\n",
            "DEBUGGING: VAL looks like: [[2.7934301121022567, 2.8075913856963965, 2.8154281074927794, 2.843270657707145, 2.8703188039315366, 2.886399585204231, 2.9092556127923657, 2.919988968054576, 2.938813177929579, 2.967653416954317, 2.992229970215423, 3.0219100411884727, 3.0523604062346776, 3.0791165224324355, 3.1100295085322265, 3.1402129346065837, 3.17118917910807, 3.2023076880497987, 3.2346036391832147, 3.2672331778070767, 9.570341651954272e-11, 9.66701176965078e-11, 9.427808463637127e-11, 9.523038852158714e-11, 9.619231163796681e-11, 9.333611149553496e-11, 9.029794726045745e-11, 9.121004773783581e-11, 8.784418093895166e-11, 8.873149589793098e-11, 8.04409584648488e-11, 7.650697222796906e-11, 7.727976992724148e-11, 7.300762532069735e-11, 6.853921415217169e-11, 6.387255393114102e-11, 5.349355304027024e-11, 4.83686892720569e-11, 4.3038945616998335e-11, 3.7502252581288084e-11, 3.175651976713996e-11, 1.952197862920006e-11, 1.3288399713929825e-11, 1.342262597366649e-11, 6.8212102632979966e-12, 0.0, 0.0, 0.0, 0.0, 0.0], [0.246414836068771, 0.2441092274803307, 0.2412601388130585, 0.24210660718673438, 0.2383833498112305, 0.23270724365117207, 0.2309359842169318, 0.22804304264637063, 0.22797138683419726, 0.20528949598560445, 0.18876747396538454, 0.16994128359621294, 0.16634926705588457, 0.14891514490055832, 0.13261845437800668, 0.09929397054514712, 0.09134519418029557, 0.08907980128054684, 0.08968578166877708, 0.08764267097773516, 0.07842741474930913, 0.0790861808800204, 0.0798722003059903, 0.08055251359160139, 0.08136382137322612, 0.08043539667064292, 0.0710322127300432, 0.06399893690696025, 0.05951209644806502, 0.04586771598474376, 0.0312860785243098, 0.03114857568785112, 0.029755844828659908, 0.029986127585690813, 0.030258043249931296, 0.029991322527511678, 0.030020600116083315, 0.030103120163405016, 0.02125252237090114, 0.021386468366610047, 0.02134188108183649, 0.021518915573332867, 0.021564334983236108, 0.02137375234758739, 0.02002870499384045, 0.019168748884773518, 0.016774548177884426, 0.012300286735438799, 0.01241399288189584, 0.0015161338405683994], [7.495162324214124, 7.569307380042287, 7.643739147176519, 7.689861580640798, 7.761100264574978, 7.821552919368063, 7.858540081623337, 7.933497985272706, 7.999324497280659, 8.067836583701192, 8.147393816893771, 8.225697958244977, 8.282591219280068, 8.365027835215049, 8.44879178566199, 8.533723431815087, 8.615852056089096, 8.702135925165203, 8.785553247288108, 8.873803122326427, 8.947592380925304, 9.037864556461848, 9.119755449638633, 9.19216875671598, 9.284617787911099, 9.377278421371164, 9.468544941739916, 9.562848059211424, 9.658592735151165, 9.756139630918366, 9.854576704633926, 9.952658169457187, 10.05309052857088, 10.153144664396855, 10.254897712221823, 10.357202623991428, 10.459549146157215, 1.3996246444445686e-10, 1.3264875230062024e-10, 1.1607434910712341e-10, 9.887318694107142e-11, 9.987190600108225e-11, 8.158840127662132e-11, 7.253670023490921e-11, 6.31638974903101e-11, 5.346674959127508e-11, 4.3441980324127686e-11, 3.30862803821194e-11, 2.2396307031155825e-11, 1.1368683772161603e-11], [1.6904335352682862, 1.6983959452247732, 1.6804088462782192, 1.6926332641844135, 1.7091606477972372, 1.7230727387244782, 1.7227889387416333, 1.738912366386309, 1.7543042735908563, 1.748086711840215, 1.722665637095036, 1.7320588664028884, 1.7174170181309263, 1.6565959896256223e-10, 1.6518933803881353e-10, 1.6341286152238947e-10, 1.6383858779800553e-10, 1.6419205754595342e-10, 1.6585056317773074e-10, 1.6752582139164722e-10, 1.6615572968256285e-10, 1.6461868507712912e-10, 1.6291300118245265e-10, 1.3638568720008584e-10, 1.1020287489589207e-10, 1.05574275767487e-10, 1.0664068259342122e-10, 1.0565082779235553e-10, 1.0457441766486737e-10, 1.0119043091545308e-10, 9.532244510318748e-11, 9.391203749859371e-11, 9.486064393797344e-11, 9.076608391739667e-11, 8.647705111853504e-11, 8.199158116989228e-11, 7.455164530700536e-11, 6.680688819758288e-11, 5.875423083913071e-11, 4.143341833794071e-11, 3.266512254566532e-11, 2.6717416246009945e-11, 2.0556518518787806e-11, 2.0764160119987684e-11, 1.7605400215643125e-11, 1.7783232541053663e-11, 1.4441248670967109e-11, 7.390781320282535e-12, 7.465435677053066e-12, 3.7895612573872005e-12]]\n",
            "DEBUGGING: traj_returns = [2.7934301121022567, 0.246414836068771, 7.495162324214124, 1.6904335352682862]\n",
            "DEBUGGING: actions = [[36], [24], [45], [27], [5], [20], [24], [51], [21], [29], [23], [23], [71], [23], [33], [1], [70], [15], [54], [0], [65], [48], [68], [71], [0], [35], [2], [30], [43], [31], [59], [7], [54], [62], [1], [80], [85], [21], [37], [56], [57], [54], [16], [70], [80], [84], [0], [54], [63], [88], [12], [6], [53], [10], [63], [40], [38], [44], [49], [6], [38], [32], [38], [38], [55], [29], [18], [45], [70], [9], [19], [41], [81], [9], [36], [49], [31], [81], [62], [8], [71], [44], [16], [60], [75], [46], [81], [79], [85], [27], [16], [38], [63], [79], [9], [83], [103], [31], [76], [7], [5], [56], [36], [11], [10], [22], [19], [58], [59], [27], [17], [6], [11], [57], [69], [13], [61], [41], [65], [15], [5], [4], [4], [49], [37], [47], [75], [69], [27], [47], [58], [21], [64], [58], [6], [4], [0], [14], [89], [72], [28], [33], [25], [44], [25], [75], [58], [63], [74], [44], [44], [3], [11], [41], [7], [24], [27], [22], [43], [12], [56], [19], [0], [62], [61], [39], [42], [2], [33], [53], [39], [25], [42], [34], [40], [69], [2], [7], [36], [50], [56], [17], [78], [8], [40], [16], [39], [22], [14], [45], [90], [94], [93], [4], [89], [43], [51], [57], [33], [55]]\n",
            "DEBUGGING: actions length = 200\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-1.1598, -0.5313,  0.2238,  ...,  0.2635,  0.5078, -0.7167],\n",
            "        [-1.1421, -0.5174,  0.2187,  ...,  0.2601,  0.5026, -0.7041],\n",
            "        [-1.1546, -0.5312,  0.2219,  ...,  0.2607,  0.5076, -0.7162],\n",
            "        ...,\n",
            "        [-1.1513, -0.5258,  0.2173,  ...,  0.2590,  0.5052, -0.7103],\n",
            "        [-1.1515, -0.5259,  0.2172,  ...,  0.2589,  0.5053, -0.7104],\n",
            "        [-1.1520, -0.5262,  0.2175,  ...,  0.2592,  0.5054, -0.7106]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-1.1519, -0.5262,  0.2174,  ...,  0.2590,  0.5054, -0.7107],\n",
            "        [-1.1521, -0.5260,  0.2171,  ...,  0.2589,  0.5052, -0.7104],\n",
            "        [-1.1511, -0.5255,  0.2170,  ...,  0.2589,  0.5051, -0.7100],\n",
            "        ...,\n",
            "        [-1.1523, -0.5264,  0.2172,  ...,  0.2590,  0.5054, -0.7107],\n",
            "        [-1.1520, -0.5262,  0.2172,  ...,  0.2591,  0.5054, -0.7105],\n",
            "        [-1.1520, -0.5262,  0.2173,  ...,  0.2590,  0.5054, -0.7106]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([7.8810, 7.8812, 7.8753, 7.8796, 7.8818, 7.8810, 7.8787, 7.8781, 7.8838,\n",
            "        7.8687, 7.8767, 7.8806, 7.8842, 7.8757, 7.8761, 7.8796, 7.8812, 7.8813,\n",
            "        7.8770, 7.8837, 7.8673, 7.8756, 7.8811, 7.8765, 7.8790, 7.8762, 7.8802,\n",
            "        7.8751, 7.8757, 7.8768, 7.8790, 7.8785, 7.8817, 7.8805, 7.8779, 7.8801,\n",
            "        7.8816, 7.8805, 7.8776, 7.8785, 7.8762, 7.8847, 7.8771, 7.8747, 7.8795,\n",
            "        7.8687, 7.8817, 7.8830, 7.8758, 7.8785, 7.8739, 7.8765, 7.8782, 7.8747,\n",
            "        7.8804, 7.8811, 7.8760, 7.8806, 7.8724, 7.8773, 7.8879, 7.8808, 7.8770,\n",
            "        7.8849, 7.8772, 7.8773, 7.8815, 7.8819, 7.8783, 7.8811, 7.8787, 7.8768,\n",
            "        7.8741, 7.8746, 7.8769, 7.8801, 7.8838, 7.8810, 7.8711, 7.8569, 7.8775,\n",
            "        7.8783, 7.8775, 7.8800, 7.8792, 7.8759, 7.8726, 7.8795, 7.8743, 7.8763,\n",
            "        7.8766, 7.8754, 7.8828, 7.8586, 7.8785, 7.8694, 7.8775, 7.8822, 7.8766,\n",
            "        7.8746, 7.8783, 7.8821, 7.8760, 7.8796, 7.8730, 7.8840, 7.8819, 7.8818],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[3.05636020e+00 3.07985098e+00 3.09520906e+00 3.11696803e+00\n",
            "  3.14474077e+00 3.16593312e+00 3.18038015e+00 3.20511059e+00\n",
            "  3.23010333e+00 3.24721655e+00 3.26276422e+00 3.28740204e+00\n",
            "  3.30467948e+00 2.89826488e+00 2.92285994e+00 2.94330758e+00\n",
            "  2.96959661e+00 2.99838085e+00 3.02746067e+00 3.05716974e+00\n",
            "  2.25650495e+00 2.27923768e+00 2.29990691e+00 2.31818032e+00\n",
            "  2.34149540e+00 2.36442845e+00 2.38489429e+00 2.40671175e+00\n",
            "  2.42952621e+00 2.45050184e+00 2.47146570e+00 2.49595169e+00\n",
            "  2.52071159e+00 2.54578270e+00 2.57128894e+00 2.59679849e+00\n",
            "  2.62239244e+00 7.52578010e-03 5.31313065e-03 5.34661714e-03\n",
            "  5.33547031e-03 5.37972893e-03 5.39108377e-03 5.34343811e-03\n",
            "  5.00717627e-03 4.79218724e-03 4.19363706e-03 3.07507169e-03\n",
            "  3.10349823e-03 3.79033464e-04]]\n",
            "DEBUGGING: baseline2 looks like: 3.0563602019133596\n",
            "DEBUGGING: ADS looks like: [ 1.13049928e+00  1.14060527e+00  1.14395921e+00  1.18122464e+00\n",
            "  1.24127893e+00  1.27231905e+00  1.29257124e+00  1.29749484e+00\n",
            "  1.33387589e+00  1.37887360e+00  1.41121867e+00  1.45857797e+00\n",
            "  1.51671082e+00  1.56249568e+00  1.58612261e+00  1.62308443e+00\n",
            "  1.67010222e+00  1.75333881e+00  1.82399133e+00  1.93186607e+00\n",
            " -1.27676878e+00 -1.23307006e+00 -1.21217616e+00 -1.16555337e+00\n",
            " -1.13384637e+00 -1.06803578e+00 -1.07386957e+00 -1.08078006e+00\n",
            " -1.02323302e+00 -9.87586536e-01 -9.25389484e-01 -8.11542907e-01\n",
            " -7.21972090e-01 -6.34996899e-01 -6.04749650e-01 -5.53193778e-01\n",
            " -5.56866868e-01 -4.12657553e-01 -2.99760393e-01 -2.72228992e-01\n",
            " -2.73160277e-01 -2.15634630e-01 -2.14678087e-01 -2.15604637e-01\n",
            " -1.78097667e-01 -1.77661734e-01 -1.48102600e-01 -1.48012962e-01\n",
            " -1.40290686e-01 -3.40255347e-02 -1.41651600e+00 -1.42287689e+00\n",
            " -1.43020876e+00 -1.41993941e+00 -1.39065652e+00 -1.38137329e+00\n",
            " -1.38574839e+00 -1.39445108e+00 -1.37696591e+00 -1.38349032e+00\n",
            " -1.39224383e+00 -1.39339078e+00 -1.36930032e+00 -1.36770570e+00\n",
            " -1.39128844e+00 -1.41783453e+00 -1.40974176e+00 -1.35988908e+00\n",
            " -1.32092652e+00 -1.24772444e+00 -1.19834137e+00 -1.15398388e+00\n",
            " -1.13230395e+00 -1.08500086e+00 -1.05248255e+00 -9.87600382e-01\n",
            " -1.00283736e+00 -1.01678112e+00 -9.63720927e-01 -9.41718820e-01\n",
            " -8.94103405e-01 -7.80394331e-01 -6.92216245e-01 -6.05010771e-01\n",
            " -5.74491607e-01 -5.23202456e-01 -5.26846268e-01 -3.82554433e-01\n",
            " -2.78507870e-01 -2.50842524e-01 -2.51818396e-01 -1.94115715e-01\n",
            " -1.93113752e-01 -1.94230885e-01 -1.58068962e-01 -1.58492985e-01\n",
            " -1.31328052e-01 -1.35712675e-01 -1.27876693e-01 -3.25094009e-02\n",
            "  5.83223149e+00  5.90232126e+00  5.97227025e+00  6.02781556e+00\n",
            "  6.13206039e+00  6.20747238e+00  6.24185571e+00  6.31100386e+00\n",
            "  6.39438720e+00  6.47905677e+00  6.56638251e+00  6.66236589e+00\n",
            "  6.74694163e+00  6.84840699e+00  6.92488489e+00  7.01659493e+00\n",
            "  7.11476510e+00  7.25316704e+00  7.37494094e+00  7.53843601e+00\n",
            "  7.67082360e+00  7.80479449e+00  7.90757929e+00  8.02661538e+00\n",
            "  8.15077142e+00  8.30924264e+00  8.39467537e+00  8.48206800e+00\n",
            "  8.63535971e+00  8.76855309e+00  8.92918722e+00  9.14111526e+00\n",
            "  9.33111844e+00  9.51814777e+00  9.65014806e+00  9.80400885e+00\n",
            "  9.90268228e+00 -4.12657553e-01 -2.99760392e-01 -2.72228992e-01\n",
            " -2.73160277e-01 -2.15634630e-01 -2.14678087e-01 -2.15604637e-01\n",
            " -1.78097667e-01 -1.77661734e-01 -1.48102600e-01 -1.48012962e-01\n",
            " -1.40290686e-01 -3.40255347e-02  2.75027002e-02  3.14098260e-02\n",
            "  8.93994689e-03  3.05872482e-02  8.01207738e-02  1.08992201e-01\n",
            "  1.06104564e-01  1.16418242e-01  1.49366981e-01  1.59306896e-01\n",
            "  1.41654334e-01  1.68726799e-01  1.81767432e-01 -1.51662084e+00\n",
            " -1.52390689e+00 -1.51712850e+00 -1.50108695e+00 -1.44896888e+00\n",
            " -1.41061231e+00 -1.33536711e+00 -1.27676878e+00 -1.23307006e+00\n",
            " -1.21217615e+00 -1.16555337e+00 -1.13384637e+00 -1.06803578e+00\n",
            " -1.07386957e+00 -1.08078006e+00 -1.02323302e+00 -9.87586536e-01\n",
            " -9.25389484e-01 -8.11542907e-01 -7.21972090e-01 -6.34996899e-01\n",
            " -6.04749650e-01 -5.53193778e-01 -5.56866868e-01 -4.12657553e-01\n",
            " -2.99760392e-01 -2.72228992e-01 -2.73160277e-01 -2.15634630e-01\n",
            " -2.14678087e-01 -2.15604637e-01 -1.78097667e-01 -1.77661734e-01\n",
            " -1.48102600e-01 -1.48012962e-01 -1.40290686e-01 -3.40255347e-02]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(4.5970, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-1.3322e-04,  3.6199e-04, -7.0712e-04,  ...,  8.6095e-04,\n",
            "         -4.6923e-04, -1.7117e-04],\n",
            "        [-6.9497e-05,  1.8905e-04, -3.6898e-04,  ...,  4.4945e-04,\n",
            "         -2.4492e-04, -7.3690e-05]])\n",
            "   Last layer:\n",
            "tensor([[-1.4268e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  5.8046e-05, -3.7572e-04,  1.9166e-04,  9.8258e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.6846e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 2.9691e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  1.1487e-04,  9.5693e-05,  1.9492e-04,  1.9231e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  9.2710e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.3199e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -4.5673e-05, -5.8743e-05, -7.3779e-05, -7.6699e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.7383e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 3.7092e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -6.4422e-05,  2.4206e-04, -1.7412e-04, -1.0856e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  9.7702e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.4112e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  1.6673e-04, -2.8293e-04,  3.7519e-04,  2.8014e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -7.9040e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.3546e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  1.1644e-04, -3.2042e-04,  2.8807e-04,  1.9532e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1780e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.9685e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  4.1553e-06,  2.7388e-04, -5.1142e-05,  6.9392e-06,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.4055e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.9551e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -6.6323e-05, -8.9985e-05, -1.0537e-04, -1.1120e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -7.1235e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-8.2763e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -6.1480e-05,  5.9415e-05, -1.2886e-04, -1.0344e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.3074e-06,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 2.5998e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  1.2310e-04,  1.7648e-05,  2.2803e-04,  2.0676e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.6585e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-8.0922e-05,  1.7526e-03,  6.1922e-04,  ..., -5.6090e-04,\n",
            "          1.9354e-05, -1.5315e-02],\n",
            "        [-4.2582e-05,  9.3010e-04,  3.2879e-04,  ..., -2.9720e-04,\n",
            "          1.0594e-05, -8.0195e-03]])\n",
            "   Last layer:\n",
            "tensor([[ 0.0124,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0035,  0.0065,\n",
            "          0.0050,  0.0070,  0.0000,  0.0000,  0.0000,  0.0000,  0.0058,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0093,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0025,  0.0055,\n",
            "          0.0034,  0.0046,  0.0000,  0.0000,  0.0000,  0.0000,  0.0043,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0039,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0010, -0.0023,\n",
            "         -0.0014, -0.0019,  0.0000,  0.0000,  0.0000,  0.0000, -0.0018,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0101,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0028, -0.0054,\n",
            "         -0.0040, -0.0055,  0.0000,  0.0000,  0.0000,  0.0000, -0.0047,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0193,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0054,  0.0105,\n",
            "          0.0076,  0.0105,  0.0000,  0.0000,  0.0000,  0.0000,  0.0090,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0166,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0046,  0.0092,\n",
            "          0.0065,  0.0089,  0.0000,  0.0000,  0.0000,  0.0000,  0.0077,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0058,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0016, -0.0032,\n",
            "         -0.0023, -0.0032,  0.0000,  0.0000,  0.0000,  0.0000, -0.0027,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0029,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0008, -0.0015,\n",
            "         -0.0012, -0.0016,  0.0000,  0.0000,  0.0000,  0.0000, -0.0013,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0053,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0015, -0.0028,\n",
            "         -0.0022, -0.0030,  0.0000,  0.0000,  0.0000,  0.0000, -0.0025,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0098,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0027,  0.0055,\n",
            "          0.0037,  0.0051,  0.0000,  0.0000,  0.0000,  0.0000,  0.0045,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000]])\n",
            "DEBUGGING: training for one iteration takes 0.004172 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 45\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0184, 0.0153, 0.0148, 0.0152, 0.0149, 0.0152, 0.0149, 0.0150, 0.0147,\n",
            "        0.0142, 0.0150, 0.0147, 0.0148, 0.0149, 0.0139, 0.0149, 0.0145, 0.0149,\n",
            "        0.0154, 0.0150, 0.0151, 0.0152, 0.0154, 0.0143, 0.0152, 0.0147, 0.0150,\n",
            "        0.0152, 0.0152, 0.0151, 0.0139, 0.0153, 0.0155, 0.0148, 0.0151, 0.0148,\n",
            "        0.0144, 0.0144, 0.0150, 0.0142, 0.0142, 0.0143, 0.0148, 0.0145, 0.0150,\n",
            "        0.0150, 0.0148, 0.0146, 0.0161, 0.0154, 0.0150, 0.0144, 0.0149, 0.0157,\n",
            "        0.0151, 0.0150, 0.0143, 0.0146, 0.0147, 0.0149, 0.0155, 0.0144, 0.0150,\n",
            "        0.0146, 0.0154, 0.0146, 0.0149], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [57]\n",
            "DEBUGGING: logits looks like: tensor([9.6678, 9.6309, 9.6246, 9.6295, 9.6257, 9.6294, 9.6251, 9.6267, 9.6234,\n",
            "        9.6163, 9.6269, 9.6227, 9.6245, 9.6258, 9.6122, 9.6254, 9.6204, 9.6258,\n",
            "        9.6320, 9.6267, 9.6282, 9.6291, 9.6319, 9.6173, 9.6295, 9.6229, 9.6264,\n",
            "        9.6298, 9.6297, 9.6279, 9.6122, 9.6311, 9.6330, 9.6238, 9.6286, 9.6236,\n",
            "        9.6194, 9.6185, 9.6263, 9.6162, 9.6164, 9.6174, 9.6238, 9.6199, 9.6263,\n",
            "        9.6264, 9.6239, 9.6218, 9.6404, 9.6318, 9.6263, 9.6189, 9.6254, 9.6356,\n",
            "        9.6283, 9.6271, 9.6176, 9.6214, 9.6222, 9.6260, 9.6336, 9.6190, 9.6267,\n",
            "        9.6216, 9.6325, 9.6212, 9.6250], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0165, 0.0130, 0.0129, 0.0130, 0.0129, 0.0130, 0.0130, 0.0129, 0.0129,\n",
            "        0.0130, 0.0130, 0.0130, 0.0129, 0.0130, 0.0129, 0.0129, 0.0129, 0.0130,\n",
            "        0.0129, 0.0129, 0.0129, 0.0129, 0.0131, 0.0129, 0.0129, 0.0129, 0.0129,\n",
            "        0.0129, 0.0129, 0.0129, 0.0129, 0.0130, 0.0129, 0.0129, 0.0129, 0.0130,\n",
            "        0.0130, 0.0129, 0.0130, 0.0130, 0.0130, 0.0129, 0.0130, 0.0130, 0.0129,\n",
            "        0.0129, 0.0130, 0.0130, 0.0127, 0.0130, 0.0126, 0.0129, 0.0130, 0.0129,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0129, 0.0131, 0.0130, 0.0129, 0.0129,\n",
            "        0.0128, 0.0131, 0.0130, 0.0128, 0.0129, 0.0129, 0.0130, 0.0130, 0.0129,\n",
            "        0.0129, 0.0129, 0.0129, 0.0130, 0.0129], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [11]\n",
            "DEBUGGING: logits looks like: tensor([9.6756, 9.6283, 9.6262, 9.6282, 9.6265, 9.6276, 9.6282, 9.6270, 9.6274,\n",
            "        9.6279, 9.6283, 9.6277, 9.6275, 9.6277, 9.6270, 9.6270, 9.6262, 9.6280,\n",
            "        9.6274, 9.6267, 9.6269, 9.6267, 9.6303, 9.6270, 9.6269, 9.6272, 9.6272,\n",
            "        9.6272, 9.6274, 9.6271, 9.6269, 9.6279, 9.6273, 9.6262, 9.6275, 9.6289,\n",
            "        9.6288, 9.6268, 9.6276, 9.6278, 9.6277, 9.6270, 9.6277, 9.6276, 9.6273,\n",
            "        9.6275, 9.6288, 9.6287, 9.6243, 9.6279, 9.6224, 9.6270, 9.6278, 9.6269,\n",
            "        9.6282, 9.6291, 9.6283, 9.6287, 9.6267, 9.6293, 9.6278, 9.6269, 9.6272,\n",
            "        9.6259, 9.6297, 9.6285, 9.6258, 9.6268, 9.6268, 9.6275, 9.6285, 9.6269,\n",
            "        9.6261, 9.6264, 9.6268, 9.6276, 9.6274], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0145, 0.0114, 0.0115, 0.0114, 0.0114, 0.0115, 0.0115, 0.0115, 0.0114,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0114, 0.0115, 0.0114, 0.0114,\n",
            "        0.0115, 0.0114, 0.0114, 0.0114, 0.0115, 0.0115, 0.0114, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0114, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0114, 0.0115, 0.0115, 0.0115, 0.0114, 0.0115, 0.0115, 0.0114,\n",
            "        0.0114, 0.0115, 0.0115, 0.0114, 0.0114, 0.0115, 0.0115, 0.0115, 0.0114,\n",
            "        0.0115, 0.0114, 0.0115, 0.0114, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0114, 0.0115, 0.0115, 0.0115, 0.0115, 0.0114, 0.0115, 0.0115,\n",
            "        0.0114, 0.0114, 0.0115, 0.0114, 0.0114, 0.0115, 0.0114, 0.0115, 0.0115,\n",
            "        0.0115, 0.0113, 0.0115, 0.0114, 0.0115, 0.0114],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [42]\n",
            "DEBUGGING: logits looks like: tensor([9.6760, 9.6287, 9.6297, 9.6283, 9.6288, 9.6295, 9.6290, 9.6292, 9.6284,\n",
            "        9.6289, 9.6294, 9.6292, 9.6290, 9.6292, 9.6288, 9.6289, 9.6285, 9.6288,\n",
            "        9.6291, 9.6287, 9.6282, 9.6288, 9.6291, 9.6293, 9.6288, 9.6298, 9.6290,\n",
            "        9.6295, 9.6289, 9.6291, 9.6299, 9.6300, 9.6288, 9.6291, 9.6292, 9.6293,\n",
            "        9.6296, 9.6283, 9.6290, 9.6292, 9.6290, 9.6288, 9.6290, 9.6295, 9.6288,\n",
            "        9.6283, 9.6299, 9.6295, 9.6287, 9.6287, 9.6291, 9.6289, 9.6290, 9.6286,\n",
            "        9.6295, 9.6281, 9.6301, 9.6280, 9.6293, 9.6293, 9.6297, 9.6290, 9.6290,\n",
            "        9.6295, 9.6286, 9.6290, 9.6289, 9.6289, 9.6291, 9.6280, 9.6297, 9.6297,\n",
            "        9.6285, 9.6286, 9.6290, 9.6287, 9.6286, 9.6295, 9.6286, 9.6296, 9.6300,\n",
            "        9.6290, 9.6268, 9.6297, 9.6285, 9.6289, 9.6282],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0126, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0099, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0099, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0099, 0.0100, 0.0100, 0.0099, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0099, 0.0099, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0099, 0.0100, 0.0099, 0.0100, 0.0100, 0.0099, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0099, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0099, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [27]\n",
            "DEBUGGING: logits looks like: tensor([9.6763, 9.6297, 9.6296, 9.6302, 9.6305, 9.6298, 9.6295, 9.6306, 9.6298,\n",
            "        9.6287, 9.6304, 9.6300, 9.6295, 9.6293, 9.6293, 9.6289, 9.6298, 9.6298,\n",
            "        9.6298, 9.6292, 9.6293, 9.6304, 9.6292, 9.6299, 9.6301, 9.6299, 9.6292,\n",
            "        9.6302, 9.6291, 9.6296, 9.6299, 9.6291, 9.6299, 9.6296, 9.6297, 9.6295,\n",
            "        9.6305, 9.6299, 9.6299, 9.6298, 9.6293, 9.6295, 9.6296, 9.6300, 9.6301,\n",
            "        9.6302, 9.6292, 9.6290, 9.6292, 9.6296, 9.6295, 9.6296, 9.6302, 9.6296,\n",
            "        9.6302, 9.6305, 9.6296, 9.6306, 9.6295, 9.6293, 9.6294, 9.6302, 9.6295,\n",
            "        9.6299, 9.6282, 9.6293, 9.6285, 9.6294, 9.6301, 9.6291, 9.6295, 9.6297,\n",
            "        9.6298, 9.6302, 9.6298, 9.6300, 9.6295, 9.6290, 9.6296, 9.6299, 9.6295,\n",
            "        9.6302, 9.6296, 9.6295, 9.6301, 9.6307, 9.6294, 9.6301, 9.6296, 9.6295,\n",
            "        9.6296, 9.6298, 9.6301, 9.6287, 9.6301, 9.6299, 9.6296, 9.6298, 9.6293,\n",
            "        9.6298], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0121, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0092, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [100]\n",
            "DEBUGGING: logits looks like: tensor([9.6826, 9.6311, 9.6311, 9.6313, 9.6312, 9.6314, 9.6312, 9.6314, 9.6312,\n",
            "        9.6312, 9.6312, 9.6309, 9.6313, 9.6313, 9.6313, 9.6313, 9.6314, 9.6312,\n",
            "        9.6312, 9.6310, 9.6313, 9.6313, 9.6312, 9.6311, 9.6310, 9.6311, 9.6311,\n",
            "        9.6312, 9.6312, 9.6313, 9.6313, 9.6311, 9.6312, 9.6311, 9.6312, 9.6314,\n",
            "        9.6312, 9.6312, 9.6314, 9.6313, 9.6312, 9.6312, 9.6312, 9.6312, 9.6310,\n",
            "        9.6311, 9.6310, 9.6312, 9.6311, 9.6294, 9.6311, 9.6313, 9.6313, 9.6312,\n",
            "        9.6309, 9.6310, 9.6310, 9.6311, 9.6312, 9.6311, 9.6313, 9.6312, 9.6314,\n",
            "        9.6312, 9.6308, 9.6314, 9.6313, 9.6309, 9.6313, 9.6313, 9.6312, 9.6314,\n",
            "        9.6312, 9.6311, 9.6312, 9.6315, 9.6314, 9.6313, 9.6314, 9.6311, 9.6311,\n",
            "        9.6313, 9.6313, 9.6305, 9.6311, 9.6310, 9.6312, 9.6313, 9.6311, 9.6313,\n",
            "        9.6313, 9.6312, 9.6313, 9.6311, 9.6311, 9.6313, 9.6313, 9.6312, 9.6312,\n",
            "        9.6310, 9.6312, 9.6310, 9.6314, 9.6313, 9.6309, 9.6310, 9.6310],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.15961254752937748 and immediate abs rewards look like: [0.01799576026496652, 0.026744476163003128, 0.03135789359180308, 0.0004539019696494506, 0.0012486800892475003, 0.014126603351996891, 0.001500459893122752, 0.0015946378978242137, 0.007623891840239594, 0.00038862336486999993, 0.009945712366288717, 0.0006095077260397375, 0.0014134768787243956, 0.00015376143778667029, 0.005302003797169164, 0.000649346372938453, 0.0002610709696000413, 0.00045165699384597247, 0.0015057073326261161, 0.0022004500094681134, 0.0009919990254729782, 0.0005260869741050556, 0.0007113821823168109, 0.0013220422215454164, 0.0011601435305692576, 0.005239952026386163, 0.00485219827032779, 0.00012154467344771547, 0.0003717393831266236, 0.00037446254077622143, 0.0002938458978860581, 0.0006600053288821073, 0.0034970698091001395, 9.184967120745569e-05, 0.0022953992558996106, 1.9895831883331994e-05, 0.0040984085055697506, 0.00039096517366488115, 0.0017052482439794403, 0.0015230851552132663, 0.0005528194208181958, 0.00023898158019619586, 0.0005299733916217519, 0.0007646232568276901, 0.0003125283888039121, 1.7595125882508e-05, 0.0009565246655256487, 8.430995103481109e-05, 0.0003784848524901463, 1.7608836060389876e-06]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.5060969044176 and immediate relative rewards look like: [0.0049887784888337635, 0.014902534462617474, 0.02640659111702596, 0.0005141690510255079, 0.0017683201395857447, 0.024014985300860953, 0.0029878411587629375, 0.0036305511000822893, 0.01953605445646528, 0.0011088945907871924, 0.031220373821654202, 0.002093169286703914, 0.005259587864087545, 0.0006164116874092445, 0.022774340690436462, 0.0029796881042000478, 0.0012731008125909417, 0.002332217562059007, 0.008208017186425103, 0.012632032930722886, 0.005983243462449872, 0.003325141410632073, 0.004701390818086151, 0.009118867069770236, 0.00833875265340982, 0.03918274904105259, 0.037735623880609324, 0.0009816362677281631, 0.0031096279475360564, 0.003240768916514047, 0.002628128972190655, 0.00609395984843888, 0.03330451572885614, 0.0009021524393808311, 0.02320925548751137, 0.00020705590342431516, 0.04383716702187374, 0.004299936416125255, 0.01925048731202091, 0.017643636758751625, 0.0065669345130157375, 0.0029085653235831036, 0.0066041617412834035, 0.009751289857473401, 0.0040771823176144525, 0.00023466466819481753, 0.01303448098445909, 0.0011736544375905884, 0.005378675862211635, 2.5537543475259048e-05]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0264, 0.0143, 0.0159, 0.0149, 0.0145, 0.0140, 0.0138, 0.0145, 0.0141,\n",
            "        0.0146, 0.0141, 0.0134, 0.0144, 0.0144, 0.0151, 0.0141, 0.0152, 0.0162,\n",
            "        0.0144, 0.0146, 0.0151, 0.0140, 0.0144, 0.0144, 0.0143, 0.0145, 0.0153,\n",
            "        0.0139, 0.0134, 0.0139, 0.0142, 0.0142, 0.0149, 0.0160, 0.0156, 0.0142,\n",
            "        0.0140, 0.0147, 0.0156, 0.0141, 0.0196, 0.0142, 0.0139, 0.0147, 0.0145,\n",
            "        0.0152, 0.0145, 0.0135, 0.0140, 0.0144, 0.0143, 0.0140, 0.0146, 0.0148,\n",
            "        0.0143, 0.0139, 0.0140, 0.0147, 0.0142, 0.0146, 0.0146, 0.0138, 0.0142,\n",
            "        0.0143, 0.0155, 0.0143, 0.0135, 0.0143], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [52]\n",
            "DEBUGGING: logits looks like: tensor([9.6986, 9.5763, 9.5977, 9.5839, 9.5793, 9.5719, 9.5689, 9.5791, 9.5728,\n",
            "        9.5795, 9.5730, 9.5629, 9.5771, 9.5767, 9.5863, 9.5728, 9.5887, 9.6008,\n",
            "        9.5778, 9.5795, 9.5873, 9.5718, 9.5767, 9.5767, 9.5764, 9.5785, 9.5900,\n",
            "        9.5705, 9.5635, 9.5701, 9.5739, 9.5747, 9.5842, 9.5982, 9.5928, 9.5742,\n",
            "        9.5720, 9.5818, 9.5931, 9.5734, 9.6390, 9.5752, 9.5703, 9.5819, 9.5787,\n",
            "        9.5879, 9.5783, 9.5639, 9.5714, 9.5778, 9.5765, 9.5720, 9.5795, 9.5822,\n",
            "        9.5755, 9.5702, 9.5713, 9.5814, 9.5742, 9.5797, 9.5802, 9.5687, 9.5739,\n",
            "        9.5765, 9.5924, 9.5764, 9.5648, 9.5761], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0138, 0.0120, 0.0137, 0.0111, 0.0117, 0.0120, 0.0096, 0.0147, 0.0111,\n",
            "        0.0123, 0.0106, 0.0139, 0.0143, 0.0110, 0.0142, 0.0136, 0.0137, 0.0137,\n",
            "        0.0113, 0.0114, 0.0128, 0.0126, 0.0142, 0.0126, 0.0153, 0.0135, 0.0114,\n",
            "        0.0110, 0.0119, 0.0117, 0.0144, 0.0140, 0.0112, 0.0135, 0.0141, 0.0117,\n",
            "        0.0134, 0.0136, 0.0126, 0.0135, 0.0145, 0.0133, 0.0127, 0.0125, 0.0133,\n",
            "        0.0139, 0.0123, 0.0130, 0.0122, 0.0118, 0.0129, 0.0129, 0.0129, 0.0129,\n",
            "        0.0119, 0.0131, 0.0131, 0.0128, 0.0130, 0.0117, 0.0139, 0.0128, 0.0134,\n",
            "        0.0112, 0.0108, 0.0149, 0.0135, 0.0153, 0.0125, 0.0122, 0.0129, 0.0130,\n",
            "        0.0142, 0.0122, 0.0105, 0.0132, 0.0139, 0.0144],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [72]\n",
            "DEBUGGING: logits looks like: tensor([9.6139, 9.5870, 9.6127, 9.5717, 9.5809, 9.5864, 9.5427, 9.6272, 9.5711,\n",
            "        9.5914, 9.5610, 9.6166, 9.6212, 9.5685, 9.6197, 9.6124, 9.6126, 9.6135,\n",
            "        9.5741, 9.5772, 9.5989, 9.5960, 9.6202, 9.5965, 9.6349, 9.6108, 9.5771,\n",
            "        9.5692, 9.5858, 9.5818, 9.6230, 9.6175, 9.5723, 9.6105, 9.6191, 9.5821,\n",
            "        9.6084, 9.6124, 9.5968, 9.6105, 9.6249, 9.6066, 9.5974, 9.5943, 9.6073,\n",
            "        9.6154, 9.5911, 9.6027, 9.5902, 9.5833, 9.6017, 9.6016, 9.6005, 9.6006,\n",
            "        9.5846, 9.6037, 9.6041, 9.6003, 9.6023, 9.5814, 9.6157, 9.5989, 9.6081,\n",
            "        9.5736, 9.5648, 9.6305, 9.6105, 9.6349, 9.5944, 9.5894, 9.6018, 9.6034,\n",
            "        9.6199, 9.5908, 9.5606, 9.6056, 9.6160, 9.6237],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0116, 0.0093, 0.0156, 0.0092, 0.0137, 0.0100, 0.0114, 0.0120, 0.0095,\n",
            "        0.0104, 0.0121, 0.0130, 0.0127, 0.0100, 0.0117, 0.0182, 0.0106, 0.0102,\n",
            "        0.0114, 0.0115, 0.0111, 0.0138, 0.0091, 0.0095, 0.0078, 0.0124, 0.0113,\n",
            "        0.0146, 0.0097, 0.0102, 0.0129, 0.0122, 0.0112, 0.0127, 0.0131, 0.0114,\n",
            "        0.0089, 0.0117, 0.0132, 0.0116, 0.0105, 0.0102, 0.0101, 0.0132, 0.0110,\n",
            "        0.0095, 0.0102, 0.0118, 0.0121, 0.0116, 0.0107, 0.0115, 0.0097, 0.0105,\n",
            "        0.0139, 0.0112, 0.0126, 0.0119, 0.0124, 0.0111, 0.0132, 0.0110, 0.0118,\n",
            "        0.0110, 0.0125, 0.0131, 0.0121, 0.0130, 0.0112, 0.0122, 0.0108, 0.0119,\n",
            "        0.0129, 0.0125, 0.0089, 0.0122, 0.0102, 0.0091, 0.0121, 0.0114, 0.0117,\n",
            "        0.0094, 0.0111, 0.0130, 0.0116, 0.0102, 0.0113],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [75]\n",
            "DEBUGGING: logits looks like: tensor([9.6259, 9.5800, 9.6842, 9.5777, 9.6591, 9.5964, 9.6216, 9.6327, 9.5849,\n",
            "        9.6040, 9.6342, 9.6474, 9.6428, 9.5960, 9.6267, 9.7147, 9.6077, 9.5993,\n",
            "        9.6225, 9.6235, 9.6163, 9.6594, 9.5775, 9.5849, 9.5463, 9.6391, 9.6197,\n",
            "        9.6710, 9.5897, 9.5985, 9.6457, 9.6359, 9.6186, 9.6436, 9.6491, 9.6212,\n",
            "        9.5717, 9.6262, 9.6515, 9.6259, 9.6057, 9.5995, 9.5982, 9.6512, 9.6154,\n",
            "        9.5843, 9.6001, 9.6282, 9.6333, 9.6258, 9.6083, 9.6228, 9.5903, 9.6058,\n",
            "        9.6612, 9.6183, 9.6419, 9.6294, 9.6387, 9.6164, 9.6515, 9.6142, 9.6287,\n",
            "        9.6145, 9.6397, 9.6496, 9.6341, 9.6472, 9.6174, 9.6353, 9.6107, 9.6310,\n",
            "        9.6469, 9.6400, 9.5722, 9.6354, 9.5996, 9.5775, 9.6339, 9.6211, 9.6266,\n",
            "        9.5825, 9.6159, 9.6477, 9.6255, 9.5998, 9.6205],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0091, 0.0118, 0.0113, 0.0112, 0.0108, 0.0109, 0.0108, 0.0120, 0.0101,\n",
            "        0.0106, 0.0102, 0.0102, 0.0106, 0.0104, 0.0097, 0.0119, 0.0122, 0.0108,\n",
            "        0.0110, 0.0106, 0.0119, 0.0100, 0.0112, 0.0102, 0.0118, 0.0113, 0.0100,\n",
            "        0.0103, 0.0105, 0.0107, 0.0103, 0.0114, 0.0109, 0.0106, 0.0086, 0.0098,\n",
            "        0.0099, 0.0107, 0.0101, 0.0103, 0.0104, 0.0107, 0.0096, 0.0107, 0.0105,\n",
            "        0.0103, 0.0102, 0.0091, 0.0096, 0.0114, 0.0110, 0.0104, 0.0103, 0.0093,\n",
            "        0.0107, 0.0101, 0.0110, 0.0099, 0.0110, 0.0102, 0.0108, 0.0120, 0.0105,\n",
            "        0.0099, 0.0104, 0.0102, 0.0103, 0.0106, 0.0108, 0.0105, 0.0105, 0.0109,\n",
            "        0.0095, 0.0105, 0.0101, 0.0109, 0.0117, 0.0108, 0.0098, 0.0109, 0.0099,\n",
            "        0.0109, 0.0113, 0.0104, 0.0094, 0.0103, 0.0105, 0.0100, 0.0097, 0.0101,\n",
            "        0.0108, 0.0106, 0.0104, 0.0101, 0.0105], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [30]\n",
            "DEBUGGING: logits looks like: tensor([9.6128, 9.6643, 9.6553, 9.6534, 9.6474, 9.6485, 9.6463, 9.6680, 9.6333,\n",
            "        9.6433, 9.6361, 9.6354, 9.6422, 9.6395, 9.6256, 9.6659, 9.6708, 9.6470,\n",
            "        9.6499, 9.6433, 9.6656, 9.6310, 9.6545, 9.6354, 9.6649, 9.6552, 9.6320,\n",
            "        9.6373, 9.6404, 9.6443, 9.6372, 9.6582, 9.6483, 9.6429, 9.6011, 9.6268,\n",
            "        9.6297, 9.6442, 9.6335, 9.6374, 9.6386, 9.6448, 9.6231, 9.6453, 9.6410,\n",
            "        9.6365, 9.6346, 9.6122, 9.6229, 9.6575, 9.6508, 9.6399, 9.6370, 9.6176,\n",
            "        9.6456, 9.6341, 9.6501, 9.6285, 9.6498, 9.6353, 9.6462, 9.6672, 9.6408,\n",
            "        9.6291, 9.6399, 9.6346, 9.6367, 9.6434, 9.6461, 9.6414, 9.6411, 9.6487,\n",
            "        9.6207, 9.6405, 9.6330, 9.6490, 9.6620, 9.6460, 9.6281, 9.6483, 9.6297,\n",
            "        9.6483, 9.6560, 9.6391, 9.6182, 9.6378, 9.6414, 9.6312, 9.6251, 9.6339,\n",
            "        9.6470, 9.6421, 9.6393, 9.6334, 9.6414], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0090, 0.0081, 0.0079, 0.0080, 0.0093, 0.0091, 0.0089, 0.0094, 0.0093,\n",
            "        0.0109, 0.0090, 0.0090, 0.0092, 0.0092, 0.0095, 0.0095, 0.0098, 0.0099,\n",
            "        0.0090, 0.0088, 0.0090, 0.0093, 0.0091, 0.0090, 0.0092, 0.0089, 0.0088,\n",
            "        0.0095, 0.0091, 0.0091, 0.0089, 0.0092, 0.0091, 0.0091, 0.0094, 0.0091,\n",
            "        0.0095, 0.0087, 0.0094, 0.0102, 0.0092, 0.0093, 0.0094, 0.0092, 0.0089,\n",
            "        0.0095, 0.0089, 0.0090, 0.0095, 0.0091, 0.0093, 0.0090, 0.0096, 0.0091,\n",
            "        0.0094, 0.0094, 0.0094, 0.0091, 0.0093, 0.0091, 0.0091, 0.0088, 0.0094,\n",
            "        0.0096, 0.0092, 0.0092, 0.0090, 0.0089, 0.0090, 0.0089, 0.0093, 0.0091,\n",
            "        0.0090, 0.0092, 0.0092, 0.0090, 0.0090, 0.0093, 0.0088, 0.0094, 0.0092,\n",
            "        0.0091, 0.0092, 0.0090, 0.0093, 0.0092, 0.0087, 0.0092, 0.0090, 0.0092,\n",
            "        0.0093, 0.0102, 0.0094, 0.0092, 0.0092, 0.0090, 0.0095, 0.0092, 0.0094,\n",
            "        0.0091, 0.0098, 0.0090, 0.0088, 0.0092, 0.0089, 0.0090, 0.0091, 0.0092,\n",
            "        0.0090], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [2]\n",
            "DEBUGGING: logits looks like: tensor([9.6143, 9.5931, 9.5878, 9.5898, 9.6208, 9.6153, 9.6116, 9.6228, 9.6208,\n",
            "        9.6525, 9.6139, 9.6143, 9.6176, 9.6193, 9.6251, 9.6248, 9.6304, 9.6321,\n",
            "        9.6141, 9.6096, 9.6144, 9.6197, 9.6158, 9.6150, 9.6175, 9.6109, 9.6094,\n",
            "        9.6243, 9.6159, 9.6167, 9.6124, 9.6182, 9.6158, 9.6170, 9.6220, 9.6152,\n",
            "        9.6240, 9.6082, 9.6222, 9.6382, 9.6180, 9.6216, 9.6221, 9.6181, 9.6118,\n",
            "        9.6251, 9.6120, 9.6142, 9.6240, 9.6164, 9.6214, 9.6139, 9.6279, 9.6173,\n",
            "        9.6223, 9.6220, 9.6230, 9.6168, 9.6206, 9.6169, 9.6153, 9.6093, 9.6222,\n",
            "        9.6260, 9.6185, 9.6186, 9.6144, 9.6110, 9.6144, 9.6116, 9.6214, 9.6173,\n",
            "        9.6148, 9.6194, 9.6195, 9.6147, 9.6137, 9.6215, 9.6096, 9.6233, 9.6174,\n",
            "        9.6167, 9.6193, 9.6138, 9.6203, 9.6192, 9.6068, 9.6193, 9.6141, 9.6185,\n",
            "        9.6197, 9.6390, 9.6235, 9.6188, 9.6177, 9.6143, 9.6253, 9.6181, 9.6234,\n",
            "        9.6160, 9.6301, 9.6131, 9.6103, 9.6182, 9.6123, 9.6138, 9.6167, 9.6194,\n",
            "        9.6143], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.886741328986318 and immediate abs rewards look like: [0.011369354286671296, 0.010197175202392827, 0.0021200941255301586, 0.004781252173415851, 0.017176935924453574, 0.00047284885522458353, 0.012567216647767054, 0.0023015557362668915, 0.00703162052332118, 0.009586459365436895, 0.005077368700767693, 0.8040594474314275, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 9.094947017729282e-13, 4.547473508864641e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 3.605724559271786 and immediate relative rewards look like: [0.003938473521185698, 0.007092769417081334, 0.002219858410641049, 0.0066799312838388275, 0.030047747715172546, 0.0009985909011227217, 0.0309687332091283, 0.00651063590993543, 0.022395638347474482, 0.03400993438685268, 0.019881927924515985, 3.4409803180226928, 2.9558577807620168e-12, 3.1832314562059726e-12, 3.410605131648481e-12, 0.0, 3.865352482534066e-12, 4.092726157978177e-12, 4.320099833420427e-12, 4.547473508864641e-12, 4.774847184308959e-12, 5.002220859751105e-12, 5.229594535193148e-12, 0.0, 0.0, 0.0, 0.0, 1.2732925824820995e-11, 6.593836587850731e-12, 6.821210263295411e-12, 7.048583938740194e-12, 7.2759576141817715e-12, 7.503331289626658e-12, 7.730704965068132e-12, 0.0, 0.0, 0.0, 0.0, 8.86757334228605e-12, 9.09494701773135e-12, 0.0, 0.0, 0.0, 1.000444171950221e-11, 2.0463630789886232e-11, 1.0459189070391053e-11, 1.0686562745831907e-11, 2.1827872842545315e-11, 2.2282620193441808e-11, 1.1368683772159018e-11]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0155, 0.0148, 0.0144, 0.0148, 0.0149, 0.0144, 0.0151, 0.0153, 0.0147,\n",
            "        0.0147, 0.0144, 0.0135, 0.0145, 0.0146, 0.0145, 0.0141, 0.0146, 0.0146,\n",
            "        0.0144, 0.0147, 0.0151, 0.0151, 0.0144, 0.0155, 0.0145, 0.0144, 0.0145,\n",
            "        0.0144, 0.0150, 0.0145, 0.0145, 0.0144, 0.0146, 0.0151, 0.0154, 0.0153,\n",
            "        0.0145, 0.0144, 0.0147, 0.0145, 0.0146, 0.0152, 0.0145, 0.0152, 0.0150,\n",
            "        0.0149, 0.0142, 0.0147, 0.0144, 0.0146, 0.0148, 0.0151, 0.0144, 0.0146,\n",
            "        0.0147, 0.0150, 0.0143, 0.0154, 0.0153, 0.0151, 0.0142, 0.0149, 0.0145,\n",
            "        0.0145, 0.0145, 0.0151, 0.0147, 0.0140], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [30]\n",
            "DEBUGGING: logits looks like: tensor([9.6273, 9.6180, 9.6125, 9.6179, 9.6193, 9.6125, 9.6221, 9.6247, 9.6165,\n",
            "        9.6158, 9.6130, 9.5991, 9.6138, 9.6151, 9.6144, 9.6079, 9.6144, 9.6157,\n",
            "        9.6127, 9.6164, 9.6212, 9.6221, 9.6122, 9.6273, 9.6140, 9.6126, 9.6130,\n",
            "        9.6127, 9.6207, 9.6135, 9.6135, 9.6123, 9.6149, 9.6220, 9.6254, 9.6250,\n",
            "        9.6141, 9.6124, 9.6164, 9.6143, 9.6152, 9.6228, 9.6139, 9.6234, 9.6203,\n",
            "        9.6194, 9.6099, 9.6161, 9.6120, 9.6144, 9.6176, 9.6220, 9.6118, 9.6155,\n",
            "        9.6165, 9.6210, 9.6115, 9.6260, 9.6246, 9.6214, 9.6090, 9.6198, 9.6134,\n",
            "        9.6138, 9.6141, 9.6213, 9.6165, 9.6070], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0127, 0.0129, 0.0128, 0.0127, 0.0126, 0.0122, 0.0128, 0.0128, 0.0129,\n",
            "        0.0126, 0.0126, 0.0128, 0.0126, 0.0126, 0.0128, 0.0126, 0.0127, 0.0131,\n",
            "        0.0126, 0.0126, 0.0124, 0.0129, 0.0130, 0.0128, 0.0126, 0.0123, 0.0129,\n",
            "        0.0130, 0.0129, 0.0125, 0.0128, 0.0129, 0.0124, 0.0125, 0.0130, 0.0128,\n",
            "        0.0129, 0.0121, 0.0126, 0.0128, 0.0126, 0.0123, 0.0126, 0.0126, 0.0126,\n",
            "        0.0124, 0.0121, 0.0130, 0.0123, 0.0128, 0.0123, 0.0128, 0.0123, 0.0131,\n",
            "        0.0126, 0.0126, 0.0126, 0.0128, 0.0129, 0.0128, 0.0127, 0.0124, 0.0127,\n",
            "        0.0126, 0.0126, 0.0127, 0.0127, 0.0127, 0.0128, 0.0126, 0.0126, 0.0126,\n",
            "        0.0130, 0.0127, 0.0125, 0.0122, 0.0132, 0.0120, 0.0127],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [62]\n",
            "DEBUGGING: logits looks like: tensor([9.6273, 9.6301, 9.6289, 9.6267, 9.6265, 9.6194, 9.6281, 9.6295, 9.6307,\n",
            "        9.6254, 9.6262, 9.6292, 9.6257, 9.6262, 9.6291, 9.6255, 9.6275, 9.6332,\n",
            "        9.6263, 9.6252, 9.6221, 9.6307, 9.6324, 9.6282, 9.6261, 9.6210, 9.6310,\n",
            "        9.6313, 9.6312, 9.6238, 9.6293, 9.6301, 9.6220, 9.6244, 9.6321, 9.6292,\n",
            "        9.6298, 9.6181, 9.6259, 9.6289, 9.6256, 9.6203, 9.6261, 9.6261, 9.6257,\n",
            "        9.6219, 9.6180, 9.6323, 9.6202, 9.6285, 9.6214, 9.6287, 9.6211, 9.6328,\n",
            "        9.6257, 9.6262, 9.6253, 9.6290, 9.6305, 9.6284, 9.6274, 9.6229, 9.6274,\n",
            "        9.6253, 9.6257, 9.6276, 9.6267, 9.6269, 9.6293, 9.6252, 9.6256, 9.6259,\n",
            "        9.6316, 9.6280, 9.6235, 9.6198, 9.6349, 9.6155, 9.6272],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0112, 0.0114, 0.0114, 0.0112, 0.0114, 0.0113, 0.0113, 0.0115, 0.0113,\n",
            "        0.0113, 0.0114, 0.0113, 0.0116, 0.0114, 0.0113, 0.0113, 0.0114, 0.0113,\n",
            "        0.0114, 0.0114, 0.0113, 0.0114, 0.0112, 0.0115, 0.0117, 0.0113, 0.0118,\n",
            "        0.0114, 0.0112, 0.0113, 0.0112, 0.0112, 0.0113, 0.0116, 0.0117, 0.0113,\n",
            "        0.0114, 0.0113, 0.0113, 0.0114, 0.0112, 0.0114, 0.0113, 0.0112, 0.0116,\n",
            "        0.0113, 0.0115, 0.0114, 0.0114, 0.0114, 0.0113, 0.0114, 0.0113, 0.0114,\n",
            "        0.0113, 0.0114, 0.0114, 0.0114, 0.0116, 0.0114, 0.0114, 0.0116, 0.0113,\n",
            "        0.0113, 0.0116, 0.0115, 0.0115, 0.0111, 0.0114, 0.0111, 0.0114, 0.0111,\n",
            "        0.0115, 0.0115, 0.0114, 0.0115, 0.0115, 0.0114, 0.0113, 0.0111, 0.0113,\n",
            "        0.0113, 0.0112, 0.0115, 0.0112, 0.0112, 0.0112, 0.0112],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [19]\n",
            "DEBUGGING: logits looks like: tensor([9.6376, 9.6407, 9.6398, 9.6370, 9.6400, 9.6396, 9.6387, 9.6423, 9.6397,\n",
            "        9.6385, 9.6407, 9.6392, 9.6439, 9.6401, 9.6390, 9.6397, 9.6414, 9.6385,\n",
            "        9.6415, 9.6398, 9.6390, 9.6411, 9.6378, 9.6417, 9.6458, 9.6391, 9.6467,\n",
            "        9.6410, 9.6378, 9.6392, 9.6363, 9.6379, 9.6382, 9.6440, 9.6465, 9.6391,\n",
            "        9.6404, 9.6381, 9.6388, 9.6412, 9.6378, 9.6406, 9.6380, 9.6375, 9.6436,\n",
            "        9.6381, 9.6423, 9.6408, 9.6402, 9.6409, 9.6381, 9.6413, 9.6393, 9.6410,\n",
            "        9.6391, 9.6407, 9.6400, 9.6398, 9.6436, 9.6402, 9.6398, 9.6448, 9.6390,\n",
            "        9.6388, 9.6447, 9.6419, 9.6416, 9.6350, 9.6411, 9.6355, 9.6409, 9.6352,\n",
            "        9.6418, 9.6416, 9.6404, 9.6424, 9.6418, 9.6399, 9.6380, 9.6350, 9.6396,\n",
            "        9.6386, 9.6368, 9.6416, 9.6377, 9.6377, 9.6375, 9.6374],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0104, 0.0103, 0.0103, 0.0103, 0.0104, 0.0103, 0.0103, 0.0103, 0.0102,\n",
            "        0.0102, 0.0103, 0.0106, 0.0104, 0.0104, 0.0103, 0.0103, 0.0104, 0.0100,\n",
            "        0.0103, 0.0102, 0.0103, 0.0105, 0.0101, 0.0103, 0.0102, 0.0103, 0.0103,\n",
            "        0.0104, 0.0104, 0.0102, 0.0103, 0.0103, 0.0105, 0.0103, 0.0101, 0.0103,\n",
            "        0.0103, 0.0104, 0.0105, 0.0103, 0.0100, 0.0104, 0.0103, 0.0103, 0.0106,\n",
            "        0.0103, 0.0104, 0.0105, 0.0103, 0.0107, 0.0103, 0.0103, 0.0103, 0.0104,\n",
            "        0.0103, 0.0104, 0.0108, 0.0102, 0.0104, 0.0104, 0.0102, 0.0103, 0.0104,\n",
            "        0.0103, 0.0104, 0.0103, 0.0103, 0.0103, 0.0102, 0.0102, 0.0103, 0.0102,\n",
            "        0.0103, 0.0104, 0.0102, 0.0103, 0.0102, 0.0103, 0.0100, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0102, 0.0102, 0.0106, 0.0101, 0.0101, 0.0103, 0.0102,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0102, 0.0103],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [93]\n",
            "DEBUGGING: logits looks like: tensor([9.6362, 9.6343, 9.6338, 9.6342, 9.6354, 9.6353, 9.6337, 9.6339, 9.6326,\n",
            "        9.6332, 9.6348, 9.6394, 9.6359, 9.6362, 9.6348, 9.6340, 9.6354, 9.6279,\n",
            "        9.6350, 9.6330, 9.6346, 9.6375, 9.6310, 9.6341, 9.6318, 9.6352, 9.6351,\n",
            "        9.6368, 9.6367, 9.6333, 9.6351, 9.6346, 9.6374, 9.6334, 9.6299, 9.6338,\n",
            "        9.6350, 9.6354, 9.6378, 9.6350, 9.6290, 9.6358, 9.6346, 9.6341, 9.6409,\n",
            "        9.6341, 9.6358, 9.6374, 9.6339, 9.6415, 9.6346, 9.6340, 9.6349, 9.6362,\n",
            "        9.6335, 9.6368, 9.6447, 9.6332, 9.6367, 9.6358, 9.6332, 9.6338, 9.6362,\n",
            "        9.6343, 9.6356, 9.6336, 9.6336, 9.6340, 9.6333, 9.6321, 9.6340, 9.6331,\n",
            "        9.6340, 9.6358, 9.6324, 9.6340, 9.6332, 9.6345, 9.6294, 9.6338, 9.6335,\n",
            "        9.6350, 9.6337, 9.6330, 9.6318, 9.6394, 9.6312, 9.6295, 9.6347, 9.6331,\n",
            "        9.6348, 9.6340, 9.6343, 9.6345, 9.6334, 9.6318, 9.6345],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0091, 0.0092, 0.0092, 0.0091, 0.0093, 0.0091, 0.0092, 0.0090, 0.0092,\n",
            "        0.0091, 0.0094, 0.0092, 0.0093, 0.0094, 0.0091, 0.0091, 0.0095, 0.0093,\n",
            "        0.0093, 0.0091, 0.0092, 0.0091, 0.0091, 0.0090, 0.0092, 0.0091, 0.0091,\n",
            "        0.0092, 0.0091, 0.0092, 0.0093, 0.0094, 0.0092, 0.0091, 0.0092, 0.0092,\n",
            "        0.0091, 0.0093, 0.0092, 0.0093, 0.0092, 0.0091, 0.0092, 0.0092, 0.0093,\n",
            "        0.0092, 0.0092, 0.0092, 0.0091, 0.0093, 0.0091, 0.0091, 0.0092, 0.0090,\n",
            "        0.0091, 0.0093, 0.0092, 0.0092, 0.0092, 0.0092, 0.0093, 0.0091, 0.0090,\n",
            "        0.0090, 0.0091, 0.0092, 0.0090, 0.0091, 0.0090, 0.0091, 0.0092, 0.0091,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0091, 0.0092, 0.0091, 0.0091, 0.0091,\n",
            "        0.0092, 0.0092, 0.0092, 0.0091, 0.0092, 0.0090, 0.0091, 0.0092, 0.0092,\n",
            "        0.0090, 0.0091, 0.0093, 0.0092, 0.0091, 0.0092, 0.0092, 0.0092, 0.0091,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0093, 0.0093, 0.0092, 0.0093, 0.0091,\n",
            "        0.0093], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [85]\n",
            "DEBUGGING: logits looks like: tensor([9.6344, 9.6365, 9.6365, 9.6357, 9.6388, 9.6343, 9.6365, 9.6338, 9.6369,\n",
            "        9.6350, 9.6415, 9.6364, 9.6386, 9.6424, 9.6352, 9.6358, 9.6433, 9.6398,\n",
            "        9.6384, 9.6352, 9.6362, 9.6357, 9.6341, 9.6327, 9.6364, 9.6351, 9.6360,\n",
            "        9.6382, 9.6344, 9.6375, 9.6385, 9.6415, 9.6382, 9.6360, 9.6368, 9.6363,\n",
            "        9.6359, 9.6399, 9.6367, 9.6392, 9.6369, 9.6356, 9.6368, 9.6368, 9.6394,\n",
            "        9.6381, 9.6370, 9.6372, 9.6347, 9.6392, 9.6359, 9.6346, 9.6372, 9.6338,\n",
            "        9.6356, 9.6385, 9.6374, 9.6368, 9.6366, 9.6370, 9.6392, 9.6351, 9.6339,\n",
            "        9.6328, 9.6356, 9.6365, 9.6339, 9.6347, 9.6334, 9.6350, 9.6368, 9.6358,\n",
            "        9.6372, 9.6369, 9.6365, 9.6379, 9.6359, 9.6363, 9.6361, 9.6353, 9.6360,\n",
            "        9.6365, 9.6372, 9.6380, 9.6360, 9.6371, 9.6336, 9.6359, 9.6368, 9.6377,\n",
            "        9.6336, 9.6357, 9.6385, 9.6363, 9.6357, 9.6382, 9.6362, 9.6379, 9.6357,\n",
            "        9.6367, 9.6381, 9.6362, 9.6367, 9.6388, 9.6390, 9.6373, 9.6389, 9.6343,\n",
            "        9.6404], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.7082653196366664 and immediate abs rewards look like: [0.008344276765456016, 0.025095668107951496, 0.00023773652173986193, 0.0007816822530912759, 0.6738059559716021, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 1.8189894035458565e-12, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 9.094947017729282e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 9.094947017729282e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.9339014377837557 and immediate relative rewards look like: [0.0022501833192214637, 0.01356551548926617, 0.00019407985209876757, 0.0008509060934727694, 0.917040752896001, 9.094947017729282e-13, 0.0, 1.2126596023640882e-12, 2.728484105319612e-12, 1.5158245029548803e-12, 3.334813906501243e-12, 1.8189894035455806e-12, 1.9705718538413444e-12, 2.1221543041365108e-12, 2.2737367544323206e-12, 4.850638409456353e-12, 1.0307606620097874e-11, 5.456968210636742e-12, 2.8800665556147096e-12, 0.0, 6.366462912412428e-12, 0.0, 3.4863963567962246e-12, 3.6379788070911612e-12, 0.0, 0.0, 0.0, 0.0, 4.395891058569153e-12, 4.547473508865331e-12, 0.0, 4.850638409455617e-12, 5.002220859750347e-12, 5.153803310046593e-12, 0.0, 0.0, 5.608550660933908e-12, 0.0, 0.0, 0.0, 6.214880462115009e-12, 6.366462912411463e-12, 0.0, 0.0, 0.0, 6.972792713592449e-12, 7.1243751638890184e-12, 0.0, 7.427540064478913e-12, 1.5158245029551102e-11]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 3\n",
            "DEBUGGING: the action_prob is: tensor([0.0151, 0.0147, 0.0150, 0.0152, 0.0153, 0.0159, 0.0142, 0.0152, 0.0156,\n",
            "        0.0151, 0.0140, 0.0148, 0.0154, 0.0146, 0.0142, 0.0144, 0.0150, 0.0145,\n",
            "        0.0145, 0.0145, 0.0157, 0.0152, 0.0153, 0.0146, 0.0146, 0.0153, 0.0150,\n",
            "        0.0153, 0.0154, 0.0146, 0.0151, 0.0152, 0.0157, 0.0149, 0.0151, 0.0150,\n",
            "        0.0151, 0.0147, 0.0153, 0.0134, 0.0138, 0.0152, 0.0152, 0.0152, 0.0155,\n",
            "        0.0152, 0.0150, 0.0152, 0.0150, 0.0144, 0.0142, 0.0154, 0.0144, 0.0149,\n",
            "        0.0151, 0.0142, 0.0147, 0.0143, 0.0150, 0.0156, 0.0136, 0.0153, 0.0154,\n",
            "        0.0146, 0.0155, 0.0150, 0.0156], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [35]\n",
            "DEBUGGING: logits looks like: tensor([9.5904, 9.5850, 9.5886, 9.5909, 9.5923, 9.6001, 9.5777, 9.5906, 9.5961,\n",
            "        9.5902, 9.5752, 9.5858, 9.5942, 9.5837, 9.5769, 9.5809, 9.5882, 9.5814,\n",
            "        9.5819, 9.5822, 9.5976, 9.5909, 9.5922, 9.5833, 9.5837, 9.5924, 9.5891,\n",
            "        9.5930, 9.5934, 9.5828, 9.5894, 9.5907, 9.5977, 9.5866, 9.5899, 9.5889,\n",
            "        9.5894, 9.5847, 9.5927, 9.5667, 9.5717, 9.5916, 9.5909, 9.5914, 9.5951,\n",
            "        9.5915, 9.5886, 9.5910, 9.5888, 9.5802, 9.5774, 9.5938, 9.5807, 9.5871,\n",
            "        9.5894, 9.5782, 9.5843, 9.5793, 9.5882, 9.5963, 9.5689, 9.5919, 9.5934,\n",
            "        9.5837, 9.5948, 9.5884, 9.5962], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0131, 0.0129, 0.0128, 0.0128, 0.0128, 0.0129, 0.0129, 0.0128, 0.0127,\n",
            "        0.0126, 0.0129, 0.0129, 0.0128, 0.0129, 0.0129, 0.0128, 0.0127, 0.0127,\n",
            "        0.0130, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0129, 0.0128, 0.0128,\n",
            "        0.0129, 0.0128, 0.0129, 0.0129, 0.0128, 0.0127, 0.0124, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0129, 0.0128, 0.0126, 0.0128, 0.0128, 0.0129, 0.0127,\n",
            "        0.0128, 0.0128, 0.0129, 0.0129, 0.0128, 0.0129, 0.0128, 0.0128, 0.0128,\n",
            "        0.0129, 0.0127, 0.0128, 0.0129, 0.0128, 0.0128, 0.0129, 0.0128, 0.0130,\n",
            "        0.0129, 0.0128, 0.0129, 0.0127, 0.0128, 0.0129, 0.0128, 0.0129, 0.0128,\n",
            "        0.0129, 0.0128, 0.0128, 0.0127, 0.0129, 0.0128],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [21]\n",
            "DEBUGGING: logits looks like: tensor([9.5870, 9.5830, 9.5817, 9.5814, 9.5818, 9.5831, 9.5833, 9.5812, 9.5807,\n",
            "        9.5786, 9.5838, 9.5824, 9.5808, 9.5824, 9.5824, 9.5817, 9.5804, 9.5805,\n",
            "        9.5842, 9.5818, 9.5820, 9.5811, 9.5821, 9.5818, 9.5838, 9.5821, 9.5815,\n",
            "        9.5826, 9.5820, 9.5832, 9.5825, 9.5813, 9.5799, 9.5757, 9.5808, 9.5818,\n",
            "        9.5809, 9.5813, 9.5829, 9.5810, 9.5791, 9.5812, 9.5820, 9.5833, 9.5795,\n",
            "        9.5824, 9.5815, 9.5829, 9.5829, 9.5818, 9.5834, 9.5818, 9.5820, 9.5822,\n",
            "        9.5837, 9.5801, 9.5820, 9.5829, 9.5810, 9.5821, 9.5837, 9.5814, 9.5839,\n",
            "        9.5825, 9.5824, 9.5836, 9.5803, 9.5823, 9.5830, 9.5820, 9.5824, 9.5818,\n",
            "        9.5824, 9.5808, 9.5809, 9.5806, 9.5830, 9.5821],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0123, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0114, 0.0115, 0.0115, 0.0114, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0114, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0114, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0114, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0114,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [43]\n",
            "DEBUGGING: logits looks like: tensor([9.5900, 9.5762, 9.5765, 9.5768, 9.5764, 9.5765, 9.5759, 9.5764, 9.5763,\n",
            "        9.5764, 9.5763, 9.5757, 9.5761, 9.5765, 9.5757, 9.5767, 9.5759, 9.5771,\n",
            "        9.5765, 9.5757, 9.5764, 9.5763, 9.5769, 9.5767, 9.5764, 9.5767, 9.5766,\n",
            "        9.5763, 9.5767, 9.5769, 9.5760, 9.5761, 9.5763, 9.5769, 9.5765, 9.5763,\n",
            "        9.5766, 9.5770, 9.5762, 9.5766, 9.5761, 9.5766, 9.5766, 9.5766, 9.5768,\n",
            "        9.5766, 9.5763, 9.5762, 9.5770, 9.5761, 9.5763, 9.5762, 9.5756, 9.5759,\n",
            "        9.5770, 9.5768, 9.5769, 9.5762, 9.5761, 9.5758, 9.5763, 9.5767, 9.5766,\n",
            "        9.5768, 9.5765, 9.5760, 9.5764, 9.5764, 9.5759, 9.5768, 9.5763, 9.5762,\n",
            "        9.5765, 9.5755, 9.5763, 9.5767, 9.5764, 9.5762, 9.5764, 9.5765, 9.5757,\n",
            "        9.5765, 9.5767, 9.5767, 9.5759, 9.5766, 9.5764],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0109, 0.0097, 0.0101, 0.0102, 0.0097, 0.0109, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0068, 0.0085, 0.0094, 0.0094, 0.0088, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0118, 0.0098, 0.0100, 0.0102, 0.0102,\n",
            "        0.0104, 0.0102, 0.0110, 0.0102, 0.0101, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0099, 0.0103, 0.0102, 0.0092, 0.0102, 0.0096, 0.0102, 0.0104, 0.0102,\n",
            "        0.0110, 0.0102, 0.0102, 0.0102, 0.0105, 0.0103, 0.0102, 0.0102, 0.0091,\n",
            "        0.0086, 0.0102, 0.0102, 0.0102, 0.0107, 0.0102, 0.0102, 0.0100, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0101, 0.0101, 0.0103, 0.0104],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [22]\n",
            "DEBUGGING: logits looks like: tensor([9.5875, 9.5631, 9.5712, 9.5733, 9.5643, 9.5867, 9.5731, 9.5730, 9.5731,\n",
            "        9.5731, 9.5732, 9.4934, 9.5378, 9.5581, 9.5572, 9.5438, 9.5731, 9.5730,\n",
            "        9.5729, 9.5731, 9.5730, 9.5730, 9.6033, 9.5661, 9.5702, 9.5731, 9.5731,\n",
            "        9.5766, 9.5739, 9.5893, 9.5735, 9.5716, 9.5730, 9.5734, 9.5731, 9.5731,\n",
            "        9.5677, 9.5763, 9.5733, 9.5532, 9.5731, 9.5604, 9.5734, 9.5767, 9.5733,\n",
            "        9.5885, 9.5731, 9.5729, 9.5732, 9.5799, 9.5761, 9.5733, 9.5732, 9.5514,\n",
            "        9.5405, 9.5731, 9.5728, 9.5731, 9.5830, 9.5730, 9.5731, 9.5688, 9.5731,\n",
            "        9.5733, 9.5731, 9.5734, 9.5731, 9.5729, 9.5731, 9.5730, 9.5733, 9.5732,\n",
            "        9.5729, 9.5731, 9.5733, 9.5730, 9.5732, 9.5733, 9.5733, 9.5727, 9.5730,\n",
            "        9.5730, 9.5729, 9.5730, 9.5733, 9.5733, 9.5732, 9.5731, 9.5733, 9.5733,\n",
            "        9.5732, 9.5729, 9.5732, 9.5732, 9.5731, 9.5723, 9.5723, 9.5752, 9.5772],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0109, 0.0092, 0.0093, 0.0092, 0.0092, 0.0094, 0.0094, 0.0090, 0.0091,\n",
            "        0.0092, 0.0094, 0.0090, 0.0092, 0.0093, 0.0094, 0.0091, 0.0093, 0.0095,\n",
            "        0.0096, 0.0095, 0.0093, 0.0096, 0.0093, 0.0095, 0.0093, 0.0094, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0094, 0.0096, 0.0092, 0.0094, 0.0093, 0.0093,\n",
            "        0.0106, 0.0093, 0.0096, 0.0091, 0.0094, 0.0094, 0.0093, 0.0093, 0.0094,\n",
            "        0.0095, 0.0092, 0.0093, 0.0094, 0.0091, 0.0093, 0.0092, 0.0091, 0.0094,\n",
            "        0.0091, 0.0095, 0.0093, 0.0095, 0.0093, 0.0094, 0.0095, 0.0095, 0.0093,\n",
            "        0.0093, 0.0091, 0.0092, 0.0091, 0.0094, 0.0094, 0.0093, 0.0095, 0.0092,\n",
            "        0.0091, 0.0095, 0.0095, 0.0096, 0.0094, 0.0095, 0.0089, 0.0093, 0.0093,\n",
            "        0.0093, 0.0094, 0.0095, 0.0093, 0.0094, 0.0092, 0.0093, 0.0096, 0.0094,\n",
            "        0.0095, 0.0095, 0.0092, 0.0095, 0.0093, 0.0091, 0.0093, 0.0091, 0.0092,\n",
            "        0.0091, 0.0094, 0.0093, 0.0093, 0.0093, 0.0091, 0.0092, 0.0093],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [56]\n",
            "DEBUGGING: logits looks like: tensor([9.5954, 9.5601, 9.5627, 9.5602, 9.5608, 9.5650, 9.5664, 9.5576, 9.5589,\n",
            "        9.5606, 9.5649, 9.5574, 9.5615, 9.5625, 9.5645, 9.5579, 9.5636, 9.5675,\n",
            "        9.5706, 9.5680, 9.5626, 9.5690, 9.5637, 9.5673, 9.5637, 9.5663, 9.5613,\n",
            "        9.5621, 9.5601, 9.5610, 9.5655, 9.5692, 9.5601, 9.5644, 9.5633, 9.5639,\n",
            "        9.5895, 9.5633, 9.5706, 9.5597, 9.5656, 9.5658, 9.5641, 9.5639, 9.5659,\n",
            "        9.5667, 9.5618, 9.5630, 9.5664, 9.5597, 9.5633, 9.5616, 9.5584, 9.5653,\n",
            "        9.5595, 9.5683, 9.5627, 9.5671, 9.5631, 9.5655, 9.5666, 9.5672, 9.5640,\n",
            "        9.5633, 9.5585, 9.5621, 9.5594, 9.5656, 9.5659, 9.5641, 9.5669, 9.5621,\n",
            "        9.5600, 9.5678, 9.5668, 9.5703, 9.5656, 9.5666, 9.5541, 9.5643, 9.5639,\n",
            "        9.5628, 9.5654, 9.5670, 9.5625, 9.5651, 9.5621, 9.5626, 9.5701, 9.5658,\n",
            "        9.5682, 9.5670, 9.5611, 9.5680, 9.5628, 9.5597, 9.5627, 9.5592, 9.5601,\n",
            "        9.5597, 9.5656, 9.5631, 9.5643, 9.5640, 9.5590, 9.5611, 9.5637],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.03812671356035935 and immediate abs rewards look like: [0.005420647241862753, 0.005032718405800551, 1.727166454656981e-05, 0.0009766985376700177, 0.00010012250822910573, 0.0006728737703269871, 0.002752791182956571, 0.00670984528005647, 0.0006435397372115403, 6.747950010321802e-05, 0.0014809740723649156, 9.961918749468168e-06, 0.0010966655422635085, 0.0004568231174744142, 2.250314355478622e-05, 5.192978278500959e-06, 0.0005650766124745132, 0.0003061630932279513, 0.0006051224713701231, 0.00014147738829706213, 0.00024576926398367505, 3.787328387261368e-05, 3.5752065741689876e-05, 8.645000843898742e-05, 0.00011596343256314867, 6.189483610796742e-05, 0.00013732448087466764, 9.724468918648199e-05, 8.690288268553559e-06, 0.0001182029845949728, 3.152490216962178e-05, 5.344681130736717e-05, 5.777918431704165e-06, 4.929947408527369e-05, 9.464069989917334e-05, 4.1072460135183064e-05, 2.9983923468535068e-05, 7.618883000759524e-05, 0.00015854702996875858, 8.548384357709438e-06, 0.00012791742710760445, 0.00023459194699171348, 0.0003782131902880792, 0.002000270105781965, 0.002238155923805607, 0.0003353738370606152, 0.00044549116773850983, 0.0003087738300564524, 0.0007725680343355634, 0.0027072141629105317]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.24587436921253406 and immediate relative rewards look like: [0.002003309478632747, 0.0037273521516835825, 1.922354097065206e-05, 0.0014494433636257128, 0.00018579749157596128, 0.0014984391422916085, 0.007153754670101476, 0.019948456017684586, 0.0021577889425770103, 0.0002514589385976007, 0.006070804995174331, 4.4572822214875394e-05, 0.0053157562105524084, 0.0023856198244203834, 0.000125931271008053, 3.099839954415026e-05, 0.00358393297233784, 0.002056461266402492, 0.004290833806858587, 0.0010562334834637811, 0.0019266938059488686, 0.0003110722916535093, 0.0003070017151809518, 0.0007746292719894795, 0.001082412280679516, 0.0006008668011996852, 0.0013844338545039104, 0.0010167324245237196, 9.41089008248057e-05, 0.0013241879356694912, 0.00036495098567283253, 0.0006386986387968827, 7.120626842695775e-05, 0.0006259721639747251, 0.0012370515603360994, 0.0005522177744350629, 0.0004143372797387085, 0.0010812934943617255, 0.0023094244798821297, 0.00012771761423837483, 0.0019589426601014327, 0.0036803683946443518, 0.006075360970676497, 0.03288286473544735, 0.03765787970437646, 0.005773027289446845, 0.00783624602925627, 0.00554785682071336, 0.014171849634151459, 0.05068879464196477]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 200 number of matrices\n",
            "DEBUGGING: ACT_MAT has 200 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.4025036958814648, 0.40153021958851615, 0.39053301527868556, 0.3678044688501612, 0.3710003028274098, 0.3729615986745698, 0.35247132664010994, 0.3530136216983303, 0.3529121925234828, 0.33674357380506825, 0.33902492849927385, 0.3109136915935552, 0.311939921522072, 0.30977811480604495, 0.3122845486046826, 0.2924345534487334, 0.29237865186316503, 0.29404601116219603, 0.294660397575896, 0.28934583877724335, 0.2795088947944651, 0.27628853669900527, 0.2757206013013871, 0.27375677826596057, 0.2673110214102933, 0.26158815025947824, 0.2246519204226522, 0.18880433994145743, 0.18971990270073663, 0.18849522702343494, 0.18712571525951607, 0.18636119827002567, 0.18208811961776444, 0.15028646857465486, 0.15089324862148892, 0.12897373043836116, 0.1300673480150877, 0.08710119292243836, 0.0836376328346597, 0.06503752072993818, 0.047872610071905616, 0.04172290460493927, 0.039206403314501176, 0.03293155714466442, 0.023414411401203053, 0.019532554629887476, 0.019492818143123897, 0.006523572887540208, 0.0054039580302521414, 2.5537543475259048e-05], [3.235211999761395, 3.2639126527678886, 3.2897170538897043, 3.32070423785764, 3.347499299569496, 3.3509611634892154, 3.383800578371811, 3.3866988334976593, 3.4143315127148726, 3.4261978528963617, 3.4264524429388983, 3.4409803181963454, 1.7540651947313661e-10, 1.7419258756805514e-10, 1.727367233453022e-10, 1.7103648304409468e-10, 1.7276412428696432e-10, 1.70604820004475e-10, 1.6819403418838062e-10, 1.655292266211719e-10, 1.62607831426573e-10, 1.594272568103677e-10, 1.5598488479860262e-10, 1.522780709731409e-10, 1.5381623330620294e-10, 1.5536993263252823e-10, 1.5693932589144267e-10, 1.5852457160751786e-10, 1.4726428866939078e-10, 1.4209136573892934e-10, 1.366365206824585e-10, 1.3089690580173566e-10, 1.248696446338928e-10, 1.185518316608749e-10, 1.1194053201596642e-10, 1.1307124446057215e-10, 1.1421337824300216e-10, 1.1536704873030521e-10, 1.1653237245485376e-10, 1.0875232233592697e-10, 1.0066401547292488e-10, 1.0168082371002513e-10, 1.0270790273739912e-10, 1.0374535630040315e-10, 9.468779250596055e-11, 7.497390072330738e-11, 6.51663754069862e-11, 5.503011379914576e-11, 3.3537617127879236e-11, 1.1368683772159018e-11], [0.8976015829871337, 0.9043953531999114, 0.8998281188996415, 0.9087212515631745, 0.9170407529996988, 1.0474522570154104e-10, 1.0488457676744254e-10, 1.0594401693681065e-10, 1.057892498327743e-10, 1.0410178356308554e-10, 1.0362218086881885e-10, 1.0130037066900767e-10, 1.0048624370248696e-10, 9.951077964509658e-11, 9.83723488292526e-11, 9.706930512608109e-11, 9.315016840063105e-11, 8.36793553338719e-11, 7.901251224569208e-11, 7.690146029300745e-11, 7.767824272020954e-11, 7.203210081595668e-11, 7.275969779389564e-11, 6.997303175464588e-11, 6.700510398742901e-11, 6.768192321962527e-11, 6.836557900972249e-11, 6.90561404138611e-11, 6.975367718571828e-11, 6.601796578499912e-11, 6.209140633952909e-11, 6.27185922621506e-11, 5.845247863908584e-11, 5.3990159373066153e-11, 4.932965258890865e-11, 4.982793190798854e-11, 5.0331244351503574e-11, 4.517443807128249e-11, 4.563074552654797e-11, 4.609166214802825e-11, 4.655723449295783e-11, 4.074985255640689e-11, 3.473069661009639e-11, 3.5081511727370095e-11, 3.5435870431686966e-11, 3.579380851685552e-11, 2.91121371750132e-11, 2.2209860617297158e-11, 2.2434202643734502e-11, 1.5158245029551102e-11], [0.17515929110366762, 0.17490503194447968, 0.1729067472654506, 0.17463386234795955, 0.17493375654983218, 0.17651308995783457, 0.1767824755712555, 0.171342142324398, 0.1529229154613267, 0.15228800658459563, 0.15357227034949295, 0.14899137914577637, 0.150451319518749, 0.14660157909918847, 0.1456726861361294, 0.1470169241061832, 0.14847063202690813, 0.14635020106522253, 0.14575125232204045, 0.14288931163149685, 0.1432657355030637, 0.14276670878496447, 0.14389458231647573, 0.14503796020332807, 0.1457205362942814, 0.14609911516525442, 0.14696792764045932, 0.1470540341272277, 0.14751242596232725, 0.14890739097121458, 0.14907394246014655, 0.1502111024994684, 0.15108323622290054, 0.15253740399441776, 0.15344589073782125, 0.15374630219947996, 0.15474149941923726, 0.15588602236312984, 0.15636841299875567, 0.1556151399180541, 0.15705800232708658, 0.15666571683533853, 0.15453065499060017, 0.14995484244436735, 0.11825452293830305, 0.08141075074133998, 0.07640174086049811, 0.06925807558711297, 0.06435375632969657, 0.05068879464196477]]\n",
            "DEBUGGING: traj_returns = [0.4025036958814648, 3.235211999761395, 0.8976015829871337, 0.17515929110366762]\n",
            "DEBUGGING: actions = [[6], [45], [36], [47], [47], [44], [28], [59], [59], [57], [28], [44], [27], [21], [54], [41], [51], [4], [26], [11], [25], [56], [32], [58], [64], [19], [75], [70], [58], [42], [50], [66], [77], [19], [89], [39], [24], [92], [79], [27], [9], [65], [9], [41], [76], [58], [52], [90], [53], [100], [37], [37], [54], [56], [33], [50], [3], [61], [10], [52], [47], [0], [23], [10], [51], [5], [68], [42], [4], [72], [57], [75], [78], [77], [52], [40], [72], [57], [60], [75], [29], [8], [46], [31], [89], [82], [40], [46], [77], [30], [70], [70], [93], [0], [81], [87], [96], [51], [0], [2], [30], [24], [40], [61], [0], [2], [59], [43], [28], [30], [26], [19], [9], [45], [2], [8], [22], [62], [41], [62], [0], [58], [80], [18], [41], [62], [51], [57], [45], [19], [50], [56], [62], [76], [63], [75], [61], [29], [76], [93], [75], [52], [21], [1], [20], [50], [13], [100], [5], [85], [37], [55], [17], [28], [20], [54], [52], [63], [46], [35], [20], [21], [8], [65], [12], [62], [58], [30], [36], [21], [69], [60], [58], [81], [55], [70], [79], [55], [44], [43], [28], [66], [10], [25], [23], [12], [8], [32], [45], [22], [53], [16], [17], [94], [98], [36], [31], [54], [11], [56]]\n",
            "DEBUGGING: actions length = 200\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-1.2679, -0.6322,  0.3302,  ...,  0.3303,  0.5672, -0.7923],\n",
            "        [-1.2670, -0.6351,  0.3363,  ...,  0.3388,  0.5695, -0.7940],\n",
            "        [-1.2521, -0.6231,  0.3298,  ...,  0.3356,  0.5650, -0.7822],\n",
            "        ...,\n",
            "        [-1.2694, -0.6327,  0.3339,  ...,  0.3387,  0.5688, -0.7907],\n",
            "        [-1.2695, -0.6327,  0.3339,  ...,  0.3387,  0.5688, -0.7906],\n",
            "        [-1.2694, -0.6327,  0.3340,  ...,  0.3388,  0.5688, -0.7906]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-1.2732, -0.6360,  0.3355,  ...,  0.3393,  0.5700, -0.7937],\n",
            "        [-1.2690, -0.6324,  0.3337,  ...,  0.3387,  0.5686, -0.7903],\n",
            "        [-1.2694, -0.6326,  0.3339,  ...,  0.3388,  0.5687, -0.7905],\n",
            "        ...,\n",
            "        [-1.2689, -0.6322,  0.3338,  ...,  0.3387,  0.5686, -0.7902],\n",
            "        [-1.2691, -0.6325,  0.3339,  ...,  0.3387,  0.5687, -0.7905],\n",
            "        [-1.2695, -0.6327,  0.3340,  ...,  0.3388,  0.5688, -0.7906]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([9.5954, 9.5601, 9.5627, 9.5602, 9.5608, 9.5650, 9.5664, 9.5576, 9.5589,\n",
            "        9.5606, 9.5649, 9.5574, 9.5615, 9.5625, 9.5645, 9.5579, 9.5636, 9.5675,\n",
            "        9.5706, 9.5680, 9.5626, 9.5690, 9.5637, 9.5673, 9.5637, 9.5663, 9.5613,\n",
            "        9.5621, 9.5601, 9.5610, 9.5655, 9.5692, 9.5601, 9.5644, 9.5633, 9.5639,\n",
            "        9.5895, 9.5633, 9.5706, 9.5597, 9.5656, 9.5658, 9.5641, 9.5639, 9.5659,\n",
            "        9.5667, 9.5618, 9.5630, 9.5664, 9.5597, 9.5633, 9.5616, 9.5584, 9.5653,\n",
            "        9.5595, 9.5683, 9.5627, 9.5671, 9.5631, 9.5655, 9.5666, 9.5672, 9.5640,\n",
            "        9.5633, 9.5585, 9.5621, 9.5594, 9.5656, 9.5659, 9.5641, 9.5669, 9.5621,\n",
            "        9.5600, 9.5678, 9.5668, 9.5703, 9.5656, 9.5666, 9.5541, 9.5643, 9.5639,\n",
            "        9.5628, 9.5654, 9.5670, 9.5625, 9.5651, 9.5621, 9.5626, 9.5701, 9.5658,\n",
            "        9.5682, 9.5670, 9.5611, 9.5680, 9.5628, 9.5597, 9.5627, 9.5592, 9.5601,\n",
            "        9.5597, 9.5656, 9.5631, 9.5643, 9.5640, 9.5590, 9.5611, 9.5637],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[1.17761914 1.18618581 1.18824623 1.19296596 1.20261853 0.97510896\n",
            "  0.9782636  0.97776365 0.98004166 0.97880736 0.97976241 0.97522135\n",
            "  0.11559781 0.11409492 0.11448931 0.10986287 0.11021232 0.11009905\n",
            "  0.11010291 0.10805879 0.10569366 0.10476381 0.1049038  0.10469868\n",
            "  0.10325789 0.10192182 0.09290496 0.08396459 0.08430808 0.08435065\n",
            "  0.08404991 0.08414308 0.08329284 0.07570597 0.07608478 0.07068001\n",
            "  0.07120221 0.0607468  0.06000151 0.05516317 0.05123265 0.04959716\n",
            "  0.04843426 0.0457216  0.03541723 0.02523583 0.02397364 0.01894541\n",
            "  0.01743943 0.01267858]]\n",
            "DEBUGGING: baseline2 looks like: 1.1776191424334153\n",
            "DEBUGGING: ADS looks like: [-1.24987688 -1.25500372 -1.27043104 -1.28404415 -1.24876954 -1.22722825\n",
            " -1.25033434 -1.25546462 -1.23844041 -1.23877597 -1.22891575 -1.23963336\n",
            " -1.19283897 -1.17635303 -1.18098283 -1.19410122 -1.1784719  -1.12581701\n",
            " -1.08767997 -1.01934065 -0.97180173 -0.93225313 -0.91238442 -0.86873454\n",
            " -0.84413125 -0.78544515 -0.82789234 -0.87030581 -0.81310171 -0.77945575\n",
            " -0.71997378 -0.60936867 -0.52599964 -0.47255193 -0.44236369 -0.41373062\n",
            " -0.41624159 -0.31790613 -0.21091061 -0.20247265 -0.22046315 -0.17030222\n",
            " -0.17185769 -0.17897997 -0.15158151 -0.15481557 -0.12591133 -0.13868357\n",
            " -0.13221605 -0.03353593  1.58283142  1.60737871  1.628753    1.66885561\n",
            "  1.72772945  1.75077131  1.78099492  1.77822059  1.82297891  1.85067831\n",
            "  1.85851177  1.89043327 -1.5047789  -1.48613115 -1.49326738 -1.48653577\n",
            " -1.47085055 -1.41986302 -1.38234036 -1.30868649 -1.25131063 -1.20854167\n",
            " -1.18810502 -1.14249132 -1.11144227 -1.0470333  -1.05254426 -1.05911015\n",
            " -1.00282161 -0.96795097 -0.90709949 -0.79572987 -0.70808776 -0.6228384\n",
            " -0.59325694 -0.54270435 -0.54630894 -0.40500732 -0.29454824 -0.26751017\n",
            " -0.26833576 -0.21202512 -0.21106409 -0.21191153 -0.17499592 -0.17434813\n",
            " -0.14540414 -0.14520715 -0.13762001 -0.03356147 -0.754779   -0.75213859\n",
            " -0.76113594 -0.74312737 -0.70272909 -1.60018985 -1.60280566 -1.60847824\n",
            " -1.5913526  -1.57551954 -1.56794067 -1.55054705 -1.5047789  -1.48613115\n",
            " -1.49326738 -1.48653577 -1.47085055 -1.41986302 -1.38234036 -1.30868649\n",
            " -1.25131063 -1.20854167 -1.18810502 -1.14249132 -1.11144227 -1.0470333\n",
            " -1.05254426 -1.05911015 -1.00282161 -0.96795097 -0.90709949 -0.79572987\n",
            " -0.70808776 -0.6228384  -0.59325694 -0.54270435 -0.54630894 -0.40500732\n",
            " -0.29454824 -0.26751017 -0.26833576 -0.21202512 -0.21106409 -0.21191153\n",
            " -0.17499592 -0.17434813 -0.14540414 -0.14520715 -0.13762001 -0.03356147\n",
            " -1.47722129 -1.48162891 -1.48805731 -1.47721476 -1.44483609 -1.42367676\n",
            " -1.42602319 -1.4371361  -1.43842969 -1.42323154 -1.4143684  -1.40155567\n",
            " -1.35432758 -1.33952957 -1.3475947  -1.33951885 -1.32237992 -1.27351282\n",
            " -1.23658911 -1.16579718 -1.10804489 -1.06577496 -1.04421043 -0.99745336\n",
            " -0.96572174 -0.90093419 -0.90557633 -0.91205612 -0.85530919 -0.81904358\n",
            " -0.75802555 -0.64551876 -0.55700452 -0.470301   -0.43981105 -0.38895805\n",
            " -0.39156744 -0.2491213  -0.13817983 -0.11189503 -0.11127776 -0.0553594\n",
            " -0.05653344 -0.06195669 -0.0567414  -0.09293738 -0.0690024  -0.07594907\n",
            " -0.07326625  0.01712732]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(-2.9148, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-8.0922e-05,  1.7526e-03,  6.1922e-04,  ..., -5.6090e-04,\n",
            "          1.9354e-05, -1.5315e-02],\n",
            "        [-4.2582e-05,  9.3010e-04,  3.2879e-04,  ..., -2.9720e-04,\n",
            "          1.0594e-05, -8.0195e-03]])\n",
            "   Last layer:\n",
            "tensor([[ 0.0124,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0035,  0.0065,\n",
            "          0.0050,  0.0070,  0.0000,  0.0000,  0.0000,  0.0000,  0.0058,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0093,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0025,  0.0055,\n",
            "          0.0034,  0.0046,  0.0000,  0.0000,  0.0000,  0.0000,  0.0043,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0039,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0010, -0.0023,\n",
            "         -0.0014, -0.0019,  0.0000,  0.0000,  0.0000,  0.0000, -0.0018,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0101,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0028, -0.0054,\n",
            "         -0.0040, -0.0055,  0.0000,  0.0000,  0.0000,  0.0000, -0.0047,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0193,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0054,  0.0105,\n",
            "          0.0076,  0.0105,  0.0000,  0.0000,  0.0000,  0.0000,  0.0090,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0166,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0046,  0.0092,\n",
            "          0.0065,  0.0089,  0.0000,  0.0000,  0.0000,  0.0000,  0.0077,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0058,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0016, -0.0032,\n",
            "         -0.0023, -0.0032,  0.0000,  0.0000,  0.0000,  0.0000, -0.0027,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0029,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0008, -0.0015,\n",
            "         -0.0012, -0.0016,  0.0000,  0.0000,  0.0000,  0.0000, -0.0013,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0053,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0015, -0.0028,\n",
            "         -0.0022, -0.0030,  0.0000,  0.0000,  0.0000,  0.0000, -0.0025,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0098,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0027,  0.0055,\n",
            "          0.0037,  0.0051,  0.0000,  0.0000,  0.0000,  0.0000,  0.0045,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0010,  0.0009,  0.0012,  ..., -0.0006,  0.0014,  0.0022],\n",
            "        [-0.0006,  0.0005,  0.0006,  ..., -0.0004,  0.0007,  0.0013]])\n",
            "   Last layer:\n",
            "tensor([[-2.2531e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -3.1373e-04, -2.6886e-03, -7.3901e-04, -4.4788e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.3690e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-6.8251e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -4.6795e-05, -1.0596e-03, -2.3687e-04, -6.7064e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.9644e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-6.2073e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -2.1781e-04, -7.2076e-05, -1.6703e-04, -3.0900e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.5389e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.0305e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  7.6048e-05,  1.5728e-03,  3.5681e-04,  1.0955e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  7.4065e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-3.1965e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -4.4522e-04, -3.8135e-03, -1.0483e-03, -6.3513e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.9421e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-3.1247e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -4.9898e-04, -3.4034e-03, -1.0068e-03, -7.1083e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.7899e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.9260e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  3.8770e-04,  1.6900e-03,  5.9817e-04,  5.5134e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  9.6709e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 6.4998e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -4.8802e-05,  3.7201e-04,  3.7549e-05, -6.8664e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.3787e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 4.7902e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  9.1899e-06,  8.6428e-04,  1.7333e-04,  1.4136e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.8892e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.8075e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  8.2855e-05, -9.5551e-04, -1.2594e-04,  1.1665e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.7785e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "DEBUGGING: training for one iteration takes 0.005544 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 46\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0182, 0.0144, 0.0153, 0.0151, 0.0145, 0.0155, 0.0145, 0.0152, 0.0150,\n",
            "        0.0156, 0.0154, 0.0149, 0.0151, 0.0157, 0.0149, 0.0147, 0.0147, 0.0157,\n",
            "        0.0160, 0.0151, 0.0147, 0.0147, 0.0147, 0.0148, 0.0151, 0.0151, 0.0149,\n",
            "        0.0148, 0.0150, 0.0152, 0.0150, 0.0145, 0.0150, 0.0148, 0.0140, 0.0153,\n",
            "        0.0147, 0.0134, 0.0148, 0.0148, 0.0149, 0.0148, 0.0140, 0.0142, 0.0149,\n",
            "        0.0154, 0.0149, 0.0152, 0.0136, 0.0142, 0.0155, 0.0150, 0.0151, 0.0148,\n",
            "        0.0152, 0.0144, 0.0147, 0.0145, 0.0151, 0.0149, 0.0146, 0.0147, 0.0154,\n",
            "        0.0150, 0.0147, 0.0151, 0.0140], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [54]\n",
            "DEBUGGING: logits looks like: tensor([11.1557, 11.1095, 11.1217, 11.1188, 11.1109, 11.1235, 11.1104, 11.1203,\n",
            "        11.1168, 11.1250, 11.1229, 11.1155, 11.1187, 11.1258, 11.1162, 11.1130,\n",
            "        11.1135, 11.1262, 11.1303, 11.1185, 11.1127, 11.1134, 11.1136, 11.1145,\n",
            "        11.1183, 11.1188, 11.1164, 11.1144, 11.1174, 11.1202, 11.1167, 11.1107,\n",
            "        11.1165, 11.1146, 11.1032, 11.1209, 11.1137, 11.0947, 11.1142, 11.1145,\n",
            "        11.1160, 11.1150, 11.1029, 11.1063, 11.1163, 11.1221, 11.1162, 11.1204,\n",
            "        11.0979, 11.1054, 11.1240, 11.1177, 11.1179, 11.1138, 11.1199, 11.1088,\n",
            "        11.1128, 11.1107, 11.1180, 11.1160, 11.1119, 11.1129, 11.1219, 11.1167,\n",
            "        11.1128, 11.1185, 11.1028], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0142, 0.0129, 0.0128, 0.0131, 0.0127, 0.0129, 0.0128, 0.0129, 0.0128,\n",
            "        0.0127, 0.0127, 0.0127, 0.0130, 0.0127, 0.0128, 0.0128, 0.0127, 0.0129,\n",
            "        0.0128, 0.0129, 0.0128, 0.0128, 0.0128, 0.0129, 0.0128, 0.0124, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0124, 0.0130, 0.0127, 0.0127, 0.0129, 0.0126,\n",
            "        0.0128, 0.0129, 0.0129, 0.0127, 0.0126, 0.0127, 0.0127, 0.0130, 0.0129,\n",
            "        0.0127, 0.0125, 0.0128, 0.0127, 0.0129, 0.0129, 0.0127, 0.0130, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0130, 0.0126, 0.0129, 0.0128, 0.0128, 0.0127,\n",
            "        0.0129, 0.0129, 0.0130, 0.0128, 0.0129, 0.0131, 0.0129, 0.0127, 0.0128,\n",
            "        0.0129, 0.0126, 0.0129, 0.0128, 0.0129, 0.0127],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [11]\n",
            "DEBUGGING: logits looks like: tensor([11.1505, 11.1304, 11.1298, 11.1338, 11.1277, 11.1318, 11.1304, 11.1306,\n",
            "        11.1301, 11.1288, 11.1279, 11.1285, 11.1324, 11.1282, 11.1290, 11.1290,\n",
            "        11.1278, 11.1305, 11.1291, 11.1306, 11.1304, 11.1296, 11.1295, 11.1308,\n",
            "        11.1295, 11.1239, 11.1295, 11.1296, 11.1295, 11.1292, 11.1241, 11.1322,\n",
            "        11.1287, 11.1275, 11.1306, 11.1257, 11.1299, 11.1310, 11.1307, 11.1275,\n",
            "        11.1267, 11.1277, 11.1285, 11.1326, 11.1308, 11.1276, 11.1251, 11.1291,\n",
            "        11.1288, 11.1314, 11.1319, 11.1284, 11.1330, 11.1299, 11.1293, 11.1300,\n",
            "        11.1290, 11.1326, 11.1271, 11.1319, 11.1298, 11.1300, 11.1285, 11.1312,\n",
            "        11.1310, 11.1331, 11.1299, 11.1313, 11.1344, 11.1313, 11.1282, 11.1295,\n",
            "        11.1306, 11.1270, 11.1318, 11.1304, 11.1317, 11.1288],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0114,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0114, 0.0115, 0.0114, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0116,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0114, 0.0114, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0114, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0114, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0114, 0.0115],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [38]\n",
            "DEBUGGING: logits looks like: tensor([11.1278, 11.1265, 11.1272, 11.1264, 11.1272, 11.1276, 11.1271, 11.1273,\n",
            "        11.1266, 11.1270, 11.1277, 11.1275, 11.1273, 11.1280, 11.1278, 11.1272,\n",
            "        11.1275, 11.1256, 11.1267, 11.1271, 11.1270, 11.1269, 11.1272, 11.1259,\n",
            "        11.1273, 11.1257, 11.1269, 11.1279, 11.1271, 11.1269, 11.1267, 11.1273,\n",
            "        11.1267, 11.1271, 11.1272, 11.1288, 11.1270, 11.1267, 11.1274, 11.1263,\n",
            "        11.1268, 11.1257, 11.1259, 11.1279, 11.1269, 11.1279, 11.1275, 11.1260,\n",
            "        11.1274, 11.1276, 11.1275, 11.1276, 11.1265, 11.1274, 11.1279, 11.1265,\n",
            "        11.1267, 11.1264, 11.1269, 11.1267, 11.1272, 11.1269, 11.1273, 11.1267,\n",
            "        11.1274, 11.1269, 11.1271, 11.1266, 11.1273, 11.1275, 11.1261, 11.1273,\n",
            "        11.1278, 11.1272, 11.1275, 11.1278, 11.1278, 11.1273, 11.1270, 11.1272,\n",
            "        11.1273, 11.1265, 11.1263, 11.1275, 11.1264, 11.1250, 11.1269],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0107,\n",
            "        0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0107, 0.0106, 0.0107, 0.0106,\n",
            "        0.0107, 0.0106, 0.0106, 0.0106, 0.0106, 0.0107, 0.0107, 0.0106, 0.0106,\n",
            "        0.0106, 0.0106, 0.0106, 0.0107, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106,\n",
            "        0.0106, 0.0106, 0.0107, 0.0106, 0.0106, 0.0106, 0.0107, 0.0106, 0.0106,\n",
            "        0.0106, 0.0107, 0.0107, 0.0106, 0.0106, 0.0107, 0.0106, 0.0106, 0.0106,\n",
            "        0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0107, 0.0106, 0.0107, 0.0106,\n",
            "        0.0106, 0.0106, 0.0106, 0.0107, 0.0106, 0.0106, 0.0106, 0.0107, 0.0106,\n",
            "        0.0106, 0.0106, 0.0107, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0107,\n",
            "        0.0106, 0.0109, 0.0106, 0.0106, 0.0106, 0.0107, 0.0106, 0.0106, 0.0106,\n",
            "        0.0107, 0.0106, 0.0106, 0.0107], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [46]\n",
            "DEBUGGING: logits looks like: tensor([11.1248, 11.1251, 11.1252, 11.1245, 11.1251, 11.1249, 11.1249, 11.1248,\n",
            "        11.1252, 11.1251, 11.1248, 11.1251, 11.1249, 11.1249, 11.1254, 11.1247,\n",
            "        11.1252, 11.1247, 11.1253, 11.1234, 11.1248, 11.1250, 11.1244, 11.1256,\n",
            "        11.1253, 11.1235, 11.1247, 11.1247, 11.1248, 11.1246, 11.1252, 11.1250,\n",
            "        11.1251, 11.1249, 11.1248, 11.1250, 11.1250, 11.1252, 11.1253, 11.1248,\n",
            "        11.1251, 11.1251, 11.1266, 11.1251, 11.1249, 11.1248, 11.1253, 11.1261,\n",
            "        11.1245, 11.1243, 11.1256, 11.1242, 11.1247, 11.1251, 11.1246, 11.1251,\n",
            "        11.1247, 11.1244, 11.1247, 11.1254, 11.1249, 11.1258, 11.1250, 11.1244,\n",
            "        11.1247, 11.1245, 11.1258, 11.1252, 11.1251, 11.1249, 11.1253, 11.1251,\n",
            "        11.1248, 11.1244, 11.1256, 11.1248, 11.1245, 11.1248, 11.1248, 11.1248,\n",
            "        11.1252, 11.1250, 11.1296, 11.1249, 11.1244, 11.1241, 11.1256, 11.1242,\n",
            "        11.1245, 11.1247, 11.1267, 11.1249, 11.1245, 11.1254],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0091,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0091, 0.0092, 0.0092, 0.0091, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0091, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [49]\n",
            "DEBUGGING: logits looks like: tensor([11.1252, 11.1250, 11.1247, 11.1248, 11.1250, 11.1244, 11.1247, 11.1249,\n",
            "        11.1251, 11.1251, 11.1247, 11.1249, 11.1250, 11.1248, 11.1246, 11.1251,\n",
            "        11.1249, 11.1250, 11.1252, 11.1249, 11.1246, 11.1250, 11.1246, 11.1248,\n",
            "        11.1247, 11.1248, 11.1252, 11.1247, 11.1247, 11.1246, 11.1247, 11.1249,\n",
            "        11.1250, 11.1251, 11.1248, 11.1241, 11.1250, 11.1249, 11.1247, 11.1250,\n",
            "        11.1249, 11.1249, 11.1249, 11.1248, 11.1251, 11.1247, 11.1247, 11.1251,\n",
            "        11.1253, 11.1244, 11.1249, 11.1254, 11.1246, 11.1250, 11.1245, 11.1248,\n",
            "        11.1249, 11.1243, 11.1237, 11.1247, 11.1250, 11.1239, 11.1249, 11.1253,\n",
            "        11.1247, 11.1249, 11.1248, 11.1249, 11.1250, 11.1251, 11.1250, 11.1250,\n",
            "        11.1247, 11.1247, 11.1250, 11.1250, 11.1250, 11.1249, 11.1246, 11.1252,\n",
            "        11.1248, 11.1245, 11.1246, 11.1248, 11.1251, 11.1250, 11.1243, 11.1250,\n",
            "        11.1249, 11.1249, 11.1245, 11.1245, 11.1249, 11.1247, 11.1252, 11.1248,\n",
            "        11.1242, 11.1245, 11.1246, 11.1245, 11.1248, 11.1245, 11.1248, 11.1249,\n",
            "        11.1250, 11.1249, 11.1247, 11.1251, 11.1245], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.09964151977783331 and immediate abs rewards look like: [0.007508820026032481, 0.026964400756696705, 0.002210057692082046, 0.0069829723179282155, 0.006240137906843302, 0.012914196143810841, 0.002361019673116971, 0.0016041768408285861, 0.007773745458052872, 0.005521773932287033, 0.0002900584768212866, 0.0002248102596240642, 0.0044532654546856065, 0.0047277876210500835, 1.9581750621000538e-05, 3.0268769023678033e-05, 8.16517649582238e-06, 0.0007465365561074577, 0.0003034800492969225, 0.001108211582959484, 0.00016471750950586284, 2.3493684693676187e-05, 0.00028039148901370936, 1.3213822512625484e-05, 2.579071178843151e-05, 4.9362351091986056e-05, 0.00036779770698558423, 6.8243393798184115e-06, 6.0401180235203356e-06, 4.211225132166874e-06, 6.151911611596006e-05, 0.00015186645441644941, 7.042876859486569e-07, 0.0021291345469762746, 0.00030620252300650463, 7.519238624809077e-05, 0.00039941484237715486, 0.0001838939065237355, 0.00016937216923906817, 0.0005896045036024589, 6.29361725259514e-05, 0.0007095810278769932, 0.0005163789251128037, 0.000792271107002307, 9.431343551113969e-05, 2.887339769586106e-05, 1.0999956430168822e-06, 6.501215148091433e-05, 0.0001835187090364343, 0.00018532071726440336]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.26932012378962306 and immediate relative rewards look like: [0.0024211760146336686, 0.01743123729319544, 0.0021618922675684447, 0.009114284339154644, 0.01020415750380654, 0.025393280943455623, 0.005439259290594534, 0.004226899671516104, 0.02305589902764083, 0.018243234756005065, 0.0010560743363332472, 0.0008930079232083186, 0.019165148789695902, 0.021944059983824857, 9.753377255694352e-05, 0.0001608162068542443, 4.60928896284668e-05, 0.004462150344090002, 0.0019151907502136438, 0.00736249051460418, 0.0011494525111171126, 0.00017176297759775866, 0.0021431461936902794, 0.0001053996854143309, 0.00021429145223242767, 0.00042655439234950553, 0.003300541063004984, 6.351614025728455e-05, 5.822504851376269e-05, 4.1994945549081664e-05, 0.0006339276534636588, 0.0016154318060711379, 7.726143266571746e-06, 0.024064721923040273, 0.0035651936908227865, 0.0009005896033257087, 0.004916854409545306, 0.002325252270533102, 0.002198124942117017, 0.007848578011850058, 0.0008588940684292899, 0.00992009434813273, 0.007392716826199855, 0.011608290633597439, 0.001413651797701808, 0.0004424108189036939, 1.7221184285651768e-05, 0.0010394656120076436, 0.0029954369379945173, 0.0030867700800275614]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0161, 0.0139, 0.0151, 0.0143, 0.0143, 0.0148, 0.0144, 0.0142, 0.0145,\n",
            "        0.0146, 0.0145, 0.0139, 0.0140, 0.0147, 0.0151, 0.0147, 0.0148, 0.0147,\n",
            "        0.0146, 0.0149, 0.0152, 0.0142, 0.0161, 0.0142, 0.0139, 0.0132, 0.0148,\n",
            "        0.0144, 0.0139, 0.0149, 0.0158, 0.0134, 0.0142, 0.0140, 0.0144, 0.0146,\n",
            "        0.0142, 0.0143, 0.0148, 0.0151, 0.0146, 0.0142, 0.0145, 0.0148, 0.0148,\n",
            "        0.0156, 0.0148, 0.0147, 0.0143, 0.0144, 0.0141, 0.0145, 0.0136, 0.0145,\n",
            "        0.0144, 0.0146, 0.0141, 0.0145, 0.0142, 0.0150, 0.0141, 0.0141, 0.0142,\n",
            "        0.0147, 0.0141, 0.0150, 0.0148, 0.0132, 0.0139],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [67]\n",
            "DEBUGGING: logits looks like: tensor([11.1523, 11.1234, 11.1394, 11.1282, 11.1293, 11.1350, 11.1298, 11.1279,\n",
            "        11.1320, 11.1322, 11.1313, 11.1224, 11.1248, 11.1343, 11.1401, 11.1339,\n",
            "        11.1355, 11.1345, 11.1324, 11.1371, 11.1404, 11.1268, 11.1520, 11.1277,\n",
            "        11.1224, 11.1131, 11.1351, 11.1300, 11.1236, 11.1370, 11.1490, 11.1162,\n",
            "        11.1266, 11.1251, 11.1303, 11.1322, 11.1269, 11.1291, 11.1351, 11.1395,\n",
            "        11.1324, 11.1279, 11.1321, 11.1355, 11.1359, 11.1456, 11.1351, 11.1336,\n",
            "        11.1288, 11.1298, 11.1255, 11.1321, 11.1189, 11.1316, 11.1297, 11.1326,\n",
            "        11.1266, 11.1309, 11.1274, 11.1383, 11.1262, 11.1263, 11.1279, 11.1336,\n",
            "        11.1256, 11.1383, 11.1361, 11.1127, 11.1234], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0109, 0.0140, 0.0126, 0.0114, 0.0134, 0.0144, 0.0125, 0.0130, 0.0124,\n",
            "        0.0130, 0.0143, 0.0105, 0.0118, 0.0126, 0.0126, 0.0132, 0.0126, 0.0127,\n",
            "        0.0130, 0.0124, 0.0129, 0.0132, 0.0133, 0.0142, 0.0115, 0.0128, 0.0125,\n",
            "        0.0124, 0.0142, 0.0130, 0.0134, 0.0131, 0.0125, 0.0122, 0.0137, 0.0123,\n",
            "        0.0137, 0.0116, 0.0133, 0.0137, 0.0126, 0.0119, 0.0136, 0.0103, 0.0109,\n",
            "        0.0111, 0.0116, 0.0125, 0.0120, 0.0125, 0.0114, 0.0107, 0.0113, 0.0131,\n",
            "        0.0127, 0.0127, 0.0129, 0.0121, 0.0121, 0.0121, 0.0142, 0.0119, 0.0149,\n",
            "        0.0137, 0.0147, 0.0130, 0.0138, 0.0131, 0.0098, 0.0132, 0.0135, 0.0110,\n",
            "        0.0111, 0.0130, 0.0139, 0.0124, 0.0131, 0.0131, 0.0139],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [73]\n",
            "DEBUGGING: logits looks like: tensor([11.1141, 11.1634, 11.1429, 11.1233, 11.1544, 11.1692, 11.1415, 11.1494,\n",
            "        11.1390, 11.1489, 11.1686, 11.1059, 11.1289, 11.1435, 11.1429, 11.1520,\n",
            "        11.1434, 11.1442, 11.1487, 11.1389, 11.1476, 11.1516, 11.1533, 11.1661,\n",
            "        11.1241, 11.1462, 11.1407, 11.1398, 11.1668, 11.1493, 11.1556, 11.1502,\n",
            "        11.1414, 11.1366, 11.1598, 11.1379, 11.1597, 11.1269, 11.1540, 11.1592,\n",
            "        11.1434, 11.1316, 11.1575, 11.1019, 11.1133, 11.1174, 11.1255, 11.1408,\n",
            "        11.1334, 11.1408, 11.1224, 11.1096, 11.1211, 11.1499, 11.1444, 11.1436,\n",
            "        11.1474, 11.1350, 11.1349, 11.1349, 11.1662, 11.1311, 11.1764, 11.1598,\n",
            "        11.1743, 11.1496, 11.1613, 11.1509, 11.0933, 11.1524, 11.1563, 11.1148,\n",
            "        11.1171, 11.1496, 11.1627, 11.1388, 11.1502, 11.1500, 11.1625],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0114, 0.0114, 0.0114, 0.0112, 0.0113, 0.0114, 0.0111, 0.0114, 0.0111,\n",
            "        0.0112, 0.0111, 0.0104, 0.0109, 0.0112, 0.0113, 0.0109, 0.0112, 0.0112,\n",
            "        0.0112, 0.0114, 0.0114, 0.0118, 0.0111, 0.0109, 0.0111, 0.0118, 0.0111,\n",
            "        0.0112, 0.0113, 0.0116, 0.0109, 0.0111, 0.0114, 0.0109, 0.0117, 0.0113,\n",
            "        0.0112, 0.0116, 0.0112, 0.0110, 0.0111, 0.0114, 0.0110, 0.0110, 0.0108,\n",
            "        0.0112, 0.0112, 0.0111, 0.0114, 0.0114, 0.0115, 0.0113, 0.0107, 0.0110,\n",
            "        0.0113, 0.0112, 0.0115, 0.0112, 0.0112, 0.0110, 0.0112, 0.0114, 0.0113,\n",
            "        0.0109, 0.0114, 0.0117, 0.0112, 0.0111, 0.0113, 0.0113, 0.0113, 0.0113,\n",
            "        0.0109, 0.0112, 0.0109, 0.0116, 0.0109, 0.0111, 0.0116, 0.0113, 0.0118,\n",
            "        0.0112, 0.0112, 0.0114, 0.0117, 0.0111, 0.0115, 0.0111, 0.0112],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [51]\n",
            "DEBUGGING: logits looks like: tensor([11.1620, 11.1613, 11.1614, 11.1572, 11.1597, 11.1611, 11.1561, 11.1610,\n",
            "        11.1560, 11.1575, 11.1556, 11.1436, 11.1522, 11.1574, 11.1589, 11.1528,\n",
            "        11.1574, 11.1573, 11.1574, 11.1616, 11.1611, 11.1685, 11.1552, 11.1528,\n",
            "        11.1561, 11.1677, 11.1562, 11.1569, 11.1596, 11.1652, 11.1524, 11.1562,\n",
            "        11.1620, 11.1531, 11.1656, 11.1599, 11.1582, 11.1640, 11.1579, 11.1541,\n",
            "        11.1551, 11.1610, 11.1537, 11.1548, 11.1503, 11.1577, 11.1574, 11.1567,\n",
            "        11.1611, 11.1611, 11.1629, 11.1599, 11.1494, 11.1535, 11.1601, 11.1580,\n",
            "        11.1625, 11.1575, 11.1576, 11.1544, 11.1582, 11.1609, 11.1597, 11.1526,\n",
            "        11.1613, 11.1668, 11.1574, 11.1558, 11.1593, 11.1586, 11.1597, 11.1603,\n",
            "        11.1517, 11.1575, 11.1520, 11.1653, 11.1532, 11.1552, 11.1650, 11.1592,\n",
            "        11.1674, 11.1568, 11.1578, 11.1620, 11.1671, 11.1556, 11.1628, 11.1552,\n",
            "        11.1577], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0105, 0.0102, 0.0096, 0.0101, 0.0102, 0.0102, 0.0103, 0.0103, 0.0105,\n",
            "        0.0103, 0.0103, 0.0100, 0.0102, 0.0106, 0.0103, 0.0103, 0.0104, 0.0100,\n",
            "        0.0094, 0.0104, 0.0095, 0.0098, 0.0099, 0.0104, 0.0102, 0.0101, 0.0103,\n",
            "        0.0100, 0.0103, 0.0104, 0.0104, 0.0103, 0.0096, 0.0106, 0.0105, 0.0103,\n",
            "        0.0101, 0.0101, 0.0105, 0.0103, 0.0103, 0.0104, 0.0098, 0.0102, 0.0103,\n",
            "        0.0103, 0.0101, 0.0101, 0.0103, 0.0099, 0.0102, 0.0102, 0.0099, 0.0104,\n",
            "        0.0102, 0.0102, 0.0094, 0.0094, 0.0108, 0.0102, 0.0103, 0.0102, 0.0104,\n",
            "        0.0101, 0.0102, 0.0108, 0.0102, 0.0105, 0.0103, 0.0104, 0.0103, 0.0096,\n",
            "        0.0099, 0.0098, 0.0103, 0.0107, 0.0102, 0.0100, 0.0100, 0.0102, 0.0107,\n",
            "        0.0105, 0.0103, 0.0101, 0.0103, 0.0102, 0.0101, 0.0105, 0.0105, 0.0096,\n",
            "        0.0103, 0.0107, 0.0104, 0.0101, 0.0105, 0.0097, 0.0102, 0.0104],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [82]\n",
            "DEBUGGING: logits looks like: tensor([11.1584, 11.1537, 11.1416, 11.1500, 11.1538, 11.1521, 11.1547, 11.1557,\n",
            "        11.1589, 11.1542, 11.1554, 11.1492, 11.1536, 11.1597, 11.1547, 11.1555,\n",
            "        11.1570, 11.1491, 11.1367, 11.1570, 11.1376, 11.1454, 11.1477, 11.1574,\n",
            "        11.1533, 11.1499, 11.1539, 11.1497, 11.1554, 11.1576, 11.1572, 11.1549,\n",
            "        11.1399, 11.1597, 11.1592, 11.1548, 11.1514, 11.1513, 11.1582, 11.1540,\n",
            "        11.1538, 11.1561, 11.1452, 11.1531, 11.1540, 11.1540, 11.1515, 11.1514,\n",
            "        11.1551, 11.1470, 11.1522, 11.1522, 11.1466, 11.1573, 11.1519, 11.1537,\n",
            "        11.1370, 11.1371, 11.1645, 11.1523, 11.1553, 11.1536, 11.1570, 11.1500,\n",
            "        11.1527, 11.1646, 11.1529, 11.1582, 11.1543, 11.1561, 11.1543, 11.1412,\n",
            "        11.1471, 11.1450, 11.1555, 11.1632, 11.1536, 11.1494, 11.1484, 11.1529,\n",
            "        11.1619, 11.1591, 11.1546, 11.1510, 11.1553, 11.1524, 11.1513, 11.1580,\n",
            "        11.1591, 11.1404, 11.1541, 11.1620, 11.1566, 11.1509, 11.1580, 11.1430,\n",
            "        11.1522, 11.1573], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0102, 0.0092, 0.0092, 0.0091, 0.0092, 0.0091, 0.0092, 0.0091, 0.0092,\n",
            "        0.0092, 0.0092, 0.0091, 0.0092, 0.0091, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0091, 0.0092, 0.0091, 0.0092, 0.0091, 0.0091, 0.0092, 0.0092,\n",
            "        0.0092, 0.0091, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0091, 0.0092, 0.0092, 0.0091, 0.0092,\n",
            "        0.0092, 0.0092, 0.0091, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0091,\n",
            "        0.0092, 0.0091, 0.0092, 0.0092, 0.0091, 0.0092, 0.0092, 0.0091, 0.0092,\n",
            "        0.0092, 0.0091, 0.0092, 0.0092, 0.0092, 0.0092, 0.0091, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0091, 0.0091, 0.0091, 0.0092, 0.0092, 0.0091, 0.0091, 0.0092,\n",
            "        0.0092, 0.0092, 0.0091, 0.0091, 0.0092, 0.0091, 0.0091, 0.0092, 0.0091,\n",
            "        0.0092], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [42]\n",
            "DEBUGGING: logits looks like: tensor([11.1750, 11.1538, 11.1540, 11.1532, 11.1542, 11.1531, 11.1541, 11.1534,\n",
            "        11.1536, 11.1543, 11.1541, 11.1531, 11.1537, 11.1535, 11.1537, 11.1541,\n",
            "        11.1540, 11.1546, 11.1546, 11.1528, 11.1542, 11.1526, 11.1545, 11.1533,\n",
            "        11.1531, 11.1544, 11.1543, 11.1544, 11.1524, 11.1537, 11.1541, 11.1543,\n",
            "        11.1541, 11.1540, 11.1548, 11.1539, 11.1537, 11.1548, 11.1536, 11.1547,\n",
            "        11.1533, 11.1545, 11.1535, 11.1534, 11.1555, 11.1536, 11.1538, 11.1533,\n",
            "        11.1537, 11.1545, 11.1543, 11.1546, 11.1538, 11.1528, 11.1551, 11.1535,\n",
            "        11.1537, 11.1535, 11.1528, 11.1540, 11.1539, 11.1533, 11.1538, 11.1541,\n",
            "        11.1533, 11.1538, 11.1549, 11.1542, 11.1542, 11.1532, 11.1538, 11.1540,\n",
            "        11.1540, 11.1539, 11.1543, 11.1542, 11.1540, 11.1543, 11.1546, 11.1539,\n",
            "        11.1537, 11.1548, 11.1537, 11.1537, 11.1540, 11.1545, 11.1545, 11.1542,\n",
            "        11.1542, 11.1536, 11.1540, 11.1534, 11.1523, 11.1535, 11.1538, 11.1541,\n",
            "        11.1529, 11.1526, 11.1542, 11.1537, 11.1538, 11.1535, 11.1534, 11.1543,\n",
            "        11.1532, 11.1529, 11.1541, 11.1533, 11.1538], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.12117175670937286 and immediate abs rewards look like: [0.005912808257107827, 0.005383229840845161, 0.009441513300771476, 0.006541754643876629, 0.008030653413698019, 0.007282686790404114, 0.0018875502037190017, 0.010292154538547038, 0.0006430647281376878, 0.0035049723433075997, 0.011227885877815424, 0.0001534614621050423, 6.800458231737139e-05, 0.0001597094965291035, 2.6215118396066828e-05, 0.00015000620123828412, 7.323618228838313e-05, 0.03053218343484332, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 0.0, 9.094947017729282e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 1.3642420526593924e-12, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.012818133032396872, 0.00020068514550075633, 0.000992084434528806, 0.0022483380457742896, 0.001769176287780283, 0.0002684623973436828, 0.00022186819433045457, 0.0001418872011527128, 0.000300402398352162, 0.00027628802627077675, 0.0005970199413241062, 2.6321177301724674e-05]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.5889342029884204 and immediate relative rewards look like: [0.0019065511600497972, 0.0034782146594012496, 0.009166473115095861, 0.008494203512458406, 0.013062085848496033, 0.014251827992231673, 0.0043197344466995085, 0.026935489454427405, 0.001899723932060609, 0.011507197611599584, 0.040595293878707334, 0.0006075350143679901, 0.0002916716732209737, 0.0007377024801511933, 0.0001297443440430312, 0.0007919145172080214, 0.0004108139881369977, 0.18134745601159716, 2.8800665556142726e-12, 3.0316490059102206e-12, 3.183231456205249e-12, 6.669627813002486e-12, 3.4863963567956963e-12, 3.637978807091713e-12, 7.579122514775551e-12, 0.0, 8.185452315955113e-12, 8.488617216548618e-12, 4.3958910585684865e-12, 4.547473508864641e-12, 4.699055959160842e-12, 9.701276818911234e-12, 1.5006662579248766e-11, 0.0, 5.305385760342886e-12, 5.4569682106375694e-12, 5.608550660932207e-12, 5.760133111228545e-12, 0.1666357294211846, 0.0026872839276372504, 0.01361758221418543, 0.03162442823557227, 0.02549637737004521, 0.003961253537844282, 0.00334844494601231, 0.0021891169859395974, 0.004735761680494506, 0.004448725906038919, 0.009814259072634697, 0.0004416059432550889]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0174, 0.0146, 0.0144, 0.0147, 0.0144, 0.0149, 0.0146, 0.0161, 0.0149,\n",
            "        0.0145, 0.0145, 0.0148, 0.0156, 0.0151, 0.0146, 0.0144, 0.0144, 0.0154,\n",
            "        0.0145, 0.0144, 0.0147, 0.0146, 0.0144, 0.0152, 0.0154, 0.0147, 0.0144,\n",
            "        0.0147, 0.0155, 0.0146, 0.0146, 0.0144, 0.0142, 0.0142, 0.0146, 0.0148,\n",
            "        0.0147, 0.0144, 0.0144, 0.0149, 0.0146, 0.0146, 0.0146, 0.0145, 0.0143,\n",
            "        0.0146, 0.0147, 0.0144, 0.0146, 0.0143, 0.0149, 0.0150, 0.0146, 0.0146,\n",
            "        0.0146, 0.0144, 0.0148, 0.0146, 0.0147, 0.0147, 0.0147, 0.0146, 0.0143,\n",
            "        0.0144, 0.0147, 0.0145, 0.0146, 0.0146], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [52]\n",
            "DEBUGGING: logits looks like: tensor([11.2388, 11.2032, 11.2014, 11.2059, 11.2014, 11.2081, 11.2044, 11.2240,\n",
            "        11.2081, 11.2021, 11.2027, 11.2069, 11.2166, 11.2107, 11.2035, 11.2009,\n",
            "        11.2015, 11.2140, 11.2022, 11.2017, 11.2053, 11.2037, 11.2016, 11.2123,\n",
            "        11.2143, 11.2050, 11.2014, 11.2049, 11.2154, 11.2037, 11.2039, 11.2010,\n",
            "        11.1990, 11.1979, 11.2037, 11.2072, 11.2058, 11.2015, 11.2015, 11.2077,\n",
            "        11.2033, 11.2040, 11.2033, 11.2024, 11.2003, 11.2041, 11.2048, 11.2007,\n",
            "        11.2043, 11.1993, 11.2081, 11.2093, 11.2045, 11.2040, 11.2037, 11.2006,\n",
            "        11.2061, 11.2039, 11.2048, 11.2047, 11.2049, 11.2040, 11.1994, 11.2013,\n",
            "        11.2047, 11.2021, 11.2041, 11.2041], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0153, 0.0124, 0.0126, 0.0124, 0.0124, 0.0124, 0.0123, 0.0125, 0.0124,\n",
            "        0.0124, 0.0124, 0.0124, 0.0125, 0.0124, 0.0124, 0.0127, 0.0124, 0.0124,\n",
            "        0.0126, 0.0125, 0.0123, 0.0125, 0.0125, 0.0124, 0.0125, 0.0124, 0.0124,\n",
            "        0.0124, 0.0125, 0.0125, 0.0130, 0.0124, 0.0124, 0.0124, 0.0125, 0.0125,\n",
            "        0.0125, 0.0123, 0.0124, 0.0125, 0.0123, 0.0124, 0.0125, 0.0125, 0.0125,\n",
            "        0.0124, 0.0127, 0.0125, 0.0124, 0.0124, 0.0124, 0.0124, 0.0127, 0.0125,\n",
            "        0.0124, 0.0127, 0.0124, 0.0125, 0.0125, 0.0125, 0.0124, 0.0125, 0.0125,\n",
            "        0.0124, 0.0124, 0.0125, 0.0124, 0.0126, 0.0124, 0.0125, 0.0125, 0.0124,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0124, 0.0124, 0.0125, 0.0124],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [2]\n",
            "DEBUGGING: logits looks like: tensor([11.2475, 11.2051, 11.2082, 11.2053, 11.2049, 11.2061, 11.2039, 11.2066,\n",
            "        11.2060, 11.2052, 11.2061, 11.2059, 11.2069, 11.2052, 11.2050, 11.2106,\n",
            "        11.2057, 11.2060, 11.2085, 11.2064, 11.2039, 11.2063, 11.2077, 11.2058,\n",
            "        11.2066, 11.2061, 11.2057, 11.2059, 11.2072, 11.2068, 11.2144, 11.2053,\n",
            "        11.2060, 11.2054, 11.2069, 11.2070, 11.2078, 11.2044, 11.2061, 11.2064,\n",
            "        11.2035, 11.2060, 11.2068, 11.2063, 11.2077, 11.2062, 11.2094, 11.2064,\n",
            "        11.2061, 11.2062, 11.2060, 11.2057, 11.2100, 11.2068, 11.2054, 11.2095,\n",
            "        11.2061, 11.2063, 11.2070, 11.2064, 11.2059, 11.2074, 11.2071, 11.2049,\n",
            "        11.2061, 11.2065, 11.2055, 11.2080, 11.2060, 11.2074, 11.2075, 11.2048,\n",
            "        11.2074, 11.2070, 11.2077, 11.2066, 11.2054, 11.2051, 11.2065, 11.2056],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0137, 0.0112, 0.0113, 0.0113, 0.0112, 0.0112, 0.0113, 0.0111, 0.0112,\n",
            "        0.0111, 0.0112, 0.0112, 0.0113, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0112, 0.0113, 0.0113, 0.0112, 0.0113, 0.0113, 0.0112, 0.0112, 0.0113,\n",
            "        0.0112, 0.0112, 0.0112, 0.0113, 0.0112, 0.0113, 0.0112, 0.0113, 0.0112,\n",
            "        0.0112, 0.0112, 0.0113, 0.0111, 0.0112, 0.0111, 0.0112, 0.0113, 0.0115,\n",
            "        0.0111, 0.0112, 0.0111, 0.0113, 0.0112, 0.0113, 0.0111, 0.0113, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0111, 0.0112, 0.0112,\n",
            "        0.0111, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0113, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0111, 0.0112, 0.0112, 0.0111, 0.0113, 0.0113,\n",
            "        0.0111, 0.0112, 0.0113, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [75]\n",
            "DEBUGGING: logits looks like: tensor([11.2494, 11.2086, 11.2102, 11.2109, 11.2088, 11.2086, 11.2108, 11.2071,\n",
            "        11.2082, 11.2076, 11.2088, 11.2093, 11.2098, 11.2076, 11.2070, 11.2074,\n",
            "        11.2067, 11.2076, 11.2095, 11.2096, 11.2098, 11.2084, 11.2101, 11.2104,\n",
            "        11.2091, 11.2089, 11.2098, 11.2087, 11.2089, 11.2089, 11.2110, 11.2079,\n",
            "        11.2097, 11.2082, 11.2102, 11.2095, 11.2087, 11.2091, 11.2097, 11.2073,\n",
            "        11.2088, 11.2073, 11.2080, 11.2103, 11.2133, 11.2074, 11.2085, 11.2077,\n",
            "        11.2112, 11.2085, 11.2106, 11.2068, 11.2098, 11.2082, 11.2091, 11.2083,\n",
            "        11.2092, 11.2093, 11.2083, 11.2085, 11.2075, 11.2086, 11.2082, 11.2070,\n",
            "        11.2094, 11.2093, 11.2092, 11.2089, 11.2093, 11.2100, 11.2087, 11.2080,\n",
            "        11.2081, 11.2080, 11.2094, 11.2074, 11.2091, 11.2086, 11.2075, 11.2107,\n",
            "        11.2097, 11.2072, 11.2087, 11.2098, 11.2084, 11.2094, 11.2085, 11.2087,\n",
            "        11.2084], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0122, 0.0100, 0.0099, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0098,\n",
            "        0.0100, 0.0100, 0.0100, 0.0099, 0.0100, 0.0099, 0.0100, 0.0101, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0099, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0101, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0099, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0099, 0.0100, 0.0100,\n",
            "        0.0100, 0.0099, 0.0100, 0.0100, 0.0100, 0.0099, 0.0101, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0099, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0099, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [53]\n",
            "DEBUGGING: logits looks like: tensor([11.2504, 11.2107, 11.2097, 11.2106, 11.2103, 11.2106, 11.2100, 11.2104,\n",
            "        11.2075, 11.2104, 11.2105, 11.2104, 11.2092, 11.2109, 11.2092, 11.2109,\n",
            "        11.2122, 11.2108, 11.2102, 11.2114, 11.2104, 11.2111, 11.2104, 11.2101,\n",
            "        11.2109, 11.2105, 11.2108, 11.2114, 11.2103, 11.2098, 11.2104, 11.2105,\n",
            "        11.2105, 11.2103, 11.2104, 11.2109, 11.2131, 11.2100, 11.2105, 11.2104,\n",
            "        11.2111, 11.2105, 11.2106, 11.2098, 11.2109, 11.2108, 11.2108, 11.2103,\n",
            "        11.2107, 11.2112, 11.2109, 11.2093, 11.2106, 11.2104, 11.2102, 11.2097,\n",
            "        11.2109, 11.2110, 11.2102, 11.2097, 11.2138, 11.2102, 11.2107, 11.2106,\n",
            "        11.2106, 11.2108, 11.2108, 11.2104, 11.2107, 11.2104, 11.2097, 11.2105,\n",
            "        11.2107, 11.2107, 11.2107, 11.2107, 11.2107, 11.2105, 11.2107, 11.2108,\n",
            "        11.2104, 11.2111, 11.2106, 11.2104, 11.2099, 11.2104, 11.2107, 11.2108,\n",
            "        11.2108, 11.2103, 11.2107, 11.2105, 11.2111, 11.2101, 11.2104, 11.2103,\n",
            "        11.2107, 11.2102, 11.2106, 11.2107], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0115, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [49]\n",
            "DEBUGGING: logits looks like: tensor([11.2512, 11.2110, 11.2113, 11.2110, 11.2113, 11.2111, 11.2116, 11.2114,\n",
            "        11.2112, 11.2112, 11.2111, 11.2106, 11.2111, 11.2111, 11.2112, 11.2111,\n",
            "        11.2114, 11.2103, 11.2113, 11.2113, 11.2113, 11.2103, 11.2110, 11.2112,\n",
            "        11.2112, 11.2110, 11.2109, 11.2110, 11.2112, 11.2112, 11.2109, 11.2113,\n",
            "        11.2110, 11.2113, 11.2112, 11.2113, 11.2112, 11.2113, 11.2113, 11.2111,\n",
            "        11.2113, 11.2113, 11.2115, 11.2112, 11.2113, 11.2110, 11.2112, 11.2112,\n",
            "        11.2112, 11.2112, 11.2113, 11.2112, 11.2110, 11.2110, 11.2111, 11.2112,\n",
            "        11.2113, 11.2114, 11.2110, 11.2113, 11.2109, 11.2112, 11.2110, 11.2105,\n",
            "        11.2112, 11.2112, 11.2115, 11.2111, 11.2111, 11.2114, 11.2112, 11.2114,\n",
            "        11.2109, 11.2111, 11.2112, 11.2111, 11.2112, 11.2109, 11.2110, 11.2112,\n",
            "        11.2112, 11.2111, 11.2112, 11.2111, 11.2110, 11.2110, 11.2107, 11.2112,\n",
            "        11.2113, 11.2112, 11.2111, 11.2109, 11.2108, 11.2109, 11.2112, 11.2112,\n",
            "        11.2112, 11.2113, 11.2114, 11.2111, 11.2113, 11.2113, 11.2111, 11.2110,\n",
            "        11.2113, 11.2112], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.20802807457812378 and immediate abs rewards look like: [0.056737522998446366, 0.004790729225760515, 0.011507806201962012, 0.03668992001666993, 0.006964224719695267, 0.027654766255636787, 0.0029480703581157286, 0.00045147223045205465, 0.012113628653651176, 4.967225595464697e-05, 0.0020280282701605756, 0.005471160733804936, 0.0034250345690907125, 0.001642725860392602, 0.0012902374010081985, 0.00783673785576866, 0.0024496587745943543, 0.00012001378513559757, 0.0002911312383275799, 5.485647534442251e-06, 0.00019073759267485002, 0.003844912441309134, 0.0008563131191294815, 0.0002844094990450685, 0.0006567543571236456, 7.698129297750711e-05, 0.00019406081673878361, 0.005325599387106195, 0.0013628379247165867, 5.774628675681015e-05, 0.004073448999861284, 0.00027159097635376384, 0.0002840839201780909, 0.001026354604618973, 0.00029843530182915856, 0.00068999018571958, 6.720789315295406e-05, 0.0001486202449996199, 0.0009666177791132213, 0.00013952915287518408, 0.0001598744954662834, 0.0004692494380833523, 1.3000635362914181e-05, 0.00012075006839040725, 0.0003726216505128832, 2.3093072059054975e-05, 0.0003256208703987795, 0.0004915895260637626, 0.0007412709946947871, 2.6744998649519403e-05]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.47916276271715613 and immediate relative rewards look like: [0.01572875666694634, 0.002698614476312515, 0.009736646700226063, 0.04152543494693412, 0.009955941750283057, 0.047536415768355336, 0.005959304512111113, 0.0010438800754028748, 0.03151396272149547, 0.00014408657203820205, 0.0064711664669253694, 0.01905601856997039, 0.012944043610536877, 0.006692485304259527, 0.005634596743898115, 0.036519130201116724, 0.012156597126740842, 0.0006310616958899391, 0.001615942177875658, 3.205370543747661e-05, 0.0011702445682550396, 0.02471464515736971, 0.005760946414766686, 0.0019970895308312163, 0.004804202527298812, 0.0005857608621183312, 0.0015334626588080886, 0.04364382218585007, 0.011585530561423122, 0.0005080337591896425, 0.037032120951266824, 0.0025517553584466506, 0.0027527637841332645, 0.010247570976273341, 0.003068271155730442, 0.007297245995369189, 0.0007306739412416976, 0.0016594789949301698, 0.011077673030411097, 0.0016405047077231384, 0.0019267855815376406, 0.005793537278548218, 0.0001643552943726577, 0.0015620408503107502, 0.00493001615637147, 0.00031235960725313483, 0.004500164253090895, 0.006939109138431919, 0.010683061653306462, 0.0003933959897404917]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 3\n",
            "DEBUGGING: the action_prob is: tensor([0.0174, 0.0154, 0.0158, 0.0155, 0.0156, 0.0162, 0.0152, 0.0152, 0.0157,\n",
            "        0.0156, 0.0152, 0.0140, 0.0166, 0.0150, 0.0151, 0.0141, 0.0151, 0.0143,\n",
            "        0.0153, 0.0139, 0.0153, 0.0158, 0.0155, 0.0154, 0.0143, 0.0156, 0.0150,\n",
            "        0.0165, 0.0147, 0.0143, 0.0159, 0.0148, 0.0149, 0.0150, 0.0148, 0.0149,\n",
            "        0.0139, 0.0154, 0.0141, 0.0147, 0.0159, 0.0162, 0.0092, 0.0147, 0.0137,\n",
            "        0.0151, 0.0148, 0.0143, 0.0155, 0.0156, 0.0150, 0.0164, 0.0164, 0.0157,\n",
            "        0.0154, 0.0144, 0.0156, 0.0153, 0.0156, 0.0155, 0.0147, 0.0154, 0.0167,\n",
            "        0.0157, 0.0138, 0.0166], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [41]\n",
            "DEBUGGING: logits looks like: tensor([11.1555, 11.1318, 11.1358, 11.1321, 11.1342, 11.1410, 11.1281, 11.1288,\n",
            "        11.1348, 11.1335, 11.1285, 11.1124, 11.1466, 11.1261, 11.1270, 11.1136,\n",
            "        11.1272, 11.1170, 11.1300, 11.1111, 11.1305, 11.1365, 11.1322, 11.1315,\n",
            "        11.1163, 11.1332, 11.1261, 11.1453, 11.1216, 11.1170, 11.1381, 11.1227,\n",
            "        11.1251, 11.1255, 11.1235, 11.1248, 11.1104, 11.1307, 11.1135, 11.1222,\n",
            "        11.1372, 11.1410, 11.0292, 11.1220, 11.1072, 11.1268, 11.1232, 11.1164,\n",
            "        11.1328, 11.1342, 11.1254, 11.1441, 11.1436, 11.1345, 11.1310, 11.1178,\n",
            "        11.1332, 11.1300, 11.1342, 11.1325, 11.1222, 11.1309, 11.1476, 11.1351,\n",
            "        11.1100, 11.1466], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0146, 0.0130, 0.0128, 0.0127, 0.0127, 0.0125, 0.0129, 0.0125, 0.0132,\n",
            "        0.0126, 0.0127, 0.0131, 0.0127, 0.0127, 0.0130, 0.0122, 0.0129, 0.0125,\n",
            "        0.0126, 0.0127, 0.0134, 0.0127, 0.0132, 0.0127, 0.0126, 0.0128, 0.0127,\n",
            "        0.0127, 0.0125, 0.0128, 0.0130, 0.0127, 0.0130, 0.0126, 0.0127, 0.0125,\n",
            "        0.0125, 0.0129, 0.0144, 0.0135, 0.0129, 0.0127, 0.0126, 0.0130, 0.0131,\n",
            "        0.0132, 0.0127, 0.0134, 0.0126, 0.0127, 0.0126, 0.0132, 0.0130, 0.0127,\n",
            "        0.0125, 0.0124, 0.0128, 0.0129, 0.0127, 0.0127, 0.0124, 0.0128, 0.0127,\n",
            "        0.0127, 0.0129, 0.0125, 0.0131, 0.0128, 0.0125, 0.0129, 0.0129, 0.0127,\n",
            "        0.0132, 0.0125, 0.0122, 0.0130, 0.0130, 0.0127],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [47]\n",
            "DEBUGGING: logits looks like: tensor([11.1496, 11.1260, 11.1235, 11.1219, 11.1225, 11.1196, 11.1247, 11.1189,\n",
            "        11.1293, 11.1203, 11.1225, 11.1274, 11.1214, 11.1220, 11.1268, 11.1137,\n",
            "        11.1252, 11.1189, 11.1198, 11.1216, 11.1334, 11.1224, 11.1292, 11.1213,\n",
            "        11.1197, 11.1234, 11.1225, 11.1218, 11.1184, 11.1240, 11.1259, 11.1224,\n",
            "        11.1262, 11.1197, 11.1218, 11.1186, 11.1195, 11.1252, 11.1473, 11.1340,\n",
            "        11.1252, 11.1218, 11.1210, 11.1272, 11.1275, 11.1293, 11.1222, 11.1329,\n",
            "        11.1201, 11.1226, 11.1198, 11.1290, 11.1261, 11.1228, 11.1182, 11.1171,\n",
            "        11.1231, 11.1251, 11.1222, 11.1227, 11.1174, 11.1235, 11.1223, 11.1226,\n",
            "        11.1249, 11.1184, 11.1280, 11.1236, 11.1188, 11.1247, 11.1250, 11.1227,\n",
            "        11.1293, 11.1182, 11.1137, 11.1271, 11.1273, 11.1215],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0133, 0.0113, 0.0114, 0.0114, 0.0115, 0.0114, 0.0113, 0.0114, 0.0113,\n",
            "        0.0114, 0.0114, 0.0114, 0.0113, 0.0114, 0.0111, 0.0113, 0.0114, 0.0112,\n",
            "        0.0113, 0.0113, 0.0114, 0.0114, 0.0114, 0.0112, 0.0114, 0.0112, 0.0114,\n",
            "        0.0115, 0.0113, 0.0109, 0.0114, 0.0113, 0.0112, 0.0113, 0.0115, 0.0112,\n",
            "        0.0114, 0.0112, 0.0114, 0.0113, 0.0115, 0.0114, 0.0118, 0.0112, 0.0113,\n",
            "        0.0115, 0.0113, 0.0114, 0.0116, 0.0113, 0.0113, 0.0114, 0.0114, 0.0113,\n",
            "        0.0114, 0.0113, 0.0113, 0.0114, 0.0113, 0.0111, 0.0113, 0.0112, 0.0114,\n",
            "        0.0114, 0.0114, 0.0117, 0.0113, 0.0115, 0.0109, 0.0113, 0.0112, 0.0114,\n",
            "        0.0114, 0.0110, 0.0113, 0.0115, 0.0114, 0.0113, 0.0114, 0.0113, 0.0112,\n",
            "        0.0114, 0.0117, 0.0112, 0.0114, 0.0114, 0.0113, 0.0111],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [81]\n",
            "DEBUGGING: logits looks like: tensor([11.1570, 11.1257, 11.1268, 11.1263, 11.1279, 11.1263, 11.1249, 11.1268,\n",
            "        11.1253, 11.1262, 11.1259, 11.1261, 11.1253, 11.1272, 11.1214, 11.1243,\n",
            "        11.1260, 11.1226, 11.1258, 11.1257, 11.1273, 11.1269, 11.1269, 11.1239,\n",
            "        11.1264, 11.1240, 11.1269, 11.1280, 11.1243, 11.1176, 11.1264, 11.1258,\n",
            "        11.1238, 11.1254, 11.1278, 11.1233, 11.1260, 11.1232, 11.1262, 11.1244,\n",
            "        11.1277, 11.1265, 11.1340, 11.1236, 11.1254, 11.1279, 11.1252, 11.1262,\n",
            "        11.1300, 11.1249, 11.1246, 11.1262, 11.1274, 11.1255, 11.1264, 11.1249,\n",
            "        11.1258, 11.1270, 11.1257, 11.1223, 11.1257, 11.1235, 11.1261, 11.1272,\n",
            "        11.1271, 11.1312, 11.1256, 11.1278, 11.1176, 11.1246, 11.1240, 11.1267,\n",
            "        11.1270, 11.1203, 11.1253, 11.1291, 11.1265, 11.1253, 11.1263, 11.1246,\n",
            "        11.1240, 11.1261, 11.1324, 11.1235, 11.1267, 11.1266, 11.1250, 11.1207],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0116, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0101, 0.0100, 0.0099,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0099, 0.0100, 0.0100, 0.0100, 0.0101,\n",
            "        0.0101, 0.0100, 0.0099, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0099, 0.0100, 0.0100, 0.0100, 0.0100, 0.0102, 0.0100,\n",
            "        0.0098, 0.0101, 0.0099, 0.0100, 0.0100, 0.0100, 0.0100, 0.0099, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0099, 0.0100, 0.0100, 0.0101, 0.0100,\n",
            "        0.0100, 0.0099, 0.0100, 0.0098, 0.0099, 0.0100, 0.0100, 0.0101, 0.0100,\n",
            "        0.0100, 0.0099, 0.0099, 0.0100, 0.0099, 0.0101, 0.0102, 0.0100, 0.0099,\n",
            "        0.0099, 0.0100, 0.0100, 0.0099, 0.0099, 0.0100, 0.0100, 0.0101, 0.0100,\n",
            "        0.0099, 0.0100, 0.0100, 0.0100, 0.0100, 0.0101, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0099, 0.0100, 0.0100, 0.0100, 0.0098, 0.0099, 0.0100,\n",
            "        0.0099], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [26]\n",
            "DEBUGGING: logits looks like: tensor([11.1537, 11.1234, 11.1229, 11.1228, 11.1227, 11.1231, 11.1264, 11.1239,\n",
            "        11.1205, 11.1238, 11.1233, 11.1231, 11.1230, 11.1222, 11.1239, 11.1239,\n",
            "        11.1228, 11.1250, 11.1248, 11.1234, 11.1209, 11.1227, 11.1227, 11.1234,\n",
            "        11.1236, 11.1229, 11.1231, 11.1227, 11.1229, 11.1222, 11.1232, 11.1227,\n",
            "        11.1229, 11.1237, 11.1283, 11.1231, 11.1196, 11.1252, 11.1225, 11.1233,\n",
            "        11.1236, 11.1234, 11.1236, 11.1223, 11.1235, 11.1235, 11.1228, 11.1236,\n",
            "        11.1232, 11.1220, 11.1231, 11.1228, 11.1246, 11.1242, 11.1243, 11.1223,\n",
            "        11.1229, 11.1200, 11.1220, 11.1228, 11.1236, 11.1257, 11.1235, 11.1226,\n",
            "        11.1221, 11.1221, 11.1227, 11.1223, 11.1246, 11.1271, 11.1232, 11.1224,\n",
            "        11.1225, 11.1227, 11.1229, 11.1225, 11.1214, 11.1244, 11.1241, 11.1253,\n",
            "        11.1235, 11.1220, 11.1238, 11.1227, 11.1235, 11.1232, 11.1250, 11.1228,\n",
            "        11.1238, 11.1226, 11.1232, 11.1244, 11.1221, 11.1227, 11.1238, 11.1239,\n",
            "        11.1194, 11.1225, 11.1235, 11.1223], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0107, 0.0092, 0.0092, 0.0092, 0.0092, 0.0090, 0.0089, 0.0090, 0.0092,\n",
            "        0.0090, 0.0093, 0.0092, 0.0092, 0.0092, 0.0091, 0.0089, 0.0092, 0.0089,\n",
            "        0.0091, 0.0091, 0.0092, 0.0092, 0.0092, 0.0091, 0.0092, 0.0091, 0.0092,\n",
            "        0.0093, 0.0091, 0.0090, 0.0092, 0.0093, 0.0092, 0.0091, 0.0092, 0.0092,\n",
            "        0.0091, 0.0092, 0.0091, 0.0098, 0.0092, 0.0091, 0.0090, 0.0092, 0.0091,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0091, 0.0092, 0.0090, 0.0092, 0.0091,\n",
            "        0.0092, 0.0090, 0.0092, 0.0092, 0.0091, 0.0091, 0.0092, 0.0090, 0.0091,\n",
            "        0.0093, 0.0091, 0.0091, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0091, 0.0092, 0.0092, 0.0091, 0.0092, 0.0091, 0.0092, 0.0092,\n",
            "        0.0091, 0.0092, 0.0091, 0.0092, 0.0092, 0.0091, 0.0092, 0.0091, 0.0092,\n",
            "        0.0091, 0.0092, 0.0092, 0.0092, 0.0091, 0.0092, 0.0092, 0.0091, 0.0089,\n",
            "        0.0093, 0.0091, 0.0092, 0.0091, 0.0091, 0.0092, 0.0094, 0.0092, 0.0092,\n",
            "        0.0092], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [63]\n",
            "DEBUGGING: logits looks like: tensor([11.1509, 11.1204, 11.1209, 11.1201, 11.1202, 11.1166, 11.1143, 11.1155,\n",
            "        11.1195, 11.1154, 11.1233, 11.1203, 11.1196, 11.1206, 11.1191, 11.1140,\n",
            "        11.1202, 11.1148, 11.1189, 11.1193, 11.1206, 11.1208, 11.1199, 11.1193,\n",
            "        11.1203, 11.1184, 11.1202, 11.1215, 11.1179, 11.1168, 11.1195, 11.1218,\n",
            "        11.1196, 11.1179, 11.1209, 11.1211, 11.1189, 11.1207, 11.1191, 11.1340,\n",
            "        11.1202, 11.1171, 11.1165, 11.1206, 11.1187, 11.1206, 11.1194, 11.1196,\n",
            "        11.1210, 11.1190, 11.1209, 11.1165, 11.1198, 11.1183, 11.1197, 11.1164,\n",
            "        11.1202, 11.1194, 11.1192, 11.1187, 11.1204, 11.1159, 11.1189, 11.1217,\n",
            "        11.1184, 11.1188, 11.1203, 11.1203, 11.1196, 11.1208, 11.1196, 11.1204,\n",
            "        11.1209, 11.1190, 11.1195, 11.1193, 11.1188, 11.1196, 11.1189, 11.1194,\n",
            "        11.1199, 11.1193, 11.1204, 11.1192, 11.1206, 11.1206, 11.1192, 11.1207,\n",
            "        11.1184, 11.1199, 11.1183, 11.1209, 11.1199, 11.1201, 11.1190, 11.1212,\n",
            "        11.1203, 11.1179, 11.1134, 11.1219, 11.1191, 11.1209, 11.1187, 11.1189,\n",
            "        11.1201, 11.1246, 11.1196, 11.1208, 11.1199], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.03265144051692914 and immediate abs rewards look like: [0.0008093529313555337, 0.001323814924035105, 4.096958400623407e-05, 0.0005455057334984303, 0.0013527804621844552, 0.0006710298175676144, 0.0010769378732220503, 0.0016836950826473185, 0.0004641370542231016, 0.004758545621371013, 0.00045724615301878657, 0.0010618800115480553, 0.001031928522479575, 0.0019086358383901825, 5.659335056407144e-05, 0.002123270071024308, 7.211019692476839e-06, 9.837675588642014e-05, 2.002958217417472e-05, 0.0005208866791690525, 7.268615809152834e-05, 0.00197836539518903, 0.0015757048254272377, 0.0009576020534041163, 0.0018283709755451127, 0.0013803830834149267, 0.0007182690196714248, 3.032751601494965e-05, 0.0012777599545188423, 6.63910841467441e-05, 0.0004269139353709761, 5.052927144788555e-05, 0.00045670865847569075, 5.135044830240076e-06, 8.892289770301431e-06, 2.3772695385559928e-05, 0.00032751824028309784, 0.0002280400749441469, 0.0006271787028708786, 2.5442789592489135e-05, 3.064040356548503e-06, 3.1566491998091806e-05, 9.073895216715755e-05, 4.663625077228062e-06, 2.1437876966956537e-06, 3.9396352804033086e-05, 6.280029083427507e-07, 0.00012473481820052257, 7.38301587261958e-05, 0.00020185545054118847]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.20286485531413906 and immediate relative rewards look like: [0.0002991126938536157, 0.0009787777563830262, 4.5459245411221224e-05, 0.0008070590378984427, 0.0025022514823237452, 0.0014901984073439137, 0.0027909213224728167, 0.0049886785758652815, 0.0015480741092630985, 0.01763810576060228, 0.0018676140824382345, 0.004732327895767928, 0.004984050199862108, 0.00993132248968549, 0.00031573362043705845, 0.012635677301030254, 4.563121356782216e-05, 0.000659147461558095, 0.00014166382912198956, 0.0038780193403147706, 0.000568318706289702, 0.01620547940247885, 0.013503782090999169, 0.008568485947500103, 0.017047754601724197, 0.013394672058275013, 0.0072415878778995454, 0.0003171715866473601, 0.013840494055947937, 0.0007442908117893705, 0.004945662116163728, 0.0006043447083302937, 0.005633173092488783, 6.526753472590327e-05, 0.0001163473609728595, 0.0003199316870142512, 0.004530200308974285, 0.00323987413238796, 0.00914589699319501, 0.0003806247040768981, 4.69845124932875e-05, 0.000495852487732382, 0.0014592988075891415, 7.674905850088298e-05, 3.6082099239218586e-05, 0.0006778158289358433, 1.1039865168312707e-05, 0.0022394083549075205, 0.001353176673651493, 0.0037752620228385654]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 200 number of matrices\n",
            "DEBUGGING: ACT_MAT has 200 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.22638015900995379, 0.22622119494476778, 0.21089894712280036, 0.21084550995477971, 0.20376891476325765, 0.19551995682772838, 0.17184512715583108, 0.16808673521741066, 0.1655149853998935, 0.1438980670426795, 0.12692407301684286, 0.12713939260657536, 0.12752160069026974, 0.10945096151573114, 0.08839080962818817, 0.08918512712690023, 0.08992354638388483, 0.09078530655985492, 0.08719510728865143, 0.08614132983680585, 0.07957458517394109, 0.07921730572002421, 0.07984398256810753, 0.07848569330749217, 0.07917201375967459, 0.07975527505802239, 0.08013002087441705, 0.0776055351630425, 0.07832527174018708, 0.07905762292088214, 0.07981376563164955, 0.07997963432139989, 0.07915576011649368, 0.07994750906386577, 0.056447259738207585, 0.05341622833069171, 0.05304609972461213, 0.0486153993081483, 0.0467577242804194, 0.04500969630131553, 0.0375364831206722, 0.037048069749740314, 0.02740199535515918, 0.020211392453494265, 0.008690001838279624, 0.007349848525836178, 0.006977209804982308, 0.007030291536057229, 0.006051339317221803, 0.0030867700800275614], [0.46281221289188584, 0.46556127447660206, 0.46675056547192, 0.46220615389578196, 0.4582948993770945, 0.44973011467535196, 0.43987705725567705, 0.4399568917262399, 0.4171933356280934, 0.4194884966626594, 0.41210232227379784, 0.37525962464150553, 0.37843645416882576, 0.38196442676323716, 0.3850774992758444, 0.38883611609272867, 0.3919638399752734, 0.39550810705771355, 0.2163238899455721, 0.21850897974009298, 0.2207161411485468, 0.22294559711652884, 0.22519757283824163, 0.22747229579268205, 0.22976999574650916, 0.232090904786798, 0.23443525736040202, 0.2368032902547642, 0.23919524267300565, 0.24161135623091895, 0.24405187497613282, 0.24651704542569067, 0.24900711658180746, 0.25152233996646545, 0.25406296966309644, 0.25662926228059707, 0.25922147704559606, 0.2618398757979672, 0.26448472302243137, 0.09883736727398665, 0.09712129630944383, 0.08434718595480648, 0.05325531082750929, 0.028039326724711194, 0.024321286047340317, 0.021184687981139403, 0.019187445449696774, 0.014597660372931583, 0.010251448956457236, 0.0004416059432550889], [0.4024585231042652, 0.39063612771446354, 0.3918560739779303, 0.3859792194724285, 0.3479331156823176, 0.3413910847798329, 0.29682289799139144, 0.2938016095750306, 0.2957148782824523, 0.26686961167773415, 0.2694197223289858, 0.26560460188086915, 0.2490389730413119, 0.23847972669775253, 0.23412852666009393, 0.23080194941029883, 0.19624527192846677, 0.18594815636537973, 0.18718898451463614, 0.18744751751187927, 0.18930854929943614, 0.19003869164765766, 0.1669939863538262, 0.16286165650410053, 0.16248946158916092, 0.15927803945642635, 0.16029523090334144, 0.1603654224692256, 0.11790060634684398, 0.10738896543981905, 0.10796053705114081, 0.07164486474734746, 0.0697910195847483, 0.06771540989961115, 0.058048322144785676, 0.05553540503944973, 0.04872541317583893, 0.048479534580401246, 0.047292985439869775, 0.03658112364591786, 0.03529355448302497, 0.033703806971199325, 0.02819219160873849, 0.0283109457720867, 0.02701909588058177, 0.022312201741626566, 0.022222062761993365, 0.01790090758474997, 0.011072523683149548, 0.0003933959897404917], [0.16349118785472577, 0.16484047996047693, 0.16551687091322617, 0.16714284006849994, 0.16801594043495102, 0.16718554439659322, 0.16736903635277708, 0.16624052023263056, 0.16288064813814676, 0.16296219598877137, 0.14679201033148392, 0.14638827903944007, 0.14308681933704256, 0.13949774660321257, 0.1308751758722496, 0.13187822449678036, 0.12044701736944455, 0.12161756177361285, 0.12218021647682299, 0.12327126530070807, 0.12059923834383161, 0.12124335316923425, 0.10609886239066202, 0.09353038414107358, 0.08582009918542775, 0.06946701473101369, 0.05663872997246332, 0.04989610312582199, 0.05007972882744912, 0.036605287647980995, 0.03622322912746628, 0.031593502031618745, 0.031302179114432775, 0.025928288911054535, 0.026124264016493568, 0.026270622884364353, 0.02621281939126273, 0.021901635436654993, 0.018850263943704072, 0.009802390859100063, 0.009516935510124409, 0.009565607068314264, 0.009161368263214023, 0.007779868136994831, 0.007780928362115099, 0.00782307703320796, 0.0072174355598708245, 0.007279187570406577, 0.005090686076261673, 0.0037752620228385654]]\n",
            "DEBUGGING: traj_returns = [0.22638015900995379, 0.46281221289188584, 0.4024585231042652, 0.16349118785472577]\n",
            "DEBUGGING: actions = [[31], [37], [18], [29], [6], [45], [50], [28], [64], [54], [56], [34], [27], [8], [20], [64], [57], [46], [75], [11], [22], [53], [49], [80], [72], [41], [51], [63], [66], [38], [3], [62], [28], [13], [71], [38], [17], [55], [51], [46], [30], [72], [70], [46], [3], [32], [52], [55], [37], [49], [54], [1], [41], [51], [21], [38], [58], [1], [44], [67], [27], [8], [17], [25], [61], [13], [55], [0], [48], [73], [59], [56], [39], [22], [61], [62], [30], [53], [78], [51], [62], [49], [77], [41], [54], [8], [0], [22], [65], [82], [4], [55], [9], [96], [42], [1], [4], [35], [14], [42], [2], [49], [10], [48], [43], [21], [2], [22], [35], [52], [7], [41], [26], [40], [58], [10], [56], [38], [46], [2], [43], [15], [17], [65], [31], [65], [79], [63], [17], [75], [6], [90], [79], [81], [36], [9], [25], [47], [27], [53], [37], [23], [89], [39], [19], [85], [73], [69], [3], [49], [29], [44], [5], [6], [21], [52], [23], [21], [21], [41], [36], [12], [47], [53], [25], [66], [71], [24], [15], [47], [78], [12], [76], [49], [72], [81], [12], [53], [31], [81], [79], [44], [58], [14], [70], [43], [45], [77], [34], [26], [20], [14], [84], [65], [47], [21], [57], [54], [97], [63]]\n",
            "DEBUGGING: actions length = 200\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-1.3690, -0.7243,  0.4398,  ...,  0.3958,  0.6229, -0.8621],\n",
            "        [-1.3677, -0.7269,  0.4458,  ...,  0.4043,  0.6251, -0.8635],\n",
            "        [-1.3525, -0.7147,  0.4386,  ...,  0.4007,  0.6204, -0.8517],\n",
            "        ...,\n",
            "        [-1.3702, -0.7246,  0.4435,  ...,  0.4045,  0.6245, -0.8600],\n",
            "        [-1.3703, -0.7246,  0.4435,  ...,  0.4045,  0.6245, -0.8601],\n",
            "        [-1.3699, -0.7243,  0.4433,  ...,  0.4043,  0.6243, -0.8598]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-1.3737, -0.7277,  0.4454,  ...,  0.4052,  0.6257, -0.8631],\n",
            "        [-1.3703, -0.7246,  0.4435,  ...,  0.4044,  0.6245, -0.8601],\n",
            "        [-1.3704, -0.7247,  0.4435,  ...,  0.4045,  0.6245, -0.8601],\n",
            "        ...,\n",
            "        [-1.3702, -0.7245,  0.4434,  ...,  0.4044,  0.6244, -0.8600],\n",
            "        [-1.3703, -0.7247,  0.4435,  ...,  0.4044,  0.6245, -0.8601],\n",
            "        [-1.3702, -0.7246,  0.4434,  ...,  0.4044,  0.6245, -0.8600]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([11.1509, 11.1204, 11.1209, 11.1201, 11.1202, 11.1166, 11.1143, 11.1155,\n",
            "        11.1195, 11.1154, 11.1233, 11.1203, 11.1196, 11.1206, 11.1191, 11.1140,\n",
            "        11.1202, 11.1148, 11.1189, 11.1193, 11.1206, 11.1208, 11.1199, 11.1193,\n",
            "        11.1203, 11.1184, 11.1202, 11.1215, 11.1179, 11.1168, 11.1195, 11.1218,\n",
            "        11.1196, 11.1179, 11.1209, 11.1211, 11.1189, 11.1207, 11.1191, 11.1340,\n",
            "        11.1202, 11.1171, 11.1165, 11.1206, 11.1187, 11.1206, 11.1194, 11.1196,\n",
            "        11.1210, 11.1190, 11.1209, 11.1165, 11.1198, 11.1183, 11.1197, 11.1164,\n",
            "        11.1202, 11.1194, 11.1192, 11.1187, 11.1204, 11.1159, 11.1189, 11.1217,\n",
            "        11.1184, 11.1188, 11.1203, 11.1203, 11.1196, 11.1208, 11.1196, 11.1204,\n",
            "        11.1209, 11.1190, 11.1195, 11.1193, 11.1188, 11.1196, 11.1189, 11.1194,\n",
            "        11.1199, 11.1193, 11.1204, 11.1192, 11.1206, 11.1206, 11.1192, 11.1207,\n",
            "        11.1184, 11.1199, 11.1183, 11.1209, 11.1199, 11.1201, 11.1190, 11.1212,\n",
            "        11.1203, 11.1179, 11.1134, 11.1219, 11.1191, 11.1209, 11.1187, 11.1189,\n",
            "        11.1201, 11.1246, 11.1196, 11.1208, 11.1199], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[0.31378552 0.31181477 0.30875561 0.30654343 0.29450322 0.28845668\n",
            "  0.26897853 0.26702144 0.26032596 0.24830459 0.23880953 0.22859797\n",
            "  0.22452096 0.21734822 0.209618   0.21017535 0.19964492 0.19846478\n",
            "  0.15322205 0.15384227 0.15254963 0.15336124 0.1445336  0.14058751\n",
            "  0.13931289 0.13514781 0.13287481 0.13116759 0.12137521 0.11616581\n",
            "  0.11701235 0.10743376 0.10731402 0.10627839 0.0986707  0.09796288\n",
            "  0.09680145 0.09520911 0.09434642 0.04755764 0.04486707 0.04116617\n",
            "  0.02950272 0.02108538 0.01695283 0.01466745 0.01390104 0.01170201\n",
            "  0.0081165  0.00192426]]\n",
            "DEBUGGING: baseline2 looks like: 0.31378552071520766\n",
            "DEBUGGING: ADS looks like: [-1.39751968 -1.4017017  -1.42129472 -1.4123796  -1.38780377 -1.37676068\n",
            " -1.40258123 -1.41184988 -1.3975179  -1.40338286 -1.41273722 -1.39528108\n",
            " -1.35001776 -1.3496848  -1.37756488 -1.37019404 -1.35388007 -1.30309051\n",
            " -1.2689938  -1.19797401 -1.14835815 -1.10687371 -1.08605739 -1.04268852\n",
            " -1.01158666 -0.94787621 -0.9528468  -0.96176116 -0.90574216 -0.87077026\n",
            " -0.81047536 -0.70110563 -0.61614958 -0.53190025 -0.52628656 -0.47982554\n",
            " -0.48369885 -0.34980047 -0.24353091 -0.21782063 -0.22604463 -0.17134175\n",
            " -0.17979909 -0.18764    -0.1629433  -0.16360082 -0.135629   -0.13533632\n",
            " -0.12881327 -0.02980157 -1.16108762 -1.16236162 -1.1654431  -1.16101895\n",
            " -1.13327778 -1.12255052 -1.1345493  -1.13997972 -1.14583955 -1.12779243\n",
            " -1.12755897 -1.14716085 -1.09910291 -1.07717134 -1.08087819 -1.07054305\n",
            " -1.05183978 -0.99836771 -1.13986502 -1.06560636 -1.00721659 -0.96314542\n",
            " -0.9407038  -0.89370192 -0.86098867 -0.79554058 -0.79854156 -0.80256341\n",
            " -0.74487219 -0.70821653 -0.64623725 -0.53456822 -0.44629822 -0.36032542\n",
            " -0.32867085 -0.2766125  -0.27752347 -0.13657599 -0.02580391 -0.16399296\n",
            " -0.16645981 -0.12404264 -0.15394577 -0.17981207 -0.14731201 -0.14976598\n",
            " -0.12341876 -0.12776895 -0.12461316 -0.03244673 -1.22144131 -1.23728676\n",
            " -1.24033759 -1.23724589 -1.24363957 -1.23088955 -1.27760346 -1.286135\n",
            " -1.26731801 -1.28041132 -1.27024157 -1.25681587 -1.22850039 -1.22065604\n",
            " -1.23182717 -1.22857722 -1.24755835 -1.20792766 -1.16899993 -1.09666782\n",
            " -1.03862418 -0.99605233 -0.99890738 -0.95831256 -0.92826921 -0.86835344\n",
            " -0.87268159 -0.87900127 -0.86616683 -0.84243892 -0.78232859 -0.7094404\n",
            " -0.62551432 -0.54413235 -0.5246855  -0.47770636 -0.48801954 -0.34993633\n",
            " -0.24299564 -0.22624921 -0.22828756 -0.17468602 -0.17900889 -0.17954045\n",
            " -0.1446142  -0.14863846 -0.12038414 -0.1244657  -0.12379209 -0.03249494\n",
            " -1.46040865 -1.46308241 -1.4666768  -1.45608227 -1.42355674 -1.40509509\n",
            " -1.40705732 -1.41369609 -1.40015224 -1.38431873 -1.39286928 -1.3760322\n",
            " -1.33445255 -1.31963802 -1.33508052 -1.32750094 -1.3233566  -1.27225826\n",
            " -1.23400869 -1.16084407 -1.1073335  -1.06484767 -1.05980251 -1.02764383\n",
            " -1.00493857 -0.95816447 -0.97633809 -0.98947059 -0.9339877  -0.9132226\n",
            " -0.8540659  -0.74949177 -0.66400316 -0.58591947 -0.55660956 -0.50697114\n",
            " -0.51053213 -0.37651423 -0.27143837 -0.25302794 -0.25406417 -0.19882422\n",
            " -0.19803971 -0.20007153 -0.16385237 -0.16312759 -0.13538877 -0.13508742\n",
            " -0.12977393 -0.02911308]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(-3.5209, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0010,  0.0009,  0.0012,  ..., -0.0006,  0.0014,  0.0022],\n",
            "        [-0.0006,  0.0005,  0.0006,  ..., -0.0004,  0.0007,  0.0013]])\n",
            "   Last layer:\n",
            "tensor([[-2.2531e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -3.1373e-04, -2.6886e-03, -7.3901e-04, -4.4788e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.3690e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-6.8251e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -4.6795e-05, -1.0596e-03, -2.3687e-04, -6.7064e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.9644e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-6.2073e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -2.1781e-04, -7.2076e-05, -1.6703e-04, -3.0900e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.5389e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.0305e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  7.6048e-05,  1.5728e-03,  3.5681e-04,  1.0955e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  7.4065e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-3.1965e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -4.4522e-04, -3.8135e-03, -1.0483e-03, -6.3513e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.9421e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-3.1247e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -4.9898e-04, -3.4034e-03, -1.0068e-03, -7.1083e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.7899e-03,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.9260e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  3.8770e-04,  1.6900e-03,  5.9817e-04,  5.5134e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  9.6709e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 6.4998e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -4.8802e-05,  3.7201e-04,  3.7549e-05, -6.8664e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.3787e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 4.7902e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  9.1899e-06,  8.6428e-04,  1.7333e-04,  1.4136e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.8892e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.8075e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  8.2855e-05, -9.5551e-04, -1.2594e-04,  1.1665e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.7785e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-9.8828e-04, -1.7814e-04,  1.2161e-03,  ..., -3.2415e-04,\n",
            "          4.4158e-04, -2.8324e-04],\n",
            "        [-5.4395e-04, -9.7899e-05,  6.6971e-04,  ..., -1.7830e-04,\n",
            "          2.4324e-04, -1.1474e-04]])\n",
            "   Last layer:\n",
            "tensor([[ 7.6700e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  1.9672e-05, -2.4354e-04,  7.1654e-05,  4.5359e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.4293e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 3.1317e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  7.8361e-05,  3.5373e-05,  1.1873e-04,  3.3107e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.2929e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.8719e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -7.1749e-05, -8.0346e-05, -1.0045e-04, -2.3272e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1799e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-3.7640e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -9.4294e-05, -1.8951e-05, -1.4633e-04, -4.3210e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.5568e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 6.1947e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  1.6107e-05, -3.7508e-04,  8.8233e-05,  6.2984e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.9767e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 5.0766e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  1.3197e-05, -2.8757e-04,  6.8963e-05,  4.8639e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.3950e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 7.7324e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  1.9088e-05,  1.4177e-04,  6.5778e-06, -1.1449e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.0468e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.2358e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  3.0813e-05,  1.5979e-04,  2.2139e-05, -8.4875e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.9414e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.0654e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -2.6758e-05,  6.7608e-05, -5.3667e-05, -2.3002e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.4827e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 4.8694e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  1.2179e-04,  1.1230e-04,  1.7457e-04,  4.2979e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0033e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "DEBUGGING: training for one iteration takes 0.005665 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 47\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0266, 0.0148, 0.0156, 0.0153, 0.0139, 0.0142, 0.0133, 0.0133, 0.0148,\n",
            "        0.0145, 0.0146, 0.0130, 0.0145, 0.0132, 0.0142, 0.0140, 0.0146, 0.0140,\n",
            "        0.0115, 0.0144, 0.0140, 0.0151, 0.0140, 0.0134, 0.0133, 0.0141, 0.0144,\n",
            "        0.0138, 0.0141, 0.0145, 0.0129, 0.0132, 0.0150, 0.0138, 0.0140, 0.0137,\n",
            "        0.0139, 0.0147, 0.0133, 0.0128, 0.0152, 0.0140, 0.0134, 0.0150, 0.0132,\n",
            "        0.0135, 0.0129, 0.0135, 0.0147, 0.0144, 0.0134, 0.0151, 0.0143, 0.0144,\n",
            "        0.0144, 0.0143, 0.0152, 0.0139, 0.0178, 0.0143, 0.0135, 0.0154, 0.0134,\n",
            "        0.0146, 0.0148, 0.0140, 0.0138, 0.0141, 0.0140, 0.0134],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [11]\n",
            "DEBUGGING: logits looks like: tensor([12.8547, 12.7379, 12.7481, 12.7446, 12.7248, 12.7302, 12.7165, 12.7171,\n",
            "        12.7376, 12.7342, 12.7345, 12.7117, 12.7337, 12.7152, 12.7299, 12.7271,\n",
            "        12.7355, 12.7270, 12.6874, 12.7326, 12.7261, 12.7413, 12.7260, 12.7177,\n",
            "        12.7157, 12.7280, 12.7324, 12.7242, 12.7284, 12.7335, 12.7101, 12.7147,\n",
            "        12.7411, 12.7243, 12.7265, 12.7224, 12.7248, 12.7362, 12.7159, 12.7089,\n",
            "        12.7432, 12.7267, 12.7176, 12.7404, 12.7143, 12.7189, 12.7100, 12.7191,\n",
            "        12.7368, 12.7321, 12.7183, 12.7418, 12.7304, 12.7326, 12.7324, 12.7306,\n",
            "        12.7431, 12.7245, 12.7745, 12.7303, 12.7192, 12.7453, 12.7181, 12.7345,\n",
            "        12.7381, 12.7264, 12.7237, 12.7279, 12.7260, 12.7181],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0246, 0.0124, 0.0124, 0.0124, 0.0124, 0.0125, 0.0124, 0.0124, 0.0123,\n",
            "        0.0127, 0.0126, 0.0124, 0.0127, 0.0126, 0.0126, 0.0124, 0.0126, 0.0125,\n",
            "        0.0124, 0.0125, 0.0126, 0.0132, 0.0124, 0.0124, 0.0123, 0.0124, 0.0124,\n",
            "        0.0126, 0.0126, 0.0122, 0.0124, 0.0125, 0.0126, 0.0125, 0.0124, 0.0125,\n",
            "        0.0123, 0.0125, 0.0124, 0.0125, 0.0126, 0.0125, 0.0123, 0.0125, 0.0126,\n",
            "        0.0125, 0.0126, 0.0126, 0.0125, 0.0124, 0.0124, 0.0125, 0.0128, 0.0124,\n",
            "        0.0125, 0.0124, 0.0124, 0.0126, 0.0127, 0.0125, 0.0124, 0.0124, 0.0124,\n",
            "        0.0124, 0.0127, 0.0125, 0.0125, 0.0125, 0.0124, 0.0126, 0.0124, 0.0126,\n",
            "        0.0125, 0.0124, 0.0124, 0.0125, 0.0129, 0.0131, 0.0125],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [40]\n",
            "DEBUGGING: logits looks like: tensor([12.8688, 12.7319, 12.7322, 12.7317, 12.7309, 12.7329, 12.7321, 12.7310,\n",
            "        12.7305, 12.7362, 12.7349, 12.7318, 12.7369, 12.7343, 12.7340, 12.7316,\n",
            "        12.7343, 12.7328, 12.7318, 12.7324, 12.7346, 12.7441, 12.7311, 12.7314,\n",
            "        12.7294, 12.7317, 12.7308, 12.7345, 12.7345, 12.7277, 12.7319, 12.7332,\n",
            "        12.7342, 12.7326, 12.7313, 12.7336, 12.7305, 12.7327, 12.7317, 12.7331,\n",
            "        12.7339, 12.7327, 12.7301, 12.7337, 12.7340, 12.7332, 12.7344, 12.7350,\n",
            "        12.7332, 12.7321, 12.7306, 12.7329, 12.7374, 12.7321, 12.7323, 12.7311,\n",
            "        12.7318, 12.7350, 12.7355, 12.7338, 12.7316, 12.7319, 12.7315, 12.7314,\n",
            "        12.7358, 12.7336, 12.7328, 12.7328, 12.7314, 12.7341, 12.7314, 12.7339,\n",
            "        12.7322, 12.7314, 12.7321, 12.7330, 12.7386, 12.7417, 12.7334],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0225, 0.0118, 0.0117, 0.0116, 0.0118, 0.0116, 0.0116, 0.0116, 0.0116,\n",
            "        0.0116, 0.0117, 0.0117, 0.0116, 0.0116, 0.0117, 0.0117, 0.0116, 0.0116,\n",
            "        0.0116, 0.0116, 0.0117, 0.0117, 0.0116, 0.0117, 0.0115, 0.0116, 0.0117,\n",
            "        0.0117, 0.0116, 0.0117, 0.0117, 0.0116, 0.0116, 0.0116, 0.0117, 0.0116,\n",
            "        0.0116, 0.0116, 0.0117, 0.0117, 0.0116, 0.0118, 0.0116, 0.0116, 0.0116,\n",
            "        0.0116, 0.0116, 0.0117, 0.0116, 0.0117, 0.0116, 0.0116, 0.0117, 0.0116,\n",
            "        0.0117, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0117, 0.0116, 0.0116,\n",
            "        0.0116, 0.0116, 0.0116, 0.0116, 0.0117, 0.0116, 0.0116, 0.0116, 0.0115,\n",
            "        0.0116, 0.0117, 0.0115, 0.0116, 0.0116, 0.0116, 0.0116, 0.0117, 0.0116,\n",
            "        0.0117, 0.0117, 0.0116, 0.0116], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [77]\n",
            "DEBUGGING: logits looks like: tensor([12.8686, 12.7393, 12.7376, 12.7361, 12.7387, 12.7367, 12.7368, 12.7363,\n",
            "        12.7360, 12.7353, 12.7369, 12.7372, 12.7362, 12.7362, 12.7371, 12.7369,\n",
            "        12.7365, 12.7359, 12.7361, 12.7359, 12.7370, 12.7376, 12.7363, 12.7371,\n",
            "        12.7347, 12.7362, 12.7372, 12.7372, 12.7362, 12.7374, 12.7378, 12.7364,\n",
            "        12.7364, 12.7367, 12.7370, 12.7365, 12.7367, 12.7367, 12.7376, 12.7374,\n",
            "        12.7367, 12.7388, 12.7363, 12.7364, 12.7368, 12.7358, 12.7364, 12.7376,\n",
            "        12.7357, 12.7372, 12.7368, 12.7360, 12.7383, 12.7365, 12.7379, 12.7368,\n",
            "        12.7364, 12.7360, 12.7367, 12.7367, 12.7382, 12.7365, 12.7352, 12.7362,\n",
            "        12.7359, 12.7365, 12.7365, 12.7380, 12.7365, 12.7359, 12.7362, 12.7348,\n",
            "        12.7368, 12.7374, 12.7345, 12.7352, 12.7368, 12.7353, 12.7368, 12.7371,\n",
            "        12.7368, 12.7381, 12.7370, 12.7359, 12.7368], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0197, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0099, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0100, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [89]\n",
            "DEBUGGING: logits looks like: tensor([12.8723, 12.7384, 12.7389, 12.7392, 12.7393, 12.7387, 12.7391, 12.7394,\n",
            "        12.7387, 12.7392, 12.7390, 12.7394, 12.7390, 12.7390, 12.7391, 12.7392,\n",
            "        12.7395, 12.7395, 12.7380, 12.7390, 12.7390, 12.7392, 12.7391, 12.7391,\n",
            "        12.7390, 12.7392, 12.7394, 12.7392, 12.7391, 12.7393, 12.7389, 12.7390,\n",
            "        12.7389, 12.7391, 12.7389, 12.7388, 12.7388, 12.7395, 12.7393, 12.7352,\n",
            "        12.7388, 12.7390, 12.7392, 12.7387, 12.7390, 12.7391, 12.7392, 12.7376,\n",
            "        12.7392, 12.7392, 12.7394, 12.7391, 12.7391, 12.7390, 12.7393, 12.7390,\n",
            "        12.7394, 12.7390, 12.7391, 12.7384, 12.7390, 12.7394, 12.7390, 12.7385,\n",
            "        12.7391, 12.7389, 12.7390, 12.7392, 12.7391, 12.7390, 12.7393, 12.7387,\n",
            "        12.7388, 12.7386, 12.7386, 12.7388, 12.7385, 12.7389, 12.7391, 12.7391,\n",
            "        12.7390, 12.7390, 12.7388, 12.7393, 12.7391, 12.7391, 12.7388, 12.7390,\n",
            "        12.7387, 12.7386, 12.7394, 12.7391, 12.7391, 12.7392, 12.7392, 12.7388,\n",
            "        12.7393, 12.7394], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0180, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [19]\n",
            "DEBUGGING: logits looks like: tensor([12.8752, 12.7423, 12.7424, 12.7423, 12.7422, 12.7421, 12.7424, 12.7422,\n",
            "        12.7423, 12.7421, 12.7424, 12.7423, 12.7424, 12.7422, 12.7423, 12.7425,\n",
            "        12.7422, 12.7424, 12.7422, 12.7423, 12.7425, 12.7422, 12.7422, 12.7423,\n",
            "        12.7423, 12.7420, 12.7422, 12.7424, 12.7421, 12.7420, 12.7420, 12.7421,\n",
            "        12.7424, 12.7427, 12.7421, 12.7422, 12.7420, 12.7421, 12.7424, 12.7422,\n",
            "        12.7423, 12.7422, 12.7423, 12.7421, 12.7423, 12.7423, 12.7426, 12.7423,\n",
            "        12.7422, 12.7424, 12.7423, 12.7422, 12.7427, 12.7425, 12.7422, 12.7424,\n",
            "        12.7422, 12.7423, 12.7421, 12.7422, 12.7421, 12.7422, 12.7421, 12.7421,\n",
            "        12.7423, 12.7423, 12.7422, 12.7422, 12.7423, 12.7421, 12.7423, 12.7425,\n",
            "        12.7424, 12.7425, 12.7424, 12.7422, 12.7424, 12.7420, 12.7423, 12.7423,\n",
            "        12.7423, 12.7425, 12.7423, 12.7424, 12.7422, 12.7423, 12.7421, 12.7422,\n",
            "        12.7420, 12.7421, 12.7422, 12.7423, 12.7423, 12.7423, 12.7423, 12.7422,\n",
            "        12.7423, 12.7421, 12.7420, 12.7423, 12.7424, 12.7424, 12.7424, 12.7423,\n",
            "        12.7426, 12.7421, 12.7424], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.10455139983287154 and immediate abs rewards look like: [0.015931637564335688, 0.004112368594178406, 0.018693608818921348, 0.004998007009817229, 0.008015284921839338, 0.0010515034073250717, 0.0004225748348289926, 0.004428009107414255, 0.01702832709906943, 0.002765500840268942, 0.00295909037004094, 0.0004263711102794332, 0.0008670360075484496, 0.004496473759445507, 0.00016996340764308115, 0.00027579798006627243, 0.007099574285803101, 0.0016047507183429843, 0.0020749419836647576, 0.0002917747628998768, 0.0006103151795286976, 3.949470965380897e-05, 0.0003329290252622741, 1.3959561329102144e-07, 2.0160000531177502e-06, 0.00046402717498494894, 3.718757261594874e-05, 0.002260489413401956, 5.384697442423203e-05, 0.0003419035788283509, 8.327110481332056e-05, 7.012238620518474e-05, 9.530186844131094e-05, 3.148668247376918e-05, 3.068242858716985e-05, 2.2184830868354766e-05, 4.364952019386692e-06, 0.00021004575182814733, 4.441913461050717e-06, 1.807104217732558e-05, 0.00015670871471229475, 0.00016474644962727325, 1.2006490123894764e-05, 0.0013224737663222186, 0.0001801100393095112, 0.00021953679924990865, 1.3633052731165662e-07, 1.960936651812517e-05, 2.3010894892649958e-05, 2.814221261360217e-05]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.32100253556508657 and immediate relative rewards look like: [0.005518900292325599, 0.002864953818768106, 0.01956286979111941, 0.007019663612454942, 0.014096498121944214, 0.0022254115551325598, 0.001043785320964337, 0.012501806434452842, 0.054171141286444485, 0.009834430391883803, 0.011586536759302128, 0.0018231795434874255, 0.0040170468522608156, 0.02244195716991865, 0.0009103408897674423, 0.001575777093970347, 0.043103022247032705, 0.01034210915261847, 0.014123365323189856, 0.0020920848798247726, 0.004595369929776441, 0.00031160445900847136, 0.002746171805863291, 1.2016640782921e-06, 1.8077178055670953e-05, 0.004327301596085236, 0.0003601921841933186, 0.02270591418048249, 0.0005606477847967009, 0.003682681026065382, 0.0009269334163211306, 0.0008057719947412055, 0.001129358423312223, 0.0003844475742020032, 0.00038565058144235875, 0.00028681352180936153, 5.799968804808058e-05, 0.0028664385717700885, 6.221750647525761e-05, 0.00025961021995086613, 0.0023075887782215195, 0.0024852564601969343, 0.00018544540621129665, 0.020901293863347988, 0.0029126628427639193, 0.0036293850959219086, 2.3029957217857254e-06, 0.0003383038820578829, 0.0004052610167157408, 0.0005057513805868285]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0139, 0.0164, 0.0182, 0.0149, 0.0146, 0.0122, 0.0109, 0.0160, 0.0181,\n",
            "        0.0127, 0.0166, 0.0132, 0.0141, 0.0138, 0.0146, 0.0162, 0.0197, 0.0134,\n",
            "        0.0137, 0.0172, 0.0123, 0.0130, 0.0139, 0.0121, 0.0152, 0.0124, 0.0167,\n",
            "        0.0152, 0.0162, 0.0119, 0.0141, 0.0131, 0.0146, 0.0148, 0.0125, 0.0138,\n",
            "        0.0147, 0.0156, 0.0159, 0.0170, 0.0132, 0.0143, 0.0150, 0.0146, 0.0142,\n",
            "        0.0153, 0.0118, 0.0169, 0.0139, 0.0150, 0.0135, 0.0142, 0.0135, 0.0163,\n",
            "        0.0173, 0.0146, 0.0152, 0.0118, 0.0147, 0.0134, 0.0146, 0.0148, 0.0134,\n",
            "        0.0136, 0.0140, 0.0139, 0.0143, 0.0152, 0.0119],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [37]\n",
            "DEBUGGING: logits looks like: tensor([12.7962, 12.8295, 12.8510, 12.8107, 12.8071, 12.7710, 12.7479, 12.8249,\n",
            "        12.8494, 12.7793, 12.8323, 12.7869, 12.7997, 12.7955, 12.8061, 12.8276,\n",
            "        12.8668, 12.7893, 12.7938, 12.8398, 12.7718, 12.7841, 12.7966, 12.7681,\n",
            "        12.8144, 12.7743, 12.8330, 12.8140, 12.8270, 12.7650, 12.7997, 12.7852,\n",
            "        12.8063, 12.8086, 12.7759, 12.7954, 12.8084, 12.8197, 12.8239, 12.8366,\n",
            "        12.7865, 12.8022, 12.8120, 12.8063, 12.8007, 12.8155, 12.7636, 12.8363,\n",
            "        12.7970, 12.8116, 12.7915, 12.8008, 12.7910, 12.8282, 12.8401, 12.8071,\n",
            "        12.8146, 12.7640, 12.8074, 12.7894, 12.8065, 12.8095, 12.7900, 12.7929,\n",
            "        12.7986, 12.7966, 12.8025, 12.8148, 12.7654], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0122, 0.0129, 0.0124, 0.0131, 0.0123, 0.0139, 0.0113, 0.0133, 0.0127,\n",
            "        0.0124, 0.0134, 0.0133, 0.0128, 0.0129, 0.0131, 0.0132, 0.0132, 0.0123,\n",
            "        0.0123, 0.0132, 0.0110, 0.0121, 0.0123, 0.0126, 0.0120, 0.0128, 0.0129,\n",
            "        0.0130, 0.0133, 0.0126, 0.0129, 0.0133, 0.0133, 0.0122, 0.0122, 0.0136,\n",
            "        0.0122, 0.0120, 0.0119, 0.0130, 0.0124, 0.0136, 0.0126, 0.0134, 0.0125,\n",
            "        0.0122, 0.0124, 0.0125, 0.0120, 0.0133, 0.0139, 0.0126, 0.0129, 0.0122,\n",
            "        0.0132, 0.0127, 0.0120, 0.0119, 0.0130, 0.0129, 0.0120, 0.0129, 0.0130,\n",
            "        0.0115, 0.0126, 0.0124, 0.0128, 0.0123, 0.0131, 0.0130, 0.0132, 0.0122,\n",
            "        0.0120, 0.0130, 0.0124, 0.0131, 0.0125, 0.0122, 0.0120],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [42]\n",
            "DEBUGGING: logits looks like: tensor([12.7887, 12.8004, 12.7915, 12.8035, 12.7899, 12.8153, 12.7743, 12.8069,\n",
            "        12.7976, 12.7922, 12.8078, 12.8066, 12.7993, 12.8000, 12.8028, 12.8053,\n",
            "        12.8051, 12.7913, 12.7910, 12.8050, 12.7682, 12.7881, 12.7903, 12.7956,\n",
            "        12.7854, 12.7994, 12.8010, 12.8011, 12.8063, 12.7956, 12.8002, 12.8059,\n",
            "        12.8062, 12.7891, 12.7894, 12.8104, 12.7890, 12.7858, 12.7845, 12.8023,\n",
            "        12.7923, 12.8101, 12.7950, 12.8073, 12.7942, 12.7889, 12.7925, 12.7934,\n",
            "        12.7859, 12.8058, 12.8148, 12.7956, 12.8007, 12.7883, 12.8049, 12.7964,\n",
            "        12.7859, 12.7841, 12.8018, 12.7998, 12.7865, 12.8008, 12.8024, 12.7775,\n",
            "        12.7960, 12.7929, 12.7994, 12.7907, 12.8029, 12.8010, 12.8047, 12.7894,\n",
            "        12.7864, 12.8022, 12.7916, 12.8035, 12.7933, 12.7889, 12.7864],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0115, 0.0120, 0.0113, 0.0118, 0.0118, 0.0115, 0.0116, 0.0119, 0.0114,\n",
            "        0.0120, 0.0119, 0.0118, 0.0113, 0.0130, 0.0119, 0.0115, 0.0109, 0.0116,\n",
            "        0.0115, 0.0120, 0.0117, 0.0117, 0.0113, 0.0117, 0.0116, 0.0122, 0.0119,\n",
            "        0.0113, 0.0118, 0.0115, 0.0116, 0.0110, 0.0115, 0.0116, 0.0109, 0.0116,\n",
            "        0.0116, 0.0117, 0.0116, 0.0119, 0.0120, 0.0117, 0.0113, 0.0112, 0.0114,\n",
            "        0.0118, 0.0114, 0.0113, 0.0117, 0.0115, 0.0118, 0.0113, 0.0112, 0.0120,\n",
            "        0.0113, 0.0120, 0.0114, 0.0119, 0.0115, 0.0121, 0.0116, 0.0115, 0.0117,\n",
            "        0.0122, 0.0122, 0.0113, 0.0116, 0.0117, 0.0114, 0.0110, 0.0115, 0.0120,\n",
            "        0.0118, 0.0120, 0.0118, 0.0116, 0.0114, 0.0116, 0.0113, 0.0118, 0.0115,\n",
            "        0.0109, 0.0118, 0.0117, 0.0116, 0.0117], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [37]\n",
            "DEBUGGING: logits looks like: tensor([12.7690, 12.7760, 12.7650, 12.7743, 12.7731, 12.7678, 12.7701, 12.7744,\n",
            "        12.7663, 12.7776, 12.7746, 12.7731, 12.7656, 12.7927, 12.7758, 12.7690,\n",
            "        12.7570, 12.7705, 12.7687, 12.7773, 12.7719, 12.7725, 12.7655, 12.7716,\n",
            "        12.7694, 12.7798, 12.7756, 12.7653, 12.7731, 12.7683, 12.7707, 12.7599,\n",
            "        12.7680, 12.7708, 12.7577, 12.7707, 12.7704, 12.7723, 12.7699, 12.7750,\n",
            "        12.7773, 12.7719, 12.7651, 12.7626, 12.7662, 12.7738, 12.7660, 12.7649,\n",
            "        12.7721, 12.7684, 12.7736, 12.7657, 12.7630, 12.7762, 12.7649, 12.7765,\n",
            "        12.7670, 12.7749, 12.7690, 12.7778, 12.7707, 12.7686, 12.7726, 12.7797,\n",
            "        12.7795, 12.7643, 12.7698, 12.7721, 12.7659, 12.7589, 12.7677, 12.7771,\n",
            "        12.7730, 12.7775, 12.7735, 12.7693, 12.7663, 12.7699, 12.7646, 12.7738,\n",
            "        12.7680, 12.7568, 12.7731, 12.7714, 12.7702, 12.7723],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0109, 0.0105, 0.0106, 0.0104, 0.0106, 0.0107, 0.0102, 0.0105, 0.0105,\n",
            "        0.0106, 0.0110, 0.0104, 0.0106, 0.0102, 0.0112, 0.0103, 0.0107, 0.0107,\n",
            "        0.0107, 0.0104, 0.0107, 0.0106, 0.0104, 0.0102, 0.0105, 0.0106, 0.0106,\n",
            "        0.0103, 0.0098, 0.0105, 0.0103, 0.0105, 0.0107, 0.0102, 0.0108, 0.0106,\n",
            "        0.0103, 0.0106, 0.0103, 0.0104, 0.0104, 0.0103, 0.0106, 0.0108, 0.0105,\n",
            "        0.0108, 0.0103, 0.0105, 0.0105, 0.0103, 0.0104, 0.0109, 0.0108, 0.0106,\n",
            "        0.0103, 0.0104, 0.0104, 0.0104, 0.0106, 0.0106, 0.0105, 0.0104, 0.0111,\n",
            "        0.0104, 0.0107, 0.0107, 0.0106, 0.0101, 0.0107, 0.0107, 0.0105, 0.0108,\n",
            "        0.0102, 0.0106, 0.0103, 0.0106, 0.0100, 0.0108, 0.0101, 0.0106, 0.0106,\n",
            "        0.0109, 0.0105, 0.0105, 0.0104, 0.0106, 0.0107, 0.0106, 0.0106, 0.0108,\n",
            "        0.0103, 0.0105, 0.0107, 0.0105, 0.0105], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [10]\n",
            "DEBUGGING: logits looks like: tensor([12.8011, 12.7934, 12.7949, 12.7908, 12.7953, 12.7964, 12.7885, 12.7928,\n",
            "        12.7936, 12.7954, 12.8032, 12.7907, 12.7959, 12.7871, 12.8062, 12.7901,\n",
            "        12.7972, 12.7971, 12.7966, 12.7917, 12.7981, 12.7961, 12.7914, 12.7871,\n",
            "        12.7940, 12.7962, 12.7960, 12.7900, 12.7806, 12.7943, 12.7899, 12.7927,\n",
            "        12.7965, 12.7875, 12.7993, 12.7961, 12.7889, 12.7953, 12.7899, 12.7926,\n",
            "        12.7914, 12.7893, 12.7948, 12.7984, 12.7941, 12.7983, 12.7894, 12.7935,\n",
            "        12.7939, 12.7896, 12.7925, 12.8004, 12.7983, 12.7956, 12.7894, 12.7919,\n",
            "        12.7924, 12.7925, 12.7954, 12.7956, 12.7945, 12.7920, 12.8048, 12.7909,\n",
            "        12.7967, 12.7980, 12.7953, 12.7854, 12.7965, 12.7971, 12.7928, 12.7997,\n",
            "        12.7869, 12.7946, 12.7899, 12.7952, 12.7844, 12.7997, 12.7850, 12.7951,\n",
            "        12.7953, 12.8004, 12.7933, 12.7936, 12.7917, 12.7958, 12.7982, 12.7955,\n",
            "        12.7963, 12.7984, 12.7894, 12.7941, 12.7980, 12.7929, 12.7938],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0095, 0.0095, 0.0094, 0.0093, 0.0096, 0.0093, 0.0096, 0.0093, 0.0097,\n",
            "        0.0094, 0.0095, 0.0096, 0.0095, 0.0093, 0.0095, 0.0097, 0.0094, 0.0094,\n",
            "        0.0095, 0.0091, 0.0096, 0.0096, 0.0094, 0.0095, 0.0095, 0.0097, 0.0097,\n",
            "        0.0096, 0.0093, 0.0085, 0.0093, 0.0097, 0.0094, 0.0093, 0.0094, 0.0094,\n",
            "        0.0094, 0.0097, 0.0090, 0.0092, 0.0095, 0.0096, 0.0094, 0.0097, 0.0092,\n",
            "        0.0094, 0.0094, 0.0095, 0.0095, 0.0091, 0.0097, 0.0097, 0.0094, 0.0094,\n",
            "        0.0098, 0.0095, 0.0097, 0.0093, 0.0094, 0.0096, 0.0094, 0.0093, 0.0094,\n",
            "        0.0093, 0.0093, 0.0092, 0.0094, 0.0094, 0.0096, 0.0095, 0.0092, 0.0093,\n",
            "        0.0094, 0.0093, 0.0094, 0.0097, 0.0093, 0.0096, 0.0097, 0.0098, 0.0094,\n",
            "        0.0094, 0.0094, 0.0091, 0.0094, 0.0096, 0.0093, 0.0091, 0.0096, 0.0093,\n",
            "        0.0091, 0.0093, 0.0092, 0.0095, 0.0093, 0.0093, 0.0097, 0.0092, 0.0097,\n",
            "        0.0095, 0.0096, 0.0095, 0.0094, 0.0093, 0.0096, 0.0094],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [61]\n",
            "DEBUGGING: logits looks like: tensor([12.7988, 12.7992, 12.7973, 12.7953, 12.8010, 12.7961, 12.8028, 12.7964,\n",
            "        12.8043, 12.7980, 12.8001, 12.8028, 12.7989, 12.7949, 12.7989, 12.8044,\n",
            "        12.7986, 12.7982, 12.7992, 12.7915, 12.8019, 12.8012, 12.7979, 12.7994,\n",
            "        12.7988, 12.8033, 12.8036, 12.8012, 12.7959, 12.7781, 12.7959, 12.8045,\n",
            "        12.7970, 12.7947, 12.7980, 12.7968, 12.7982, 12.8039, 12.7894, 12.7927,\n",
            "        12.7995, 12.8028, 12.7976, 12.8044, 12.7935, 12.7972, 12.7983, 12.8007,\n",
            "        12.8007, 12.7920, 12.8049, 12.8042, 12.7983, 12.7985, 12.8055, 12.7994,\n",
            "        12.8037, 12.7965, 12.7978, 12.8027, 12.7985, 12.7955, 12.7969, 12.7949,\n",
            "        12.7953, 12.7935, 12.7979, 12.7975, 12.8011, 12.7992, 12.7925, 12.7950,\n",
            "        12.7968, 12.7964, 12.7985, 12.8035, 12.7956, 12.8021, 12.8037, 12.8061,\n",
            "        12.7973, 12.7979, 12.7986, 12.7918, 12.7983, 12.8019, 12.7955, 12.7903,\n",
            "        12.8011, 12.7963, 12.7906, 12.7952, 12.7942, 12.7991, 12.7963, 12.7960,\n",
            "        12.8037, 12.7943, 12.8043, 12.7988, 12.8026, 12.8005, 12.7980, 12.7945,\n",
            "        12.8028, 12.7979], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.7082653196380306 and immediate abs rewards look like: [0.03063765095475901, 0.017358033789150795, 0.02651350491805715, 0.005729889028316393, 0.6280262409286479, 9.094947017729282e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 0.0, 1.3642420526593924e-12, 4.547473508864641e-13, 0.0, 1.8189894035458565e-12, 2.2737367544323206e-12, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 9.094947017729282e-13, 1.8189894035458565e-12, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.9112605031958904 and immediate relative rewards look like: [0.008261989991023589, 0.00943979943214397, 0.021730780157912483, 0.00630740074280419, 0.8655205326846502, 1.8189894035453052e-12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1221543041368323e-12, 0.0, 2.425319204727441e-12, 2.5769016550232964e-12, 0.0, 2.8800665556147096e-12, 0.0, 3.183231456205249e-12, 3.334813906501243e-12, 6.972792713592449e-12, 0.0, 1.1368683772158155e-11, 3.9411437076832865e-12, 0.0, 1.6977234433094658e-11, 2.1979455292859092e-11, 9.094947017727904e-12, 4.699055959160842e-12, 0.0, 5.002220859751105e-12, 5.153803310045812e-12, 0.0, 1.0913936421275139e-11, 2.243420264372543e-11, 1.1520266222460583e-11, 5.9117155615240335e-12, 6.0632980118186026e-12, 1.2429760924226251e-11, 0.0, 0.0, 0.0, 0.0, 6.972792713592449e-12, 0.0, 0.0, 0.0, 7.579122514775551e-12]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0176, 0.0145, 0.0145, 0.0145, 0.0139, 0.0145, 0.0154, 0.0140, 0.0144,\n",
            "        0.0146, 0.0144, 0.0149, 0.0145, 0.0146, 0.0142, 0.0146, 0.0146, 0.0143,\n",
            "        0.0142, 0.0143, 0.0145, 0.0140, 0.0144, 0.0151, 0.0147, 0.0144, 0.0148,\n",
            "        0.0146, 0.0148, 0.0149, 0.0143, 0.0148, 0.0145, 0.0135, 0.0144, 0.0142,\n",
            "        0.0142, 0.0153, 0.0144, 0.0143, 0.0138, 0.0141, 0.0142, 0.0144, 0.0142,\n",
            "        0.0138, 0.0139, 0.0146, 0.0143, 0.0142, 0.0142, 0.0146, 0.0147, 0.0147,\n",
            "        0.0146, 0.0142, 0.0149, 0.0148, 0.0144, 0.0144, 0.0144, 0.0142, 0.0146,\n",
            "        0.0143, 0.0146, 0.0146, 0.0145, 0.0146, 0.0148],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [43]\n",
            "DEBUGGING: logits looks like: tensor([12.8184, 12.7787, 12.7798, 12.7788, 12.7711, 12.7790, 12.7907, 12.7716,\n",
            "        12.7771, 12.7809, 12.7770, 12.7844, 12.7792, 12.7808, 12.7755, 12.7811,\n",
            "        12.7810, 12.7763, 12.7752, 12.7768, 12.7798, 12.7720, 12.7773, 12.7866,\n",
            "        12.7821, 12.7772, 12.7828, 12.7809, 12.7830, 12.7844, 12.7758, 12.7826,\n",
            "        12.7788, 12.7643, 12.7771, 12.7744, 12.7746, 12.7896, 12.7779, 12.7758,\n",
            "        12.7685, 12.7740, 12.7749, 12.7782, 12.7749, 12.7691, 12.7703, 12.7808,\n",
            "        12.7757, 12.7753, 12.7752, 12.7798, 12.7815, 12.7817, 12.7802, 12.7748,\n",
            "        12.7840, 12.7836, 12.7775, 12.7777, 12.7781, 12.7755, 12.7811, 12.7760,\n",
            "        12.7809, 12.7807, 12.7789, 12.7808, 12.7826], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0157, 0.0121, 0.0122, 0.0130, 0.0131, 0.0126, 0.0130, 0.0129, 0.0131,\n",
            "        0.0132, 0.0130, 0.0127, 0.0130, 0.0129, 0.0129, 0.0125, 0.0129, 0.0128,\n",
            "        0.0130, 0.0131, 0.0122, 0.0128, 0.0127, 0.0123, 0.0126, 0.0129, 0.0130,\n",
            "        0.0133, 0.0129, 0.0130, 0.0126, 0.0132, 0.0128, 0.0122, 0.0130, 0.0127,\n",
            "        0.0130, 0.0129, 0.0120, 0.0128, 0.0120, 0.0128, 0.0125, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0132, 0.0130, 0.0128, 0.0134, 0.0126, 0.0124,\n",
            "        0.0125, 0.0131, 0.0129, 0.0131, 0.0128, 0.0122, 0.0128, 0.0129, 0.0127,\n",
            "        0.0129, 0.0124, 0.0130, 0.0129, 0.0127, 0.0129, 0.0129, 0.0127, 0.0130,\n",
            "        0.0129, 0.0131, 0.0121, 0.0126, 0.0126, 0.0128],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [11]\n",
            "DEBUGGING: logits looks like: tensor([12.8211, 12.7681, 12.7704, 12.7831, 12.7852, 12.7766, 12.7831, 12.7817,\n",
            "        12.7846, 12.7857, 12.7833, 12.7787, 12.7830, 12.7820, 12.7815, 12.7749,\n",
            "        12.7818, 12.7794, 12.7824, 12.7847, 12.7711, 12.7806, 12.7788, 12.7723,\n",
            "        12.7762, 12.7820, 12.7835, 12.7873, 12.7816, 12.7832, 12.7762, 12.7859,\n",
            "        12.7799, 12.7705, 12.7829, 12.7789, 12.7827, 12.7810, 12.7679, 12.7803,\n",
            "        12.7670, 12.7794, 12.7750, 12.7803, 12.7803, 12.7799, 12.7803, 12.7808,\n",
            "        12.7863, 12.7827, 12.7801, 12.7891, 12.7771, 12.7730, 12.7747, 12.7847,\n",
            "        12.7821, 12.7840, 12.7806, 12.7702, 12.7801, 12.7814, 12.7792, 12.7820,\n",
            "        12.7738, 12.7829, 12.7812, 12.7790, 12.7818, 12.7820, 12.7784, 12.7838,\n",
            "        12.7816, 12.7845, 12.7692, 12.7768, 12.7771, 12.7801],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0137, 0.0113, 0.0114, 0.0115, 0.0113, 0.0114, 0.0113, 0.0113, 0.0113,\n",
            "        0.0114, 0.0113, 0.0114, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0114,\n",
            "        0.0114, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0115,\n",
            "        0.0113, 0.0113, 0.0114, 0.0114, 0.0113, 0.0114, 0.0113, 0.0113, 0.0113,\n",
            "        0.0114, 0.0113, 0.0113, 0.0113, 0.0114, 0.0113, 0.0114, 0.0113, 0.0113,\n",
            "        0.0113, 0.0113, 0.0114, 0.0114, 0.0113, 0.0114, 0.0113, 0.0113, 0.0113,\n",
            "        0.0114, 0.0113, 0.0114, 0.0113, 0.0113, 0.0113, 0.0113, 0.0114, 0.0113,\n",
            "        0.0113, 0.0113, 0.0113, 0.0113, 0.0114, 0.0113, 0.0114, 0.0113, 0.0112,\n",
            "        0.0113, 0.0114, 0.0113, 0.0113, 0.0113, 0.0114, 0.0114, 0.0113, 0.0113,\n",
            "        0.0114, 0.0113, 0.0115, 0.0113, 0.0113, 0.0113, 0.0113],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [7]\n",
            "DEBUGGING: logits looks like: tensor([12.8228, 12.7844, 12.7855, 12.7889, 12.7850, 12.7859, 12.7844, 12.7851,\n",
            "        12.7849, 12.7856, 12.7845, 12.7866, 12.7850, 12.7852, 12.7852, 12.7841,\n",
            "        12.7846, 12.7863, 12.7856, 12.7846, 12.7854, 12.7846, 12.7852, 12.7851,\n",
            "        12.7852, 12.7846, 12.7881, 12.7852, 12.7852, 12.7862, 12.7869, 12.7848,\n",
            "        12.7856, 12.7846, 12.7849, 12.7844, 12.7866, 12.7849, 12.7843, 12.7850,\n",
            "        12.7858, 12.7853, 12.7863, 12.7849, 12.7843, 12.7854, 12.7844, 12.7858,\n",
            "        12.7858, 12.7850, 12.7856, 12.7854, 12.7851, 12.7849, 12.7856, 12.7849,\n",
            "        12.7869, 12.7848, 12.7843, 12.7851, 12.7847, 12.7860, 12.7841, 12.7855,\n",
            "        12.7854, 12.7846, 12.7855, 12.7856, 12.7850, 12.7857, 12.7854, 12.7822,\n",
            "        12.7846, 12.7860, 12.7852, 12.7845, 12.7853, 12.7859, 12.7860, 12.7846,\n",
            "        12.7850, 12.7858, 12.7850, 12.7873, 12.7853, 12.7855, 12.7839, 12.7852],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0122, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0098, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0100, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [42]\n",
            "DEBUGGING: logits looks like: tensor([12.8245, 12.7865, 12.7865, 12.7866, 12.7865, 12.7866, 12.7867, 12.7866,\n",
            "        12.7867, 12.7867, 12.7865, 12.7868, 12.7865, 12.7866, 12.7866, 12.7866,\n",
            "        12.7866, 12.7866, 12.7866, 12.7867, 12.7864, 12.7803, 12.7865, 12.7867,\n",
            "        12.7866, 12.7866, 12.7866, 12.7867, 12.7867, 12.7867, 12.7866, 12.7858,\n",
            "        12.7866, 12.7865, 12.7864, 12.7864, 12.7867, 12.7867, 12.7866, 12.7865,\n",
            "        12.7867, 12.7866, 12.7867, 12.7865, 12.7866, 12.7868, 12.7865, 12.7866,\n",
            "        12.7866, 12.7865, 12.7867, 12.7866, 12.7867, 12.7867, 12.7866, 12.7867,\n",
            "        12.7865, 12.7864, 12.7866, 12.7866, 12.7865, 12.7867, 12.7866, 12.7868,\n",
            "        12.7866, 12.7866, 12.7866, 12.7864, 12.7866, 12.7865, 12.7867, 12.7865,\n",
            "        12.7867, 12.7866, 12.7866, 12.7866, 12.7867, 12.7865, 12.7866, 12.7866,\n",
            "        12.7867, 12.7868, 12.7867, 12.7865, 12.7865, 12.7864, 12.7865, 12.7865,\n",
            "        12.7866, 12.7863, 12.7865, 12.7864, 12.7864, 12.7864, 12.7868, 12.7866,\n",
            "        12.7866, 12.7867, 12.7867], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0094, 0.0092, 0.0092, 0.0093, 0.0092, 0.0096, 0.0093, 0.0095, 0.0097,\n",
            "        0.0095, 0.0095, 0.0094, 0.0095, 0.0094, 0.0095, 0.0094, 0.0094, 0.0096,\n",
            "        0.0092, 0.0095, 0.0091, 0.0095, 0.0095, 0.0095, 0.0094, 0.0100, 0.0094,\n",
            "        0.0096, 0.0095, 0.0095, 0.0094, 0.0093, 0.0094, 0.0097, 0.0094, 0.0096,\n",
            "        0.0095, 0.0095, 0.0095, 0.0094, 0.0095, 0.0095, 0.0095, 0.0094, 0.0096,\n",
            "        0.0095, 0.0094, 0.0095, 0.0094, 0.0093, 0.0090, 0.0096, 0.0092, 0.0096,\n",
            "        0.0097, 0.0094, 0.0094, 0.0094, 0.0095, 0.0094, 0.0095, 0.0095, 0.0095,\n",
            "        0.0101, 0.0090, 0.0095, 0.0093, 0.0094, 0.0095, 0.0094, 0.0093, 0.0095,\n",
            "        0.0094, 0.0095, 0.0094, 0.0090, 0.0096, 0.0095, 0.0094, 0.0093, 0.0092,\n",
            "        0.0095, 0.0095, 0.0093, 0.0093, 0.0094, 0.0095, 0.0096, 0.0092, 0.0093,\n",
            "        0.0090, 0.0095, 0.0095, 0.0095, 0.0090, 0.0095, 0.0096, 0.0092, 0.0094,\n",
            "        0.0094, 0.0099, 0.0095, 0.0093, 0.0094, 0.0096, 0.0095],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [63]\n",
            "DEBUGGING: logits looks like: tensor([12.7901, 12.7840, 12.7841, 12.7861, 12.7847, 12.7926, 12.7880, 12.7905,\n",
            "        12.7950, 12.7913, 12.7905, 12.7900, 12.7905, 12.7894, 12.7918, 12.7883,\n",
            "        12.7899, 12.7937, 12.7858, 12.7905, 12.7828, 12.7919, 12.7904, 12.7904,\n",
            "        12.7899, 12.8007, 12.7899, 12.7924, 12.7918, 12.7910, 12.7897, 12.7862,\n",
            "        12.7887, 12.7951, 12.7885, 12.7924, 12.7921, 12.7920, 12.7908, 12.7890,\n",
            "        12.7918, 12.7921, 12.7922, 12.7886, 12.7923, 12.7902, 12.7891, 12.7905,\n",
            "        12.7887, 12.7869, 12.7797, 12.7931, 12.7858, 12.7929, 12.7960, 12.7892,\n",
            "        12.7893, 12.7896, 12.7907, 12.7885, 12.7902, 12.7906, 12.7912, 12.8034,\n",
            "        12.7812, 12.7908, 12.7875, 12.7885, 12.7913, 12.7901, 12.7873, 12.7917,\n",
            "        12.7892, 12.7909, 12.7887, 12.7803, 12.7934, 12.7909, 12.7888, 12.7880,\n",
            "        12.7853, 12.7912, 12.7914, 12.7878, 12.7869, 12.7899, 12.7918, 12.7941,\n",
            "        12.7848, 12.7868, 12.7793, 12.7914, 12.7914, 12.7902, 12.7814, 12.7919,\n",
            "        12.7925, 12.7858, 12.7883, 12.7898, 12.7994, 12.7919, 12.7875, 12.7902,\n",
            "        12.7930, 12.7914], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.708265319621205 and immediate abs rewards look like: [0.011022115781997854, 0.005557941906317865, 0.0036487678025878267, 0.00947910708737254, 0.0006055815438230638, 0.006734932638210012, 0.004189079213574587, 0.011314678277813073, 0.013290325636262423, 8.126492821247666e-05, 0.00030935713311919244, 0.00023506116940552602, 0.0039661371820329805, 0.008212021978579287, 0.0003715349639605847, 0.0012023152235087764, 0.00021410940416899393, 0.006636724829149898, 0.0014471465187853028, 0.0032114545733747946, 0.0037220255330794316, 0.00133886890444046, 0.0022703120989717718, 0.0020190793052279332, 0.004320768842262623, 0.0007217956099339062, 0.0017841174653767666, 0.0004935752963319828, 0.0005047910021858115, 0.0006620395547543012, 0.00027273195155430585, 0.0003286477058281889, 0.0003845101200568024, 0.0016992824680528429, 2.1998980628268328e-06, 0.00012763168297169614, 0.00046952211368989083, 0.00038348568386936677, 0.00018798404244080302, 4.7623996579204686e-05, 3.999533828391577e-05, 6.692311626466108e-05, 3.249771862101625e-05, 1.658446853980422e-05, 2.5031181394297164e-06, 8.27748590381816e-06, 8.384616194234695e-06, 0.5946195026899659, 4.547473508864641e-13, 9.094947017729282e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 8.30963934906674 and immediate relative rewards look like: [0.0029723104556951727, 0.00300653303009581, 0.002965123685012159, 0.010280925476157875, 0.0008231236869599245, 0.010986983507738322, 0.007987420931681932, 0.02468413966737549, 0.032719452252290955, 0.0002231067970515816, 0.000934269464062556, 0.0007744945412358779, 0.014157785801517317, 0.031603532068898876, 0.0015354296250553233, 0.005300559975750803, 0.001003256512365049, 0.03292905522007931, 0.00759301541440947, 0.017744082501364788, 0.02161254402097319, 0.008152957462788164, 0.014458685617257406, 0.013426200683763188, 0.029945569688678206, 0.0052088235070148915, 0.013372921087674223, 0.0038385365102828785, 0.004066524322550839, 0.005517976760869445, 0.0023493746407890924, 0.0029225911227193516, 0.003526540358514422, 0.016058983463738664, 2.1411610501921103e-05, 0.0012777326582758765, 0.004831168542635348, 0.004053067866607934, 0.0020393090158182087, 0.0005299147245254267, 0.00045616204007031894, 0.0007819089586447355, 0.00038874083031713926, 0.0002030004449713223, 3.133564362946731e-05, 0.00010592569988169058, 0.0001096294174205483, 7.940127211728443, 7.427540064478913e-12, 1.5158245029551102e-11]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 3\n",
            "DEBUGGING: the action_prob is: tensor([0.0161, 0.0137, 0.0158, 0.0138, 0.0143, 0.0147, 0.0180, 0.0157, 0.0128,\n",
            "        0.0136, 0.0181, 0.0149, 0.0131, 0.0148, 0.0130, 0.0132, 0.0142, 0.0173,\n",
            "        0.0127, 0.0117, 0.0149, 0.0161, 0.0118, 0.0164, 0.0127, 0.0140, 0.0139,\n",
            "        0.0127, 0.0165, 0.0155, 0.0147, 0.0139, 0.0140, 0.0138, 0.0144, 0.0128,\n",
            "        0.0148, 0.0150, 0.0163, 0.0176, 0.0152, 0.0138, 0.0145, 0.0145, 0.0118,\n",
            "        0.0145, 0.0145, 0.0189, 0.0146, 0.0147, 0.0137, 0.0128, 0.0136, 0.0145,\n",
            "        0.0138, 0.0124, 0.0125, 0.0142, 0.0145, 0.0140, 0.0158, 0.0136, 0.0165,\n",
            "        0.0140, 0.0168, 0.0138, 0.0139, 0.0147, 0.0144],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [8]\n",
            "DEBUGGING: logits looks like: tensor([12.7232, 12.6912, 12.7202, 12.6932, 12.7006, 12.7057, 12.7465, 12.7183,\n",
            "        12.6781, 12.6906, 12.7475, 12.7078, 12.6821, 12.7066, 12.6803, 12.6836,\n",
            "        12.6987, 12.7377, 12.6762, 12.6597, 12.7081, 12.7236, 12.6622, 12.7280,\n",
            "        12.6763, 12.6957, 12.6939, 12.6768, 12.7281, 12.7167, 12.7053, 12.6940,\n",
            "        12.6955, 12.6929, 12.7020, 12.6780, 12.7075, 12.7089, 12.7262, 12.7416,\n",
            "        12.7119, 12.6933, 12.7024, 12.7027, 12.6623, 12.7034, 12.7022, 12.7561,\n",
            "        12.7048, 12.7055, 12.6919, 12.6786, 12.6896, 12.7029, 12.6929, 12.6723,\n",
            "        12.6732, 12.6993, 12.7026, 12.6957, 12.7199, 12.6894, 12.7282, 12.6964,\n",
            "        12.7326, 12.6936, 12.6942, 12.7055, 12.7015], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0148, 0.0104, 0.0117, 0.0114, 0.0132, 0.0140, 0.0153, 0.0148, 0.0139,\n",
            "        0.0111, 0.0121, 0.0138, 0.0129, 0.0088, 0.0133, 0.0150, 0.0100, 0.0122,\n",
            "        0.0107, 0.0129, 0.0131, 0.0129, 0.0131, 0.0102, 0.0112, 0.0129, 0.0136,\n",
            "        0.0114, 0.0132, 0.0130, 0.0136, 0.0112, 0.0133, 0.0128, 0.0115, 0.0121,\n",
            "        0.0124, 0.0099, 0.0123, 0.0131, 0.0138, 0.0143, 0.0132, 0.0151, 0.0160,\n",
            "        0.0102, 0.0135, 0.0145, 0.0138, 0.0128, 0.0132, 0.0122, 0.0129, 0.0130,\n",
            "        0.0131, 0.0126, 0.0150, 0.0149, 0.0113, 0.0135, 0.0113, 0.0109, 0.0133,\n",
            "        0.0127, 0.0136, 0.0140, 0.0127, 0.0128, 0.0114, 0.0130, 0.0146, 0.0130,\n",
            "        0.0162, 0.0133, 0.0102, 0.0113, 0.0128, 0.0146],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [20]\n",
            "DEBUGGING: logits looks like: tensor([12.7397, 12.6685, 12.6925, 12.6866, 12.7160, 12.7288, 12.7456, 12.7399,\n",
            "        12.7268, 12.6826, 12.6994, 12.7249, 12.7125, 12.6343, 12.7184, 12.7423,\n",
            "        12.6613, 12.7010, 12.6748, 12.7115, 12.7152, 12.7115, 12.7156, 12.6642,\n",
            "        12.6844, 12.7119, 12.7231, 12.6874, 12.7160, 12.7130, 12.7226, 12.6829,\n",
            "        12.7175, 12.7109, 12.6896, 12.6994, 12.7046, 12.6583, 12.7030, 12.7156,\n",
            "        12.7257, 12.7321, 12.7168, 12.7432, 12.7544, 12.6642, 12.7205, 12.7351,\n",
            "        12.7250, 12.7097, 12.7171, 12.7014, 12.7116, 12.7141, 12.7143, 12.7064,\n",
            "        12.7418, 12.7409, 12.6851, 12.7214, 12.6850, 12.6781, 12.7180, 12.7080,\n",
            "        12.7227, 12.7284, 12.7082, 12.7104, 12.6879, 12.7138, 12.7366, 12.7132,\n",
            "        12.7578, 12.7180, 12.6642, 12.6859, 12.7102, 12.7360],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0109, 0.0111, 0.0134, 0.0109, 0.0121, 0.0122, 0.0115, 0.0119, 0.0118,\n",
            "        0.0115, 0.0108, 0.0115, 0.0111, 0.0112, 0.0100, 0.0115, 0.0126, 0.0107,\n",
            "        0.0109, 0.0110, 0.0123, 0.0114, 0.0114, 0.0111, 0.0099, 0.0127, 0.0114,\n",
            "        0.0111, 0.0110, 0.0110, 0.0115, 0.0114, 0.0121, 0.0126, 0.0117, 0.0112,\n",
            "        0.0111, 0.0133, 0.0113, 0.0098, 0.0116, 0.0126, 0.0115, 0.0115, 0.0112,\n",
            "        0.0113, 0.0115, 0.0116, 0.0109, 0.0114, 0.0115, 0.0108, 0.0119, 0.0112,\n",
            "        0.0116, 0.0112, 0.0126, 0.0111, 0.0117, 0.0118, 0.0125, 0.0116, 0.0124,\n",
            "        0.0118, 0.0117, 0.0131, 0.0118, 0.0118, 0.0112, 0.0114, 0.0107, 0.0111,\n",
            "        0.0111, 0.0110, 0.0111, 0.0124, 0.0113, 0.0112, 0.0115, 0.0119, 0.0111,\n",
            "        0.0114, 0.0113, 0.0114, 0.0112, 0.0113, 0.0114],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [31]\n",
            "DEBUGGING: logits looks like: tensor([12.7147, 12.7180, 12.7562, 12.7144, 12.7354, 12.7371, 12.7267, 12.7325,\n",
            "        12.7308, 12.7259, 12.7136, 12.7254, 12.7183, 12.7215, 12.6986, 12.7266,\n",
            "        12.7449, 12.7108, 12.7151, 12.7171, 12.7389, 12.7238, 12.7239, 12.7185,\n",
            "        12.6968, 12.7461, 12.7247, 12.7186, 12.7172, 12.7168, 12.7251, 12.7250,\n",
            "        12.7364, 12.7447, 12.7299, 12.7212, 12.7187, 12.7555, 12.7219, 12.6948,\n",
            "        12.7276, 12.7436, 12.7260, 12.7258, 12.7215, 12.7221, 12.7265, 12.7279,\n",
            "        12.7146, 12.7243, 12.7259, 12.7129, 12.7325, 12.7202, 12.7274, 12.7207,\n",
            "        12.7436, 12.7197, 12.7293, 12.7305, 12.7419, 12.7280, 12.7405, 12.7310,\n",
            "        12.7291, 12.7519, 12.7311, 12.7312, 12.7204, 12.7247, 12.7119, 12.7188,\n",
            "        12.7183, 12.7171, 12.7180, 12.7414, 12.7233, 12.7204, 12.7262, 12.7326,\n",
            "        12.7185, 12.7237, 12.7225, 12.7242, 12.7200, 12.7226, 12.7236],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0107, 0.0116, 0.0098, 0.0095, 0.0105, 0.0099, 0.0091, 0.0099, 0.0100,\n",
            "        0.0072, 0.0103, 0.0102, 0.0104, 0.0101, 0.0092, 0.0094, 0.0109, 0.0117,\n",
            "        0.0109, 0.0103, 0.0098, 0.0104, 0.0098, 0.0107, 0.0116, 0.0111, 0.0101,\n",
            "        0.0106, 0.0113, 0.0109, 0.0105, 0.0098, 0.0120, 0.0111, 0.0109, 0.0109,\n",
            "        0.0098, 0.0092, 0.0107, 0.0101, 0.0086, 0.0099, 0.0102, 0.0103, 0.0112,\n",
            "        0.0103, 0.0090, 0.0091, 0.0098, 0.0098, 0.0107, 0.0098, 0.0097, 0.0114,\n",
            "        0.0083, 0.0095, 0.0105, 0.0097, 0.0098, 0.0104, 0.0099, 0.0093, 0.0085,\n",
            "        0.0113, 0.0105, 0.0107, 0.0115, 0.0115, 0.0091, 0.0107, 0.0108, 0.0095,\n",
            "        0.0095, 0.0103, 0.0116, 0.0101, 0.0109, 0.0104, 0.0125, 0.0100, 0.0116,\n",
            "        0.0093, 0.0117, 0.0103, 0.0112, 0.0104, 0.0107, 0.0096, 0.0109, 0.0096,\n",
            "        0.0097, 0.0122, 0.0104, 0.0106, 0.0096, 0.0100, 0.0127],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [88]\n",
            "DEBUGGING: logits looks like: tensor([12.7269, 12.7426, 12.7099, 12.7032, 12.7228, 12.7106, 12.6943, 12.7114,\n",
            "        12.7138, 12.6464, 12.7193, 12.7172, 12.7206, 12.7146, 12.6955, 12.7017,\n",
            "        12.7307, 12.7440, 12.7302, 12.7185, 12.7095, 12.7213, 12.7086, 12.7260,\n",
            "        12.7432, 12.7347, 12.7158, 12.7252, 12.7375, 12.7306, 12.7230, 12.7099,\n",
            "        12.7489, 12.7348, 12.7305, 12.7297, 12.7082, 12.6969, 12.7269, 12.7151,\n",
            "        12.6841, 12.7103, 12.7165, 12.7192, 12.7351, 12.7193, 12.6924, 12.6943,\n",
            "        12.7092, 12.7088, 12.7263, 12.7084, 12.7071, 12.7393, 12.6750, 12.7029,\n",
            "        12.7220, 12.7065, 12.7098, 12.7213, 12.7112, 12.6989, 12.6815, 12.7376,\n",
            "        12.7238, 12.7274, 12.7405, 12.7405, 12.6952, 12.7266, 12.7281, 12.7033,\n",
            "        12.7030, 12.7189, 12.7433, 12.7147, 12.7305, 12.7217, 12.7582, 12.7139,\n",
            "        12.7436, 12.6986, 12.7454, 12.7183, 12.7363, 12.7201, 12.7270, 12.7054,\n",
            "        12.7313, 12.7057, 12.7064, 12.7537, 12.7215, 12.7246, 12.7048, 12.7135,\n",
            "        12.7602], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0093, 0.0088, 0.0091, 0.0096, 0.0091, 0.0092, 0.0089, 0.0095, 0.0088,\n",
            "        0.0090, 0.0091, 0.0091, 0.0093, 0.0095, 0.0095, 0.0095, 0.0090, 0.0093,\n",
            "        0.0088, 0.0096, 0.0094, 0.0090, 0.0089, 0.0086, 0.0095, 0.0090, 0.0096,\n",
            "        0.0092, 0.0088, 0.0097, 0.0092, 0.0094, 0.0092, 0.0088, 0.0098, 0.0088,\n",
            "        0.0092, 0.0092, 0.0093, 0.0091, 0.0096, 0.0096, 0.0088, 0.0093, 0.0094,\n",
            "        0.0087, 0.0097, 0.0092, 0.0094, 0.0092, 0.0094, 0.0089, 0.0089, 0.0090,\n",
            "        0.0095, 0.0096, 0.0094, 0.0091, 0.0096, 0.0095, 0.0085, 0.0090, 0.0094,\n",
            "        0.0094, 0.0093, 0.0094, 0.0093, 0.0096, 0.0091, 0.0095, 0.0098, 0.0094,\n",
            "        0.0091, 0.0094, 0.0092, 0.0091, 0.0093, 0.0093, 0.0097, 0.0094, 0.0089,\n",
            "        0.0089, 0.0094, 0.0090, 0.0097, 0.0094, 0.0092, 0.0096, 0.0091, 0.0093,\n",
            "        0.0095, 0.0088, 0.0092, 0.0092, 0.0095, 0.0104, 0.0092, 0.0093, 0.0095,\n",
            "        0.0093, 0.0093, 0.0094, 0.0090, 0.0092, 0.0095, 0.0090, 0.0090, 0.0096],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [52]\n",
            "DEBUGGING: logits looks like: tensor([12.6876, 12.6770, 12.6834, 12.6926, 12.6832, 12.6851, 12.6795, 12.6905,\n",
            "        12.6769, 12.6810, 12.6833, 12.6821, 12.6875, 12.6918, 12.6907, 12.6913,\n",
            "        12.6801, 12.6871, 12.6771, 12.6944, 12.6893, 12.6802, 12.6787, 12.6716,\n",
            "        12.6911, 12.6814, 12.6943, 12.6860, 12.6757, 12.6959, 12.6848, 12.6888,\n",
            "        12.6851, 12.6772, 12.6971, 12.6755, 12.6843, 12.6846, 12.6867, 12.6840,\n",
            "        12.6937, 12.6941, 12.6770, 12.6866, 12.6903, 12.6748, 12.6960, 12.6845,\n",
            "        12.6900, 12.6842, 12.6901, 12.6790, 12.6784, 12.6797, 12.6908, 12.6931,\n",
            "        12.6891, 12.6827, 12.6935, 12.6918, 12.6682, 12.6812, 12.6884, 12.6898,\n",
            "        12.6880, 12.6902, 12.6875, 12.6930, 12.6822, 12.6911, 12.6968, 12.6895,\n",
            "        12.6822, 12.6894, 12.6856, 12.6825, 12.6868, 12.6876, 12.6965, 12.6895,\n",
            "        12.6790, 12.6785, 12.6888, 12.6813, 12.6959, 12.6896, 12.6840, 12.6939,\n",
            "        12.6826, 12.6874, 12.6913, 12.6770, 12.6857, 12.6856, 12.6922, 12.7087,\n",
            "        12.6846, 12.6865, 12.6920, 12.6865, 12.6881, 12.6893, 12.6805, 12.6854,\n",
            "        12.6905, 12.6809, 12.6801, 12.6931], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.7058461509345761 and immediate abs rewards look like: [0.001391986146245472, 0.0034341110808782105, 0.005519493684005283, 0.0032590227178843634, 0.6922415372928299, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 0.0, 1.3642420526593924e-12, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 0.0, 9.094947017729282e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 0.0, 0.0, 4.547473508864641e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 1.2996437456272354 and immediate relative rewards look like: [0.0005144365453931946, 0.002539596437319346, 0.006130454688541955, 0.004836241203187818, 1.2856230165533864, 1.3642420526597025e-12, 1.5916157281026244e-12, 1.8189894035454429e-12, 0.0, 2.2737367544323206e-12, 2.5011104298761213e-12, 2.7284841053187847e-12, 2.955857780762689e-12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.139089236967266e-12, 0.0, 6.593836587855229e-12, 0.0, 0.0, 2.1827872842550278e-11, 1.5006662579263552e-11, 7.730704965071648e-12, 0.0, 0.0, 8.412825991399586e-12, 0.0, 1.7735146684568068e-11, 1.81898940354627e-11, 9.322320693170395e-12, 0.0, 9.777068044058979e-12, 1.0004441719499936e-11, 1.0231815394945443e-11, 1.0459189070386297e-11, 2.1373125491654094e-11, 0.0, 0.0, 1.1368683772161603e-11]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 200 number of matrices\n",
            "DEBUGGING: ACT_MAT has 200 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.2753129261717786, 0.2725192180600535, 0.27237804468816706, 0.25536886353237137, 0.2508577776968853, 0.23915280765145563, 0.23932060211749806, 0.24068365332983205, 0.2304867140357366, 0.1780965381305981, 0.1699617249886003, 0.15997493760535167, 0.15974925056753964, 0.1573052562780594, 0.13622555465468764, 0.13668203410598, 0.13647096667879763, 0.09431105498158074, 0.08481711699895178, 0.07140782997551709, 0.07001590413706295, 0.0660813476841278, 0.06643408406577708, 0.06433122450496344, 0.06497982105139913, 0.06561792310438733, 0.06190971869525464, 0.062171238900061945, 0.039863964363211574, 0.039700319776176635, 0.036381453282940665, 0.03581264632991873, 0.03536047912644195, 0.03457688959912093, 0.03453782022719084, 0.034497141056311596, 0.034555886398487105, 0.03484635021256467, 0.03230294105130766, 0.032566387419022624, 0.032633108281890665, 0.030631837882494087, 0.02843089032555268, 0.028530752443779173, 0.007706523818617359, 0.004842283813993373, 0.0012251502202742064, 0.0012351992167196168, 0.000905954883496701, 0.0005057513805868285], [0.8764413641019609, 0.8769488627383206, 0.8762717811173502, 0.8631727282418562, 0.8655205328273252, 1.441161286516122e-10, 1.4373448408895646e-10, 1.4518634756460248e-10, 1.466528763278813e-10, 1.4813421851301142e-10, 1.4963052375051659e-10, 1.5114194318234e-10, 1.526686294771111e-10, 1.5421073684556678e-10, 1.5362483084992923e-10, 1.5517659681811034e-10, 1.542942198114979e-10, 1.532498163196713e-10, 1.5479779426229425e-10, 1.5345225020876722e-10, 1.550022729381487e-10, 1.5335256715347823e-10, 1.5153308408785554e-10, 1.460204963376395e-10, 1.4749545084610049e-10, 1.3750178492317406e-10, 1.3490973860150583e-10, 1.362724632338443e-10, 1.2050023111186833e-10, 9.951593517071641e-11, 9.133433146766517e-11, 8.751037930151952e-11, 8.839432252678739e-11, 8.423444612831948e-11, 7.987943719017542e-11, 8.068630019209638e-11, 7.047713512204165e-11, 4.852821462456184e-11, 3.738176606272854e-11, 3.178792979919647e-11, 2.598447655290694e-11, 1.3691631948162312e-11, 1.3829931260770012e-11, 1.3969627536131326e-11, 1.4110734884981138e-11, 1.4253267560587009e-11, 7.354014996964202e-12, 7.428297976731518e-12, 7.503331289627795e-12, 7.579122514775551e-12], [5.2618535315312025, 5.312001233409603, 5.36262090947425, 5.413793723019433, 5.458093734892197, 5.512394556772967, 5.5569773467325545, 5.60504032909179, 5.636723423661025, 5.6606100721300345, 5.7175625912454375, 5.774372042203409, 5.8319167148102755, 5.876524170715918, 5.903960241057595, 5.962045264073272, 6.016913842522749, 6.076677359606449, 6.104796267056939, 6.158791163275282, 6.2030778593675935, 6.24390435893598, 6.298738789366861, 6.347757680555155, 6.398314626132719, 6.432696016610143, 6.492411306164775, 6.544483217249597, 6.606711798726579, 6.66933866101417, 6.7311320042962635, 6.796750130965125, 6.862452060446874, 6.928207596048849, 6.981968295540515, 7.052471599929307, 7.122418047748516, 7.189481696167556, 7.258008715455503, 7.329262026706752, 7.402759709072956, 7.477074289932209, 7.551810485831883, 7.627698732324814, 7.704541143312972, 7.782333139059943, 7.8608355690505665, 7.940127211750653, 2.2434202643734502e-11, 1.5158245029551102e-11], [1.248694035866182, 1.2607874740614027, 1.2709574521455387, 1.2776030277343402, 1.285623016698134, 1.462097039551261e-10, 1.4630854737622868e-10, 1.4617871883649097e-10, 1.458179085181268e-10, 1.4729081668497658e-10, 1.4648189891974168e-10, 1.4543513988875308e-10, 1.4414813715498414e-10, 1.4261846401436512e-10, 1.4405905455996478e-10, 1.4551419652521695e-10, 1.4698403689415853e-10, 1.4846872413551368e-10, 1.4996840821769058e-10, 1.514832406239299e-10, 1.5301337436760596e-10, 1.5455896400768278e-10, 1.5612016566432605e-10, 1.5769713703467278e-10, 1.592900374087604e-10, 1.6089902768561656e-10, 1.6252427038951167e-10, 1.5796482944701456e-10, 1.595604337848632e-10, 1.5451171434041208e-10, 1.5607243872768897e-10, 1.5764892800776663e-10, 1.3719298501537005e-10, 1.2342052771323889e-10, 1.1685840681633056e-10, 1.1803879476397026e-10, 1.192311058221922e-10, 1.1193765639474e-10, 1.1306833979266668e-10, 9.629615465464507e-11, 7.889521274664885e-11, 7.027564853886713e-11, 7.098550357461327e-11, 6.182670255611544e-11, 5.2345718016783335e-11, 4.2539295577614033e-11, 3.2404147987098725e-11, 1.1142446965095588e-11, 1.1254996934439987e-11, 1.1368683772161603e-11]]\n",
            "DEBUGGING: traj_returns = [0.2753129261717786, 0.8764413641019609, 5.2618535315312025, 1.248694035866182]\n",
            "DEBUGGING: actions = [[23], [18], [29], [17], [2], [63], [52], [20], [61], [11], [57], [71], [58], [62], [16], [16], [4], [6], [41], [40], [49], [71], [31], [45], [9], [1], [36], [14], [3], [77], [49], [4], [86], [69], [19], [80], [58], [66], [76], [89], [90], [67], [72], [34], [5], [46], [44], [48], [76], [19], [12], [22], [11], [53], [0], [62], [34], [60], [9], [37], [1], [33], [57], [10], [52], [53], [25], [62], [66], [42], [4], [37], [23], [68], [56], [59], [42], [36], [60], [37], [88], [52], [24], [75], [2], [83], [28], [17], [13], [10], [60], [95], [50], [52], [2], [50], [41], [11], [94], [61], [44], [26], [13], [14], [35], [38], [1], [61], [26], [43], [47], [3], [39], [58], [15], [42], [19], [33], [13], [11], [31], [27], [39], [6], [18], [74], [13], [21], [28], [7], [66], [31], [25], [44], [2], [87], [17], [15], [28], [42], [7], [17], [65], [44], [79], [47], [88], [0], [77], [63], [45], [58], [38], [10], [0], [21], [6], [25], [49], [8], [16], [47], [19], [63], [35], [67], [70], [24], [73], [20], [60], [56], [33], [49], [10], [76], [77], [81], [58], [31], [83], [77], [33], [82], [78], [83], [68], [57], [6], [88], [27], [78], [55], [44], [58], [23], [94], [68], [54], [52]]\n",
            "DEBUGGING: actions length = 200\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-1.4626, -0.8103,  0.5450,  ...,  0.4546,  0.6754, -0.9280],\n",
            "        [-1.4609, -0.8125,  0.5507,  ...,  0.4631,  0.6775, -0.9291],\n",
            "        [-1.4453, -0.8001,  0.5429,  ...,  0.4592,  0.6725, -0.9172],\n",
            "        ...,\n",
            "        [-1.4617, -0.8085,  0.5484,  ...,  0.4635,  0.6762, -0.9244],\n",
            "        [-1.4627, -0.8095,  0.5489,  ...,  0.4637,  0.6766, -0.9254],\n",
            "        [-1.4637, -0.8102,  0.5492,  ...,  0.4639,  0.6769, -0.9258]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-1.4632, -0.8098,  0.5490,  ...,  0.4637,  0.6768, -0.9256],\n",
            "        [-1.4620, -0.8089,  0.5485,  ...,  0.4634,  0.6764, -0.9247],\n",
            "        [-1.4628, -0.8094,  0.5488,  ...,  0.4636,  0.6766, -0.9252],\n",
            "        ...,\n",
            "        [-1.4625, -0.8092,  0.5486,  ...,  0.4635,  0.6765, -0.9250],\n",
            "        [-1.4624, -0.8091,  0.5487,  ...,  0.4636,  0.6765, -0.9249],\n",
            "        [-1.4638, -0.8103,  0.5493,  ...,  0.4639,  0.6770, -0.9260]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([12.6876, 12.6770, 12.6834, 12.6926, 12.6832, 12.6851, 12.6795, 12.6905,\n",
            "        12.6769, 12.6810, 12.6833, 12.6821, 12.6875, 12.6918, 12.6907, 12.6913,\n",
            "        12.6801, 12.6871, 12.6771, 12.6944, 12.6893, 12.6802, 12.6787, 12.6716,\n",
            "        12.6911, 12.6814, 12.6943, 12.6860, 12.6757, 12.6959, 12.6848, 12.6888,\n",
            "        12.6851, 12.6772, 12.6971, 12.6755, 12.6843, 12.6846, 12.6867, 12.6840,\n",
            "        12.6937, 12.6941, 12.6770, 12.6866, 12.6903, 12.6748, 12.6960, 12.6845,\n",
            "        12.6900, 12.6842, 12.6901, 12.6790, 12.6784, 12.6797, 12.6908, 12.6931,\n",
            "        12.6891, 12.6827, 12.6935, 12.6918, 12.6682, 12.6812, 12.6884, 12.6898,\n",
            "        12.6880, 12.6902, 12.6875, 12.6930, 12.6822, 12.6911, 12.6968, 12.6895,\n",
            "        12.6822, 12.6894, 12.6856, 12.6825, 12.6868, 12.6876, 12.6965, 12.6895,\n",
            "        12.6790, 12.6785, 12.6888, 12.6813, 12.6959, 12.6896, 12.6840, 12.6939,\n",
            "        12.6826, 12.6874, 12.6913, 12.6770, 12.6857, 12.6856, 12.6922, 12.7087,\n",
            "        12.6846, 12.6865, 12.6920, 12.6865, 12.6881, 12.6893, 12.6805, 12.6854,\n",
            "        12.6905, 12.6809, 12.6801, 12.6931], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[1.91557546e+00 1.93056420e+00 1.94555705e+00 1.95248459e+00\n",
            "  1.96502377e+00 1.43788684e+00 1.44907449e+00 1.46143100e+00\n",
            "  1.46680253e+00 1.45967665e+00 1.47188108e+00 1.48358675e+00\n",
            "  1.49791649e+00 1.50845736e+00 1.51004645e+00 1.52468182e+00\n",
            "  1.53834620e+00 1.54274710e+00 1.54740335e+00 1.55754975e+00\n",
            "  1.56827344e+00 1.57749643e+00 1.59129322e+00 1.60302223e+00\n",
            "  1.61582361e+00 1.62457849e+00 1.63858026e+00 1.65166361e+00\n",
            "  1.66164394e+00 1.67725975e+00 1.69187836e+00 1.70814069e+00\n",
            "  1.72445313e+00 1.74069612e+00 1.75412653e+00 1.77174219e+00\n",
            "  1.78924348e+00 1.80608201e+00 1.82257791e+00 1.84045710e+00\n",
            "  1.85884820e+00 1.87692653e+00 1.89506034e+00 1.91405737e+00\n",
            "  1.92806192e+00 1.94679386e+00 1.96551518e+00 1.98534060e+00\n",
            "  2.26488731e-04 1.26437854e-04]]\n",
            "DEBUGGING: baseline2 looks like: 1.9155754644177811\n",
            "DEBUGGING: ADS looks like: [-1.35466348 -1.3617087  -1.36634403 -1.37471582 -1.34849514 -1.33032796\n",
            " -1.33249426 -1.33678409 -1.33054138 -1.3673593  -1.36828748 -1.3616365\n",
            " -1.31821464 -1.30285804 -1.3306487  -1.3240576  -1.30930229 -1.30266625\n",
            " -1.27535543 -1.21840406 -1.16500726 -1.12816395 -1.10832962 -1.06688149\n",
            " -1.0367177  -0.97444995 -0.98368384 -0.98995164 -0.95831965 -0.9252824\n",
            " -0.87060745 -0.76458628 -0.68138544 -0.60078855 -0.57260002 -0.52454672\n",
            " -0.52828278 -0.3928959  -0.28990838 -0.26313117 -0.26418273 -0.21251917\n",
            " -0.21393393 -0.2148666  -0.20051904 -0.20310512 -0.17935833 -0.1795267\n",
            " -0.1311537  -0.03170005 -0.75353505 -0.75727906 -0.76245029 -0.76691195\n",
            " -0.73383238 -1.56948076 -1.57181486 -1.57746774 -1.56102809 -1.54545584\n",
            " -1.5382492  -1.52161144 -1.47796389 -1.4601633  -1.46687425 -1.46073964\n",
            " -1.44577326 -1.3969773  -1.36017254 -1.28981189 -1.23502316 -1.1942453\n",
            " -1.1747637  -1.13121271 -1.10169752 -1.04006788 -1.04559356 -1.05212288\n",
            " -0.99818361 -0.96498272 -0.9069889  -0.80039892 -0.71674592 -0.63536544\n",
            " -0.60713784 -0.55904386 -0.56283867 -0.42774225 -0.32221132 -0.29569755\n",
            " -0.29681584 -0.243151   -0.24236482 -0.24339735 -0.20822556 -0.2079474\n",
            " -0.18058348 -0.1807619  -0.13205965 -0.0322058   3.63187712  3.67777331\n",
            "  3.72389884  3.78370904  3.85874082  3.94291379  3.98516248  4.02757259\n",
            "  4.07569533  4.11515423  4.17931339  4.2527606   4.35395283  4.41636087\n",
            "  4.43708599  4.50130563  4.57114058  4.67970005  4.74462372  4.86897927\n",
            "  4.96805469  5.04965906  5.12397509  5.21654497  5.2966171   5.39262814\n",
            "  5.44681775  5.49236034  5.60852819  5.70435595  5.8241431   5.99635121\n",
            "  6.14570614  6.29284216  6.37483046  6.49342774  6.55957938  6.76173945\n",
            "  6.93579739  7.03356447  7.10594387  7.23392329  7.30944567  7.38430138\n",
            "  7.49631558  7.57438574  7.68025209  7.75936531 -0.13205965 -0.0322058\n",
            " -0.38128237 -0.37344045 -0.36776462 -0.35248165 -0.3137299  -1.56948076\n",
            " -1.57181486 -1.57746774 -1.56102809 -1.54545584 -1.5382492  -1.52161144\n",
            " -1.47796389 -1.4601633  -1.46687425 -1.46073964 -1.44577326 -1.3969773\n",
            " -1.36017254 -1.28981189 -1.23502316 -1.1942453  -1.1747637  -1.13121271\n",
            " -1.10169752 -1.04006788 -1.04559356 -1.05212288 -0.99818361 -0.96498272\n",
            " -0.9069889  -0.80039892 -0.71674592 -0.63536544 -0.60713784 -0.55904386\n",
            " -0.56283867 -0.42774225 -0.32221132 -0.29569755 -0.29681584 -0.243151\n",
            " -0.24236482 -0.24339735 -0.20822556 -0.2079474  -0.18058348 -0.1807619\n",
            " -0.13205965 -0.0322058 ]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(2.9177, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-9.8828e-04, -1.7814e-04,  1.2161e-03,  ..., -3.2415e-04,\n",
            "          4.4158e-04, -2.8324e-04],\n",
            "        [-5.4395e-04, -9.7899e-05,  6.6971e-04,  ..., -1.7830e-04,\n",
            "          2.4324e-04, -1.1474e-04]])\n",
            "   Last layer:\n",
            "tensor([[ 7.6700e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  1.9672e-05, -2.4354e-04,  7.1654e-05,  4.5359e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.4293e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 3.1317e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  7.8361e-05,  3.5373e-05,  1.1873e-04,  3.3107e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.2929e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.8719e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -7.1749e-05, -8.0346e-05, -1.0045e-04, -2.3272e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1799e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-3.7640e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -9.4294e-05, -1.8951e-05, -1.4633e-04, -4.3210e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.5568e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 6.1947e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  1.6107e-05, -3.7508e-04,  8.8233e-05,  6.2984e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.9767e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 5.0766e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  1.3197e-05, -2.8757e-04,  6.8963e-05,  4.8639e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.3950e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 7.7324e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  1.9088e-05,  1.4177e-04,  6.5778e-06, -1.1449e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.0468e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.2358e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  3.0813e-05,  1.5979e-04,  2.2139e-05, -8.4875e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.9414e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.0654e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -2.6758e-05,  6.7608e-05, -5.3667e-05, -2.3002e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.4827e-05,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 4.8694e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  1.2179e-04,  1.1230e-04,  1.7457e-04,  4.2979e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0033e-04,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0005, -0.0022, -0.0015,  ...,  0.0028, -0.0018, -0.0135],\n",
            "        [ 0.0003, -0.0012, -0.0008,  ...,  0.0016, -0.0010, -0.0077]])\n",
            "   Last layer:\n",
            "tensor([[ 0.0142,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0032,  0.0092,\n",
            "          0.0037,  0.0029,  0.0000,  0.0000,  0.0000,  0.0000,  0.0054,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0072,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0016,  0.0047,\n",
            "          0.0019,  0.0014,  0.0000,  0.0000,  0.0000,  0.0000,  0.0027,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0045,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0010, -0.0030,\n",
            "         -0.0012, -0.0009,  0.0000,  0.0000,  0.0000,  0.0000, -0.0017,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0079,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0017, -0.0053,\n",
            "         -0.0020, -0.0014,  0.0000,  0.0000,  0.0000,  0.0000, -0.0030,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0197,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0045,  0.0128,\n",
            "          0.0051,  0.0040,  0.0000,  0.0000,  0.0000,  0.0000,  0.0075,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0155,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0035,  0.0100,\n",
            "          0.0041,  0.0032,  0.0000,  0.0000,  0.0000,  0.0000,  0.0059,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0049,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0011, -0.0031,\n",
            "         -0.0013, -0.0011,  0.0000,  0.0000,  0.0000,  0.0000, -0.0019,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0043,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0010, -0.0028,\n",
            "         -0.0011, -0.0009,  0.0000,  0.0000,  0.0000,  0.0000, -0.0016,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0048,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0011, -0.0033,\n",
            "         -0.0012, -0.0008,  0.0000,  0.0000,  0.0000,  0.0000, -0.0018,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0067,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0015,  0.0045,\n",
            "          0.0017,  0.0012,  0.0000,  0.0000,  0.0000,  0.0000,  0.0026,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000]])\n",
            "DEBUGGING: training for one iteration takes 0.008329 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 48\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0296, 0.0144, 0.0128, 0.0135, 0.0149, 0.0150, 0.0148, 0.0133, 0.0133,\n",
            "        0.0134, 0.0122, 0.0145, 0.0151, 0.0147, 0.0146, 0.0121, 0.0136, 0.0122,\n",
            "        0.0145, 0.0157, 0.0143, 0.0135, 0.0165, 0.0095, 0.0148, 0.0086, 0.0132,\n",
            "        0.0138, 0.0150, 0.0135, 0.0199, 0.0159, 0.0154, 0.0142, 0.0137, 0.0131,\n",
            "        0.0153, 0.0145, 0.0155, 0.0147, 0.0142, 0.0159, 0.0099, 0.0145, 0.0144,\n",
            "        0.0152, 0.0140, 0.0148, 0.0142, 0.0139, 0.0138, 0.0130, 0.0144, 0.0128,\n",
            "        0.0141, 0.0123, 0.0156, 0.0139, 0.0110, 0.0146, 0.0155, 0.0146, 0.0149,\n",
            "        0.0154, 0.0127, 0.0128, 0.0146, 0.0135, 0.0150, 0.0155],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [31]\n",
            "DEBUGGING: logits looks like: tensor([16.1416, 15.9975, 15.9748, 15.9843, 16.0042, 16.0057, 16.0027, 15.9814,\n",
            "        15.9815, 15.9834, 15.9649, 15.9984, 16.0069, 16.0015, 15.9997, 15.9624,\n",
            "        15.9867, 15.9642, 15.9990, 16.0153, 15.9969, 15.9849, 16.0245, 15.9148,\n",
            "        16.0037, 15.8947, 15.9808, 15.9892, 16.0055, 15.9850, 16.0624, 16.0178,\n",
            "        16.0115, 15.9950, 15.9879, 15.9791, 16.0094, 15.9991, 16.0125, 16.0016,\n",
            "        15.9955, 16.0169, 15.9218, 15.9985, 15.9972, 16.0090, 15.9922, 16.0034,\n",
            "        15.9955, 15.9901, 15.9892, 15.9771, 15.9972, 15.9735, 15.9927, 15.9662,\n",
            "        16.0135, 15.9906, 15.9430, 16.0007, 16.0120, 15.9998, 16.0043, 16.0105,\n",
            "        15.9717, 15.9745, 16.0001, 15.9852, 16.0055, 16.0128],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0280, 0.0132, 0.0133, 0.0130, 0.0137, 0.0134, 0.0131, 0.0131, 0.0131,\n",
            "        0.0131, 0.0129, 0.0129, 0.0132, 0.0130, 0.0134, 0.0130, 0.0134, 0.0132,\n",
            "        0.0133, 0.0135, 0.0130, 0.0132, 0.0136, 0.0130, 0.0134, 0.0128, 0.0129,\n",
            "        0.0132, 0.0131, 0.0128, 0.0131, 0.0129, 0.0127, 0.0134, 0.0135, 0.0138,\n",
            "        0.0125, 0.0130, 0.0130, 0.0134, 0.0133, 0.0131, 0.0129, 0.0127, 0.0131,\n",
            "        0.0126, 0.0131, 0.0132, 0.0128, 0.0130, 0.0131, 0.0131, 0.0131, 0.0138,\n",
            "        0.0135, 0.0128, 0.0131, 0.0130, 0.0141, 0.0124, 0.0129, 0.0121, 0.0134,\n",
            "        0.0125, 0.0132, 0.0131, 0.0138, 0.0129, 0.0132, 0.0133, 0.0132, 0.0135,\n",
            "        0.0131, 0.0131, 0.0132], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [38]\n",
            "DEBUGGING: logits looks like: tensor([16.1615, 16.0115, 16.0124, 16.0073, 16.0185, 16.0141, 16.0100, 16.0101,\n",
            "        16.0089, 16.0100, 16.0070, 16.0072, 16.0108, 16.0086, 16.0148, 16.0088,\n",
            "        16.0137, 16.0107, 16.0133, 16.0157, 16.0083, 16.0113, 16.0172, 16.0085,\n",
            "        16.0144, 16.0051, 16.0058, 16.0109, 16.0093, 16.0053, 16.0089, 16.0069,\n",
            "        16.0039, 16.0148, 16.0155, 16.0196, 16.0005, 16.0084, 16.0084, 16.0135,\n",
            "        16.0132, 16.0101, 16.0065, 16.0041, 16.0091, 16.0013, 16.0099, 16.0114,\n",
            "        16.0052, 16.0076, 16.0096, 16.0094, 16.0090, 16.0195, 16.0153, 16.0055,\n",
            "        16.0097, 16.0079, 16.0245, 15.9984, 16.0070, 15.9935, 16.0148, 16.0004,\n",
            "        16.0106, 16.0099, 16.0206, 16.0063, 16.0111, 16.0133, 16.0117, 16.0153,\n",
            "        16.0093, 16.0090, 16.0118], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0183, 0.0134, 0.0085, 0.0073, 0.0167, 0.0105, 0.0164, 0.0125, 0.0113,\n",
            "        0.0106, 0.0093, 0.0140, 0.0095, 0.0125, 0.0121, 0.0141, 0.0123, 0.0118,\n",
            "        0.0121, 0.0100, 0.0117, 0.0135, 0.0112, 0.0166, 0.0117, 0.0120, 0.0120,\n",
            "        0.0104, 0.0135, 0.0143, 0.0107, 0.0127, 0.0104, 0.0165, 0.0120, 0.0114,\n",
            "        0.0132, 0.0111, 0.0110, 0.0117, 0.0114, 0.0129, 0.0130, 0.0165, 0.0100,\n",
            "        0.0113, 0.0122, 0.0131, 0.0128, 0.0086, 0.0128, 0.0104, 0.0120, 0.0130,\n",
            "        0.0105, 0.0072, 0.0100, 0.0126, 0.0119, 0.0108, 0.0112, 0.0104, 0.0116,\n",
            "        0.0112, 0.0103, 0.0102, 0.0118, 0.0139, 0.0095, 0.0110, 0.0141, 0.0119,\n",
            "        0.0128, 0.0103, 0.0170, 0.0112, 0.0112, 0.0108, 0.0106, 0.0097, 0.0092,\n",
            "        0.0080, 0.0086, 0.0091, 0.0108], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [33]\n",
            "DEBUGGING: logits looks like: tensor([16.0778, 16.0158, 15.9250, 15.8928, 16.0594, 15.9663, 16.0563, 16.0014,\n",
            "        15.9811, 15.9683, 15.9425, 16.0242, 15.9459, 16.0014, 15.9954, 16.0255,\n",
            "        15.9980, 15.9899, 15.9948, 15.9569, 15.9880, 16.0165, 15.9796, 16.0580,\n",
            "        15.9883, 15.9928, 15.9933, 15.9641, 16.0163, 16.0290, 15.9714, 16.0048,\n",
            "        15.9642, 16.0570, 15.9932, 15.9831, 16.0117, 15.9773, 15.9758, 15.9891,\n",
            "        15.9824, 16.0086, 16.0090, 16.0573, 15.9570, 15.9810, 15.9973, 16.0106,\n",
            "        16.0066, 15.9271, 16.0070, 15.9652, 15.9941, 16.0087, 15.9676, 15.8899,\n",
            "        15.9576, 16.0034, 15.9922, 15.9719, 15.9802, 15.9643, 15.9874, 15.9794,\n",
            "        15.9631, 15.9604, 15.9904, 16.0231, 15.9467, 15.9754, 16.0255, 15.9919,\n",
            "        16.0059, 15.9635, 16.0627, 15.9803, 15.9798, 15.9715, 15.9684, 15.9510,\n",
            "        15.9409, 15.9132, 15.9258, 15.9372, 15.9726], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0100, 0.0108, 0.0106, 0.0102, 0.0103, 0.0109, 0.0109, 0.0108, 0.0106,\n",
            "        0.0107, 0.0131, 0.0109, 0.0109, 0.0106, 0.0107, 0.0109, 0.0107, 0.0107,\n",
            "        0.0104, 0.0107, 0.0104, 0.0121, 0.0110, 0.0121, 0.0112, 0.0108, 0.0107,\n",
            "        0.0108, 0.0105, 0.0106, 0.0105, 0.0108, 0.0108, 0.0107, 0.0107, 0.0104,\n",
            "        0.0101, 0.0093, 0.0107, 0.0108, 0.0109, 0.0107, 0.0106, 0.0107, 0.0108,\n",
            "        0.0110, 0.0112, 0.0111, 0.0112, 0.0108, 0.0104, 0.0108, 0.0108, 0.0110,\n",
            "        0.0112, 0.0113, 0.0107, 0.0110, 0.0106, 0.0104, 0.0104, 0.0106, 0.0108,\n",
            "        0.0105, 0.0107, 0.0113, 0.0104, 0.0106, 0.0112, 0.0106, 0.0105, 0.0109,\n",
            "        0.0108, 0.0106, 0.0113, 0.0106, 0.0107, 0.0106, 0.0109, 0.0109, 0.0103,\n",
            "        0.0115, 0.0098, 0.0105, 0.0108, 0.0109, 0.0107, 0.0107, 0.0105, 0.0100,\n",
            "        0.0107, 0.0106, 0.0106], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [31]\n",
            "DEBUGGING: logits looks like: tensor([16.0073, 16.0231, 16.0195, 16.0119, 16.0139, 16.0253, 16.0243, 16.0224,\n",
            "        16.0193, 16.0209, 16.0614, 16.0250, 16.0247, 16.0190, 16.0213, 16.0244,\n",
            "        16.0204, 16.0207, 16.0150, 16.0220, 16.0146, 16.0457, 16.0259, 16.0450,\n",
            "        16.0300, 16.0232, 16.0219, 16.0231, 16.0173, 16.0192, 16.0174, 16.0232,\n",
            "        16.0226, 16.0212, 16.0206, 16.0159, 16.0106, 15.9939, 16.0216, 16.0230,\n",
            "        16.0247, 16.0220, 16.0202, 16.0208, 16.0227, 16.0267, 16.0308, 16.0292,\n",
            "        16.0296, 16.0230, 16.0154, 16.0221, 16.0230, 16.0267, 16.0306, 16.0318,\n",
            "        16.0205, 16.0276, 16.0200, 16.0147, 16.0151, 16.0200, 16.0232, 16.0172,\n",
            "        16.0218, 16.0317, 16.0158, 16.0184, 16.0297, 16.0200, 16.0173, 16.0246,\n",
            "        16.0233, 16.0195, 16.0328, 16.0186, 16.0221, 16.0199, 16.0246, 16.0256,\n",
            "        16.0136, 16.0362, 16.0036, 16.0172, 16.0228, 16.0245, 16.0212, 16.0217,\n",
            "        16.0180, 16.0073, 16.0212, 16.0188, 16.0193], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0096, 0.0082, 0.0082, 0.0084, 0.0108, 0.0107, 0.0108, 0.0097, 0.0085,\n",
            "        0.0089, 0.0083, 0.0099, 0.0091, 0.0086, 0.0096, 0.0091, 0.0091, 0.0096,\n",
            "        0.0084, 0.0096, 0.0086, 0.0091, 0.0095, 0.0093, 0.0094, 0.0090, 0.0097,\n",
            "        0.0098, 0.0082, 0.0091, 0.0098, 0.0096, 0.0099, 0.0090, 0.0101, 0.0092,\n",
            "        0.0085, 0.0092, 0.0102, 0.0093, 0.0079, 0.0093, 0.0101, 0.0107, 0.0092,\n",
            "        0.0097, 0.0096, 0.0095, 0.0096, 0.0090, 0.0093, 0.0098, 0.0099, 0.0091,\n",
            "        0.0094, 0.0089, 0.0093, 0.0093, 0.0095, 0.0097, 0.0093, 0.0091, 0.0094,\n",
            "        0.0095, 0.0093, 0.0087, 0.0089, 0.0107, 0.0095, 0.0092, 0.0101, 0.0086,\n",
            "        0.0095, 0.0092, 0.0096, 0.0094, 0.0096, 0.0094, 0.0095, 0.0092, 0.0091,\n",
            "        0.0093, 0.0092, 0.0098, 0.0096, 0.0095, 0.0088, 0.0090, 0.0083, 0.0096,\n",
            "        0.0094, 0.0089, 0.0092, 0.0088, 0.0101, 0.0090, 0.0091, 0.0095, 0.0092,\n",
            "        0.0096, 0.0110, 0.0085, 0.0099, 0.0090, 0.0089, 0.0095, 0.0098],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [29]\n",
            "DEBUGGING: logits looks like: tensor([16.0556, 16.0238, 16.0235, 16.0294, 16.0787, 16.0775, 16.0800, 16.0577,\n",
            "        16.0302, 16.0400, 16.0263, 16.0613, 16.0453, 16.0344, 16.0547, 16.0447,\n",
            "        16.0447, 16.0550, 16.0296, 16.0553, 16.0329, 16.0451, 16.0533, 16.0493,\n",
            "        16.0519, 16.0421, 16.0568, 16.0595, 16.0235, 16.0458, 16.0592, 16.0555,\n",
            "        16.0622, 16.0424, 16.0656, 16.0471, 16.0323, 16.0480, 16.0677, 16.0495,\n",
            "        16.0178, 16.0491, 16.0668, 16.0778, 16.0480, 16.0568, 16.0567, 16.0533,\n",
            "        16.0561, 16.0436, 16.0490, 16.0593, 16.0627, 16.0452, 16.0523, 16.0414,\n",
            "        16.0496, 16.0497, 16.0535, 16.0587, 16.0503, 16.0441, 16.0520, 16.0533,\n",
            "        16.0495, 16.0351, 16.0403, 16.0774, 16.0535, 16.0468, 16.0667, 16.0343,\n",
            "        16.0541, 16.0469, 16.0565, 16.0506, 16.0561, 16.0511, 16.0536, 16.0472,\n",
            "        16.0459, 16.0493, 16.0478, 16.0595, 16.0562, 16.0531, 16.0375, 16.0434,\n",
            "        16.0275, 16.0565, 16.0515, 16.0413, 16.0471, 16.0380, 16.0661, 16.0424,\n",
            "        16.0447, 16.0539, 16.0475, 16.0556, 16.0824, 16.0316, 16.0617, 16.0438,\n",
            "        16.0415, 16.0540, 16.0608], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.8867413289817705 and immediate abs rewards look like: [0.010429303515593347, 0.014901368266691861, 0.0132388987772174, 0.013195728593473177, 9.595442452337011e-05, 0.004164279390806769, 0.017150030640095792, 0.003429095976116514, 0.0015604153522872366, 0.002335302197934652, 0.005687419099103863, 0.000540641884526849, 0.00314891668222117, 0.0038469580608762044, 0.0004800575152330566, 0.0010287784043612191, 0.000863313619447581, 0.00312069628489553, 0.00011838875798275694, 0.0016725816990401654, 1.0525764992053155e-05, 0.001157320018592145, 5.4855787311680615e-05, 0.0016940925061135204, 0.00017206615848408546, 0.00021044546929260832, 0.0008290525715892727, 0.00039073865445971023, 0.781214102899412, 0.0, 0.0, 0.0, 9.094947017729282e-13, 9.094947017729282e-13, 4.547473508864641e-13, 1.3642420526593924e-12, 9.094947017729282e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 0.0, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0]\n",
            "DEBUGGING: the total relative reward of the trajectory = 8.411557778952671 and immediate relative rewards look like: [0.0036128292517656558, 0.010361440716310217, 0.013880110578273267, 0.01853220902776598, 0.00016923322016495617, 0.008813662546680272, 0.04240984370459394, 0.009750178277912185, 0.0049975285271953804, 0.008314896896884562, 0.02229374140134618, 0.0023165786829211117, 0.014619903002084166, 0.019256357602521416, 0.002578166436852506, 0.00589444462645732, 0.005257491858612957, 0.020128871932428287, 0.0008069477659239705, 0.012000991819156787, 7.934753581089019e-05, 0.009139833137758627, 0.0004530987596807941, 0.014601568272822347, 0.0015457915057597153, 0.001966324665984681, 0.008044906109061738, 0.003933226661613142, 8.145798254246202, 0.0, 0.0, 0.0, 1.5006662579253316e-11, 1.546140993014681e-11, 7.958078640513122e-12, 2.4556356947874646e-11, 1.682565198279152e-11, 0.0, 8.86757334228605e-12, 9.094947017727214e-12, 0.0, 0.0, 9.777068044058979e-12, 0.0, 2.0463630789895538e-11, 1.0459189070386297e-11, 1.0686562745831907e-11, 2.182787284255524e-11, 1.1141310096715838e-11, 0.0]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0178, 0.0147, 0.0154, 0.0147, 0.0148, 0.0155, 0.0157, 0.0150, 0.0151,\n",
            "        0.0151, 0.0149, 0.0141, 0.0148, 0.0154, 0.0162, 0.0153, 0.0137, 0.0146,\n",
            "        0.0150, 0.0168, 0.0131, 0.0153, 0.0141, 0.0158, 0.0148, 0.0148, 0.0140,\n",
            "        0.0147, 0.0144, 0.0144, 0.0140, 0.0150, 0.0149, 0.0155, 0.0136, 0.0152,\n",
            "        0.0137, 0.0142, 0.0147, 0.0162, 0.0132, 0.0155, 0.0153, 0.0168, 0.0141,\n",
            "        0.0163, 0.0134, 0.0149, 0.0152, 0.0144, 0.0158, 0.0154, 0.0133, 0.0145,\n",
            "        0.0148, 0.0134, 0.0151, 0.0194, 0.0145, 0.0148, 0.0151, 0.0143, 0.0143,\n",
            "        0.0144, 0.0150, 0.0145, 0.0157], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [63]\n",
            "DEBUGGING: logits looks like: tensor([15.9901, 15.9519, 15.9616, 15.9524, 15.9530, 15.9623, 15.9658, 15.9566,\n",
            "        15.9580, 15.9576, 15.9552, 15.9432, 15.9533, 15.9612, 15.9710, 15.9601,\n",
            "        15.9374, 15.9504, 15.9560, 15.9787, 15.9288, 15.9605, 15.9439, 15.9664,\n",
            "        15.9529, 15.9535, 15.9429, 15.9521, 15.9479, 15.9476, 15.9427, 15.9556,\n",
            "        15.9548, 15.9623, 15.9369, 15.9582, 15.9383, 15.9448, 15.9527, 15.9709,\n",
            "        15.9305, 15.9627, 15.9595, 15.9784, 15.9439, 15.9724, 15.9339, 15.9543,\n",
            "        15.9582, 15.9482, 15.9661, 15.9613, 15.9318, 15.9499, 15.9535, 15.9339,\n",
            "        15.9574, 16.0077, 15.9499, 15.9532, 15.9574, 15.9462, 15.9464, 15.9476,\n",
            "        15.9556, 15.9494, 15.9656], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0128, 0.0123, 0.0112, 0.0133, 0.0129, 0.0152, 0.0124, 0.0130, 0.0133,\n",
            "        0.0097, 0.0145, 0.0116, 0.0119, 0.0134, 0.0123, 0.0129, 0.0145, 0.0124,\n",
            "        0.0133, 0.0140, 0.0109, 0.0105, 0.0128, 0.0125, 0.0122, 0.0118, 0.0143,\n",
            "        0.0118, 0.0125, 0.0142, 0.0148, 0.0126, 0.0106, 0.0134, 0.0120, 0.0127,\n",
            "        0.0114, 0.0116, 0.0124, 0.0127, 0.0103, 0.0113, 0.0134, 0.0123, 0.0112,\n",
            "        0.0125, 0.0125, 0.0125, 0.0120, 0.0149, 0.0110, 0.0159, 0.0119, 0.0147,\n",
            "        0.0114, 0.0134, 0.0122, 0.0103, 0.0132, 0.0133, 0.0125, 0.0110, 0.0145,\n",
            "        0.0140, 0.0130, 0.0144, 0.0114, 0.0123, 0.0138, 0.0128, 0.0122, 0.0158,\n",
            "        0.0142, 0.0149, 0.0122, 0.0125, 0.0108, 0.0111, 0.0122],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [29]\n",
            "DEBUGGING: logits looks like: tensor([15.9354, 15.9267, 15.9089, 15.9435, 15.9371, 15.9704, 15.9294, 15.9392,\n",
            "        15.9430, 15.8796, 15.9606, 15.9164, 15.9205, 15.9442, 15.9274, 15.9368,\n",
            "        15.9598, 15.9294, 15.9426, 15.9532, 15.9038, 15.8953, 15.9353, 15.9314,\n",
            "        15.9255, 15.9194, 15.9569, 15.9194, 15.9300, 15.9567, 15.9639, 15.9320,\n",
            "        15.8968, 15.9447, 15.9217, 15.9333, 15.9117, 15.9154, 15.9287, 15.9344,\n",
            "        15.8925, 15.9096, 15.9438, 15.9266, 15.9082, 15.9305, 15.9303, 15.9307,\n",
            "        15.9227, 15.9662, 15.9053, 15.9788, 15.9202, 15.9627, 15.9123, 15.9450,\n",
            "        15.9253, 15.8914, 15.9420, 15.9426, 15.9304, 15.9046, 15.9604, 15.9528,\n",
            "        15.9382, 15.9592, 15.9126, 15.9271, 15.9502, 15.9350, 15.9262, 15.9780,\n",
            "        15.9562, 15.9652, 15.9251, 15.9302, 15.9018, 15.9065, 15.9253],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0101, 0.0119, 0.0114, 0.0118, 0.0126, 0.0138, 0.0109, 0.0124, 0.0109,\n",
            "        0.0107, 0.0131, 0.0119, 0.0093, 0.0103, 0.0104, 0.0124, 0.0105, 0.0111,\n",
            "        0.0145, 0.0126, 0.0113, 0.0135, 0.0126, 0.0131, 0.0122, 0.0111, 0.0150,\n",
            "        0.0100, 0.0102, 0.0113, 0.0102, 0.0112, 0.0122, 0.0117, 0.0141, 0.0106,\n",
            "        0.0114, 0.0125, 0.0132, 0.0100, 0.0105, 0.0139, 0.0100, 0.0093, 0.0105,\n",
            "        0.0123, 0.0134, 0.0115, 0.0128, 0.0083, 0.0113, 0.0099, 0.0091, 0.0112,\n",
            "        0.0118, 0.0115, 0.0105, 0.0097, 0.0128, 0.0122, 0.0121, 0.0118, 0.0118,\n",
            "        0.0130, 0.0113, 0.0112, 0.0121, 0.0097, 0.0101, 0.0112, 0.0133, 0.0088,\n",
            "        0.0090, 0.0102, 0.0101, 0.0087, 0.0102, 0.0141, 0.0124, 0.0119, 0.0126,\n",
            "        0.0121, 0.0094, 0.0126, 0.0127, 0.0122, 0.0126],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [55]\n",
            "DEBUGGING: logits looks like: tensor([15.9696, 16.0020, 15.9927, 15.9998, 16.0128, 16.0314, 15.9836, 16.0097,\n",
            "        15.9840, 15.9815, 16.0211, 16.0020, 15.9524, 15.9741, 15.9760, 16.0105,\n",
            "        15.9773, 15.9881, 16.0412, 16.0130, 15.9922, 16.0268, 16.0141, 16.0215,\n",
            "        16.0069, 15.9883, 16.0480, 15.9673, 15.9717, 15.9925, 15.9717, 15.9907,\n",
            "        16.0066, 15.9986, 16.0360, 15.9784, 15.9931, 16.0115, 16.0224, 15.9663,\n",
            "        15.9766, 16.0331, 15.9665, 15.9527, 15.9778, 16.0089, 16.0258, 15.9952,\n",
            "        16.0158, 15.9310, 15.9916, 15.9649, 15.9480, 15.9908, 15.9998, 15.9955,\n",
            "        15.9776, 15.9609, 16.0173, 16.0065, 16.0059, 16.0004, 16.0007, 16.0204,\n",
            "        15.9911, 15.9904, 16.0053, 15.9616, 15.9696, 15.9902, 16.0246, 15.9426,\n",
            "        15.9460, 15.9720, 15.9695, 15.9389, 15.9711, 16.0358, 16.0102, 16.0019,\n",
            "        16.0130, 16.0060, 15.9552, 16.0141, 16.0149, 16.0071, 16.0136],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0102, 0.0104, 0.0107, 0.0100, 0.0099, 0.0101, 0.0104, 0.0101, 0.0107,\n",
            "        0.0100, 0.0099, 0.0100, 0.0102, 0.0103, 0.0102, 0.0103, 0.0101, 0.0107,\n",
            "        0.0106, 0.0099, 0.0105, 0.0098, 0.0099, 0.0106, 0.0102, 0.0105, 0.0106,\n",
            "        0.0101, 0.0101, 0.0109, 0.0108, 0.0098, 0.0102, 0.0100, 0.0103, 0.0105,\n",
            "        0.0105, 0.0107, 0.0100, 0.0104, 0.0103, 0.0098, 0.0104, 0.0100, 0.0107,\n",
            "        0.0102, 0.0102, 0.0097, 0.0105, 0.0104, 0.0100, 0.0104, 0.0104, 0.0106,\n",
            "        0.0106, 0.0100, 0.0099, 0.0108, 0.0108, 0.0104, 0.0100, 0.0100, 0.0101,\n",
            "        0.0101, 0.0103, 0.0099, 0.0104, 0.0106, 0.0100, 0.0102, 0.0103, 0.0101,\n",
            "        0.0105, 0.0108, 0.0100, 0.0101, 0.0105, 0.0105, 0.0103, 0.0101, 0.0108,\n",
            "        0.0106, 0.0104, 0.0103, 0.0104, 0.0108, 0.0099, 0.0104, 0.0106, 0.0100,\n",
            "        0.0105, 0.0108, 0.0106, 0.0104, 0.0104, 0.0103, 0.0102],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [79]\n",
            "DEBUGGING: logits looks like: tensor([15.9727, 15.9769, 15.9822, 15.9696, 15.9672, 15.9717, 15.9766, 15.9713,\n",
            "        15.9825, 15.9687, 15.9675, 15.9684, 15.9736, 15.9754, 15.9731, 15.9757,\n",
            "        15.9716, 15.9826, 15.9809, 15.9666, 15.9780, 15.9648, 15.9669, 15.9801,\n",
            "        15.9723, 15.9785, 15.9804, 15.9700, 15.9714, 15.9854, 15.9849, 15.9647,\n",
            "        15.9736, 15.9693, 15.9745, 15.9783, 15.9784, 15.9825, 15.9680, 15.9772,\n",
            "        15.9743, 15.9654, 15.9759, 15.9695, 15.9814, 15.9730, 15.9734, 15.9633,\n",
            "        15.9785, 15.9766, 15.9693, 15.9759, 15.9770, 15.9805, 15.9801, 15.9692,\n",
            "        15.9678, 15.9837, 15.9850, 15.9770, 15.9689, 15.9697, 15.9711, 15.9713,\n",
            "        15.9752, 15.9671, 15.9770, 15.9809, 15.9681, 15.9736, 15.9745, 15.9713,\n",
            "        15.9789, 15.9843, 15.9682, 15.9702, 15.9789, 15.9788, 15.9743, 15.9714,\n",
            "        15.9839, 15.9813, 15.9770, 15.9744, 15.9759, 15.9838, 15.9670, 15.9761,\n",
            "        15.9813, 15.9693, 15.9791, 15.9843, 15.9807, 15.9761, 15.9773, 15.9748,\n",
            "        15.9726], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0093, 0.0092, 0.0094, 0.0094, 0.0090, 0.0086, 0.0093, 0.0091, 0.0094,\n",
            "        0.0089, 0.0092, 0.0092, 0.0089, 0.0089, 0.0096, 0.0094, 0.0091, 0.0095,\n",
            "        0.0092, 0.0088, 0.0093, 0.0095, 0.0095, 0.0090, 0.0092, 0.0092, 0.0096,\n",
            "        0.0096, 0.0086, 0.0093, 0.0091, 0.0083, 0.0092, 0.0090, 0.0091, 0.0093,\n",
            "        0.0091, 0.0089, 0.0091, 0.0088, 0.0093, 0.0093, 0.0092, 0.0092, 0.0089,\n",
            "        0.0094, 0.0090, 0.0093, 0.0094, 0.0091, 0.0093, 0.0093, 0.0091, 0.0090,\n",
            "        0.0092, 0.0095, 0.0090, 0.0092, 0.0090, 0.0095, 0.0099, 0.0088, 0.0091,\n",
            "        0.0092, 0.0093, 0.0089, 0.0091, 0.0093, 0.0092, 0.0094, 0.0093, 0.0091,\n",
            "        0.0097, 0.0092, 0.0091, 0.0089, 0.0091, 0.0091, 0.0091, 0.0092, 0.0094,\n",
            "        0.0086, 0.0091, 0.0094, 0.0091, 0.0089, 0.0093, 0.0093, 0.0091, 0.0095,\n",
            "        0.0089, 0.0093, 0.0093, 0.0088, 0.0097, 0.0094, 0.0091, 0.0094, 0.0092,\n",
            "        0.0090, 0.0091, 0.0094, 0.0091, 0.0094, 0.0090, 0.0094, 0.0090, 0.0086,\n",
            "        0.0090], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [35]\n",
            "DEBUGGING: logits looks like: tensor([15.9709, 15.9680, 15.9712, 15.9711, 15.9632, 15.9537, 15.9705, 15.9664,\n",
            "        15.9719, 15.9610, 15.9671, 15.9682, 15.9622, 15.9602, 15.9761, 15.9721,\n",
            "        15.9646, 15.9735, 15.9689, 15.9598, 15.9706, 15.9748, 15.9733, 15.9638,\n",
            "        15.9680, 15.9685, 15.9759, 15.9759, 15.9547, 15.9690, 15.9666, 15.9474,\n",
            "        15.9667, 15.9624, 15.9663, 15.9692, 15.9650, 15.9612, 15.9647, 15.9595,\n",
            "        15.9708, 15.9692, 15.9687, 15.9670, 15.9601, 15.9713, 15.9626, 15.9706,\n",
            "        15.9711, 15.9651, 15.9699, 15.9704, 15.9652, 15.9641, 15.9683, 15.9732,\n",
            "        15.9640, 15.9685, 15.9629, 15.9752, 15.9821, 15.9585, 15.9662, 15.9686,\n",
            "        15.9697, 15.9613, 15.9651, 15.9699, 15.9668, 15.9718, 15.9707, 15.9646,\n",
            "        15.9782, 15.9676, 15.9662, 15.9615, 15.9659, 15.9657, 15.9652, 15.9675,\n",
            "        15.9731, 15.9535, 15.9664, 15.9720, 15.9655, 15.9605, 15.9699, 15.9701,\n",
            "        15.9661, 15.9743, 15.9604, 15.9700, 15.9707, 15.9598, 15.9794, 15.9712,\n",
            "        15.9646, 15.9722, 15.9673, 15.9629, 15.9657, 15.9724, 15.9658, 15.9723,\n",
            "        15.9638, 15.9726, 15.9625, 15.9546, 15.9640], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.10131109042185926 and immediate abs rewards look like: [0.0027557819103094516, 0.005205815304634598, 0.0006096725464885822, 0.0023913070972412243, 0.002112913710789144, 0.005291541468977812, 0.00027918598607357126, 0.005090378388558747, 0.004822622992833203, 0.007770181100113405, 0.0008420266713073943, 0.0005965107725387497, 0.00035474307878757827, 0.0035271630258648656, 0.0007319121918953897, 0.0008242216777034628, 0.05810511249183037, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 9.094947017729282e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.42875641322307123 and immediate relative rewards look like: [0.0008885860947086886, 0.0033601564511980537, 0.0005912741652651333, 0.0030928008638439596, 0.003418568652564346, 0.010280708121959711, 0.0006339076757505323, 0.013210332226895086, 0.014103186460661605, 0.02528736919313721, 0.0030219734802570818, 0.0023360975860020894, 0.0015053354220018861, 0.016120550146631458, 0.0035882020898512124, 0.0043111642678191125, 0.32300620025328025, 0.0, 2.8800665556142726e-12, 3.0316490059102206e-12, 0.0, 3.3348139065007367e-12, 0.0, 0.0, 0.0, 3.9411437076832865e-12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.305385760342081e-12, 0.0, 5.608550660933908e-12, 0.0, 5.9117155615240335e-12, 6.0632980118186026e-12, 6.214880462115009e-12, 6.366462912409532e-12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.427540064478913e-12, 1.5158245029546504e-11]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0145, 0.0161, 0.0132, 0.0150, 0.0151, 0.0156, 0.0135, 0.0144, 0.0141,\n",
            "        0.0143, 0.0143, 0.0155, 0.0141, 0.0161, 0.0148, 0.0136, 0.0139, 0.0153,\n",
            "        0.0151, 0.0151, 0.0149, 0.0146, 0.0149, 0.0157, 0.0147, 0.0158, 0.0143,\n",
            "        0.0147, 0.0151, 0.0145, 0.0155, 0.0149, 0.0144, 0.0155, 0.0149, 0.0149,\n",
            "        0.0146, 0.0139, 0.0143, 0.0158, 0.0140, 0.0150, 0.0147, 0.0164, 0.0143,\n",
            "        0.0153, 0.0154, 0.0150, 0.0143, 0.0135, 0.0140, 0.0140, 0.0146, 0.0156,\n",
            "        0.0151, 0.0139, 0.0147, 0.0136, 0.0144, 0.0141, 0.0146, 0.0152, 0.0131,\n",
            "        0.0154, 0.0155, 0.0141, 0.0141, 0.0143], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [61]\n",
            "DEBUGGING: logits looks like: tensor([16.0741, 16.0957, 16.0563, 16.0818, 16.0826, 16.0896, 16.0609, 16.0733,\n",
            "        16.0689, 16.0714, 16.0717, 16.0883, 16.0690, 16.0950, 16.0786, 16.0622,\n",
            "        16.0666, 16.0852, 16.0824, 16.0821, 16.0801, 16.0764, 16.0799, 16.0905,\n",
            "        16.0780, 16.0915, 16.0723, 16.0779, 16.0828, 16.0749, 16.0882, 16.0794,\n",
            "        16.0734, 16.0883, 16.0805, 16.0805, 16.0753, 16.0664, 16.0714, 16.0917,\n",
            "        16.0673, 16.0811, 16.0768, 16.0995, 16.0714, 16.0858, 16.0872, 16.0814,\n",
            "        16.0716, 16.0601, 16.0671, 16.0681, 16.0759, 16.0887, 16.0830, 16.0663,\n",
            "        16.0773, 16.0624, 16.0737, 16.0687, 16.0765, 16.0839, 16.0544, 16.0868,\n",
            "        16.0881, 16.0687, 16.0689, 16.0716], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0130, 0.0151, 0.0116, 0.0124, 0.0141, 0.0132, 0.0149, 0.0142, 0.0133,\n",
            "        0.0125, 0.0118, 0.0122, 0.0132, 0.0143, 0.0145, 0.0122, 0.0139, 0.0125,\n",
            "        0.0139, 0.0133, 0.0140, 0.0120, 0.0139, 0.0148, 0.0125, 0.0121, 0.0176,\n",
            "        0.0129, 0.0129, 0.0126, 0.0156, 0.0153, 0.0150, 0.0128, 0.0142, 0.0130,\n",
            "        0.0116, 0.0123, 0.0120, 0.0136, 0.0125, 0.0119, 0.0147, 0.0145, 0.0118,\n",
            "        0.0119, 0.0148, 0.0136, 0.0119, 0.0125, 0.0153, 0.0134, 0.0152, 0.0112,\n",
            "        0.0121, 0.0157, 0.0130, 0.0135, 0.0117, 0.0125, 0.0124, 0.0119, 0.0110,\n",
            "        0.0116, 0.0131, 0.0129, 0.0128, 0.0133, 0.0116, 0.0122, 0.0122, 0.0123,\n",
            "        0.0142, 0.0120, 0.0119, 0.0139], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [7]\n",
            "DEBUGGING: logits looks like: tensor([16.0803, 16.1092, 16.0563, 16.0709, 16.0960, 16.0825, 16.1064, 16.0967,\n",
            "        16.0849, 16.0724, 16.0604, 16.0669, 16.0833, 16.0989, 16.1012, 16.0671,\n",
            "        16.0926, 16.0721, 16.0934, 16.0843, 16.0951, 16.0641, 16.0926, 16.1056,\n",
            "        16.0721, 16.0659, 16.1400, 16.0780, 16.0778, 16.0736, 16.1166, 16.1120,\n",
            "        16.1080, 16.0771, 16.0969, 16.0804, 16.0564, 16.0681, 16.0631, 16.0889,\n",
            "        16.0713, 16.0615, 16.1043, 16.1011, 16.0604, 16.0613, 16.1054, 16.0890,\n",
            "        16.0617, 16.0716, 16.1128, 16.0855, 16.1109, 16.0503, 16.0660, 16.1174,\n",
            "        16.0799, 16.0876, 16.0578, 16.0710, 16.0696, 16.0626, 16.0458, 16.0570,\n",
            "        16.0818, 16.0786, 16.0772, 16.0842, 16.0573, 16.0665, 16.0676, 16.0681,\n",
            "        16.0974, 16.0631, 16.0627, 16.0927], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0111, 0.0113, 0.0113, 0.0115, 0.0114, 0.0113, 0.0120, 0.0111, 0.0113,\n",
            "        0.0115, 0.0114, 0.0114, 0.0114, 0.0112, 0.0113, 0.0116, 0.0113, 0.0115,\n",
            "        0.0120, 0.0113, 0.0118, 0.0115, 0.0114, 0.0111, 0.0117, 0.0114, 0.0115,\n",
            "        0.0116, 0.0114, 0.0117, 0.0114, 0.0119, 0.0118, 0.0116, 0.0114, 0.0113,\n",
            "        0.0114, 0.0113, 0.0114, 0.0119, 0.0113, 0.0118, 0.0119, 0.0118, 0.0116,\n",
            "        0.0116, 0.0117, 0.0116, 0.0114, 0.0115, 0.0115, 0.0113, 0.0119, 0.0112,\n",
            "        0.0115, 0.0113, 0.0117, 0.0118, 0.0114, 0.0115, 0.0115, 0.0112, 0.0114,\n",
            "        0.0112, 0.0115, 0.0114, 0.0113, 0.0116, 0.0116, 0.0120, 0.0117, 0.0115,\n",
            "        0.0115, 0.0120, 0.0114, 0.0111, 0.0112, 0.0112, 0.0116, 0.0114, 0.0113,\n",
            "        0.0115, 0.0115, 0.0118, 0.0115, 0.0112, 0.0115],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [17]\n",
            "DEBUGGING: logits looks like: tensor([16.0892, 16.0913, 16.0915, 16.0954, 16.0947, 16.0923, 16.1040, 16.0884,\n",
            "        16.0916, 16.0956, 16.0930, 16.0943, 16.0936, 16.0900, 16.0926, 16.0980,\n",
            "        16.0925, 16.0950, 16.1034, 16.0914, 16.1007, 16.0960, 16.0937, 16.0891,\n",
            "        16.0990, 16.0933, 16.0962, 16.0976, 16.0935, 16.0995, 16.0939, 16.1029,\n",
            "        16.1011, 16.0974, 16.0936, 16.0925, 16.0946, 16.0923, 16.0942, 16.1028,\n",
            "        16.0929, 16.0999, 16.1017, 16.1001, 16.0972, 16.0978, 16.0982, 16.0965,\n",
            "        16.0945, 16.0954, 16.0958, 16.0920, 16.1021, 16.0909, 16.0962, 16.0928,\n",
            "        16.0985, 16.1007, 16.0943, 16.0958, 16.0959, 16.0904, 16.0940, 16.0909,\n",
            "        16.0956, 16.0940, 16.0927, 16.0967, 16.0980, 16.1037, 16.0988, 16.0962,\n",
            "        16.0963, 16.1041, 16.0945, 16.0889, 16.0900, 16.0898, 16.0966, 16.0941,\n",
            "        16.0928, 16.0960, 16.0952, 16.0999, 16.0958, 16.0902, 16.0958],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0102, 0.0103, 0.0104, 0.0103, 0.0100, 0.0104, 0.0103, 0.0102, 0.0102,\n",
            "        0.0102, 0.0104, 0.0103, 0.0103, 0.0101, 0.0104, 0.0101, 0.0101, 0.0101,\n",
            "        0.0103, 0.0104, 0.0104, 0.0105, 0.0104, 0.0102, 0.0102, 0.0105, 0.0102,\n",
            "        0.0104, 0.0103, 0.0104, 0.0103, 0.0105, 0.0103, 0.0102, 0.0101, 0.0103,\n",
            "        0.0103, 0.0104, 0.0102, 0.0104, 0.0103, 0.0099, 0.0102, 0.0103, 0.0101,\n",
            "        0.0105, 0.0107, 0.0103, 0.0105, 0.0104, 0.0101, 0.0108, 0.0104, 0.0101,\n",
            "        0.0105, 0.0102, 0.0103, 0.0103, 0.0102, 0.0103, 0.0103, 0.0102, 0.0104,\n",
            "        0.0104, 0.0106, 0.0105, 0.0104, 0.0099, 0.0105, 0.0103, 0.0103, 0.0102,\n",
            "        0.0105, 0.0106, 0.0102, 0.0102, 0.0102, 0.0102, 0.0103, 0.0102, 0.0102,\n",
            "        0.0103, 0.0103, 0.0102, 0.0104, 0.0104, 0.0104, 0.0101, 0.0104, 0.0106,\n",
            "        0.0105, 0.0107, 0.0103, 0.0104, 0.0102, 0.0103, 0.0104],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [13]\n",
            "DEBUGGING: logits looks like: tensor([16.0929, 16.0945, 16.0962, 16.0954, 16.0897, 16.0972, 16.0942, 16.0937,\n",
            "        16.0936, 16.0930, 16.0965, 16.0950, 16.0953, 16.0907, 16.0968, 16.0915,\n",
            "        16.0915, 16.0908, 16.0960, 16.0973, 16.0962, 16.0985, 16.0979, 16.0927,\n",
            "        16.0926, 16.0984, 16.0933, 16.0975, 16.0947, 16.0963, 16.0958, 16.0982,\n",
            "        16.0943, 16.0928, 16.0918, 16.0949, 16.0949, 16.0964, 16.0928, 16.0964,\n",
            "        16.0944, 16.0877, 16.0936, 16.0946, 16.0918, 16.0996, 16.1023, 16.0943,\n",
            "        16.0990, 16.0963, 16.0920, 16.1045, 16.0973, 16.0909, 16.0982, 16.0932,\n",
            "        16.0953, 16.0955, 16.0928, 16.0955, 16.0945, 16.0923, 16.0973, 16.0971,\n",
            "        16.1006, 16.0990, 16.0961, 16.0880, 16.0982, 16.0946, 16.0958, 16.0936,\n",
            "        16.0986, 16.1004, 16.0938, 16.0932, 16.0924, 16.0936, 16.0951, 16.0933,\n",
            "        16.0940, 16.0954, 16.0954, 16.0933, 16.0969, 16.0975, 16.0964, 16.0920,\n",
            "        16.0972, 16.0999, 16.0985, 16.1027, 16.0948, 16.0969, 16.0930, 16.0941,\n",
            "        16.0967], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0102, 0.0100, 0.0098, 0.0097, 0.0099, 0.0099, 0.0100, 0.0102, 0.0097,\n",
            "        0.0102, 0.0098, 0.0098, 0.0101, 0.0105, 0.0098, 0.0095, 0.0100, 0.0096,\n",
            "        0.0101, 0.0100, 0.0101, 0.0102, 0.0101, 0.0099, 0.0102, 0.0101, 0.0097,\n",
            "        0.0096, 0.0097, 0.0103, 0.0099, 0.0099, 0.0093, 0.0096, 0.0098, 0.0100,\n",
            "        0.0100, 0.0101, 0.0101, 0.0100, 0.0099, 0.0098, 0.0098, 0.0098, 0.0099,\n",
            "        0.0096, 0.0102, 0.0105, 0.0097, 0.0101, 0.0102, 0.0103, 0.0099, 0.0093,\n",
            "        0.0099, 0.0097, 0.0099, 0.0099, 0.0098, 0.0099, 0.0099, 0.0097, 0.0099,\n",
            "        0.0101, 0.0098, 0.0100, 0.0100, 0.0099, 0.0100, 0.0102, 0.0098, 0.0100,\n",
            "        0.0097, 0.0096, 0.0092, 0.0097, 0.0103, 0.0098, 0.0099, 0.0096, 0.0094,\n",
            "        0.0098, 0.0100, 0.0093, 0.0101, 0.0099, 0.0101, 0.0100, 0.0105, 0.0094,\n",
            "        0.0101, 0.0100, 0.0097, 0.0097, 0.0101, 0.0097, 0.0100, 0.0098, 0.0096,\n",
            "        0.0096, 0.0106], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [12]\n",
            "DEBUGGING: logits looks like: tensor([16.0579, 16.0544, 16.0506, 16.0484, 16.0518, 16.0520, 16.0547, 16.0577,\n",
            "        16.0477, 16.0582, 16.0490, 16.0495, 16.0552, 16.0641, 16.0505, 16.0439,\n",
            "        16.0537, 16.0450, 16.0553, 16.0533, 16.0566, 16.0573, 16.0570, 16.0523,\n",
            "        16.0585, 16.0555, 16.0485, 16.0449, 16.0487, 16.0591, 16.0518, 16.0527,\n",
            "        16.0397, 16.0451, 16.0500, 16.0541, 16.0539, 16.0566, 16.0565, 16.0548,\n",
            "        16.0524, 16.0491, 16.0508, 16.0510, 16.0527, 16.0450, 16.0581, 16.0640,\n",
            "        16.0486, 16.0553, 16.0583, 16.0601, 16.0528, 16.0388, 16.0528, 16.0477,\n",
            "        16.0514, 16.0511, 16.0505, 16.0530, 16.0528, 16.0483, 16.0530, 16.0558,\n",
            "        16.0499, 16.0540, 16.0542, 16.0530, 16.0545, 16.0576, 16.0503, 16.0545,\n",
            "        16.0477, 16.0459, 16.0365, 16.0475, 16.0601, 16.0496, 16.0515, 16.0466,\n",
            "        16.0415, 16.0498, 16.0538, 16.0406, 16.0553, 16.0514, 16.0569, 16.0538,\n",
            "        16.0631, 16.0413, 16.0560, 16.0546, 16.0490, 16.0486, 16.0565, 16.0490,\n",
            "        16.0533, 16.0499, 16.0459, 16.0462, 16.0658], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.6072478073194816 and immediate abs rewards look like: [0.008648887428307717, 0.014646748271616161, 0.042850228653833256, 0.006788778977806942, 0.534313163971774, 1.1368683772161603e-12, 1.3642420526593924e-12, 4.547473508864641e-13, 2.2737367544323206e-13, 1.1368683772161603e-12, 6.821210263296962e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 0.0, 4.547473508864641e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 4.547473508864641e-13, 0.0, 9.094947017729282e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 0.0, 2.2737367544323206e-13, 6.821210263296962e-13, 9.094947017729282e-13, 6.821210263296962e-13, 2.2737367544323206e-13, 4.547473508864641e-13, 2.2737367544323206e-13, 0.0, 2.2737367544323206e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 2.2737367544323206e-13, 2.2737367544323206e-13, 0.0, 0.0, 2.2737367544323206e-13, 6.821210263296962e-13, 0.0, 4.547473508864641e-13, 2.2737367544323206e-13, 2.2737367544323206e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.8099687739621788 and immediate relative rewards look like: [0.0023976416066557654, 0.00814025046843827, 0.03586841559439562, 0.007668549606513306, 0.7558939165585434, 2.273736754432493e-12, 3.1832314562042835e-12, 1.2126596023640882e-12, 6.821210263296962e-13, 3.789561257387488e-12, 2.501110429874794e-12, 9.094947017728593e-13, 9.85285926920523e-13, 0.0, 2.273736754432148e-12, 1.2126596023639961e-12, 1.2884508275116482e-12, 1.3642420526594958e-12, 2.8800665556142726e-12, 0.0, 6.366462912409532e-12, 1.6674069532506214e-12, 1.7431981783982444e-12, 1.8189894035458565e-12, 1.894780628693457e-12, 1.9705718538413444e-12, 2.0463630789889334e-12, 0.0, 2.1979455292845764e-12, 6.821210263296445e-12, 9.398111918321684e-12, 7.275957614182323e-12, 2.501110429875742e-12, 5.153803310046593e-12, 2.652692880171443e-12, 0.0, 2.804275330466741e-12, 5.760133111228545e-12, 0.0, 6.0632980118186026e-12, 3.1074402310575047e-12, 3.1832314562050073e-12, 0.0, 0.0, 3.410605131648481e-12, 1.0459189070387881e-11, 0.0, 7.27595761418453e-12, 3.713770032239457e-12, 3.789561257387488e-12]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 3\n",
            "DEBUGGING: the action_prob is: tensor([0.0179, 0.0154, 0.0143, 0.0130, 0.0145, 0.0149, 0.0149, 0.0150, 0.0148,\n",
            "        0.0141, 0.0146, 0.0161, 0.0162, 0.0149, 0.0155, 0.0147, 0.0147, 0.0140,\n",
            "        0.0147, 0.0145, 0.0159, 0.0150, 0.0145, 0.0139, 0.0144, 0.0162, 0.0133,\n",
            "        0.0140, 0.0136, 0.0149, 0.0136, 0.0138, 0.0144, 0.0140, 0.0130, 0.0151,\n",
            "        0.0112, 0.0163, 0.0122, 0.0142, 0.0164, 0.0152, 0.0150, 0.0151, 0.0146,\n",
            "        0.0159, 0.0163, 0.0149, 0.0158, 0.0127, 0.0160, 0.0164, 0.0145, 0.0144,\n",
            "        0.0136, 0.0153, 0.0154, 0.0147, 0.0145, 0.0146, 0.0133, 0.0128, 0.0160,\n",
            "        0.0159, 0.0141, 0.0159, 0.0143, 0.0142], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [61]\n",
            "DEBUGGING: logits looks like: tensor([15.9937, 15.9632, 15.9483, 15.9298, 15.9510, 15.9573, 15.9563, 15.9576,\n",
            "        15.9555, 15.9464, 15.9532, 15.9724, 15.9740, 15.9564, 15.9647, 15.9546,\n",
            "        15.9548, 15.9438, 15.9538, 15.9508, 15.9694, 15.9584, 15.9512, 15.9426,\n",
            "        15.9499, 15.9731, 15.9348, 15.9449, 15.9381, 15.9572, 15.9389, 15.9422,\n",
            "        15.9500, 15.9445, 15.9291, 15.9593, 15.8998, 15.9748, 15.9173, 15.9478,\n",
            "        15.9766, 15.9602, 15.9576, 15.9597, 15.9529, 15.9699, 15.9754, 15.9570,\n",
            "        15.9689, 15.9255, 15.9709, 15.9762, 15.9508, 15.9504, 15.9380, 15.9625,\n",
            "        15.9641, 15.9540, 15.9521, 15.9530, 15.9336, 15.9265, 15.9712, 15.9696,\n",
            "        15.9457, 15.9695, 15.9492, 15.9479], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0159, 0.0138, 0.0135, 0.0133, 0.0127, 0.0132, 0.0138, 0.0125, 0.0128,\n",
            "        0.0127, 0.0129, 0.0130, 0.0128, 0.0128, 0.0135, 0.0130, 0.0129, 0.0127,\n",
            "        0.0131, 0.0131, 0.0127, 0.0133, 0.0124, 0.0134, 0.0128, 0.0129, 0.0132,\n",
            "        0.0129, 0.0133, 0.0131, 0.0128, 0.0133, 0.0131, 0.0126, 0.0128, 0.0130,\n",
            "        0.0127, 0.0130, 0.0134, 0.0129, 0.0125, 0.0127, 0.0127, 0.0130, 0.0130,\n",
            "        0.0128, 0.0128, 0.0130, 0.0130, 0.0130, 0.0128, 0.0128, 0.0136, 0.0129,\n",
            "        0.0129, 0.0125, 0.0132, 0.0123, 0.0134, 0.0128, 0.0131, 0.0130, 0.0129,\n",
            "        0.0130, 0.0124, 0.0126, 0.0128, 0.0129, 0.0128, 0.0131, 0.0128, 0.0132,\n",
            "        0.0128, 0.0138, 0.0128, 0.0119, 0.0130], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [58]\n",
            "DEBUGGING: logits looks like: tensor([15.9921, 15.9630, 15.9585, 15.9566, 15.9464, 15.9550, 15.9633, 15.9433,\n",
            "        15.9484, 15.9470, 15.9503, 15.9509, 15.9480, 15.9477, 15.9598, 15.9509,\n",
            "        15.9501, 15.9464, 15.9526, 15.9534, 15.9464, 15.9561, 15.9422, 15.9572,\n",
            "        15.9484, 15.9505, 15.9546, 15.9494, 15.9555, 15.9525, 15.9488, 15.9565,\n",
            "        15.9532, 15.9460, 15.9490, 15.9509, 15.9474, 15.9508, 15.9582, 15.9503,\n",
            "        15.9440, 15.9468, 15.9472, 15.9515, 15.9518, 15.9485, 15.9491, 15.9513,\n",
            "        15.9516, 15.9515, 15.9483, 15.9487, 15.9603, 15.9502, 15.9494, 15.9440,\n",
            "        15.9541, 15.9398, 15.9571, 15.9486, 15.9526, 15.9523, 15.9493, 15.9513,\n",
            "        15.9428, 15.9458, 15.9479, 15.9495, 15.9486, 15.9528, 15.9490, 15.9550,\n",
            "        15.9478, 15.9633, 15.9489, 15.9340, 15.9515], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0135, 0.0111, 0.0111, 0.0110, 0.0110, 0.0111, 0.0111, 0.0111, 0.0110,\n",
            "        0.0110, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0110, 0.0109, 0.0111, 0.0110, 0.0110, 0.0110, 0.0112, 0.0110, 0.0112,\n",
            "        0.0111, 0.0111, 0.0110, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0110,\n",
            "        0.0110, 0.0111, 0.0112, 0.0111, 0.0111, 0.0110, 0.0112, 0.0110, 0.0111,\n",
            "        0.0111, 0.0111, 0.0110, 0.0110, 0.0111, 0.0112, 0.0111, 0.0110, 0.0111,\n",
            "        0.0110, 0.0111, 0.0110, 0.0110, 0.0110, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0113, 0.0110, 0.0110, 0.0111, 0.0111, 0.0113, 0.0111, 0.0111, 0.0112,\n",
            "        0.0112, 0.0111, 0.0111, 0.0111, 0.0112, 0.0110, 0.0111, 0.0110, 0.0110,\n",
            "        0.0111, 0.0112, 0.0109, 0.0110, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [12]\n",
            "DEBUGGING: logits looks like: tensor([15.9917, 15.9527, 15.9525, 15.9504, 15.9506, 15.9521, 15.9511, 15.9525,\n",
            "        15.9506, 15.9501, 15.9511, 15.9520, 15.9515, 15.9521, 15.9519, 15.9514,\n",
            "        15.9521, 15.9511, 15.9506, 15.9490, 15.9513, 15.9509, 15.9505, 15.9507,\n",
            "        15.9533, 15.9495, 15.9537, 15.9521, 15.9510, 15.9510, 15.9523, 15.9516,\n",
            "        15.9528, 15.9519, 15.9518, 15.9505, 15.9510, 15.9518, 15.9533, 15.9519,\n",
            "        15.9511, 15.9508, 15.9544, 15.9506, 15.9525, 15.9525, 15.9524, 15.9503,\n",
            "        15.9506, 15.9524, 15.9544, 15.9524, 15.9503, 15.9519, 15.9503, 15.9513,\n",
            "        15.9501, 15.9493, 15.9495, 15.9522, 15.9516, 15.9511, 15.9525, 15.9550,\n",
            "        15.9507, 15.9499, 15.9520, 15.9528, 15.9558, 15.9519, 15.9519, 15.9530,\n",
            "        15.9539, 15.9520, 15.9517, 15.9513, 15.9537, 15.9510, 15.9518, 15.9497,\n",
            "        15.9506, 15.9524, 15.9537, 15.9476, 15.9504, 15.9521, 15.9520, 15.9518,\n",
            "        15.9526, 15.9520], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0102, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0102, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0104, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0104, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [37]\n",
            "DEBUGGING: logits looks like: tensor([15.9495, 15.9506, 15.9506, 15.9503, 15.9508, 15.9506, 15.9507, 15.9499,\n",
            "        15.9508, 15.9507, 15.9508, 15.9510, 15.9493, 15.9503, 15.9507, 15.9498,\n",
            "        15.9507, 15.9504, 15.9508, 15.9513, 15.9500, 15.9505, 15.9510, 15.9505,\n",
            "        15.9510, 15.9504, 15.9505, 15.9503, 15.9507, 15.9511, 15.9503, 15.9507,\n",
            "        15.9498, 15.9509, 15.9503, 15.9511, 15.9504, 15.9502, 15.9502, 15.9503,\n",
            "        15.9501, 15.9506, 15.9501, 15.9505, 15.9494, 15.9509, 15.9505, 15.9509,\n",
            "        15.9511, 15.9501, 15.9505, 15.9501, 15.9510, 15.9503, 15.9510, 15.9513,\n",
            "        15.9506, 15.9504, 15.9500, 15.9507, 15.9507, 15.9507, 15.9507, 15.9511,\n",
            "        15.9512, 15.9504, 15.9492, 15.9510, 15.9507, 15.9507, 15.9509, 15.9501,\n",
            "        15.9513, 15.9504, 15.9510, 15.9507, 15.9499, 15.9509, 15.9506, 15.9512,\n",
            "        15.9501, 15.9498, 15.9503, 15.9505, 15.9517, 15.9503, 15.9502, 15.9507,\n",
            "        15.9506, 15.9504, 15.9506, 15.9502, 15.9507, 15.9506, 15.9504, 15.9508,\n",
            "        15.9510], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0094, 0.0093, 0.0093, 0.0093, 0.0093, 0.0094, 0.0094, 0.0093, 0.0094,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0094, 0.0094, 0.0093, 0.0093, 0.0094, 0.0093, 0.0094, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0094, 0.0093, 0.0093, 0.0093, 0.0094, 0.0094,\n",
            "        0.0094, 0.0093, 0.0093, 0.0094, 0.0093, 0.0093, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0093, 0.0094, 0.0093, 0.0093, 0.0093, 0.0093, 0.0094, 0.0093,\n",
            "        0.0094, 0.0093, 0.0093, 0.0093, 0.0093, 0.0094, 0.0094, 0.0093, 0.0094,\n",
            "        0.0094, 0.0094, 0.0094, 0.0094, 0.0093, 0.0093, 0.0094, 0.0093, 0.0093,\n",
            "        0.0094, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0094, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0094, 0.0094, 0.0094,\n",
            "        0.0093, 0.0093, 0.0093, 0.0094, 0.0093, 0.0094, 0.0093, 0.0094],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [87]\n",
            "DEBUGGING: logits looks like: tensor([15.9504, 15.9498, 15.9504, 15.9501, 15.9499, 15.9506, 15.9506, 15.9503,\n",
            "        15.9506, 15.9502, 15.9501, 15.9501, 15.9501, 15.9499, 15.9499, 15.9502,\n",
            "        15.9501, 15.9503, 15.9504, 15.9510, 15.9505, 15.9501, 15.9503, 15.9504,\n",
            "        15.9504, 15.9504, 15.9501, 15.9500, 15.9503, 15.9501, 15.9507, 15.9504,\n",
            "        15.9503, 15.9502, 15.9505, 15.9505, 15.9504, 15.9503, 15.9495, 15.9504,\n",
            "        15.9500, 15.9502, 15.9507, 15.9504, 15.9510, 15.9505, 15.9501, 15.9508,\n",
            "        15.9501, 15.9502, 15.9503, 15.9501, 15.9505, 15.9503, 15.9508, 15.9498,\n",
            "        15.9503, 15.9501, 15.9503, 15.9505, 15.9504, 15.9502, 15.9508, 15.9505,\n",
            "        15.9506, 15.9507, 15.9504, 15.9502, 15.9500, 15.9505, 15.9502, 15.9502,\n",
            "        15.9505, 15.9492, 15.9500, 15.9502, 15.9503, 15.9501, 15.9502, 15.9501,\n",
            "        15.9500, 15.9502, 15.9504, 15.9504, 15.9507, 15.9502, 15.9501, 15.9502,\n",
            "        15.9503, 15.9502, 15.9503, 15.9502, 15.9503, 15.9502, 15.9501, 15.9504,\n",
            "        15.9509, 15.9507, 15.9510, 15.9501, 15.9503, 15.9502, 15.9508, 15.9500,\n",
            "        15.9506, 15.9503, 15.9505], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.09242224604440707 and immediate abs rewards look like: [0.005601399716397282, 0.005777949339062616, 0.002681175510133471, 0.0018688664035835245, 0.0012181289152977115, 0.0024243313519036747, 0.00021559348624577979, 0.003710162402057904, 0.009681461670879798, 0.0001596571555637638, 0.014891451806761324, 9.0025098870683e-05, 0.0005440000486487406, 0.005082815519472206, 0.008627751466065092, 0.012360847938452935, 0.003705118314428546, 0.001083887643289927, 0.0015445303884007444, 0.00044168472186356666, 0.0017407179084329982, 0.0001054646450029395, 0.0003491242873678857, 0.00038006617478458793, 1.8038807411357993e-05, 0.0004036691302644613, 0.0004287835390641703, 0.002400805782599491, 0.0001817147335714253, 0.00042516436042205896, 0.00010814249117174768, 0.00029460314954121714, 4.181006715953117e-06, 0.001082344595943141, 5.4961606565484544e-05, 0.0015017633631941862, 0.00028850043781858403, 1.446420355932787e-06, 2.5693743282317882e-05, 0.00012050713394273771, 0.0001822575950427563, 0.00046251646017481107, 3.0922528821974993e-06, 9.713014605949866e-06, 8.97725522008841e-06, 1.238862978425459e-05, 5.2795685405726545e-06, 1.5432165582751622e-05, 2.9235288820927963e-05, 6.682159892079653e-05]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.38928442470754576 and immediate relative rewards look like: [0.0018061392595239527, 0.003732875441402872, 0.0026031405233757276, 0.002421399058768996, 0.001974032767997765, 0.004716347813198324, 0.0004897086633852351, 0.009632020594080214, 0.028310082956925053, 0.0005203725082190022, 0.053392257300514806, 0.00035383968435598014, 0.0023164172095146876, 0.023312254035858352, 0.042468267306971015, 0.06508414885470468, 0.020812688946177653, 0.006454550213073699, 0.009712142182168098, 0.0029250237658183516, 0.012105932253353496, 0.0007688304124907654, 0.0026608724322638466, 0.0030229910675128907, 0.00014947512978670467, 0.003478744976241099, 0.0038378119030363107, 0.022287359179872557, 0.0017485478359647606, 0.004232467891358092, 0.001112589364515579, 0.003128814325595954, 4.579622685951491e-05, 0.012214620150367745, 0.0006387324917506494, 0.017951613001334146, 0.0035462086879889856, 1.8261474319077193e-05, 0.00033292768538163367, 0.0016015274255553502, 0.002482838733511444, 0.006454785338901384, 4.418913788879482e-05, 0.00014202972127698769, 0.00013425485638369296, 0.00018938968453106487, 8.246568910852297e-05, 0.00024617609368256797, 0.0004760838619116853, 0.0011103785887660285]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 200 number of matrices\n",
            "DEBUGGING: ACT_MAT has 200 number of matrices\n",
            "DEBUGGING: VAL looks like: [[6.385803493618333, 6.446657236733906, 6.501308884866259, 6.552958357866653, 6.6004304533726135, 6.666930525406514, 6.725370568545286, 6.750465378626962, 6.808803232675809, 6.872531014291528, 6.933551633731963, 6.981068578111735, 7.049244443867489, 7.1056813544095, 7.158005047279776, 7.227703920043357, 7.294757045875657, 7.363130862643479, 7.417173727990961, 7.491279575984885, 7.5548268526926545, 7.631058086017013, 7.698907326140661, 7.77621639129392, 7.840014972748583, 7.917645637619014, 7.995635669649524, 8.06827349852572, 8.145798254408188, 1.636225736302556e-10, 1.6527532689924808e-10, 1.6694477464570512e-10, 1.6863108550071224e-10, 1.5517618476915042e-10, 1.4112603519091274e-10, 1.3451308742464608e-10, 1.1106740452199135e-10, 9.519368943353518e-11, 9.615524185205574e-11, 8.816936213108049e-11, 7.987314657914473e-11, 8.067994603954013e-11, 8.149489498943448e-11, 7.244224943977323e-11, 7.317398933310428e-11, 5.3242786407281556e-11, 4.3215754885752786e-11, 3.285776983830392e-11, 1.1141310096715838e-11, 0.0], [0.3720363698283604, 0.3748967512461128, 0.3752894896918331, 0.37848304598643234, 0.3791820657803923, 0.37955908800790705, 0.37300846453125996, 0.3761359160156661, 0.36659149877653635, 0.3560488003190654, 0.3341024556827557, 0.33442472949747337, 0.3354430625368397, 0.33731083546953317, 0.3244346316392947, 0.3240873025751954, 0.3230062003104811, 5.777866769673668e-11, 5.836229060276432e-11, 5.604265055267682e-11, 5.354646620885515e-11, 5.408733960490419e-11, 5.126517747313481e-11, 5.178300754862102e-11, 5.230606823093032e-11, 5.2834412354475076e-11, 4.9387140047264435e-11, 4.988600004774185e-11, 5.038989903812309e-11, 5.089888791729605e-11, 5.141301809827884e-11, 5.1932341513412966e-11, 5.245691061960906e-11, 5.298677840364551e-11, 5.352199838752072e-11, 4.870364911836226e-11, 4.919560517006289e-11, 4.4027327786998974e-11, 4.4472048269695934e-11, 3.894983101835545e-11, 3.321872020862308e-11, 2.7276605804553604e-11, 2.1121356456711184e-11, 2.133470349162746e-11, 2.1550205547098444e-11, 2.176788439100853e-11, 2.1987762011119728e-11, 2.2209860617292654e-11, 2.2434202643729953e-11, 1.5158245029546504e-11], [0.7791605900331632, 0.7846090388146539, 0.7843119074204199, 0.7560035270969943, 0.7558939166570515, 9.950307925894772e-11, 9.821145707526791e-11, 9.598810668592286e-11, 9.573277483187755e-11, 9.601076141974531e-11, 9.315272743672507e-11, 9.156728990590938e-11, 9.157353050922881e-11, 9.150327735586695e-11, 9.24275528847141e-11, 9.106446073765852e-11, 9.07593950861561e-11, 9.037469117034792e-11, 8.990954456332164e-11, 8.790856364414885e-11, 8.879652893348368e-11, 8.326269295057995e-11, 8.241948080538316e-11, 8.149119457271203e-11, 8.047697491834967e-11, 7.937595382793557e-11, 7.818725451928709e-11, 7.690999135383652e-11, 7.768685995337023e-11, 7.625142871119763e-11, 7.013153378575878e-11, 6.134689077518898e-11, 5.461710420303703e-11, 5.264241795268817e-11, 4.796829761882988e-11, 4.5773338119857006e-11, 4.623569507056263e-11, 4.3870120949591806e-11, 3.849493721046794e-11, 3.8883774960068625e-11, 3.3151996917424265e-11, 3.034803705693612e-11, 2.7439197576496073e-11, 2.7716361188379872e-11, 2.7996324432706942e-11, 2.4834059900059054e-11, 1.4520071545122395e-11, 1.4666738934467066e-11, 7.46543567705307e-12, 3.789561257387488e-12], [0.3290256033021116, 0.33052471115412896, 0.330092763346188, 0.33079759881092147, 0.3316931310627803, 0.3330495942371541, 0.33164974386258156, 0.3345050860597943, 0.32815461158152937, 0.30287326123697406, 0.3054069583118738, 0.25456030405187774, 0.2567742064318402, 0.25702806992154087, 0.23607658170270962, 0.19556395393508952, 0.13179778290947963, 0.11210615551848684, 0.10671879323779104, 0.09798651621780095, 0.09602170954745717, 0.08476341140818554, 0.0848430111067624, 0.08301226128737228, 0.08079724264632263, 0.08146239143084437, 0.07877136005515482, 0.07569045267890759, 0.053942518685893975, 0.05272118267669618, 0.04897849978316978, 0.04834940446328707, 0.04567736377544557, 0.046092492473319244, 0.034220073053486365, 0.03392054602195527, 0.016130235374364768, 0.012711138067046244, 0.012821087467401179, 0.012614302810120753, 0.011124015539965053, 0.008728461420660211, 0.0022966425068270983, 0.002275205423170003, 0.0021547229312050657, 0.0020408768432539116, 0.0018701890492149966, 0.001805781171824721, 0.0015753586647900535, 0.0011103785887660285]]\n",
            "DEBUGGING: traj_returns = [6.385803493618333, 0.3720363698283604, 0.7791605900331632, 0.3290256033021116]\n",
            "DEBUGGING: actions = [[46], [28], [31], [5], [12], [60], [28], [27], [18], [31], [5], [4], [47], [51], [38], [59], [46], [59], [9], [38], [62], [28], [56], [10], [14], [28], [13], [10], [0], [33], [72], [47], [45], [38], [77], [71], [10], [90], [28], [31], [59], [92], [19], [20], [59], [102], [39], [20], [41], [29], [42], [46], [46], [17], [7], [46], [41], [35], [56], [63], [40], [61], [11], [2], [12], [55], [0], [25], [20], [29], [72], [74], [19], [11], [74], [34], [26], [57], [24], [55], [62], [61], [17], [3], [77], [82], [8], [21], [68], [79], [28], [67], [11], [66], [25], [27], [83], [21], [85], [35], [58], [12], [17], [53], [0], [54], [9], [15], [48], [61], [56], [58], [26], [26], [58], [57], [50], [52], [3], [7], [18], [75], [36], [43], [20], [58], [23], [49], [84], [17], [45], [48], [21], [64], [79], [1], [29], [18], [43], [13], [48], [38], [79], [72], [69], [71], [17], [97], [102], [12], [13], [45], [47], [41], [19], [56], [25], [3], [20], [61], [25], [22], [67], [5], [51], [28], [54], [58], [46], [58], [73], [29], [48], [29], [10], [65], [65], [72], [60], [12], [37], [23], [41], [86], [61], [56], [42], [88], [28], [37], [36], [85], [92], [71], [86], [92], [51], [86], [21], [87]]\n",
            "DEBUGGING: actions length = 200\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-1.6294, -0.9735,  0.7162,  ...,  0.5646,  0.7726, -1.0501],\n",
            "        [-1.6179, -0.9637,  0.7106,  ...,  0.5585,  0.7680, -1.0431],\n",
            "        [-1.5783, -0.9184,  0.6927,  ...,  0.5581,  0.7501, -1.0028],\n",
            "        ...,\n",
            "        [-1.6246, -0.9698,  0.7135,  ...,  0.5625,  0.7707, -1.0472],\n",
            "        [-1.6246, -0.9698,  0.7135,  ...,  0.5625,  0.7707, -1.0472],\n",
            "        [-1.6247, -0.9699,  0.7135,  ...,  0.5625,  0.7707, -1.0472]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-1.6247, -0.9699,  0.7135,  ...,  0.5625,  0.7707, -1.0472],\n",
            "        [-1.6246, -0.9698,  0.7135,  ...,  0.5625,  0.7707, -1.0472],\n",
            "        [-1.6247, -0.9699,  0.7135,  ...,  0.5625,  0.7707, -1.0472],\n",
            "        ...,\n",
            "        [-1.6247, -0.9699,  0.7135,  ...,  0.5625,  0.7707, -1.0472],\n",
            "        [-1.6247, -0.9698,  0.7135,  ...,  0.5625,  0.7707, -1.0472],\n",
            "        [-1.6247, -0.9699,  0.7135,  ...,  0.5625,  0.7707, -1.0472]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([15.9504, 15.9498, 15.9504, 15.9501, 15.9499, 15.9506, 15.9506, 15.9503,\n",
            "        15.9506, 15.9502, 15.9501, 15.9501, 15.9501, 15.9499, 15.9499, 15.9502,\n",
            "        15.9501, 15.9503, 15.9504, 15.9510, 15.9505, 15.9501, 15.9503, 15.9504,\n",
            "        15.9504, 15.9504, 15.9501, 15.9500, 15.9503, 15.9501, 15.9507, 15.9504,\n",
            "        15.9503, 15.9502, 15.9505, 15.9505, 15.9504, 15.9503, 15.9495, 15.9504,\n",
            "        15.9500, 15.9502, 15.9507, 15.9504, 15.9510, 15.9505, 15.9501, 15.9508,\n",
            "        15.9501, 15.9502, 15.9503, 15.9501, 15.9505, 15.9503, 15.9508, 15.9498,\n",
            "        15.9503, 15.9501, 15.9503, 15.9505, 15.9504, 15.9502, 15.9508, 15.9505,\n",
            "        15.9506, 15.9507, 15.9504, 15.9502, 15.9500, 15.9505, 15.9502, 15.9502,\n",
            "        15.9505, 15.9492, 15.9500, 15.9502, 15.9503, 15.9501, 15.9502, 15.9501,\n",
            "        15.9500, 15.9502, 15.9504, 15.9504, 15.9507, 15.9502, 15.9501, 15.9502,\n",
            "        15.9503, 15.9502, 15.9503, 15.9502, 15.9503, 15.9502, 15.9501, 15.9504,\n",
            "        15.9509, 15.9507, 15.9510, 15.9501, 15.9503, 15.9502, 15.9508, 15.9500,\n",
            "        15.9506, 15.9503, 15.9505], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[1.96650651e+00 1.98417193e+00 1.99775076e+00 2.00456063e+00\n",
            "  2.01679989e+00 1.84488480e+00 1.85750719e+00 1.86527660e+00\n",
            "  1.87588734e+00 1.88286327e+00 1.89326526e+00 1.89251340e+00\n",
            "  1.91036543e+00 1.92500506e+00 1.92962907e+00 1.93683879e+00\n",
            "  1.93739026e+00 1.86880925e+00 1.88097313e+00 1.89731652e+00\n",
            "  1.91271214e+00 1.92895537e+00 1.94593758e+00 1.96480716e+00\n",
            "  1.98020305e+00 1.99977701e+00 2.01860176e+00 2.03599099e+00\n",
            "  2.04993519e+00 1.31802957e-02 1.22446250e-02 1.20873512e-02\n",
            "  1.14193410e-02 1.15231232e-02 8.55501832e-03 8.48013656e-03\n",
            "  4.03255890e-03 3.17778456e-03 3.20527191e-03 3.15357574e-03\n",
            "  2.78100392e-03 2.18211539e-03 5.74160659e-04 5.68801386e-04\n",
            "  5.38680763e-04 5.10219236e-04 4.67547282e-04 4.51445310e-04\n",
            "  3.93839676e-04 2.77594652e-04]]\n",
            "DEBUGGING: baseline2 looks like: 1.966506514195492\n",
            "DEBUGGING: ADS looks like: [ 4.74895912  4.8052876   4.8552597   4.91523131  4.99255821  5.09182927\n",
            "  5.14772525  5.16712398  5.24134944  5.32018931  5.38805721  5.45188771\n",
            "  5.56245603  5.63603149  5.68168682  5.75724797  5.83895079  5.95652433\n",
            "  6.0463726   6.18906963  6.3059733   6.4218187   6.50840538  6.62799155\n",
            "  6.72038877  6.85799186  6.9301848   6.99607168  7.12615033 -0.94555818\n",
            " -0.88872882 -0.78431093 -0.7023515  -0.62263396 -0.59492186 -0.54780786\n",
            " -0.55143446 -0.41907767 -0.315701   -0.28972727 -0.29081513 -0.23823327\n",
            " -0.23743031 -0.23844167 -0.20398705 -0.20371399 -0.17690764 -0.1770821\n",
            " -0.12937259 -0.0315542  -1.264808   -1.26647288 -1.2707597  -1.259244\n",
            " -1.22869017 -1.19554217 -1.20463685 -1.20720548 -1.20086229 -1.19629291\n",
            " -1.21139197 -1.19475614 -1.15134535 -1.13233903 -1.15188359 -1.14636864\n",
            " -1.13280006 -1.40660653 -1.37080113 -1.30220994 -1.24885355 -1.20923938\n",
            " -1.19050194 -1.14822485 -1.11962621 -1.05965378 -1.06545087 -1.07220182\n",
            " -1.01964793 -0.94555818 -0.88872882 -0.78431093 -0.7023515  -0.62263396\n",
            " -0.59492186 -0.54780786 -0.55143446 -0.41907767 -0.315701   -0.28972727\n",
            " -0.29081513 -0.23823327 -0.23743031 -0.23844167 -0.20398705 -0.20371399\n",
            " -0.17690764 -0.1770821  -0.12937259 -0.0315542  -0.85768378 -0.8567606\n",
            " -0.86173728 -0.88172352 -0.85197832 -1.57510125 -1.57764532 -1.58334139\n",
            " -1.56745379 -1.55234171 -1.54549443 -1.52918087 -1.48678841 -1.46964987\n",
            " -1.47631823 -1.47045595 -1.45580626 -1.40660653 -1.37080113 -1.30220994\n",
            " -1.24885355 -1.20923938 -1.19050194 -1.14822485 -1.11962621 -1.05965378\n",
            " -1.06545087 -1.07220182 -1.01964793 -0.94555818 -0.88872882 -0.78431093\n",
            " -0.7023515  -0.62263396 -0.59492186 -0.54780786 -0.55143446 -0.41907767\n",
            " -0.315701   -0.28972727 -0.29081513 -0.23823327 -0.23743031 -0.23844167\n",
            " -0.20398705 -0.20371399 -0.17690764 -0.1770821  -0.12937259 -0.0315542\n",
            " -1.30781877 -1.31084492 -1.31595642 -1.30692945 -1.27617911 -1.24205166\n",
            " -1.24599558 -1.24883631 -1.23929918 -1.24946844 -1.24008747 -1.27462056\n",
            " -1.2300142  -1.2126218  -1.24024164 -1.27489199 -1.32400848 -1.29450037\n",
            " -1.26408233 -1.20422343 -1.15283184 -1.12447597 -1.10565893 -1.06521258\n",
            " -1.03882896 -0.97819139 -0.98667951 -0.99651137 -0.96570541 -0.89283699\n",
            " -0.83975032 -0.73596153 -0.65667413 -0.57654147 -0.56070179 -0.51388732\n",
            " -0.53530423 -0.40636653 -0.30287991 -0.27711297 -0.27969111 -0.22950481\n",
            " -0.23513367 -0.23616646 -0.20183233 -0.20167311 -0.17503745 -0.17527632\n",
            " -0.12779724 -0.03044382]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(0.5610, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0005, -0.0022, -0.0015,  ...,  0.0028, -0.0018, -0.0135],\n",
            "        [ 0.0003, -0.0012, -0.0008,  ...,  0.0016, -0.0010, -0.0077]])\n",
            "   Last layer:\n",
            "tensor([[ 0.0142,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0032,  0.0092,\n",
            "          0.0037,  0.0029,  0.0000,  0.0000,  0.0000,  0.0000,  0.0054,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0072,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0016,  0.0047,\n",
            "          0.0019,  0.0014,  0.0000,  0.0000,  0.0000,  0.0000,  0.0027,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0045,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0010, -0.0030,\n",
            "         -0.0012, -0.0009,  0.0000,  0.0000,  0.0000,  0.0000, -0.0017,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0079,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0017, -0.0053,\n",
            "         -0.0020, -0.0014,  0.0000,  0.0000,  0.0000,  0.0000, -0.0030,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0197,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0045,  0.0128,\n",
            "          0.0051,  0.0040,  0.0000,  0.0000,  0.0000,  0.0000,  0.0075,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0155,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0035,  0.0100,\n",
            "          0.0041,  0.0032,  0.0000,  0.0000,  0.0000,  0.0000,  0.0059,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0049,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0011, -0.0031,\n",
            "         -0.0013, -0.0011,  0.0000,  0.0000,  0.0000,  0.0000, -0.0019,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0043,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0010, -0.0028,\n",
            "         -0.0011, -0.0009,  0.0000,  0.0000,  0.0000,  0.0000, -0.0016,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0048,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0011, -0.0033,\n",
            "         -0.0012, -0.0008,  0.0000,  0.0000,  0.0000,  0.0000, -0.0018,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0067,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0015,  0.0045,\n",
            "          0.0017,  0.0012,  0.0000,  0.0000,  0.0000,  0.0000,  0.0026,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-3.2145e-04,  3.8813e-03,  6.5832e-05,  ..., -1.1912e-03,\n",
            "         -2.5651e-03, -1.5844e-02],\n",
            "        [-1.8011e-04,  2.1775e-03,  3.7160e-05,  ..., -6.6800e-04,\n",
            "         -1.4388e-03, -8.8273e-03]])\n",
            "   Last layer:\n",
            "tensor([[ 0.0117,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0035,  0.0045,\n",
            "          0.0048,  0.0061,  0.0000,  0.0000,  0.0000,  0.0000,  0.0051,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0103,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0029,  0.0045,\n",
            "          0.0039,  0.0046,  0.0000,  0.0000,  0.0000,  0.0000,  0.0043,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0064,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0019, -0.0027,\n",
            "         -0.0025, -0.0030,  0.0000,  0.0000,  0.0000,  0.0000, -0.0027,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0093,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0028, -0.0038,\n",
            "         -0.0037, -0.0046,  0.0000,  0.0000,  0.0000,  0.0000, -0.0040,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0186,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0055,  0.0075,\n",
            "          0.0075,  0.0093,  0.0000,  0.0000,  0.0000,  0.0000,  0.0079,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0143,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0042,  0.0059,\n",
            "          0.0057,  0.0070,  0.0000,  0.0000,  0.0000,  0.0000,  0.0061,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0039,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0011, -0.0016,\n",
            "         -0.0015, -0.0018,  0.0000,  0.0000,  0.0000,  0.0000, -0.0016,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0040,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0012, -0.0015,\n",
            "         -0.0017, -0.0021,  0.0000,  0.0000,  0.0000,  0.0000, -0.0017,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0058,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0017, -0.0023,\n",
            "         -0.0024, -0.0030,  0.0000,  0.0000,  0.0000,  0.0000, -0.0025,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0096,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0028,  0.0040,\n",
            "          0.0037,  0.0045,  0.0000,  0.0000,  0.0000,  0.0000,  0.0041,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000]])\n",
            "DEBUGGING: training for one iteration takes 0.005244 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 49\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0196, 0.0142, 0.0148, 0.0148, 0.0151, 0.0146, 0.0148, 0.0149, 0.0147,\n",
            "        0.0151, 0.0152, 0.0143, 0.0146, 0.0144, 0.0146, 0.0156, 0.0144, 0.0151,\n",
            "        0.0149, 0.0155, 0.0152, 0.0148, 0.0147, 0.0148, 0.0148, 0.0147, 0.0148,\n",
            "        0.0150, 0.0144, 0.0151, 0.0146, 0.0149, 0.0147, 0.0150, 0.0154, 0.0152,\n",
            "        0.0147, 0.0149, 0.0149, 0.0147, 0.0154, 0.0151, 0.0148, 0.0150, 0.0148,\n",
            "        0.0149, 0.0149, 0.0148, 0.0146, 0.0144, 0.0148, 0.0149, 0.0147, 0.0146,\n",
            "        0.0148, 0.0150, 0.0148, 0.0154, 0.0150, 0.0148, 0.0154, 0.0148, 0.0150,\n",
            "        0.0146, 0.0148, 0.0147, 0.0151], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [53]\n",
            "DEBUGGING: logits looks like: tensor([21.3923, 21.3279, 21.3358, 21.3359, 21.3397, 21.3331, 21.3362, 21.3375,\n",
            "        21.3339, 21.3393, 21.3409, 21.3289, 21.3338, 21.3302, 21.3338, 21.3458,\n",
            "        21.3298, 21.3395, 21.3377, 21.3445, 21.3415, 21.3353, 21.3340, 21.3364,\n",
            "        21.3363, 21.3348, 21.3361, 21.3382, 21.3311, 21.3403, 21.3328, 21.3377,\n",
            "        21.3349, 21.3387, 21.3438, 21.3410, 21.3348, 21.3375, 21.3366, 21.3339,\n",
            "        21.3437, 21.3393, 21.3362, 21.3386, 21.3356, 21.3373, 21.3368, 21.3354,\n",
            "        21.3329, 21.3308, 21.3356, 21.3371, 21.3345, 21.3338, 21.3355, 21.3386,\n",
            "        21.3361, 21.3436, 21.3383, 21.3354, 21.3437, 21.3363, 21.3386, 21.3334,\n",
            "        21.3362, 21.3345, 21.3394], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0166, 0.0124, 0.0124, 0.0124, 0.0125, 0.0125, 0.0126, 0.0124, 0.0124,\n",
            "        0.0125, 0.0124, 0.0125, 0.0125, 0.0125, 0.0124, 0.0125, 0.0125, 0.0125,\n",
            "        0.0122, 0.0125, 0.0125, 0.0124, 0.0125, 0.0125, 0.0124, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0123, 0.0124, 0.0124, 0.0125, 0.0124, 0.0123, 0.0123,\n",
            "        0.0124, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0124, 0.0125, 0.0125,\n",
            "        0.0124, 0.0124, 0.0125, 0.0125, 0.0126, 0.0124, 0.0124, 0.0125, 0.0124,\n",
            "        0.0125, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0124, 0.0125,\n",
            "        0.0123, 0.0123, 0.0124, 0.0124, 0.0124, 0.0126, 0.0126, 0.0124, 0.0124,\n",
            "        0.0125, 0.0125, 0.0123, 0.0126, 0.0124, 0.0124, 0.0125, 0.0125],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [25]\n",
            "DEBUGGING: logits looks like: tensor([21.3952, 21.3379, 21.3374, 21.3373, 21.3388, 21.3392, 21.3409, 21.3375,\n",
            "        21.3381, 21.3389, 21.3371, 21.3396, 21.3386, 21.3391, 21.3380, 21.3384,\n",
            "        21.3382, 21.3386, 21.3346, 21.3386, 21.3387, 21.3373, 21.3390, 21.3386,\n",
            "        21.3382, 21.3386, 21.3386, 21.3387, 21.3390, 21.3360, 21.3373, 21.3372,\n",
            "        21.3390, 21.3381, 21.3366, 21.3363, 21.3372, 21.3386, 21.3393, 21.3391,\n",
            "        21.3389, 21.3385, 21.3380, 21.3383, 21.3393, 21.3381, 21.3380, 21.3387,\n",
            "        21.3385, 21.3411, 21.3368, 21.3366, 21.3396, 21.3379, 21.3382, 21.3376,\n",
            "        21.3368, 21.3374, 21.3381, 21.3379, 21.3376, 21.3376, 21.3385, 21.3360,\n",
            "        21.3362, 21.3373, 21.3377, 21.3376, 21.3399, 21.3408, 21.3380, 21.3381,\n",
            "        21.3383, 21.3389, 21.3358, 21.3400, 21.3379, 21.3376, 21.3389, 21.3386],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0174, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117,\n",
            "        0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117,\n",
            "        0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117,\n",
            "        0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117,\n",
            "        0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117,\n",
            "        0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117,\n",
            "        0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117,\n",
            "        0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117,\n",
            "        0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117,\n",
            "        0.0117, 0.0117, 0.0117, 0.0117], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [4]\n",
            "DEBUGGING: logits looks like: tensor([21.4198, 21.3403, 21.3405, 21.3401, 21.3401, 21.3403, 21.3403, 21.3404,\n",
            "        21.3402, 21.3403, 21.3404, 21.3402, 21.3404, 21.3402, 21.3403, 21.3403,\n",
            "        21.3405, 21.3398, 21.3399, 21.3402, 21.3403, 21.3405, 21.3403, 21.3405,\n",
            "        21.3398, 21.3404, 21.3400, 21.3401, 21.3399, 21.3403, 21.3401, 21.3403,\n",
            "        21.3401, 21.3403, 21.3399, 21.3402, 21.3403, 21.3401, 21.3403, 21.3403,\n",
            "        21.3405, 21.3399, 21.3403, 21.3401, 21.3400, 21.3402, 21.3402, 21.3400,\n",
            "        21.3403, 21.3401, 21.3404, 21.3401, 21.3403, 21.3401, 21.3403, 21.3403,\n",
            "        21.3401, 21.3402, 21.3402, 21.3399, 21.3402, 21.3401, 21.3401, 21.3403,\n",
            "        21.3403, 21.3400, 21.3404, 21.3398, 21.3404, 21.3401, 21.3398, 21.3402,\n",
            "        21.3400, 21.3402, 21.3402, 21.3400, 21.3402, 21.3402, 21.3404, 21.3401,\n",
            "        21.3402, 21.3404, 21.3404, 21.3402, 21.3405], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0102, 0.0119, 0.0099, 0.0077, 0.0095, 0.0103, 0.0105, 0.0099, 0.0112,\n",
            "        0.0107, 0.0095, 0.0107, 0.0094, 0.0094, 0.0112, 0.0101, 0.0100, 0.0097,\n",
            "        0.0107, 0.0096, 0.0097, 0.0083, 0.0103, 0.0103, 0.0098, 0.0093, 0.0106,\n",
            "        0.0108, 0.0101, 0.0106, 0.0100, 0.0091, 0.0103, 0.0097, 0.0092, 0.0101,\n",
            "        0.0113, 0.0104, 0.0091, 0.0105, 0.0109, 0.0108, 0.0101, 0.0104, 0.0103,\n",
            "        0.0102, 0.0078, 0.0098, 0.0113, 0.0103, 0.0107, 0.0109, 0.0098, 0.0103,\n",
            "        0.0108, 0.0102, 0.0092, 0.0108, 0.0106, 0.0093, 0.0109, 0.0104, 0.0108,\n",
            "        0.0094, 0.0114, 0.0106, 0.0107, 0.0098, 0.0105, 0.0088, 0.0093, 0.0097,\n",
            "        0.0108, 0.0109, 0.0116, 0.0103, 0.0099, 0.0112, 0.0112, 0.0102, 0.0108,\n",
            "        0.0099, 0.0101, 0.0102, 0.0092, 0.0111, 0.0104, 0.0098, 0.0110, 0.0091,\n",
            "        0.0103, 0.0102, 0.0106, 0.0106, 0.0108, 0.0108, 0.0099, 0.0104],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [53]\n",
            "DEBUGGING: logits looks like: tensor([21.3510, 21.3806, 21.3444, 21.2954, 21.3371, 21.3525, 21.3564, 21.3448,\n",
            "        21.3695, 21.3604, 21.3367, 21.3601, 21.3332, 21.3333, 21.3700, 21.3490,\n",
            "        21.3469, 21.3409, 21.3599, 21.3385, 21.3407, 21.3088, 21.3532, 21.3523,\n",
            "        21.3422, 21.3321, 21.3591, 21.3622, 21.3493, 21.3586, 21.3475, 21.3283,\n",
            "        21.3528, 21.3405, 21.3298, 21.3483, 21.3714, 21.3553, 21.3270, 21.3564,\n",
            "        21.3638, 21.3611, 21.3492, 21.3549, 21.3530, 21.3509, 21.2970, 21.3421,\n",
            "        21.3712, 21.3522, 21.3595, 21.3640, 21.3426, 21.3524, 21.3624, 21.3501,\n",
            "        21.3293, 21.3621, 21.3575, 21.3318, 21.3633, 21.3550, 21.3611, 21.3338,\n",
            "        21.3724, 21.3576, 21.3607, 21.3416, 21.3570, 21.3215, 21.3329, 21.3402,\n",
            "        21.3612, 21.3628, 21.3765, 21.3532, 21.3452, 21.3683, 21.3689, 21.3495,\n",
            "        21.3623, 21.3453, 21.3480, 21.3497, 21.3291, 21.3683, 21.3553, 21.3430,\n",
            "        21.3658, 21.3280, 21.3516, 21.3504, 21.3574, 21.3581, 21.3614, 21.3611,\n",
            "        21.3454, 21.3548], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0099, 0.0091, 0.0090, 0.0089, 0.0076, 0.0098, 0.0106, 0.0080, 0.0089,\n",
            "        0.0094, 0.0091, 0.0099, 0.0085, 0.0079, 0.0090, 0.0110, 0.0101, 0.0091,\n",
            "        0.0110, 0.0096, 0.0106, 0.0089, 0.0098, 0.0095, 0.0107, 0.0100, 0.0109,\n",
            "        0.0083, 0.0086, 0.0097, 0.0087, 0.0079, 0.0096, 0.0105, 0.0080, 0.0103,\n",
            "        0.0097, 0.0101, 0.0096, 0.0093, 0.0081, 0.0089, 0.0080, 0.0094, 0.0085,\n",
            "        0.0091, 0.0082, 0.0104, 0.0088, 0.0110, 0.0097, 0.0092, 0.0098, 0.0091,\n",
            "        0.0097, 0.0094, 0.0094, 0.0101, 0.0096, 0.0091, 0.0087, 0.0100, 0.0092,\n",
            "        0.0095, 0.0096, 0.0096, 0.0077, 0.0104, 0.0096, 0.0089, 0.0106, 0.0081,\n",
            "        0.0090, 0.0096, 0.0105, 0.0106, 0.0081, 0.0096, 0.0081, 0.0082, 0.0097,\n",
            "        0.0077, 0.0088, 0.0076, 0.0121, 0.0094, 0.0099, 0.0078, 0.0099, 0.0095,\n",
            "        0.0088, 0.0100, 0.0076, 0.0078, 0.0091, 0.0092, 0.0075, 0.0076, 0.0091,\n",
            "        0.0088, 0.0091, 0.0087, 0.0108, 0.0097, 0.0097, 0.0087, 0.0082, 0.0090],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [78]\n",
            "DEBUGGING: logits looks like: tensor([21.3685, 21.3517, 21.3492, 21.3489, 21.3164, 21.3672, 21.3832, 21.3273,\n",
            "        21.3482, 21.3587, 21.3527, 21.3701, 21.3376, 21.3250, 21.3508, 21.3901,\n",
            "        21.3727, 21.3532, 21.3906, 21.3634, 21.3826, 21.3470, 21.3663, 21.3615,\n",
            "        21.3838, 21.3713, 21.3890, 21.3341, 21.3412, 21.3642, 21.3435, 21.3235,\n",
            "        21.3636, 21.3813, 21.3271, 21.3770, 21.3654, 21.3722, 21.3628, 21.3575,\n",
            "        21.3282, 21.3474, 21.3253, 21.3589, 21.3389, 21.3512, 21.3302, 21.3794,\n",
            "        21.3456, 21.3900, 21.3659, 21.3538, 21.3666, 21.3523, 21.3648, 21.3592,\n",
            "        21.3597, 21.3728, 21.3626, 21.3515, 21.3429, 21.3704, 21.3548, 21.3613,\n",
            "        21.3628, 21.3631, 21.3198, 21.3787, 21.3637, 21.3477, 21.3827, 21.3295,\n",
            "        21.3509, 21.3623, 21.3802, 21.3825, 21.3290, 21.3630, 21.3285, 21.3308,\n",
            "        21.3660, 21.3199, 21.3448, 21.3160, 21.4095, 21.3589, 21.3688, 21.3224,\n",
            "        21.3686, 21.3607, 21.3449, 21.3717, 21.3163, 21.3209, 21.3520, 21.3546,\n",
            "        21.3148, 21.3158, 21.3512, 21.3453, 21.3527, 21.3443, 21.3863, 21.3645,\n",
            "        21.3658, 21.3423, 21.3316, 21.3508], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.7082653196262072 and immediate abs rewards look like: [0.04907008932195822, 0.03354056410080375, 0.004434018369465775, 0.003341435646689206, 0.008344270240741025, 0.00010843221980394446, 0.0014542587200594426, 0.0007103462021404994, 0.0005752830702476786, 0.0018208035812676826, 8.440010788035579e-07, 7.874450966482982e-05, 0.00031802183411855367, 0.0019459827594801027, 0.0010521380795580626, 2.688383756321855e-05, 1.8818982425727881e-06, 0.001504151467088377, 0.00014576308194591547, 0.00022238334577195928, 0.0004883992164650408, 0.0002428363700346381, 3.100914227616158e-05, 6.29671535534726e-05, 2.3921066713228356e-05, 9.190886839860468e-05, 0.0004749807662847161, 0.0004944161887578957, 4.531689228315372e-06, 3.5442438274913e-05, 2.064438012894243e-05, 1.9722479009942617e-06, 0.5975959938041342, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 5.579862147787619 and immediate relative rewards look like: [0.013232626334026372, 0.01833220803475595, 0.003668869854709508, 0.0036909495130530074, 0.01153199119064971, 0.00018024297569868168, 0.0028203402986600005, 0.001575059125121717, 0.0014353123695682371, 0.005048410833085775, 2.5754112178334117e-06, 0.0002621274646221454, 0.0011468873825449442, 0.007558329568215599, 0.00438083936961292, 0.00011943495034669273, 8.88318052495613e-06, 0.007517747477381314, 0.0007693185815121133, 0.001235534622141822, 0.002849336539814824, 0.001484379123091941, 0.00019817794366288417, 0.0004199202064051389, 0.000166176504681444, 0.0006640223888359728, 0.0035637139364123266, 0.0038474331920686985, 3.65290325727867e-05, 0.0002955462455871883, 0.0001778887239857641, 1.754279756308477e-05, 5.481623792526963, 0.0, 5.305385760342081e-12, 1.0913936421273484e-11, 5.608550660933908e-12, 0.0, 0.0, 6.063298011819521e-12, 1.2429760924228135e-11, 6.366462912411463e-12, 6.518045362705985e-12, 6.669627813002486e-12, 6.821210263296962e-12, 6.9727927135935075e-12, 0.0, 7.275957614183426e-12, 0.0, 7.579122514773252e-12]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0085, 0.0084, 0.0125, 0.0097, 0.0189, 0.0249, 0.0186, 0.0183, 0.0157,\n",
            "        0.0173, 0.0126, 0.0097, 0.0184, 0.0215, 0.0106, 0.0124, 0.0172, 0.0187,\n",
            "        0.0168, 0.0109, 0.0159, 0.0135, 0.0193, 0.0104, 0.0151, 0.0204, 0.0101,\n",
            "        0.0187, 0.0162, 0.0116, 0.0100, 0.0202, 0.0138, 0.0100, 0.0117, 0.0139,\n",
            "        0.0110, 0.0133, 0.0089, 0.0137, 0.0149, 0.0134, 0.0137, 0.0116, 0.0130,\n",
            "        0.0106, 0.0093, 0.0132, 0.0249, 0.0094, 0.0158, 0.0134, 0.0139, 0.0182,\n",
            "        0.0149, 0.0164, 0.0152, 0.0144, 0.0151, 0.0156, 0.0129, 0.0145, 0.0139,\n",
            "        0.0170, 0.0151, 0.0153, 0.0175, 0.0169, 0.0107],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [35]\n",
            "DEBUGGING: logits looks like: tensor([21.2134, 21.2106, 21.2910, 21.2393, 21.3730, 21.4283, 21.3696, 21.3668,\n",
            "        21.3355, 21.3552, 21.2920, 21.2402, 21.3681, 21.3991, 21.2582, 21.2886,\n",
            "        21.3541, 21.3706, 21.3490, 21.2633, 21.3383, 21.3061, 21.3772, 21.2531,\n",
            "        21.3283, 21.3883, 21.2474, 21.3709, 21.3422, 21.2746, 21.2456, 21.3861,\n",
            "        21.3097, 21.2451, 21.2774, 21.3118, 21.2654, 21.3021, 21.2230, 21.3088,\n",
            "        21.3249, 21.3037, 21.3091, 21.2763, 21.2980, 21.2574, 21.2318, 21.3011,\n",
            "        21.4283, 21.2330, 21.3377, 21.3049, 21.3112, 21.3650, 21.3252, 21.3446,\n",
            "        21.3300, 21.3190, 21.3286, 21.3346, 21.2974, 21.3200, 21.3117, 21.3516,\n",
            "        21.3287, 21.3312, 21.3572, 21.3508, 21.2592], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0164, 0.0083, 0.0156, 0.0139, 0.0125, 0.0149, 0.0108, 0.0119, 0.0098,\n",
            "        0.0135, 0.0139, 0.0123, 0.0134, 0.0171, 0.0132, 0.0138, 0.0137, 0.0149,\n",
            "        0.0119, 0.0088, 0.0123, 0.0128, 0.0126, 0.0124, 0.0102, 0.0153, 0.0065,\n",
            "        0.0147, 0.0152, 0.0132, 0.0120, 0.0107, 0.0122, 0.0124, 0.0145, 0.0128,\n",
            "        0.0137, 0.0127, 0.0149, 0.0162, 0.0136, 0.0143, 0.0138, 0.0166, 0.0137,\n",
            "        0.0108, 0.0121, 0.0153, 0.0134, 0.0094, 0.0147, 0.0120, 0.0151, 0.0123,\n",
            "        0.0166, 0.0150, 0.0113, 0.0155, 0.0165, 0.0135, 0.0127, 0.0120, 0.0161,\n",
            "        0.0117, 0.0163, 0.0143, 0.0153, 0.0116, 0.0140, 0.0139, 0.0169, 0.0120,\n",
            "        0.0138, 0.0127, 0.0131], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [62]\n",
            "DEBUGGING: logits looks like: tensor([21.2860, 21.1505, 21.2756, 21.2528, 21.2315, 21.2663, 21.2019, 21.2213,\n",
            "        21.1833, 21.2462, 21.2529, 21.2285, 21.2457, 21.2938, 21.2423, 21.2517,\n",
            "        21.2494, 21.2660, 21.2218, 21.1601, 21.2274, 21.2368, 21.2323, 21.2299,\n",
            "        21.1916, 21.2721, 21.0997, 21.2640, 21.2711, 21.2421, 21.2237, 21.1997,\n",
            "        21.2257, 21.2299, 21.2613, 21.2357, 21.2494, 21.2345, 21.2664, 21.2839,\n",
            "        21.2484, 21.2587, 21.2517, 21.2887, 21.2501, 21.2017, 21.2252, 21.2724,\n",
            "        21.2460, 21.1736, 21.2632, 21.2238, 21.2693, 21.2287, 21.2886, 21.2680,\n",
            "        21.2104, 21.2740, 21.2865, 21.2468, 21.2353, 21.2226, 21.2821, 21.2189,\n",
            "        21.2849, 21.2586, 21.2712, 21.2163, 21.2539, 21.2524, 21.2919, 21.2235,\n",
            "        21.2510, 21.2341, 21.2408], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0106, 0.0129, 0.0134, 0.0120, 0.0149, 0.0113, 0.0120, 0.0111, 0.0100,\n",
            "        0.0090, 0.0108, 0.0097, 0.0118, 0.0124, 0.0100, 0.0118, 0.0133, 0.0112,\n",
            "        0.0153, 0.0114, 0.0093, 0.0112, 0.0121, 0.0118, 0.0113, 0.0102, 0.0119,\n",
            "        0.0112, 0.0092, 0.0095, 0.0099, 0.0119, 0.0122, 0.0108, 0.0116, 0.0106,\n",
            "        0.0117, 0.0117, 0.0147, 0.0111, 0.0120, 0.0105, 0.0117, 0.0105, 0.0113,\n",
            "        0.0120, 0.0119, 0.0091, 0.0128, 0.0117, 0.0112, 0.0117, 0.0132, 0.0109,\n",
            "        0.0118, 0.0122, 0.0119, 0.0114, 0.0116, 0.0094, 0.0118, 0.0118, 0.0109,\n",
            "        0.0118, 0.0106, 0.0164, 0.0102, 0.0104, 0.0102, 0.0109, 0.0113, 0.0099,\n",
            "        0.0122, 0.0152, 0.0162, 0.0114, 0.0127, 0.0118, 0.0110, 0.0103, 0.0115,\n",
            "        0.0110, 0.0094, 0.0112, 0.0122, 0.0116, 0.0083],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [63]\n",
            "DEBUGGING: logits looks like: tensor([21.2378, 21.2766, 21.2847, 21.2623, 21.3052, 21.2509, 21.2625, 21.2471,\n",
            "        21.2265, 21.2052, 21.2411, 21.2192, 21.2588, 21.2696, 21.2254, 21.2591,\n",
            "        21.2828, 21.2477, 21.3106, 21.2527, 21.2123, 21.2477, 21.2640, 21.2586,\n",
            "        21.2502, 21.2301, 21.2610, 21.2491, 21.2096, 21.2152, 21.2243, 21.2612,\n",
            "        21.2649, 21.2420, 21.2549, 21.2371, 21.2578, 21.2568, 21.3028, 21.2468,\n",
            "        21.2624, 21.2348, 21.2580, 21.2359, 21.2506, 21.2622, 21.2613, 21.2059,\n",
            "        21.2754, 21.2567, 21.2488, 21.2565, 21.2812, 21.2435, 21.2590, 21.2654,\n",
            "        21.2611, 21.2514, 21.2553, 21.2143, 21.2587, 21.2595, 21.2429, 21.2592,\n",
            "        21.2368, 21.3245, 21.2290, 21.2334, 21.2302, 21.2439, 21.2509, 21.2240,\n",
            "        21.2658, 21.3090, 21.3228, 21.2516, 21.2729, 21.2590, 21.2451, 21.2314,\n",
            "        21.2541, 21.2445, 21.2131, 21.2482, 21.2652, 21.2557, 21.1884],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0122, 0.0129, 0.0122, 0.0105, 0.0107, 0.0108, 0.0103, 0.0100, 0.0118,\n",
            "        0.0098, 0.0114, 0.0116, 0.0063, 0.0100, 0.0094, 0.0099, 0.0121, 0.0108,\n",
            "        0.0098, 0.0100, 0.0103, 0.0113, 0.0094, 0.0109, 0.0107, 0.0108, 0.0110,\n",
            "        0.0112, 0.0099, 0.0120, 0.0073, 0.0130, 0.0105, 0.0106, 0.0115, 0.0106,\n",
            "        0.0113, 0.0119, 0.0094, 0.0104, 0.0120, 0.0111, 0.0098, 0.0074, 0.0082,\n",
            "        0.0067, 0.0104, 0.0106, 0.0098, 0.0101, 0.0127, 0.0097, 0.0108, 0.0102,\n",
            "        0.0097, 0.0103, 0.0108, 0.0090, 0.0111, 0.0113, 0.0102, 0.0104, 0.0115,\n",
            "        0.0086, 0.0118, 0.0110, 0.0096, 0.0095, 0.0093, 0.0099, 0.0094, 0.0110,\n",
            "        0.0107, 0.0105, 0.0087, 0.0095, 0.0107, 0.0108, 0.0113, 0.0119, 0.0107,\n",
            "        0.0112, 0.0129, 0.0067, 0.0103, 0.0098, 0.0109, 0.0110, 0.0106, 0.0110,\n",
            "        0.0087, 0.0100, 0.0111, 0.0104, 0.0098, 0.0103],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [0]\n",
            "DEBUGGING: logits looks like: tensor([21.3222, 21.3336, 21.3219, 21.2915, 21.2950, 21.2980, 21.2881, 21.2825,\n",
            "        21.3146, 21.2773, 21.3080, 21.3117, 21.1880, 21.2812, 21.2693, 21.2809,\n",
            "        21.3208, 21.2972, 21.2783, 21.2820, 21.2888, 21.3056, 21.2703, 21.2997,\n",
            "        21.2949, 21.2967, 21.3016, 21.3045, 21.2804, 21.3182, 21.2193, 21.3352,\n",
            "        21.2910, 21.2927, 21.3103, 21.2937, 21.3071, 21.3169, 21.2704, 21.2889,\n",
            "        21.3178, 21.3028, 21.2781, 21.2204, 21.2424, 21.2025, 21.2896, 21.2929,\n",
            "        21.2784, 21.2843, 21.3296, 21.2752, 21.2968, 21.2859, 21.2753, 21.2873,\n",
            "        21.2966, 21.2608, 21.3024, 21.3067, 21.2855, 21.2905, 21.3101, 21.2517,\n",
            "        21.3142, 21.3008, 21.2748, 21.2721, 21.2682, 21.2791, 21.2688, 21.3014,\n",
            "        21.2961, 21.2925, 21.2551, 21.2711, 21.2964, 21.2981, 21.3065, 21.3170,\n",
            "        21.2946, 21.3048, 21.3336, 21.2011, 21.2883, 21.2774, 21.2986, 21.3013,\n",
            "        21.2934, 21.3018, 21.2533, 21.2820, 21.3026, 21.2907, 21.2771, 21.2888],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0099, 0.0084, 0.0091, 0.0095, 0.0092, 0.0092, 0.0098, 0.0084, 0.0077,\n",
            "        0.0087, 0.0093, 0.0120, 0.0095, 0.0096, 0.0102, 0.0101, 0.0084, 0.0092,\n",
            "        0.0088, 0.0092, 0.0091, 0.0088, 0.0097, 0.0080, 0.0083, 0.0105, 0.0100,\n",
            "        0.0085, 0.0088, 0.0097, 0.0102, 0.0092, 0.0096, 0.0091, 0.0083, 0.0108,\n",
            "        0.0105, 0.0081, 0.0097, 0.0099, 0.0091, 0.0094, 0.0084, 0.0085, 0.0094,\n",
            "        0.0094, 0.0099, 0.0111, 0.0091, 0.0092, 0.0089, 0.0065, 0.0090, 0.0085,\n",
            "        0.0065, 0.0094, 0.0086, 0.0084, 0.0098, 0.0085, 0.0099, 0.0093, 0.0119,\n",
            "        0.0099, 0.0140, 0.0092, 0.0098, 0.0094, 0.0091, 0.0095, 0.0095, 0.0093,\n",
            "        0.0083, 0.0088, 0.0071, 0.0082, 0.0081, 0.0101, 0.0089, 0.0099, 0.0096,\n",
            "        0.0097, 0.0079, 0.0090, 0.0095, 0.0124, 0.0097, 0.0092, 0.0094, 0.0097,\n",
            "        0.0098, 0.0109, 0.0095, 0.0082, 0.0110, 0.0094, 0.0091, 0.0093, 0.0098,\n",
            "        0.0095, 0.0101, 0.0111, 0.0092, 0.0090, 0.0100, 0.0089, 0.0080],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [39]\n",
            "DEBUGGING: logits looks like: tensor([21.2517, 21.2182, 21.2343, 21.2427, 21.2369, 21.2368, 21.2478, 21.2175,\n",
            "        21.2010, 21.2258, 21.2382, 21.2898, 21.2429, 21.2442, 21.2561, 21.2553,\n",
            "        21.2172, 21.2353, 21.2266, 21.2368, 21.2350, 21.2276, 21.2464, 21.2078,\n",
            "        21.2167, 21.2623, 21.2536, 21.2192, 21.2281, 21.2472, 21.2566, 21.2352,\n",
            "        21.2448, 21.2349, 21.2150, 21.2689, 21.2635, 21.2108, 21.2476, 21.2507,\n",
            "        21.2334, 21.2405, 21.2170, 21.2210, 21.2397, 21.2409, 21.2501, 21.2732,\n",
            "        21.2345, 21.2363, 21.2295, 21.1682, 21.2319, 21.2214, 21.1655, 21.2400,\n",
            "        21.2218, 21.2168, 21.2482, 21.2192, 21.2507, 21.2373, 21.2884, 21.2515,\n",
            "        21.3205, 21.2351, 21.2484, 21.2406, 21.2346, 21.2427, 21.2435, 21.2384,\n",
            "        21.2161, 21.2277, 21.1843, 21.2143, 21.2095, 21.2543, 21.2298, 21.2509,\n",
            "        21.2438, 21.2470, 21.2062, 21.2323, 21.2426, 21.2962, 21.2469, 21.2354,\n",
            "        21.2406, 21.2478, 21.2490, 21.2706, 21.2415, 21.2135, 21.2711, 21.2395,\n",
            "        21.2338, 21.2387, 21.2491, 21.2425, 21.2550, 21.2737, 21.2368, 21.2314,\n",
            "        21.2538, 21.2298, 21.2077], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.7058461509354856 and immediate abs rewards look like: [0.007174113898599899, 0.0003030951438631746, 0.0017263400213778368, 0.0011508763618621742, 0.0031081770821401733, 0.004566402024920535, 3.236147085772245e-05, 0.6877847849182217, 0.0, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 1.3642420526593924e-12, 9.094947017729282e-13, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 0.0, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13]\n",
            "DEBUGGING: the total relative reward of the trajectory = 2.069670892234677 and immediate relative rewards look like: [0.002651338434802358, 0.00022462540071935028, 0.0019193150290735487, 0.0017071247944676212, 0.0057655103385053, 0.010176266366531395, 8.428039694159452e-05, 2.047142431276958, 0.0, 2.2737367544328376e-12, 0.0, 0.0, 0.0, 0.0, 3.410605131648481e-12, 1.091393642127762e-11, 7.730704965066375e-12, 0.0, 4.320099833421409e-12, 9.094947017727214e-12, 9.549694368617918e-12, 5.002220859749968e-12, 5.229594535194337e-12, 5.456968210636329e-12, 0.0, 0.0, 0.0, 0.0, 0.0, 6.821210263296962e-12, 7.0485839387417965e-12, 0.0, 7.503331289626658e-12, 7.730704965068132e-12, 7.958078640513122e-12, 8.185452315958215e-12, 8.412825991399586e-12, 8.640199666844783e-12, 0.0, 0.0, 0.0, 9.549694368615746e-12, 1.9554136088122403e-11, 0.0, 2.0463630789886232e-11, 1.0459189070391053e-11, 0.0, 0.0, 0.0, 1.1368683772161603e-11]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0245, 0.0142, 0.0154, 0.0141, 0.0143, 0.0145, 0.0150, 0.0146, 0.0144,\n",
            "        0.0141, 0.0152, 0.0136, 0.0141, 0.0142, 0.0144, 0.0149, 0.0152, 0.0145,\n",
            "        0.0150, 0.0146, 0.0141, 0.0137, 0.0144, 0.0150, 0.0142, 0.0151, 0.0154,\n",
            "        0.0140, 0.0141, 0.0148, 0.0150, 0.0140, 0.0145, 0.0146, 0.0143, 0.0170,\n",
            "        0.0148, 0.0144, 0.0140, 0.0144, 0.0143, 0.0148, 0.0142, 0.0149, 0.0144,\n",
            "        0.0146, 0.0143, 0.0143, 0.0144, 0.0144, 0.0144, 0.0150, 0.0154, 0.0144,\n",
            "        0.0148, 0.0143, 0.0146, 0.0148, 0.0148, 0.0144, 0.0151, 0.0148, 0.0144,\n",
            "        0.0140, 0.0142, 0.0144, 0.0145, 0.0145], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [59]\n",
            "DEBUGGING: logits looks like: tensor([21.4353, 21.3255, 21.3421, 21.3246, 21.3272, 21.3307, 21.3366, 21.3317,\n",
            "        21.3286, 21.3248, 21.3393, 21.3170, 21.3250, 21.3259, 21.3283, 21.3351,\n",
            "        21.3396, 21.3304, 21.3373, 21.3319, 21.3250, 21.3189, 21.3291, 21.3369,\n",
            "        21.3266, 21.3389, 21.3421, 21.3232, 21.3249, 21.3349, 21.3364, 21.3230,\n",
            "        21.3296, 21.3310, 21.3277, 21.3623, 21.3342, 21.3286, 21.3229, 21.3294,\n",
            "        21.3279, 21.3341, 21.3253, 21.3353, 21.3292, 21.3320, 21.3275, 21.3270,\n",
            "        21.3287, 21.3286, 21.3282, 21.3373, 21.3421, 21.3284, 21.3337, 21.3271,\n",
            "        21.3315, 21.3346, 21.3340, 21.3286, 21.3387, 21.3341, 21.3288, 21.3225,\n",
            "        21.3266, 21.3292, 21.3307, 21.3297], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0220, 0.0129, 0.0127, 0.0127, 0.0133, 0.0127, 0.0130, 0.0131, 0.0129,\n",
            "        0.0129, 0.0129, 0.0129, 0.0129, 0.0127, 0.0129, 0.0130, 0.0129, 0.0128,\n",
            "        0.0128, 0.0130, 0.0128, 0.0129, 0.0129, 0.0128, 0.0128, 0.0129, 0.0127,\n",
            "        0.0129, 0.0129, 0.0128, 0.0129, 0.0128, 0.0128, 0.0129, 0.0129, 0.0127,\n",
            "        0.0130, 0.0127, 0.0127, 0.0128, 0.0131, 0.0126, 0.0127, 0.0130, 0.0129,\n",
            "        0.0129, 0.0128, 0.0129, 0.0131, 0.0129, 0.0130, 0.0128, 0.0129, 0.0130,\n",
            "        0.0128, 0.0130, 0.0128, 0.0128, 0.0127, 0.0128, 0.0129, 0.0128, 0.0127,\n",
            "        0.0130, 0.0131, 0.0131, 0.0127, 0.0127, 0.0128, 0.0128, 0.0129, 0.0127,\n",
            "        0.0128, 0.0128, 0.0130, 0.0129, 0.0130], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [70]\n",
            "DEBUGGING: logits looks like: tensor([21.4341, 21.3271, 21.3241, 21.3232, 21.3333, 21.3235, 21.3289, 21.3303,\n",
            "        21.3270, 21.3272, 21.3276, 21.3271, 21.3273, 21.3235, 21.3278, 21.3278,\n",
            "        21.3266, 21.3261, 21.3259, 21.3281, 21.3257, 21.3270, 21.3269, 21.3253,\n",
            "        21.3260, 21.3273, 21.3240, 21.3269, 21.3273, 21.3254, 21.3266, 21.3251,\n",
            "        21.3259, 21.3269, 21.3275, 21.3237, 21.3278, 21.3247, 21.3242, 21.3258,\n",
            "        21.3295, 21.3225, 21.3247, 21.3280, 21.3265, 21.3276, 21.3247, 21.3273,\n",
            "        21.3306, 21.3266, 21.3280, 21.3254, 21.3265, 21.3281, 21.3252, 21.3280,\n",
            "        21.3252, 21.3252, 21.3247, 21.3257, 21.3267, 21.3252, 21.3244, 21.3292,\n",
            "        21.3302, 21.3308, 21.3240, 21.3245, 21.3261, 21.3249, 21.3275, 21.3247,\n",
            "        21.3260, 21.3256, 21.3281, 21.3270, 21.3284], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0202, 0.0112, 0.0113, 0.0112, 0.0112, 0.0113, 0.0112, 0.0112, 0.0113,\n",
            "        0.0114, 0.0113, 0.0112, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113, 0.0113,\n",
            "        0.0111, 0.0112, 0.0112, 0.0112, 0.0113, 0.0112, 0.0113, 0.0114, 0.0113,\n",
            "        0.0112, 0.0114, 0.0113, 0.0112, 0.0114, 0.0113, 0.0112, 0.0113, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0111, 0.0113, 0.0112, 0.0113, 0.0112, 0.0112,\n",
            "        0.0113, 0.0113, 0.0114, 0.0112, 0.0112, 0.0112, 0.0112, 0.0113, 0.0112,\n",
            "        0.0113, 0.0113, 0.0113, 0.0113, 0.0112, 0.0113, 0.0112, 0.0113, 0.0112,\n",
            "        0.0113, 0.0113, 0.0112, 0.0112, 0.0113, 0.0113, 0.0112, 0.0112, 0.0111,\n",
            "        0.0113, 0.0113, 0.0113, 0.0113, 0.0112, 0.0112, 0.0112, 0.0113, 0.0112,\n",
            "        0.0114, 0.0114, 0.0113, 0.0112, 0.0113, 0.0113, 0.0112],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [30]\n",
            "DEBUGGING: logits looks like: tensor([21.4324, 21.3151, 21.3173, 21.3141, 21.3142, 21.3163, 21.3148, 21.3151,\n",
            "        21.3175, 21.3187, 21.3160, 21.3149, 21.3167, 21.3159, 21.3164, 21.3167,\n",
            "        21.3159, 21.3158, 21.3128, 21.3154, 21.3156, 21.3156, 21.3170, 21.3153,\n",
            "        21.3165, 21.3182, 21.3168, 21.3155, 21.3177, 21.3172, 21.3153, 21.3177,\n",
            "        21.3166, 21.3153, 21.3168, 21.3156, 21.3154, 21.3155, 21.3149, 21.3137,\n",
            "        21.3158, 21.3154, 21.3158, 21.3148, 21.3147, 21.3162, 21.3165, 21.3175,\n",
            "        21.3140, 21.3149, 21.3152, 21.3152, 21.3163, 21.3152, 21.3161, 21.3166,\n",
            "        21.3159, 21.3161, 21.3154, 21.3175, 21.3153, 21.3171, 21.3152, 21.3171,\n",
            "        21.3158, 21.3153, 21.3147, 21.3158, 21.3163, 21.3153, 21.3155, 21.3136,\n",
            "        21.3166, 21.3170, 21.3166, 21.3168, 21.3151, 21.3150, 21.3153, 21.3169,\n",
            "        21.3153, 21.3191, 21.3175, 21.3166, 21.3150, 21.3172, 21.3172, 21.3150],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0179, 0.0101, 0.0096, 0.0101, 0.0106, 0.0100, 0.0101, 0.0102, 0.0103,\n",
            "        0.0101, 0.0100, 0.0100, 0.0100, 0.0101, 0.0096, 0.0096, 0.0098, 0.0101,\n",
            "        0.0098, 0.0102, 0.0100, 0.0101, 0.0101, 0.0100, 0.0101, 0.0100, 0.0101,\n",
            "        0.0101, 0.0101, 0.0099, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0097,\n",
            "        0.0100, 0.0100, 0.0100, 0.0093, 0.0101, 0.0101, 0.0087, 0.0096, 0.0101,\n",
            "        0.0100, 0.0102, 0.0101, 0.0119, 0.0101, 0.0096, 0.0100, 0.0099, 0.0103,\n",
            "        0.0101, 0.0101, 0.0094, 0.0098, 0.0100, 0.0098, 0.0100, 0.0101, 0.0100,\n",
            "        0.0101, 0.0100, 0.0101, 0.0100, 0.0101, 0.0101, 0.0101, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0101, 0.0100, 0.0101, 0.0101, 0.0100, 0.0101, 0.0100,\n",
            "        0.0100, 0.0101, 0.0101, 0.0100, 0.0099, 0.0101, 0.0100, 0.0102, 0.0100,\n",
            "        0.0100, 0.0101, 0.0101, 0.0099, 0.0097, 0.0100, 0.0100, 0.0103, 0.0102],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [35]\n",
            "DEBUGGING: logits looks like: tensor([21.4311, 21.3171, 21.3062, 21.3159, 21.3267, 21.3145, 21.3153, 21.3182,\n",
            "        21.3204, 21.3171, 21.3148, 21.3148, 21.3149, 21.3161, 21.3061, 21.3067,\n",
            "        21.3100, 21.3154, 21.3105, 21.3186, 21.3148, 21.3157, 21.3154, 21.3148,\n",
            "        21.3168, 21.3148, 21.3163, 21.3165, 21.3171, 21.3118, 21.3153, 21.3161,\n",
            "        21.3162, 21.3159, 21.3157, 21.3077, 21.3142, 21.3135, 21.3150, 21.3007,\n",
            "        21.3158, 21.3159, 21.2863, 21.3060, 21.3154, 21.3151, 21.3173, 21.3158,\n",
            "        21.3486, 21.3163, 21.3063, 21.3148, 21.3127, 21.3204, 21.3160, 21.3158,\n",
            "        21.3017, 21.3098, 21.3139, 21.3111, 21.3147, 21.3154, 21.3150, 21.3168,\n",
            "        21.3140, 21.3152, 21.3142, 21.3157, 21.3162, 21.3155, 21.3140, 21.3146,\n",
            "        21.3146, 21.3137, 21.3155, 21.3144, 21.3152, 21.3169, 21.3139, 21.3156,\n",
            "        21.3135, 21.3142, 21.3153, 21.3164, 21.3149, 21.3132, 21.3162, 21.3139,\n",
            "        21.3186, 21.3143, 21.3150, 21.3165, 21.3169, 21.3122, 21.3078, 21.3135,\n",
            "        21.3143, 21.3192, 21.3177], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0169, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0092, 0.0093, 0.0093, 0.0093,\n",
            "        0.0092, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0092, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0092, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0092, 0.0093, 0.0093, 0.0093, 0.0093],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [22]\n",
            "DEBUGGING: logits looks like: tensor([21.4359, 21.3164, 21.3162, 21.3160, 21.3160, 21.3172, 21.3167, 21.3165,\n",
            "        21.3161, 21.3163, 21.3160, 21.3161, 21.3161, 21.3161, 21.3164, 21.3160,\n",
            "        21.3168, 21.3161, 21.3162, 21.3161, 21.3167, 21.3165, 21.3162, 21.3155,\n",
            "        21.3164, 21.3163, 21.3162, 21.3156, 21.3162, 21.3165, 21.3164, 21.3164,\n",
            "        21.3160, 21.3160, 21.3162, 21.3159, 21.3162, 21.3162, 21.3162, 21.3163,\n",
            "        21.3168, 21.3164, 21.3162, 21.3162, 21.3159, 21.3162, 21.3162, 21.3170,\n",
            "        21.3160, 21.3165, 21.3170, 21.3162, 21.3160, 21.3162, 21.3162, 21.3162,\n",
            "        21.3161, 21.3157, 21.3157, 21.3157, 21.3163, 21.3164, 21.3167, 21.3161,\n",
            "        21.3165, 21.3160, 21.3161, 21.3156, 21.3160, 21.3162, 21.3162, 21.3165,\n",
            "        21.3160, 21.3159, 21.3168, 21.3162, 21.3167, 21.3165, 21.3163, 21.3163,\n",
            "        21.3170, 21.3160, 21.3165, 21.3166, 21.3162, 21.3162, 21.3164, 21.3159,\n",
            "        21.3166, 21.3163, 21.3162, 21.3167, 21.3160, 21.3164, 21.3164, 21.3159,\n",
            "        21.3163, 21.3164, 21.3166, 21.3158, 21.3164, 21.3163, 21.3152, 21.3163,\n",
            "        21.3163, 21.3160, 21.3164], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.19677955311453843 and immediate abs rewards look like: [0.08755268610684652, 0.026166394062784093, 0.0017833844942742871, 0.0026589366409552895, 0.0004058649203670939, 0.011945509364295503, 0.015562905296519602, 0.0028066373042747728, 0.0012927351567668666, 0.00139807262235081, 0.0025597623666726577, 6.655326092186442e-05, 0.00252687145029995, 0.0002555012374614307, 0.0003281845313267695, 1.5073781014507404e-05, 0.001516922923428865, 0.003607416944078068, 0.0005796930236101616, 0.00012471956870285794, 0.006217957619810477, 0.0009640476498589123, 0.0015097894770406128, 0.004191001602293909, 0.004635468546666743, 0.004218314996933259, 0.0014696217010623513, 0.0004047651077598857, 0.00011295289982626855, 0.00013320026528162998, 0.001112401580940059, 0.00019983561310255027, 0.00019143511394759116, 0.0016560204080633412, 0.00019412031974752608, 2.8691398938462953e-05, 0.00026033558765448106, 4.0996894995259936e-05, 0.0021395394624050823, 6.828209097875515e-05, 0.0005951370869752282, 0.0010071175863686221, 0.0004708132137238863, 0.00012293899112592044, 0.0006697743053791783, 6.365597755575436e-06, 0.00015126185871849884, 0.0005577935248766153, 0.00015543676568086084, 0.0001403107896749134]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.4382597566220915 and immediate relative rewards look like: [0.024271325615498426, 0.014868557168605405, 0.0015314468266052544, 0.003045968568768025, 0.000581620620901438, 0.02054445952899674, 0.031334092498255035, 0.006487137193445838, 0.0033641951515906827, 0.004044093722791792, 0.008148158790957784, 0.00023128078555268163, 0.00951313893548152, 0.0010366599165995143, 0.0014267792627765102, 6.99086504222784e-05, 0.007474860071174456, 0.0188300068314291, 0.0031973329600641347, 0.0007242256052853988, 0.03791334112516309, 0.006169230113293924, 0.01010359247037872, 0.029278679166315214, 0.03377428767514577, 0.032007519741915715, 0.011594284659280677, 0.0033130063726074894, 0.0009576524952982769, 0.0011682969635627273, 0.010082473932857775, 0.001870285709864987, 0.0018477619289216116, 0.016469461272283086, 0.0019883107269223124, 0.00030229022968258223, 0.002819089060913135, 0.0004559751611182177, 0.02442287253027753, 0.0007999276157821332, 0.00714650075160817, 0.012390754390189286, 0.005932169076684257, 0.0015852532840030104, 0.008833096679445174, 8.583299515333181e-05, 0.002083940644032381, 0.007848596761293753, 0.002233048523576508, 0.0020569758593186474]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 3\n",
            "DEBUGGING: the action_prob is: tensor([0.0466, 0.0140, 0.0134, 0.0132, 0.0140, 0.0137, 0.0133, 0.0138, 0.0141,\n",
            "        0.0144, 0.0146, 0.0131, 0.0121, 0.0143, 0.0133, 0.0139, 0.0136, 0.0139,\n",
            "        0.0142, 0.0138, 0.0139, 0.0139, 0.0138, 0.0141, 0.0140, 0.0138, 0.0141,\n",
            "        0.0139, 0.0138, 0.0130, 0.0134, 0.0137, 0.0140, 0.0142, 0.0138, 0.0142,\n",
            "        0.0131, 0.0135, 0.0139, 0.0129, 0.0137, 0.0143, 0.0137, 0.0138, 0.0142,\n",
            "        0.0145, 0.0141, 0.0143, 0.0142, 0.0138, 0.0140, 0.0141, 0.0137, 0.0146,\n",
            "        0.0140, 0.0144, 0.0128, 0.0140, 0.0139, 0.0131, 0.0145, 0.0139, 0.0140,\n",
            "        0.0141, 0.0141, 0.0134, 0.0124, 0.0135, 0.0144, 0.0143],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [52]\n",
            "DEBUGGING: logits looks like: tensor([21.4734, 21.2325, 21.2237, 21.2203, 21.2333, 21.2279, 21.2231, 21.2292,\n",
            "        21.2347, 21.2386, 21.2413, 21.2195, 21.2034, 21.2369, 21.2226, 21.2307,\n",
            "        21.2269, 21.2319, 21.2349, 21.2296, 21.2314, 21.2306, 21.2300, 21.2337,\n",
            "        21.2330, 21.2294, 21.2335, 21.2313, 21.2293, 21.2176, 21.2246, 21.2289,\n",
            "        21.2327, 21.2356, 21.2296, 21.2360, 21.2200, 21.2258, 21.2315, 21.2165,\n",
            "        21.2281, 21.2370, 21.2278, 21.2303, 21.2350, 21.2394, 21.2337, 21.2375,\n",
            "        21.2362, 21.2301, 21.2323, 21.2336, 21.2279, 21.2408, 21.2327, 21.2384,\n",
            "        21.2145, 21.2327, 21.2319, 21.2202, 21.2392, 21.2312, 21.2327, 21.2345,\n",
            "        21.2342, 21.2239, 21.2081, 21.2260, 21.2382, 21.2373],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0418, 0.0125, 0.0125, 0.0126, 0.0125, 0.0125, 0.0124, 0.0125, 0.0123,\n",
            "        0.0125, 0.0125, 0.0124, 0.0123, 0.0125, 0.0126, 0.0127, 0.0126, 0.0125,\n",
            "        0.0125, 0.0125, 0.0126, 0.0125, 0.0119, 0.0123, 0.0124, 0.0123, 0.0124,\n",
            "        0.0125, 0.0125, 0.0125, 0.0124, 0.0125, 0.0124, 0.0125, 0.0121, 0.0125,\n",
            "        0.0125, 0.0123, 0.0125, 0.0122, 0.0124, 0.0125, 0.0124, 0.0123, 0.0125,\n",
            "        0.0124, 0.0124, 0.0123, 0.0125, 0.0125, 0.0124, 0.0124, 0.0124, 0.0125,\n",
            "        0.0124, 0.0125, 0.0125, 0.0124, 0.0126, 0.0125, 0.0123, 0.0125, 0.0125,\n",
            "        0.0125, 0.0123, 0.0124, 0.0125, 0.0125, 0.0125, 0.0126, 0.0125, 0.0122,\n",
            "        0.0125, 0.0126, 0.0125, 0.0125, 0.0124, 0.0124],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [58]\n",
            "DEBUGGING: logits looks like: tensor([21.4807, 21.2386, 21.2385, 21.2406, 21.2387, 21.2391, 21.2381, 21.2395,\n",
            "        21.2363, 21.2390, 21.2385, 21.2377, 21.2366, 21.2391, 21.2404, 21.2418,\n",
            "        21.2411, 21.2392, 21.2388, 21.2384, 21.2399, 21.2394, 21.2289, 21.2351,\n",
            "        21.2382, 21.2357, 21.2370, 21.2383, 21.2392, 21.2388, 21.2372, 21.2390,\n",
            "        21.2377, 21.2385, 21.2332, 21.2385, 21.2393, 21.2360, 21.2392, 21.2341,\n",
            "        21.2380, 21.2387, 21.2377, 21.2360, 21.2397, 21.2371, 21.2381, 21.2354,\n",
            "        21.2389, 21.2393, 21.2372, 21.2382, 21.2377, 21.2389, 21.2382, 21.2391,\n",
            "        21.2387, 21.2381, 21.2402, 21.2397, 21.2367, 21.2389, 21.2397, 21.2393,\n",
            "        21.2352, 21.2377, 21.2385, 21.2387, 21.2383, 21.2414, 21.2388, 21.2339,\n",
            "        21.2386, 21.2407, 21.2388, 21.2395, 21.2381, 21.2372],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0365, 0.0110, 0.0109, 0.0109, 0.0109, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0109, 0.0110, 0.0109, 0.0110, 0.0109, 0.0109, 0.0109, 0.0109,\n",
            "        0.0110, 0.0110, 0.0109, 0.0109, 0.0110, 0.0109, 0.0109, 0.0109, 0.0109,\n",
            "        0.0110, 0.0109, 0.0109, 0.0109, 0.0109, 0.0110, 0.0109, 0.0109, 0.0109,\n",
            "        0.0109, 0.0110, 0.0109, 0.0109, 0.0109, 0.0109, 0.0110, 0.0109, 0.0110,\n",
            "        0.0109, 0.0109, 0.0110, 0.0109, 0.0110, 0.0110, 0.0110, 0.0109, 0.0110,\n",
            "        0.0109, 0.0109, 0.0110, 0.0110, 0.0109, 0.0109, 0.0110, 0.0109, 0.0110,\n",
            "        0.0109, 0.0110, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0110, 0.0110,\n",
            "        0.0109, 0.0109, 0.0110, 0.0109, 0.0109, 0.0110, 0.0109, 0.0110, 0.0109,\n",
            "        0.0110, 0.0109, 0.0109, 0.0110, 0.0109, 0.0110, 0.0110, 0.0109],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [59]\n",
            "DEBUGGING: logits looks like: tensor([21.4866, 21.2465, 21.2457, 21.2454, 21.2458, 21.2469, 21.2460, 21.2462,\n",
            "        21.2463, 21.2462, 21.2458, 21.2460, 21.2459, 21.2465, 21.2457, 21.2457,\n",
            "        21.2457, 21.2459, 21.2461, 21.2463, 21.2458, 21.2458, 21.2466, 21.2456,\n",
            "        21.2457, 21.2459, 21.2457, 21.2470, 21.2459, 21.2460, 21.2460, 21.2459,\n",
            "        21.2461, 21.2455, 21.2458, 21.2460, 21.2457, 21.2463, 21.2456, 21.2459,\n",
            "        21.2456, 21.2447, 21.2471, 21.2458, 21.2466, 21.2457, 21.2454, 21.2463,\n",
            "        21.2459, 21.2462, 21.2464, 21.2473, 21.2459, 21.2464, 21.2458, 21.2459,\n",
            "        21.2464, 21.2462, 21.2454, 21.2455, 21.2465, 21.2457, 21.2464, 21.2460,\n",
            "        21.2470, 21.2460, 21.2457, 21.2458, 21.2457, 21.2459, 21.2461, 21.2462,\n",
            "        21.2460, 21.2455, 21.2462, 21.2458, 21.2456, 21.2468, 21.2458, 21.2469,\n",
            "        21.2460, 21.2462, 21.2457, 21.2458, 21.2461, 21.2452, 21.2461, 21.2465,\n",
            "        21.2459], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0332, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [8]\n",
            "DEBUGGING: logits looks like: tensor([21.4913, 21.2504, 21.2504, 21.2504, 21.2504, 21.2504, 21.2505, 21.2504,\n",
            "        21.2504, 21.2504, 21.2504, 21.2504, 21.2504, 21.2504, 21.2504, 21.2504,\n",
            "        21.2504, 21.2505, 21.2504, 21.2504, 21.2505, 21.2504, 21.2504, 21.2505,\n",
            "        21.2504, 21.2504, 21.2505, 21.2504, 21.2504, 21.2504, 21.2504, 21.2504,\n",
            "        21.2504, 21.2505, 21.2504, 21.2504, 21.2504, 21.2504, 21.2505, 21.2504,\n",
            "        21.2504, 21.2504, 21.2504, 21.2503, 21.2504, 21.2504, 21.2505, 21.2504,\n",
            "        21.2504, 21.2504, 21.2504, 21.2505, 21.2504, 21.2504, 21.2504, 21.2504,\n",
            "        21.2504, 21.2504, 21.2504, 21.2504, 21.2504, 21.2505, 21.2504, 21.2504,\n",
            "        21.2505, 21.2504, 21.2504, 21.2504, 21.2504, 21.2504, 21.2504, 21.2504,\n",
            "        21.2504, 21.2504, 21.2504, 21.2504, 21.2504, 21.2504, 21.2505, 21.2504,\n",
            "        21.2504, 21.2504, 21.2505, 21.2505, 21.2504, 21.2504, 21.2504, 21.2504,\n",
            "        21.2504, 21.2505, 21.2505, 21.2504, 21.2504, 21.2504, 21.2505, 21.2504,\n",
            "        21.2505, 21.2504], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: the action_prob is: tensor([0.0308, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [27]\n",
            "DEBUGGING: logits looks like: tensor([21.4952, 21.2542, 21.2542, 21.2542, 21.2543, 21.2543, 21.2542, 21.2542,\n",
            "        21.2542, 21.2542, 21.2542, 21.2543, 21.2542, 21.2542, 21.2542, 21.2542,\n",
            "        21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542,\n",
            "        21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542,\n",
            "        21.2542, 21.2543, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542,\n",
            "        21.2543, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542,\n",
            "        21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542,\n",
            "        21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542,\n",
            "        21.2542, 21.2542, 21.2543, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542,\n",
            "        21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2543,\n",
            "        21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2543,\n",
            "        21.2542, 21.2543, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542,\n",
            "        21.2542, 21.2542, 21.2542, 21.2543, 21.2542, 21.2542, 21.2542, 21.2543,\n",
            "        21.2542, 21.2542], grad_fn=<MeanBackward1>)\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total abs reward of the trajectory = 0.10144798317287496 and immediate abs rewards look like: [0.013212290489263978, 0.020244628810814902, 0.027742428746933, 0.005343655442175077, 0.001956855527623702, 0.003700656917771994, 0.0004967742420376453, 0.005056231904291053, 0.007738340259038523, 0.004150608780491893, 0.0016136029112203687, 0.0022223196824597835, 0.00019702795361808967, 0.0019044668920287222, 0.0012036384632665431, 0.0011691840390994912, 0.0002492355065442098, 0.0002112986608153733, 0.0015437788938470476, 1.1142152288812213e-05, 8.050882206589449e-05, 2.2575173716177233e-05, 8.787192200543359e-07, 8.534180506103439e-05, 0.00018222886092189583, 3.907492555299541e-06, 1.3190297067922074e-05, 1.5623194485669956e-05, 0.0003179050672770245, 5.3160954848863184e-05, 0.0001405022335347894, 9.984699136111885e-05, 8.59998458508926e-05, 0.00013197812131693354, 2.8821994419558905e-05, 1.1853009254991775e-05, 1.2923426311317598e-05, 6.645985195063986e-06, 1.0582298273220658e-06, 1.8531427940615686e-05, 6.356995072565041e-07, 1.0297182143403916e-05, 8.676265133544803e-05, 1.7593922621017555e-05, 2.5868612283375114e-06, 6.675326403637882e-06, 2.6660360163077712e-05, 3.656750777736306e-07, 4.873753823630977e-06, 9.883812708721962e-06]\n",
            "DEBUGGING: the total relative reward of the trajectory = 0.19139026214474597 and immediate relative rewards look like: [0.004576887564070705, 0.01409042925245648, 0.029168941574369382, 0.00756478647742401, 0.00346935800567895, 0.007878651188458912, 0.0012355193707459138, 0.014374279391744795, 0.0247936506383099, 0.014816990881862373, 0.006345729244998226, 0.009539608961407351, 0.0009169789799248088, 0.009545974700657576, 0.006468481789547531, 0.006705099486637155, 0.0015192989472346883, 0.001363931210251667, 0.010519493439135589, 7.996422449648184e-05, 0.0006066820529258776, 0.00017822339362228812, 7.252577983924425e-06, 0.0007350005237356567, 0.0016348768796887402, 3.646093419157422e-05, 0.00012781306058520184, 0.00015699536106709408, 0.0033086957450127364, 0.0005724333989568343, 0.0015633785401525436, 0.0011469014143661446, 0.0010187516035132973, 0.001610834635622494, 0.0003621452143147626, 0.00015318854608353474, 0.00017166288731937384, 9.066548708770205e-05, 1.4816467564359509e-05, 0.0002661148503353724, 9.35704794970872e-06, 0.0001552640491145153, 0.001339387059225018, 0.0002779289009297371, 4.179330772406562e-05, 0.0001102432075108754, 0.00044986935842577266, 6.301780908013786e-06, 8.574058182981141e-05, 0.00017742794758646434]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 200 number of matrices\n",
            "DEBUGGING: ACT_MAT has 200 number of matrices\n",
            "DEBUGGING: VAL looks like: [[4.064480566521345, 4.092169636552847, 4.114987301533426, 4.152846900685573, 4.19106661734598, 4.221752147631647, 4.2642140451070185, 4.304438085665009, 4.34632628943423, 4.3887787647117795, 4.428010458463326, 4.47273523540617, 4.517649603981361, 4.562123956160421, 4.600571339992126, 4.642616667295468, 4.689391143782951, 4.736749758184269, 4.777002031017058, 4.824477487308632, 4.871961568370192, 4.918295183667047, 4.96647556014541, 5.016441800203785, 5.066688767674122, 5.117699587039839, 5.16872279257677, 5.2173324026670285, 5.266146433813091, 5.319302934121736, 5.372734735228433, 5.42682509747924, 5.4816237926077545, 8.160786375823036e-11, 8.24321856143741e-11, 7.790585843841619e-11, 6.766860809812395e-11, 6.268692670423237e-11, 6.332012798407309e-11, 6.395972523643746e-11, 5.84812396208262e-11, 4.651664514807886e-11, 4.055573963198728e-11, 3.43815093629104e-11, 2.7991799545361525e-11, 2.1384433618247034e-11, 1.4557213035003563e-11, 1.470425559091269e-11, 7.503331289625519e-12, 7.579122514773252e-12], [1.9297769848959017, 1.9465915620819185, 1.966027208768888, 1.983947367413954, 2.0022628713328148, 2.016664001004353, 2.0267552875129513, 2.047142431430313, 1.5490389369627062e-10, 1.5646857949118245e-10, 1.5575236640075718e-10, 1.5732562262702747e-10, 1.5891477033033078e-10, 1.6051997003063714e-10, 1.6214138386933045e-10, 1.603341199370525e-10, 1.509294782987625e-10, 1.4464522558959205e-10, 1.461062884743354e-10, 1.432183723645596e-10, 1.3547820742104283e-10, 1.2720051823477264e-10, 1.2343262361113402e-10, 1.1939699906660575e-10, 1.1509094025855497e-10, 1.1625347500864138e-10, 1.174277525339812e-10, 1.1861389144846586e-10, 1.1981201156410694e-10, 1.210222339031383e-10, 1.1535456933317308e-10, 1.0939998524690028e-10, 1.1050503560292958e-10, 1.0404212556899286e-10, 9.72842632362876e-11, 9.02284692886611e-11, 8.28717343158615e-11, 7.521101850955749e-11, 6.724325135627545e-11, 6.792247611744996e-11, 6.860856173479794e-11, 6.930157750989691e-11, 6.035543751644562e-11, 4.121343578618507e-11, 4.162973311735866e-11, 2.1379901340881236e-11, 1.1031022495444631e-11, 1.1142446965095588e-11, 1.1254996934439987e-11, 1.1368683772161603e-11], [0.3570120085449478, 0.33610169992873673, 0.32447792197993064, 0.32620856076093474, 0.32642686080016836, 0.3291366062416838, 0.3117092391037243, 0.28320721879340327, 0.27951523393935096, 0.27894044321995987, 0.2776730803001698, 0.2722473954638505, 0.27476375220030086, 0.26792991238870645, 0.26958914391121913, 0.2708710754024673, 0.27353653207277273, 0.268749163637978, 0.25244359273388783, 0.2517638987614381, 0.2535754274304573, 0.2178404912174689, 0.2138093546506818, 0.20576339614172026, 0.17826739088424753, 0.14595262950414317, 0.11509607046689643, 0.10454725839153106, 0.10225682022113491, 0.10232239164225923, 0.10217585321080455, 0.09302361543226947, 0.0920740704266712, 0.09113768535126222, 0.07542244856462539, 0.07417589680576069, 0.07461980462230111, 0.07252597531453331, 0.07279797995294454, 0.04886374487138083, 0.048549310359190606, 0.04182101980563882, 0.0297275408236864, 0.024035729037375903, 0.022677248235730196, 0.013983991470994972, 0.014038543914991555, 0.012075356839352703, 0.004269454624301969, 0.0020569758593186474], [0.17442580882787256, 0.17156456693313318, 0.15906478553603706, 0.13120792319360372, 0.12489205728907041, 0.12264919119534491, 0.11592983839079393, 0.11585284749499801, 0.10250360414470022, 0.07849490253170739, 0.06432112287863133, 0.05856100367033647, 0.04951656031204961, 0.04909048619406546, 0.03994395100344231, 0.03381360526656038, 0.02738232907062952, 0.02612427285191397, 0.025010446102689196, 0.01463732592278142, 0.014704405755843373, 0.014240124952441913, 0.014203940968504672, 0.014340089283354292, 0.013742513898604682, 0.012229936382743375, 0.012316641867224042, 0.012311948289534183, 0.012277730230774837, 0.009059630793699092, 0.008572926661355817, 0.007080351637579064, 0.005993384063851434, 0.005024881273068825, 0.003448531957016496, 0.003117562366365387, 0.0029943169901836897, 0.002851165760469006, 0.0027883841145265697, 0.0028015834817800103, 0.0025610794257016544, 0.002577497351264592, 0.0024467003052020975, 0.0011184982282596767, 0.0008490599265958986, 0.0008154208271432656, 0.0007123006258913032, 0.0002650820883490208, 0.0002613942499404111, 0.00017742794758646434]]\n",
            "DEBUGGING: traj_returns = [4.064480566521345, 1.9297769848959017, 0.3570120085449478, 0.17442580882787256]\n",
            "DEBUGGING: actions = [[37], [10], [3], [56], [57], [21], [7], [51], [66], [53], [26], [31], [62], [21], [61], [39], [55], [9], [38], [25], [76], [6], [60], [64], [76], [35], [62], [8], [1], [4], [46], [30], [0], [44], [86], [40], [70], [4], [96], [53], [40], [93], [41], [64], [38], [92], [58], [32], [19], [78], [42], [59], [42], [55], [60], [49], [5], [0], [5], [35], [56], [4], [56], [18], [57], [56], [63], [61], [39], [62], [31], [64], [58], [48], [77], [51], [84], [42], [32], [63], [10], [64], [33], [15], [13], [27], [47], [62], [33], [0], [51], [40], [48], [52], [22], [73], [59], [80], [5], [39], [57], [57], [25], [3], [7], [30], [28], [38], [8], [59], [14], [67], [54], [56], [62], [18], [63], [24], [61], [70], [4], [33], [71], [47], [15], [33], [12], [81], [38], [30], [56], [64], [66], [81], [6], [32], [58], [72], [36], [35], [22], [63], [10], [93], [69], [85], [94], [19], [82], [22], [51], [8], [10], [37], [19], [38], [6], [45], [43], [52], [21], [7], [50], [53], [7], [41], [15], [14], [58], [58], [25], [49], [63], [26], [13], [70], [84], [18], [35], [59], [54], [10], [40], [5], [27], [40], [45], [12], [49], [8], [75], [95], [12], [61], [64], [49], [57], [7], [66], [27]]\n",
            "DEBUGGING: actions length = 200\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-1.8617, -1.2059,  0.9597,  ...,  0.7055,  0.9120, -1.2285],\n",
            "        [-1.8688, -1.2222,  0.9580,  ...,  0.7000,  0.9169, -1.2406],\n",
            "        [-1.8276, -1.1902,  0.9316,  ...,  0.6806,  0.9008, -1.2135],\n",
            "        ...,\n",
            "        [-1.8444, -1.1988,  0.9476,  ...,  0.6942,  0.9069, -1.2223],\n",
            "        [-1.8444, -1.1988,  0.9476,  ...,  0.6942,  0.9069, -1.2223],\n",
            "        [-1.8444, -1.1988,  0.9476,  ...,  0.6942,  0.9069, -1.2223]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-1.8634, -1.2163,  0.9580,  ...,  0.6992,  0.9148, -1.2372],\n",
            "        [-1.8444, -1.1988,  0.9476,  ...,  0.6942,  0.9069, -1.2223],\n",
            "        [-1.8444, -1.1988,  0.9476,  ...,  0.6942,  0.9069, -1.2223],\n",
            "        ...,\n",
            "        [-1.8444, -1.1988,  0.9476,  ...,  0.6942,  0.9069, -1.2223],\n",
            "        [-1.8444, -1.1988,  0.9476,  ...,  0.6942,  0.9069, -1.2223],\n",
            "        [-1.8444, -1.1988,  0.9476,  ...,  0.6942,  0.9069, -1.2223]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([21.4952, 21.2542, 21.2542, 21.2542, 21.2543, 21.2543, 21.2542, 21.2542,\n",
            "        21.2542, 21.2542, 21.2542, 21.2543, 21.2542, 21.2542, 21.2542, 21.2542,\n",
            "        21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542,\n",
            "        21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542,\n",
            "        21.2542, 21.2543, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542,\n",
            "        21.2543, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542,\n",
            "        21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542,\n",
            "        21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542,\n",
            "        21.2542, 21.2542, 21.2543, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542,\n",
            "        21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2543,\n",
            "        21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2543,\n",
            "        21.2542, 21.2543, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542, 21.2542,\n",
            "        21.2542, 21.2542, 21.2542, 21.2543, 21.2542, 21.2542, 21.2542, 21.2543,\n",
            "        21.2542, 21.2542], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[1.63142384e+00 1.63660687e+00 1.64113930e+00 1.64855269e+00\n",
            "  1.66116210e+00 1.67255049e+00 1.67965210e+00 1.68766015e+00\n",
            "  1.18208628e+00 1.18655353e+00 1.19250117e+00 1.20088591e+00\n",
            "  1.21048248e+00 1.21978609e+00 1.22752611e+00 1.23682534e+00\n",
            "  1.24757750e+00 1.25790580e+00 1.26361402e+00 1.27271968e+00\n",
            "  1.28506035e+00 1.28759395e+00 1.29862221e+00 1.30913632e+00\n",
            "  1.31467467e+00 1.31897054e+00 1.32403388e+00 1.33354790e+00\n",
            "  1.34517025e+00 1.35767124e+00 1.37087088e+00 1.38173227e+00\n",
            "  1.39492281e+00 2.40406417e-02 1.97177452e-02 1.93233648e-02\n",
            "  1.94035304e-02 1.88442853e-02 1.88965910e-02 1.29163321e-02\n",
            "  1.27775975e-02 1.10996293e-02 8.04356031e-03 6.28855684e-03\n",
            "  5.88157706e-03 3.69985309e-03 3.68771114e-03 3.08510974e-03\n",
            "  1.13271222e-03 5.58600956e-04]]\n",
            "DEBUGGING: baseline2 looks like: 1.631423842197517\n",
            "DEBUGGING: ADS looks like: [ 2.42774461  2.45089526  2.46903631  2.51490334  2.58212858  2.64470191\n",
            "  2.68452859  2.71901032  2.78657985  2.84375282  2.8895759   2.95012027\n",
            "  3.03638731  3.09747137  3.12922896  3.17683333  3.23774946  3.33311724\n",
            "  3.40834465  3.52285735  3.62238388  3.70748871  3.77381121  3.86499873\n",
            "  3.94316159  4.05285947  4.09810026  4.13990366  4.23998806  4.3655025\n",
            "  4.47436308  4.63056574  4.76542087 -0.61066209 -0.58341778 -0.53723817\n",
            " -0.54079385 -0.411073   -0.30976491 -0.28419105 -0.28525438 -0.2336906\n",
            " -0.23284258 -0.23379861 -0.20002494 -0.1997137  -0.17344324 -0.17360216\n",
            " -0.1268078  -0.03093429  0.29304102  0.30531718  0.32007622  0.34600381\n",
            "  0.39332483  0.43961376  0.44706983  0.46171466 -1.55974644 -1.54502594\n",
            " -1.53843456 -1.52261497 -1.48126229 -1.46465259 -1.47134238 -1.46578334\n",
            " -1.45164168 -1.40363251 -1.36865738 -1.30162014 -1.24957769 -1.21080647\n",
            " -1.19266435 -1.15144307 -1.12352718 -1.06484011 -1.07062253 -1.07742874\n",
            " -1.02615837 -0.95380044 -0.89837166 -0.79625936 -0.71620292 -0.61066209\n",
            " -0.58341778 -0.53723817 -0.54079385 -0.411073   -0.30976491 -0.28419105\n",
            " -0.28525438 -0.2336906  -0.23284258 -0.23379861 -0.20002494 -0.1997137\n",
            " -0.17344324 -0.17360216 -0.1268078  -0.03093429 -1.27972395 -1.30517268\n",
            " -1.32147307 -1.311735   -1.28251118 -1.24791363 -1.26797622 -1.30222055\n",
            " -1.2802312  -1.2660855  -1.26076148 -1.25036757 -1.20649854 -1.19672268\n",
            " -1.20175324 -1.19491226 -1.17810515 -1.13488335 -1.11621379 -1.04985624\n",
            " -0.99600226 -0.99296598 -0.97885499 -0.94567968 -0.94525979 -0.91888748\n",
            " -0.95552646 -0.97288149 -0.92390155 -0.85147805 -0.79619581 -0.70323574\n",
            " -0.62412885 -0.51952441 -0.50799533 -0.46306228 -0.46617404 -0.33854702\n",
            " -0.23696693 -0.2353273  -0.23670507 -0.19186958 -0.20311504 -0.20976288\n",
            " -0.1773477  -0.18572971 -0.1594047  -0.1615268  -0.12253834 -0.02887731\n",
            " -1.46231015 -1.46970981 -1.4868862  -1.50673564 -1.48404598 -1.45440105\n",
            " -1.46375562 -1.46957492 -1.45724283 -1.46653104 -1.47411344 -1.46405396\n",
            " -1.43174573 -1.4155621  -1.43139843 -1.43196973 -1.42425935 -1.37750824\n",
            " -1.34364694 -1.28698281 -1.23487328 -1.19656635 -1.17846041 -1.13710299\n",
            " -1.10978466 -1.05261018 -1.05830589 -1.0651168  -1.01388064 -0.94474081\n",
            " -0.88979873 -0.78917901 -0.71020954 -0.60563721 -0.57996925 -0.53412061\n",
            " -0.53779953 -0.40822183 -0.30697652 -0.28138947 -0.2826933  -0.2311131\n",
            " -0.23039588 -0.23268011 -0.19917589 -0.19889828 -0.17273094 -0.17333708\n",
            " -0.1265464  -0.03075686]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(-0.3404, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: BEFORE the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-3.2145e-04,  3.8813e-03,  6.5832e-05,  ..., -1.1912e-03,\n",
            "         -2.5651e-03, -1.5844e-02],\n",
            "        [-1.8011e-04,  2.1775e-03,  3.7160e-05,  ..., -6.6800e-04,\n",
            "         -1.4388e-03, -8.8273e-03]])\n",
            "   Last layer:\n",
            "tensor([[ 0.0117,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0035,  0.0045,\n",
            "          0.0048,  0.0061,  0.0000,  0.0000,  0.0000,  0.0000,  0.0051,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0103,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0029,  0.0045,\n",
            "          0.0039,  0.0046,  0.0000,  0.0000,  0.0000,  0.0000,  0.0043,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0064,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0019, -0.0027,\n",
            "         -0.0025, -0.0030,  0.0000,  0.0000,  0.0000,  0.0000, -0.0027,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0093,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0028, -0.0038,\n",
            "         -0.0037, -0.0046,  0.0000,  0.0000,  0.0000,  0.0000, -0.0040,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0186,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0055,  0.0075,\n",
            "          0.0075,  0.0093,  0.0000,  0.0000,  0.0000,  0.0000,  0.0079,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0143,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0042,  0.0059,\n",
            "          0.0057,  0.0070,  0.0000,  0.0000,  0.0000,  0.0000,  0.0061,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0039,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0011, -0.0016,\n",
            "         -0.0015, -0.0018,  0.0000,  0.0000,  0.0000,  0.0000, -0.0016,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0040,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0012, -0.0015,\n",
            "         -0.0017, -0.0021,  0.0000,  0.0000,  0.0000,  0.0000, -0.0017,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0058,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0017, -0.0023,\n",
            "         -0.0024, -0.0030,  0.0000,  0.0000,  0.0000,  0.0000, -0.0025,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0096,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0028,  0.0040,\n",
            "          0.0037,  0.0045,  0.0000,  0.0000,  0.0000,  0.0000,  0.0041,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000]])\n",
            "DEBUGGING: AFTER the backward pass, the gradients of the network looks like:\n",
            "   First layer:\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0033,  0.0015, -0.0024,  ...,  0.0008,  0.0015, -0.0282],\n",
            "        [-0.0018,  0.0008, -0.0014,  ...,  0.0004,  0.0009, -0.0159]])\n",
            "   Last layer:\n",
            "tensor([[ 0.0226,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0064,  0.0093,\n",
            "          0.0083,  0.0092,  0.0000,  0.0000,  0.0000,  0.0000,  0.0088,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0181,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0050,  0.0077,\n",
            "          0.0064,  0.0070,  0.0000,  0.0000,  0.0000,  0.0000,  0.0070,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0129,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0036, -0.0054,\n",
            "         -0.0047, -0.0051,  0.0000,  0.0000,  0.0000,  0.0000, -0.0050,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0163,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0046, -0.0068,\n",
            "         -0.0060, -0.0066,  0.0000,  0.0000,  0.0000,  0.0000, -0.0063,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0343,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0096,  0.0143,\n",
            "          0.0125,  0.0138,  0.0000,  0.0000,  0.0000,  0.0000,  0.0133,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0256,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0072,  0.0107,\n",
            "          0.0093,  0.0102,  0.0000,  0.0000,  0.0000,  0.0000,  0.0099,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0063,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0017, -0.0026,\n",
            "         -0.0023, -0.0024,  0.0000,  0.0000,  0.0000,  0.0000, -0.0024,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0082,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0023, -0.0034,\n",
            "         -0.0030, -0.0034,  0.0000,  0.0000,  0.0000,  0.0000, -0.0032,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0106,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0030, -0.0043,\n",
            "         -0.0039, -0.0044,  0.0000,  0.0000,  0.0000,  0.0000, -0.0041,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0167,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0047,  0.0070,\n",
            "          0.0060,  0.0066,  0.0000,  0.0000,  0.0000,  0.0000,  0.0065,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000]])\n",
            "DEBUGGING: training for one iteration takes 0.004417 min:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329,
          "referenced_widgets": [
            "c3bbdea7eb114147a5e441be84d0f0ef",
            "2f96f4aa52a54181a1abe40c14059aac",
            "d5b3791c0ac54fb390106d7f94b4d23d",
            "359e3fd0e6f440e0be84078e0bf2d836",
            "eaccc9080697496e8a1877fb228f1896",
            "1212079cfed84271b7ec828d244d1597",
            "b1197b0edc0249a5bcf2c1f284af8813",
            "f7df16b4afe44f4191845b91a33b369b"
          ]
        },
        "id": "Oe4fCU1V-kAs",
        "outputId": "027e6165-b295-4ffd-b260-55ee4ab83a34"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3bbdea7eb114147a5e441be84d0f0ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>training loss</td><td>â–ƒâ–â–‚â–‚â–ƒâ–‚â–„â–‚â–…â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–…â–…â–ƒâ–‚â–‚â–‚â–…â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–‡â–‚â–ˆâ–‚â–„â–‚â–ƒâ–‚â–‚â–„â–ƒ</td></tr><tr><td>training reward</td><td>â–‡â–‚â–‚â–â–ƒâ–â–ˆâ–†â–‡â–‚â–‚â–‡â–‡â–‚â–‚â–ˆâ–‚â–‚â–â–â–‚â–â–‚â–†â–ˆâ–‚â–â–â–†â–‡â–‚â–‡â–‚â–‡â–‚â–â–â–â–‡â–‚</td></tr><tr><td>training reward moving average</td><td>â–â–â–â–â–â–â–â–â–â–â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>training loss</td><td>-0.34036</td></tr><tr><td>training reward</td><td>0.10145</td></tr><tr><td>training reward moving average</td><td>0.38111</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">desert-flower-2814</strong>: <a href=\"https://wandb.ai/ieor4575-spring2022/finalproject/runs/3rbp8ihl\" target=\"_blank\">https://wandb.ai/ieor4575-spring2022/finalproject/runs/3rbp8ihl</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220509_023104-3rbp8ihl/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = [   54246.9141, 27220988.0000,  3948314.5000,  3902082.5000,\n",
        "        27219886.0000,  3816622.7500, 27175632.0000,  3976903.5000,\n",
        "         3977309.0000,  3961905.7500,  3904790.7500, 23420360.0000,\n",
        "        27218672.0000, 27206276.0000, 27190054.0000,  3976984.7500,\n",
        "         3931598.0000,  3963138.5000, 19419398.0000,  3931559.5000,\n",
        "         7862683.0000, 19299310.0000,  3915878.7500, 15562425.0000,\n",
        "        27141808.0000, 27323768.0000, 11622510.0000, 15451909.0000,\n",
        "        23389744.0000,  7781165.5000, 11871555.0000, 11657742.0000,\n",
        "        27473456.0000, 11592561.0000,  4054315.2500,  7718911.0000,\n",
        "        27136936.0000,  3914171.2500, 23255876.0000, 15599427.0000,\n",
        "        19661078.0000,  3915166.7500,  7554840.5000, 27196042.0000,\n",
        "        11791202.0000, 23477022.0000, 15708443.0000, 23297932.0000,\n",
        "        23511102.0000,  7689591.5000, 27405178.0000,  4021836.7500,\n",
        "        15558291.0000,  3948567.5000,  7915618.5000,  3864714.5000,\n",
        "         3995934.2500, 19536616.0000,  3890406.7500, 27284138.0000,\n",
        "        11731948.0000, 23232848.0000, 19200360.0000, 22949674.0000,\n",
        "        26724900.0000,  3268551.2500, 22647966.0000, 25925040.0000,\n",
        "        25380838.0000,  5669201.5000,  7810961.0000, 13092715.0000,\n",
        "        14364957.0000, 15622157.0000, 27473452.0000]"
      ],
      "metadata": {
        "id": "R5xFgG-ZGZkL"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_t = torch.FloatTensor(a)\n",
        "print(a_t)\n",
        "b_t = a_t /1000000\n",
        "print(b_t)\n",
        "print(torch.nn.functional.softmax(b_t, dim=0))\n",
        "b_t = a_t / a_t.max()\n",
        "print(b_t)\n",
        "torch.nn.functional.softmax(b_t, dim=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5GfEt5DGbcN",
        "outputId": "8bd651ea-5d2c-47d9-de25-661702bd23fd"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([   54246.9141, 27220988.0000,  3948314.5000,  3902082.5000,\n",
            "        27219886.0000,  3816622.7500, 27175632.0000,  3976903.5000,\n",
            "         3977309.0000,  3961905.7500,  3904790.7500, 23420360.0000,\n",
            "        27218672.0000, 27206276.0000, 27190054.0000,  3976984.7500,\n",
            "         3931598.0000,  3963138.5000, 19419398.0000,  3931559.5000,\n",
            "         7862683.0000, 19299310.0000,  3915878.7500, 15562425.0000,\n",
            "        27141808.0000, 27323768.0000, 11622510.0000, 15451909.0000,\n",
            "        23389744.0000,  7781165.5000, 11871555.0000, 11657742.0000,\n",
            "        27473456.0000, 11592561.0000,  4054315.2500,  7718911.0000,\n",
            "        27136936.0000,  3914171.2500, 23255876.0000, 15599427.0000,\n",
            "        19661078.0000,  3915166.7500,  7554840.5000, 27196042.0000,\n",
            "        11791202.0000, 23477022.0000, 15708443.0000, 23297932.0000,\n",
            "        23511102.0000,  7689591.5000, 27405178.0000,  4021836.7500,\n",
            "        15558291.0000,  3948567.5000,  7915618.5000,  3864714.5000,\n",
            "         3995934.2500, 19536616.0000,  3890406.7500, 27284138.0000,\n",
            "        11731948.0000, 23232848.0000, 19200360.0000, 22949674.0000,\n",
            "        26724900.0000,  3268551.2500, 22647966.0000, 25925040.0000,\n",
            "        25380838.0000,  5669201.5000,  7810961.0000, 13092715.0000,\n",
            "        14364957.0000, 15622157.0000, 27473452.0000])\n",
            "tensor([ 0.0542, 27.2210,  3.9483,  3.9021, 27.2199,  3.8166, 27.1756,  3.9769,\n",
            "         3.9773,  3.9619,  3.9048, 23.4204, 27.2187, 27.2063, 27.1901,  3.9770,\n",
            "         3.9316,  3.9631, 19.4194,  3.9316,  7.8627, 19.2993,  3.9159, 15.5624,\n",
            "        27.1418, 27.3238, 11.6225, 15.4519, 23.3897,  7.7812, 11.8716, 11.6577,\n",
            "        27.4735, 11.5926,  4.0543,  7.7189, 27.1369,  3.9142, 23.2559, 15.5994,\n",
            "        19.6611,  3.9152,  7.5548, 27.1960, 11.7912, 23.4770, 15.7084, 23.2979,\n",
            "        23.5111,  7.6896, 27.4052,  4.0218, 15.5583,  3.9486,  7.9156,  3.8647,\n",
            "         3.9959, 19.5366,  3.8904, 27.2841, 11.7319, 23.2328, 19.2004, 22.9497,\n",
            "        26.7249,  3.2686, 22.6480, 25.9250, 25.3808,  5.6692,  7.8110, 13.0927,\n",
            "        14.3650, 15.6222, 27.4735])\n",
            "tensor([1.0010e-13, 6.2920e-02, 4.9158e-12, 4.6937e-12, 6.2851e-02, 4.3093e-12,\n",
            "        6.0130e-02, 5.0584e-12, 5.0605e-12, 4.9831e-12, 4.7065e-12, 1.4067e-03,\n",
            "        6.2775e-02, 6.2001e-02, 6.1004e-02, 5.0588e-12, 4.8344e-12, 4.9893e-12,\n",
            "        2.5740e-05, 4.8342e-12, 2.4637e-10, 2.2827e-05, 4.7589e-12, 5.4393e-07,\n",
            "        5.8130e-02, 6.9731e-02, 1.0579e-08, 4.8702e-07, 1.3643e-03, 2.2708e-10,\n",
            "        1.3571e-08, 1.0959e-08, 8.0991e-02, 1.0267e-08, 5.4655e-12, 2.1338e-10,\n",
            "        5.7848e-02, 4.7508e-12, 1.1933e-03, 5.6443e-07, 3.2777e-05, 4.7556e-12,\n",
            "        1.8109e-10, 6.1370e-02, 1.2523e-08, 1.4887e-03, 6.2944e-07, 1.2446e-03,\n",
            "        1.5403e-03, 2.0721e-10, 7.5646e-02, 5.2909e-12, 5.4168e-07, 4.9171e-12,\n",
            "        2.5976e-10, 4.5216e-12, 5.1556e-12, 2.8941e-05, 4.6393e-12, 6.7022e-02,\n",
            "        1.1803e-08, 1.1662e-03, 2.0676e-05, 8.7858e-04, 3.8313e-02, 2.4910e-12,\n",
            "        6.4976e-04, 1.7217e-02, 9.9914e-03, 2.7477e-11, 2.3395e-10, 4.6021e-08,\n",
            "        1.6424e-07, 5.7741e-07, 8.0991e-02])\n",
            "tensor([0.0020, 0.9908, 0.1437, 0.1420, 0.9908, 0.1389, 0.9892, 0.1448, 0.1448,\n",
            "        0.1442, 0.1421, 0.8525, 0.9907, 0.9903, 0.9897, 0.1448, 0.1431, 0.1443,\n",
            "        0.7068, 0.1431, 0.2862, 0.7025, 0.1425, 0.5665, 0.9879, 0.9946, 0.4230,\n",
            "        0.5624, 0.8514, 0.2832, 0.4321, 0.4243, 1.0000, 0.4220, 0.1476, 0.2810,\n",
            "        0.9878, 0.1425, 0.8465, 0.5678, 0.7156, 0.1425, 0.2750, 0.9899, 0.4292,\n",
            "        0.8545, 0.5718, 0.8480, 0.8558, 0.2799, 0.9975, 0.1464, 0.5663, 0.1437,\n",
            "        0.2881, 0.1407, 0.1454, 0.7111, 0.1416, 0.9931, 0.4270, 0.8456, 0.6989,\n",
            "        0.8353, 0.9728, 0.1190, 0.8244, 0.9436, 0.9238, 0.2064, 0.2843, 0.4766,\n",
            "        0.5229, 0.5686, 1.0000])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0074, 0.0199, 0.0085, 0.0085, 0.0199, 0.0085, 0.0199, 0.0085, 0.0085,\n",
              "        0.0085, 0.0085, 0.0173, 0.0199, 0.0199, 0.0199, 0.0085, 0.0085, 0.0085,\n",
              "        0.0150, 0.0085, 0.0098, 0.0149, 0.0085, 0.0130, 0.0198, 0.0200, 0.0113,\n",
              "        0.0130, 0.0173, 0.0098, 0.0114, 0.0113, 0.0201, 0.0113, 0.0086, 0.0098,\n",
              "        0.0198, 0.0085, 0.0172, 0.0130, 0.0151, 0.0085, 0.0097, 0.0199, 0.0113,\n",
              "        0.0174, 0.0131, 0.0172, 0.0174, 0.0098, 0.0200, 0.0085, 0.0130, 0.0085,\n",
              "        0.0099, 0.0085, 0.0085, 0.0150, 0.0085, 0.0199, 0.0113, 0.0172, 0.0149,\n",
              "        0.0170, 0.0195, 0.0083, 0.0168, 0.0190, 0.0186, 0.0091, 0.0098, 0.0119,\n",
              "        0.0125, 0.0130, 0.0201])"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYdGyAom2SMj",
        "outputId": "5258aef4-2453-4b24-c42e-97792fdde7f2"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2., 0., 1., 3., 0., 0., 0., 3., 2., 3., 1., 1., 2., 0., 4., 4., 0.,\n",
              "       2., 1., 2., 2., 2., 4., 1., 3., 2., 0., 1., 2., 0., 3., 0., 3., 1.,\n",
              "       3., 0., 4., 1., 4., 4., 0., 0., 1., 2., 4., 0., 0., 1., 1., 1., 2.,\n",
              "       3., 4., 4., 3., 3., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s[1][-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WUaedhX198a",
        "outputId": "3fa9e10f-f056-47ac-de33-8c50d67f4b35"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29572964.0"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s[-2] / np.max(s[-2], axis=1, keepdims=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmmYGIMyEQNl",
        "outputId": "ca0deba8-4eae-4a03-b72f-c02a05bf76a4"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.54545455, 0.63636364, 0.81818182, ..., 0.72727273, 0.81818182,\n",
              "        0.63636364],\n",
              "       [0.60089703, 0.79261811, 0.85848273, ..., 0.73489363, 0.71923282,\n",
              "        0.61737557],\n",
              "       [0.60085379, 0.79252935, 0.85855567, ..., 0.73486304, 0.71906795,\n",
              "        0.6173248 ],\n",
              "       ...,\n",
              "       [0.6009077 , 0.79259915, 0.85847814, ..., 0.73489227, 0.71926816,\n",
              "        0.61743666],\n",
              "       [0.6008974 , 0.79256889, 0.85851286, ..., 0.73490929, 0.71925082,\n",
              "        0.61742936],\n",
              "       [0.6009055 , 0.79262137, 0.85848451, ..., 0.73492612, 0.71928375,\n",
              "        0.61740443]])"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(s[-2]), len(s[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GEP-MOCEZKu",
        "outputId": "935d59ba-e3c0-4d2b-9b52-dc93277b408c"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(106, 110)"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([0.0057, 0.0235, 0.0144, 0.0181, 0.0181, 0.0123, 0.0293, 0.0095, 0.0122,\n",
        "        0.0146, 0.0152, 0.0095, 0.0184, 0.0151, 0.0143, 0.0095, 0.0220, 0.0120,\n",
        "        0.0209, 0.0226, 0.0142, 0.0124, 0.0145, 0.0133, 0.0218, 0.0146, 0.0098,\n",
        "        0.0186, 0.0146, 0.0126, 0.0182, 0.0204, 0.0155, 0.0180, 0.0098, 0.0184,\n",
        "        0.0248, 0.0150, 0.0215, 0.0114, 0.0186, 0.0175, 0.0141, 0.0199, 0.0250,\n",
        "        0.0178, 0.0310, 0.0170, 0.0153, 0.0204, 0.0121, 0.0247, 0.0187, 0.0197,\n",
        "        0.0130, 0.0127, 0.0240, 0.0189, 0.0104, 0.0125])\n",
        "a.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlcbwh-Ts6Jt",
        "outputId": "b5ebc057-dbf9-40d4-f2ac-ab76b8693993"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.env_now.env.oldobj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg-stUzCuAdm",
        "outputId": "1a3b0377-c3c1-4c3f-e9d6-70565a29198c"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-2102.7852933458"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.env_now.env.newobj\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyF1UWObuLoZ",
        "outputId": "f5453d42-2014-4de8-f5dd-4c3f37805ec3"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-2102.7852933458"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.env_now.env.ip_obj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPL2qJZ0nNrJ",
        "outputId": "a3b12c33-c11b-44f0-a466-78d6eb3160a4"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-2100.0"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OH notes:\n",
        "\n",
        "baseline is what you use to interprete the reward\n",
        "\n",
        "don't use nn, use \"mean\" of rewards in this episode (I think so)\n",
        "\n",
        "Maybe look in to the instances, and look at how your agent is solving them\n",
        "\n",
        "Cutting off early, but only after solved some LP's\n",
        "- counterfactual exploration\n",
        "  - more than to do it more random\n",
        "- all prob entirely the same\n",
        "- activation function ?\n",
        "\n",
        "not learning? \n",
        "\n",
        "reward shaping?\n",
        "- someone: amplify the reward (shouldn't, since every step wil GIVE a reward), compare it to other POSSIBLE states\n",
        "- pre-trained on the instances and pre-trained the baseline?\n",
        "- get the max reward from the LP solver (isn't this cheating lol)\n",
        "- recalcluate the max-gap-to-go (lol remaining max gap) every step\n",
        "- go in the environment to make if return the shaped new-gap reward\n",
        "- the original reward doesn't help you across LPs?\n",
        "- moving average of *returns* from all *previous* episodes\n",
        "  - return is the discounted sum of the reward\n",
        "- advantage? Q - running averaged RETURN (but different states are mixed in, how do you deal with that)\n",
        "- Think about what's wrong with this base line, and write it down\n",
        "  - something about the states\n",
        "\n",
        "\n",
        "differences between LPs\n",
        "\n",
        "mode: \n",
        "- standardize the constraints, and the b vector by itself\n",
        "  - do you normalize by row or columns?\n",
        "  - He thinks it's more sense to normalize by rows\n",
        "  - Normalize it twice? LOL\n",
        "- or a normalization layer\\\n",
        "\n",
        "\n",
        "When doing softmax you can use a lamda (parameter) to mitigate large score difference\n",
        "- softmax comes with a scalar parameter\n",
        "\n",
        "Professor didn't think normalizing the input is necessary & and it shouldn't make a difference anyways (but come numerical value can be lost after normalization)\n",
        "\n",
        "iteration: \n",
        "\n",
        "plot random policy together, (as a bseline to see if your model is really learning)\n",
        "\n",
        "For baseline, Prof is suggesting Q network can work too.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ha4GPGL8Dduc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sKnhC54KYNNL"
      },
      "execution_count": 156,
      "outputs": []
    }
  ]
}