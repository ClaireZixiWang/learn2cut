{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_policy_gradient.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "67502f850aa0440d99224a3c7ae9bcba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32b2a0d3060c4c0a9b85112f194d9c73",
              "IPY_MODEL_de2f282e3e9f49d58aa4b6c135cbe29d"
            ],
            "layout": "IPY_MODEL_712ba6cace53471ab81cfbd326eb9517"
          }
        },
        "32b2a0d3060c4c0a9b85112f194d9c73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0da138971674feca76c536de92b2e8e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7113ab569bcb4a058583df5abbd031b3",
            "value": "0.255 MB of 0.255 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "de2f282e3e9f49d58aa4b6c135cbe29d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79a46285cbd14a499e611b013668cefc",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_22da20e3ebb040d59d0e4556e7c0dc1a",
            "value": 1
          }
        },
        "712ba6cace53471ab81cfbd326eb9517": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0da138971674feca76c536de92b2e8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7113ab569bcb4a058583df5abbd031b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79a46285cbd14a499e611b013668cefc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22da20e3ebb040d59d0e4556e7c0dc1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ClaireZixiWang/learn2cut/blob/main/Project_policy_gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## See README.md file for further details about the project and the environment.\n",
        "\n",
        "### State-Action Description\n",
        "\n",
        "### State\n",
        "State s is an array with give components\n",
        "\n",
        "* s[0]:  constraint matrix $A$of the current LP ($\\max  -c^Tx \\text{ s.t. }Ax \\le  b$) . Dimension is $m \\times n$. See by printing s[0].shape. Here $n$ is the (fixed) number of variables. For instances of size 60 by 60 used in the above command, $n$ will remain fixed as 60. And $m$ is the current number of constraints. Initially, $m$ is to the number of constraints in the IP instance. (For instances generated with --num-c=60, $m$ is 60 at the first step).  But $m$ will increase by one in every step of the episode as one new constraint (cut) is added on taking an action.\n",
        "* s[1]: rhs $b$ for the current LP ($Ax\\le b$). Dimension same as the number $m$ in matrix A.\n",
        "* s[2]: coefficient vector $c$ from the LP objective ($-c^Tx$). Dimension same as the number of variables, i.e., $n$.\n",
        "* s[3],  s[4]: Gomory cuts available in the current round of Gomory's cutting plane algorithm. Each cut $i$ is of the form $D_i x\\le d_i$.   s[3] gives the matrix $D$ (of dimension $k \\times n$) of cuts and s[4] gives the rhs $d$ (of dimension $k$). The number of cuts $k$ available in each round changes, you can find it out by printing the size of last component of state, i.e., s[4].size or s[-1].size.\n",
        "\n",
        "### Actions\n",
        "There are k=s[4].size actions available in each state $s$, with $i^{th}$ action corresponding to the $i^{th}$ cut with inequality $D_i x\\le d_i$ in $s[3], s[4]$."
      ],
      "metadata": {
        "id": "5TN-sMTvcG_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***QUESTIONS***:\n",
        "1. By \"current\" LP, you mean the LP that the agent was running in the last state? As in, the LP with all the added constraints? \n",
        "  * ==> I think so.\n",
        "1. What do you mean Gomory cuts *available*? As in, after doing Simplex methods, the *variables* that you can choose to cut?\n",
        "  * Yes I think so.\n",
        "2. Isn't the number of variables (n) changing? in the C-G cutting plane method?\n",
        "  * No, as the spec says, **$n$ is the fixed number of variables**.\n",
        "  * If you look that cuttng plane lecture notes, you can see that after each step, the dummy variable is not added in the constraint. They are merely there for the sake of the LP solver (simplex method), but not really relevant for us.\n",
        "    * This is not correct, I think they are still very much relevant, it's just that I think among the 60 variables a lot of them are space holders for dummy variables so that our $n$ is fixed, so that we don't have to worry about using LSTM. Since each time the sequence [a, b] will be of size n+1. And we can just use a fixed-input-size network to do that.\n",
        "    * But still need to verify with the TA about the place holder understanding.\n",
        "3. dimension of s[3] and s[4]? Where is the \"available all\" stored? In which dimension?\n",
        "  * Each row of D is an \"available cut\". Therefore each $D_i x\\le d_i$ is an \"available\" cut in CG method solved from the simplex method.\n",
        "4. pointing towards the slides: why does the number of constraints m increase 1 in each step, if you can choose *multiple* cuts in one step? (OR in the algorithm we just choose one cut each time? or is that a more vanilla version to start, but to expand on multiple cuts a time later?)\n",
        "5. What do you mean by each \"instance\"?"
      ],
      "metadata": {
        "id": "XJE0bz30UL1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYROdPaaZOVa",
        "outputId": "9e5a031b-6052-49dd-a220-9ce2ae34674a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -i https://pypi.gurobi.com gurobipy"
      ],
      "metadata": {
        "id": "xSXTKB2zurrt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3bcf39d-bcc6-4bb3-e2e3-6a8b2a83723c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.gurobi.com\n",
            "Requirement already satisfied: gurobipy in /usr/local/lib/python3.7/dist-packages (9.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qqq"
      ],
      "metadata": {
        "id": "YULy9ymNvDxN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/IEOR_RL/Project_learn2cut\n",
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "lTnvB0_iZUrX",
        "outputId": "f026d5aa-84b4-462a-95a0-370746d1d8c4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive/IEOR_RL/Project_learn2cut'\n",
            "/content/drive/MyDrive/IEOR_RL/Project_learn2cut\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/IEOR_RL/Project_learn2cut'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import time"
      ],
      "metadata": {
        "id": "Q8PiSPj5us0O"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(4, 40),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(40, 20), \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(20, 10)\n",
        "        )\n",
        "\n",
        "datapoint = torch.FloatTensor([\n",
        "                               [[1,2,3,4],\n",
        "                                [2,3,4,5]],\n",
        "                               [[3,4,5,6],\n",
        "                                [4,5,6,7]]\n",
        "                              ])\n"
      ],
      "metadata": {
        "id": "ojB8UvBAPAWk"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(datapoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r_nraniPaKl",
        "outputId": "ad2cf321-9886-4022-bcc8-3de366ccc360"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.2300,  0.1828, -0.2706,  0.1732, -0.1912,  0.3011, -0.4173,\n",
              "           0.1674,  0.2598, -0.0838],\n",
              "         [-0.3068,  0.2656, -0.3352,  0.2436, -0.2090,  0.3181, -0.5276,\n",
              "           0.2421,  0.3396, -0.0769]],\n",
              "\n",
              "        [[-0.3779,  0.3449, -0.3861,  0.3090, -0.2254,  0.3335, -0.6280,\n",
              "           0.3175,  0.4117, -0.0791],\n",
              "         [-0.4490,  0.4243, -0.4370,  0.3744, -0.2417,  0.3489, -0.7284,\n",
              "           0.3929,  0.4839, -0.0812]]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Code for Policy Model\n",
        "\n",
        "class Policy(object):\n",
        "\n",
        "    # inputsize = n+1 = 61\n",
        "    def __init__(self, lr, input_size, attention_size=10, temperature=1) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_size, 40),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(40, 30), \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(30, 20), \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(20, attention_size)\n",
        "        )\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "        # RECORD HYPER-PARAMS\n",
        "        self.input_size = input_size\n",
        "        self.attention_size = attention_size\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def compute_logits_big_batch(self, obs_matrix, act_matrix, batchsize):\n",
        "        '''\n",
        "        Function that takes in a batch of observations and computes the action logits for each of observations in this batch\n",
        "        Args: obs_matrix: np.array(m * batchsize, n+1) # TODO maybe change this to tensor\n",
        "              act_matrix: np.array(k * batchsize, n+1)\n",
        "        Return: batch_logit: tensor(batchsize, k)\n",
        "        '''\n",
        "\n",
        "        # Get the batch result\n",
        "\n",
        "        # transform to tensor\n",
        "        obs_attention = self.model(torch.FloatTensor(obs_matrix)) # tensor(m * batchsize, u)\n",
        "        act_attention = self.model(torch.FloatTensor(act_matrix)) # tensor(k * batchsize, u)\n",
        "\n",
        "        assert obs_attention.shape == (obs_matrix.shape[0], self.attention_size)\n",
        "        assert act_attention.shape == (act_matrix.shape[0], self.attention_size)\n",
        "\n",
        "        # split a batch of output of size tensor(m * batchsize, u) into (batchsize, m, u)\n",
        "        batch_obs_output = torch.reshape(obs_attention, (batchsize, obs_attention.shape[0]/batchsize, obs_attention.shape[1]))\n",
        "        \n",
        "        # split a batch of output of size tensor(k * batchsize, u) into (batchsize, k, u)\n",
        "        batch_act_output = torch.reshape(act_attention, (batchsize, act_attention.shape[0]/batchsize, act_attention.shape[1]))\n",
        "\n",
        "        # To do batch matrix multiplication, transpose (batchsize, k, u) into (batchsize, u, k)\n",
        "        # (batchsize, m, u) @ (batchsize, u, k) =  (batchsize, m, k)\n",
        "        # (batchsize, m, k) == mean across all observation ==> (batchsize, k)\n",
        "        batch_logit = torch.bmm(batch_obs_output, batch_act_output.transpose(0, 2, 1)).mean(dim=1)\n",
        "\n",
        "        assert batch_logit.shape == (batchsize, act_matrix.shape[0]/batchsize)\n",
        "\n",
        "        return batch_logit\n",
        "\n",
        "    def compute_batch_selected_prob(self, batch_probs):\n",
        "        '''\n",
        "        Function that takes in a batch of probabilities and return the max probability for each \"datapoint\" in the batch\n",
        "        Args:    batch_logit: tensor(batchsize, k)\n",
        "        Return:  batch_selected_prob: tensor(batchsize,)\n",
        "        '''\n",
        "        # TODO\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def compute_one_step_logits(self, obs, act):\n",
        "        '''\n",
        "        Function that takes in ONE observation and action space, computes the action logits\n",
        "        Args:   obs_matrix: np.array(m, n+1)\n",
        "                act_matrix: np.array(k, n+1)\n",
        "        Return: logit: tensor(k)\n",
        "        '''\n",
        "\n",
        "        obs_attention = self.model(torch.FloatTensor(obs)) #-> (m, 10)\n",
        "        act_attention = self.model(torch.FloatTensor(act)) #-> (k, 10)\n",
        "        # print(\"DEBUGGING: obs_attention looks like:\", obs_attention)\n",
        "        # print(\"DEBUGGING: act_attention looks like:\", act_attention)\n",
        "\n",
        "        # attention matrix multiplication & mean to get the score\n",
        "        logits = torch.mm(obs_attention, act_attention.transpose(1, 0)).mean(dim=0)\n",
        "        # print(\"DEBUGGING: logits looks like:\", logits)\n",
        "\n",
        "        # print(\"DEBUGGING: act.shape =\", act_attention.shape)\n",
        "        # print(\"DEBUGGING: logits.shape =\", logits.shape)\n",
        "        assert logits.shape[0] == act_attention.shape[0]\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def predict_prob(self, obs, act):\n",
        "        # Function that uses softmax to transform logits to probabilities\n",
        "        # TODO: What shape should obs and act take?\n",
        "        # Args:   obs_matrix: np.array(m, n+1)\n",
        "        #         act_matrix: np.array(k, n+1)\n",
        "        # Return: probs: tensor(k)\n",
        "\n",
        "        logits = self.compute_one_step_logits(obs, act)\n",
        "\n",
        "        # TODO: softmax parameter lamda?\n",
        "        probs = torch.nn.functional.softmax(logits/self.temperature, dim=0)\n",
        "        return probs\n",
        "\n",
        "    def selected_prob(self, probs, action):\n",
        "\n",
        "\n",
        "        # TODO: do I need this function?\n",
        "        one_hot = torch.zeros()\n",
        "        return probs.max()\n",
        "\n",
        "    def choose_action(self, obs, act):\n",
        "\n",
        "        # TODO: is this returning a number?\n",
        "        return torch.argmax(self.predict_prob(obs, act)).item()\n",
        "\n",
        "\n",
        "    def train(self, obs_matrix, act_matrix, actions, Qs):\n",
        "        \"\"\"\n",
        "        Args: obs_matrix: np.array(batchsize * m, n+1) => changed to [np.array(m, n+1)] * batchsize, note that m are varied!\n",
        "              act_matrix: np.array(batchsize * k, n+1)  \n",
        "              actions: => [[action number]] * batchsize      \n",
        "              Qs: np.array(batchsize, )\n",
        "        \"\"\"\n",
        "        # Convert numpy array to tensor\n",
        "\n",
        "        # use compute_batch_prob to compute the batch logits for every datapoint in batch.\n",
        "        # use compute_batch_selected_prob the compute the max probabilty for every datapoint in batch.\n",
        "        start = time.time()\n",
        "        print(\"DEBUGGING: I'm inside the training now!\")\n",
        "        \n",
        "        Qs = torch.FloatTensor(Qs)\n",
        "\n",
        "\n",
        "        # Try using a for loop first, see whether it really cost too much time & whether it works at all\n",
        "        prob_selected = torch.zeros(len(actions))\n",
        "        for i in range(len(obs_matrix)):\n",
        "            probs = self.predict_prob(obs_matrix[i], act_matrix[i])\n",
        "            one_hot = torch.zeros_like(probs)\n",
        "            one_hot[actions[i][0]] = 1\n",
        "            prob_selected[i] = torch.sum(probs * one_hot)\n",
        "\n",
        "\n",
        "        # For robustness add in noise for prob_selected # TODO: why do this?\n",
        "\n",
        "        # define loss function as in lab 4\n",
        "        # TODO define loss function as described in the text above\n",
        "        loss = - (torch.sum(Qs * torch.log(prob_selected)) / (len(obs_matrix) + 1))\n",
        "        print(\"DEBUGGING: the loss =\", loss)\n",
        "\n",
        "        # backward pass\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # step\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "        print(\"DEBUGGING: training for one iteration takes %f min:\" % ((time.time() - start)/60))\n",
        "\n",
        "\n",
        "        # return detached loss (why?)\n",
        "        return loss.detach().cpu().data.numpy()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HViRnY1ssGfc"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(policy.model, (61,))\n",
        "print(policy.model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGm4an0pWPHr",
        "outputId": "b1a0b79b-fe08-4af7-ccbf-0ecd51c0c679"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                   [-1, 40]           2,480\n",
            "              ReLU-2                   [-1, 40]               0\n",
            "            Linear-3                   [-1, 30]           1,230\n",
            "              ReLU-4                   [-1, 30]               0\n",
            "            Linear-5                   [-1, 20]             620\n",
            "              ReLU-6                   [-1, 20]               0\n",
            "            Linear-7                   [-1, 10]             210\n",
            "================================================================\n",
            "Total params: 4,540\n",
            "Trainable params: 4,540\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.02\n",
            "Estimated Total Size (MB): 0.02\n",
            "----------------------------------------------------------------\n",
            "Sequential(\n",
            "  (0): Linear(in_features=61, out_features=40, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=40, out_features=30, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=30, out_features=20, bias=True)\n",
            "  (5): ReLU()\n",
            "  (6): Linear(in_features=20, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(policy.model[6].weight.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfkXKSJWW6OR",
        "outputId": "47a4e28e-faf9-47fc-fb45-37db9f49c5ea"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 4.8296e-07,  0.0000e+00,  6.9802e-07, -5.9552e-09,  0.0000e+00,\n",
            "          0.0000e+00,  4.6265e-07, -1.4381e-07,  7.3074e-08,  0.0000e+00,\n",
            "          4.4568e-07,  1.4648e-07,  2.0036e-07,  6.6785e-07,  7.2504e-07,\n",
            "          0.0000e+00,  0.0000e+00,  3.8387e-07,  0.0000e+00, -1.3685e-08],\n",
            "        [ 6.4677e-07,  0.0000e+00,  8.1809e-07, -9.2750e-08,  0.0000e+00,\n",
            "          0.0000e+00,  5.6302e-07, -2.8808e-07,  2.8375e-08,  0.0000e+00,\n",
            "          6.2300e-07,  1.1607e-07,  2.2978e-07,  8.1646e-07,  9.5672e-07,\n",
            "          0.0000e+00,  0.0000e+00,  4.8390e-07,  0.0000e+00, -1.5723e-07],\n",
            "        [ 1.5878e-07,  0.0000e+00,  1.9228e-07, -2.8985e-08,  0.0000e+00,\n",
            "          0.0000e+00,  1.3421e-07, -7.7694e-08,  2.1374e-09,  0.0000e+00,\n",
            "          1.5489e-07,  2.2688e-08,  5.3623e-08,  1.9501e-07,  2.3384e-07,\n",
            "          0.0000e+00,  0.0000e+00,  1.1669e-07,  0.0000e+00, -4.7916e-08],\n",
            "        [ 1.4809e-07,  0.0000e+00,  2.0150e-07, -1.0690e-08,  0.0000e+00,\n",
            "          0.0000e+00,  1.3606e-07, -5.4266e-08,  1.4892e-08,  0.0000e+00,\n",
            "          1.3948e-07,  3.6572e-08,  5.7264e-08,  1.9654e-07,  2.2064e-07,\n",
            "          0.0000e+00,  0.0000e+00,  1.1464e-07,  0.0000e+00, -1.8974e-08],\n",
            "        [ 5.3941e-07,  0.0000e+00,  6.2693e-07, -1.1795e-07,  0.0000e+00,\n",
            "          0.0000e+00,  4.4403e-07, -2.8577e-07, -8.4196e-09,  0.0000e+00,\n",
            "          5.3251e-07,  5.9285e-08,  1.7369e-07,  6.4497e-07,  7.9172e-07,\n",
            "          0.0000e+00,  0.0000e+00,  3.8985e-07,  0.0000e+00, -1.9396e-07],\n",
            "        [ 8.7138e-08,  0.0000e+00,  8.4566e-08, -3.1165e-08,  0.0000e+00,\n",
            "          0.0000e+00,  6.3431e-08, -5.9850e-08, -1.1316e-08,  0.0000e+00,\n",
            "          8.9616e-08, -1.8484e-09,  2.2507e-08,  9.3052e-08,  1.2572e-07,\n",
            "          0.0000e+00,  0.0000e+00,  5.8587e-08,  0.0000e+00, -5.1025e-08],\n",
            "        [ 1.5119e-08,  0.0000e+00,  2.3394e-08,  9.7690e-10,  0.0000e+00,\n",
            "          0.0000e+00,  1.5259e-08, -3.2177e-09,  3.2020e-09,  0.0000e+00,\n",
            "          1.3599e-08,  5.6586e-09,  6.7799e-09,  2.1933e-08,  2.2883e-08,\n",
            "          0.0000e+00,  0.0000e+00,  1.2415e-08,  0.0000e+00,  1.3987e-09],\n",
            "        [ 8.6208e-07,  0.0000e+00,  1.0744e-06, -1.3536e-07,  0.0000e+00,\n",
            "          0.0000e+00,  7.4337e-07, -3.9767e-07,  2.8030e-08,  0.0000e+00,\n",
            "          8.3360e-07,  1.4330e-07,  3.0103e-07,  1.0776e-06,  1.2725e-06,\n",
            "          0.0000e+00,  0.0000e+00,  6.4014e-07,  0.0000e+00, -2.2608e-07],\n",
            "        [-4.7208e-07,  0.0000e+00, -4.6757e-07,  1.6125e-07,  0.0000e+00,\n",
            "          0.0000e+00, -3.4905e-07,  3.1557e-07,  5.5135e-08,  0.0000e+00,\n",
            "         -4.8315e-07,  3.5116e-09, -1.2516e-07, -5.1152e-07, -6.8186e-07,\n",
            "          0.0000e+00,  0.0000e+00, -3.2051e-07,  0.0000e+00,  2.6312e-07],\n",
            "        [ 3.1545e-07,  0.0000e+00,  3.7506e-07, -6.3014e-08,  0.0000e+00,\n",
            "          0.0000e+00,  2.6298e-07, -1.6094e-07, -5.8694e-10,  0.0000e+00,\n",
            "          3.0930e-07,  3.9747e-08,  1.0377e-07,  3.8172e-07,  4.6360e-07,\n",
            "          0.0000e+00,  0.0000e+00,  2.2970e-07,  0.0000e+00, -1.0479e-07]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP model for policy model:\n",
        "#   model.forward\n",
        "#   model.train --> What is in this function? what are the function arguments?\n",
        "# Baseline function b(s) ==> Okay maybe we stil need the V model as a proper baseline\n",
        "\n",
        "\n",
        "# Q value model ==> Discard this right now, just do vanilla policy gradient\n",
        "#   Can I just use the one in Lab4? What does it mean? what does the states and actions mean? --> Print out the s, r to check\n",
        "#   What is a Q-value in our set-up?\n",
        "#   How do I used this? \n",
        "#   (What's the baseline function??)"
      ],
      "metadata": {
        "id": "ACiQz-gESgOO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "2xI0riE6md5V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "18ea7017-c038-4ce2-d865-f86a08d8790b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/My Drive/IEOR_RL/Project_learn2cut/wandb/run-20220507_205056-3rwptiep</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/ieor4575-spring2022/finalproject/runs/3rwptiep\" target=\"_blank\">sandy-dawn-2077</a></strong> to <a href=\"https://wandb.ai/ieor4575-spring2022/finalproject\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# import gymenv_v2\n",
        "from gymenv_v2 import make_multiple_env\n",
        "\n",
        "\n",
        "import wandb\n",
        "wandb.login()\n",
        "run=wandb.init(project=\"finalproject\", entity=\"ieor4575-spring2022\", tags=[\"training-easy\"])\n",
        "#run=wandb.init(project=\"finalproject\", entity=\"ieor-4575\", tags=[\"training-hard\"])\n",
        "#run=wandb.init(project=\"finalproject\", entity=\"ieor-4575\", tags=[\"test\"])\n",
        "\n",
        "### TRAINING\n",
        "\n",
        "# Setup: You may generate your own instances on which you train the cutting agent.\n",
        "custom_config = {\n",
        "    \"load_dir\"        : 'instances/randomip_n60_m60',   # this is the location of the randomly generated instances (you may specify a different directory)\n",
        "    \"idx_list\"        : list(range(20)),                # take the first 20 instances from the directory\n",
        "    \"timelimit\"       : 50,                             # the maximum horizon length is 50\n",
        "    \"reward_type\"     : 'obj'                           # DO NOT CHANGE reward_type\n",
        "}\n",
        "\n",
        "# Easy Setup: Use the following environment settings. We will evaluate your agent with the same easy config below:\n",
        "easy_config = {\n",
        "    \"load_dir\"        : 'instances/train_10_n60_m60',\n",
        "    \"idx_list\"        : list(range(1)),\n",
        "    \"timelimit\"       : 50,\n",
        "    \"reward_type\"     : 'obj'\n",
        "}\n",
        "\n",
        "# Hard Setup: Use the following environment settings. We will evaluate your agent with the same hard config below:\n",
        "hard_config = {\n",
        "    \"load_dir\"        : 'instances/train_100_n60_m60',\n",
        "    \"idx_list\"        : list(range(99)),\n",
        "    \"timelimit\"       : 50,\n",
        "    \"reward_type\"     : 'obj'\n",
        "}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._C import dtype\n",
        "def discounted_rewards(r, gamma):\n",
        "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "    discounted_r = np.zeros_like(r, dtype=float)\n",
        "    running_sum = 0\n",
        "    for i in reversed(range(0,len(r))):\n",
        "        discounted_r[i] = running_sum * gamma + r[i]\n",
        "        running_sum = discounted_r[i]\n",
        "    return list(discounted_r)"
      ],
      "metadata": {
        "id": "COyIO0TEDRjt"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = make_multiple_env(**easy_config) \n",
        "s = env.reset()   # samples a RANDOM INSTANCE every time env.reset() is called\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFhvgi_q53V4",
        "outputId": "0ae42ec8-a76c-435c-e3c2-74e463682883"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading training instances, dir instances/train_10_n60_m60 idx 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "A, b, c0, cuts_a, cuts_b = s\n",
        "concat = np.hstack((s[0], np.expand_dims(s[1], axis=1)))\n",
        "concat\n",
        "# np.linalg.norm(concat, axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJXouQZx9DQ7",
        "outputId": "ed3047dd-3a5e-4df2-c340-1e7d3fa2fd52"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  2,   0,   1, ...,   0,   0, 586],\n",
              "       [  2,   3,   0, ...,   1,   1, 580],\n",
              "       [  0,   4,   4, ...,   4,   4, 564],\n",
              "       ...,\n",
              "       [  4,   2,   3, ...,   1,   1, 598],\n",
              "       [  4,   4,   4, ...,   2,   0, 592],\n",
              "       [  1,   2,   2, ...,   2,   3, 580]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.expand_dims(s[1], axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSUhT5go6czx",
        "outputId": "a6fe47dd-b4f8-432e-d131-afea34516993"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[586],\n",
              "       [580],\n",
              "       [564],\n",
              "       [575],\n",
              "       [562],\n",
              "       [563],\n",
              "       [568],\n",
              "       [594],\n",
              "       [585],\n",
              "       [585],\n",
              "       [543],\n",
              "       [552],\n",
              "       [569],\n",
              "       [598],\n",
              "       [548],\n",
              "       [547],\n",
              "       [544],\n",
              "       [557],\n",
              "       [542],\n",
              "       [547],\n",
              "       [582],\n",
              "       [542],\n",
              "       [562],\n",
              "       [578],\n",
              "       [548],\n",
              "       [557],\n",
              "       [588],\n",
              "       [540],\n",
              "       [573],\n",
              "       [580],\n",
              "       [589],\n",
              "       [557],\n",
              "       [568],\n",
              "       [571],\n",
              "       [597],\n",
              "       [566],\n",
              "       [568],\n",
              "       [581],\n",
              "       [562],\n",
              "       [556],\n",
              "       [543],\n",
              "       [554],\n",
              "       [592],\n",
              "       [589],\n",
              "       [597],\n",
              "       [561],\n",
              "       [587],\n",
              "       [576],\n",
              "       [587],\n",
              "       [587],\n",
              "       [590],\n",
              "       [544],\n",
              "       [589],\n",
              "       [586],\n",
              "       [588],\n",
              "       [573],\n",
              "       [581],\n",
              "       [598],\n",
              "       [592],\n",
              "       [580]])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.hstack((s[0], np.expand_dims(s[1], axis=1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjIckPCE7n2z",
        "outputId": "53329e0f-871f-4584-8770-500b1b1f4ebc"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  2,   0,   1, ...,   0,   0, 586],\n",
              "       [  2,   3,   0, ...,   1,   1, 580],\n",
              "       [  0,   4,   4, ...,   4,   4, 564],\n",
              "       ...,\n",
              "       [  4,   2,   3, ...,   1,   1, 598],\n",
              "       [  4,   4,   4, ...,   2,   0, 592],\n",
              "       [  1,   2,   2, ...,   2,   3, 580]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([\n",
        "              [1,2,3,4],\n",
        "              [3,3,3,3],\n",
        "              [2,2,2,2],\n",
        "              [1,1,1,1]\n",
        "])\n",
        "print(a.mean(axis=0, keepdims=True))\n",
        "a = a - a.mean(axis=0, keepdims=True)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMUMT8x_A8I0",
        "outputId": "96566bc2-13c2-4644-9db2-e57d5cc1022a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.75 2.   2.25 2.5 ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.75,  0.  ,  0.75,  1.5 ],\n",
              "       [ 1.25,  1.  ,  0.75,  0.5 ],\n",
              "       [ 0.25,  0.  , -0.25, -0.5 ],\n",
              "       [-0.75, -1.  , -1.25, -1.5 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "empty = [np.array([1,2]), np.array([3,4])]\n",
        "a = []\n",
        "b = a + empty + empty\n",
        "\n",
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBFhGXfSJz3l",
        "outputId": "bdee9605-88a8-467c-9777-cd322c8fc264"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([1, 2]), array([3, 4]), array([1, 2]), array([3, 4])]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "empty = [np.array([10,20])]\n",
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJY9NttjZiM7",
        "outputId": "0a4aa47a-c63f-4b8a-d5dd-cb831df74c3a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([1, 2]), array([3, 4]), array([1, 2]), array([3, 4])]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%wandb\n",
        "# create env\n",
        "env = make_multiple_env(**easy_config) \n",
        "# Parameter initialization\n",
        "numtrajs = 3  # num of trajecories from the current policy to collect in each iteration\n",
        "lr_pg = 1e-2  # learning rate for PG\n",
        "attention_size = 10\n",
        "iterations = 10\n",
        "discount_gamma = .99\n",
        "\n",
        "# Network initialize\n",
        "policy = Policy(lr_pg, 61, attention_size)\n",
        "\n",
        "#To record training reward for logging and plotting purposes\n",
        "rrecord = []\n",
        "\n",
        "# For every training iteration, we roll out 5 random instances using the policy network\n",
        "# collect observation and action matrices from these 5 roll-outs, consider this a BATCH\n",
        "# train the network using this BATCH\n",
        "for ite in range(iterations):\n",
        "\n",
        "    print(\"==========================================================================================================\")\n",
        "    print(\"Outer iteration no\", ite)\n",
        "\n",
        "    # To record traectories generated from current policy\n",
        "    OBS_MAT = []  # observations: [obs_matrices_batch_traj1, obs_matrices_batch_traj2, ....]\n",
        "    ACT_MAT = []  # actions\n",
        "    ADS = []  # advantages (to compute policy gradient)\n",
        "    VAL = []  # Monte carlo value predictions (to compute baseline, and policy gradient) => 2 d list (numtrajs, #steps per traj)\n",
        "    traj_returns = []  # a list of 5 numbers, each number represents the RETURN of that roll-out\n",
        "    actions = []\n",
        "    \n",
        "    # collect some trajectories\n",
        "    for num in range(numtrajs):\n",
        "        print(\"-------------------------------------------------------------------------------------------\")\n",
        "        print(\"Running trajectories:\", num)\n",
        "        # Initialize a list of obs_matrices and act_matrices, to store all the obs_matrix and act_matrix in the trajectory\n",
        "        obs_matrices = []  # states: [obs_matrix_state1, obs_matrix_state2, ...]\n",
        "        act_matrices = []  # actions matrices\n",
        "        \n",
        "        # this is used to collect all the immedaite rewards in this trajectory\n",
        "        rews = []  # instant rewards\n",
        "\n",
        "        # gym loop\n",
        "        s = env.reset()   # samples a RANDOM INSTANCE every time env.reset() is called\n",
        "        done = False\n",
        "\n",
        "        # TODO: wandb logging -> what reward average should we log??\n",
        "        t = 0 # TODO: how is this used?\n",
        "        repisode = 0  # TODO: how is this used?\n",
        "\n",
        "        log_itr = 0\n",
        "        \n",
        "\n",
        "        # roll out ONE policy\n",
        "        while not done:\n",
        "\n",
        "            # TODO compute the running average RETURN as basline\n",
        "            A, b, c0, cuts_a, cuts_b = s\n",
        "\n",
        "            # choose the action according to the model output probabilities\n",
        "\n",
        "            # Concat [A,b] and [cuts_a, cuts_b]\n",
        "            assert A.shape[0] == b.shape[0]\n",
        "            assert cuts_a.shape[0] == cuts_b.shape[0]\n",
        "\n",
        "            obs_matrix = np.hstack((A, np.expand_dims(b, axis=1)))\n",
        "            act_matrix = np.hstack((cuts_a, np.expand_dims(cuts_b, axis=1)))\n",
        "\n",
        "            assert obs_matrix.shape == (A.shape[0], A.shape[1]+1)\n",
        "            assert act_matrix.shape == (cuts_a.shape[0], cuts_a.shape[1]+1)\n",
        "\n",
        "            # Normalize on a row (MIGHT NOT NEED THIS)\n",
        "            \n",
        "            # The reason we want to normalize a row: we want the numeric space of the model input to be just between 0, 1\n",
        "            # Right now I'm normalizing such that the largest number has value 1 --> each row divided by the largest num in that row\n",
        "            #   => This would result in b vector always be 1 (does it make sense?)\n",
        "            # another option is to normalize such that the SUM of the row is 1\n",
        "            #   => I think this makes more sense, consider this differentiates the max among datapoints\n",
        "            #   => According to prof in OH this might cause some information loss\n",
        "            \n",
        "            obs_matrix = obs_matrix / obs_matrix.max(axis=1, keepdims=True)\n",
        "            act_matrix = act_matrix / act_matrix.max(axis=1, keepdims=True)\n",
        "            \n",
        "            # print(\"DEBUGGING: obs_matrix.shape = \", obs_matrix.shape)\n",
        "            # print(\"DEBUGGING: act_matrix.shape = \", act_matrix.shape)\n",
        "\n",
        "            action_prob = policy.predict_prob(obs_matrix, act_matrix)\n",
        "            action = random.choices(range(0, len(cuts_b)), action_prob) # this returns a list\n",
        "            actions.append(action)\n",
        "\n",
        "            if log_itr %10 == 0:\n",
        "                print(\"DEBUGGING: the action_prob is:\", action_prob)\n",
        "                print(\"DEBUGGING: the actual action to take is:\", action)\n",
        "\n",
        "            # take the action in the environment\n",
        "            # TODO: why does the environment.step function takes in a list?\n",
        "            # TODO: remember to go in the environment to change the returned r to the NORMALIZED r!!\n",
        "            s, r, done, _ = env.step(action)\n",
        "            \n",
        "            # Record the observed immediate reward & observed matrices along the trajectory\n",
        "            rews.append(r) # rews = list(len of trajectory)\n",
        "            obs_matrices.append(obs_matrix)\n",
        "            act_matrices.append(act_matrix)\n",
        "\n",
        "            log_itr += 1\n",
        "\n",
        "            # TODO: do we need to also record the one step of observation and action where the environment terminates?\n",
        "            #   ==> RN I'm thinking maybe don't need to\n",
        "        print(\"-------------------------------------------------------------------------------------------\")\n",
        "        #Below is for logging training performance\n",
        "        print(\"DEBUGGING: the total reward of the trajectory =\", np.sum(rews), \"and immediate rewards look like:\", rews)\n",
        "        rrecord.append(np.sum(rews))\n",
        "\n",
        "        # After the policy roll out for this trajectory,\n",
        "        # compute the monte-carlo RETURN of this trajectory (i.e. discounted sum of rewards), add to big list\n",
        "        # TODO: one of the next steps could be: to make the basline state-dependent\n",
        "        v_hat = discounted_rewards(rews, discount_gamma) # This is a list\n",
        "        traj_returns.append(v_hat[0])\n",
        "        # OBS_MAT.append(np.concatenate(obs_matrices, axis=1))\n",
        "        # ACT_MAT.append(np.concatenate(act_matrices, aixs=1))\n",
        "        VAL.append(v_hat) # VAL -> 2d list\n",
        "        OBS_MAT += obs_matrices\n",
        "        ACT_MAT += act_matrices\n",
        "\n",
        "        # TODO: do I need to specify batchsize somewhere?\n",
        "\n",
        "    print(\"+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\")\n",
        "    \n",
        "    # After collecting 5 (or however many) trajectories,\n",
        "    print(\"DEBUGGING: OBS_MAT has %d number of matrices\" % len(OBS_MAT))\n",
        "    print(\"DEBUGGING: ACT_MAT has %d number of matrices\" % len(ACT_MAT))\n",
        "    print(\"DEBUGGING: VAL looks like:\", VAL)\n",
        "    # print(\"DEBUGGING: OBS_MAT looks like:\", OBS_MAT)\n",
        "    # print(\"DEBUGGING: ACT_MAT looks like:\", ACT_MAT)\n",
        "    print(\"DEBUGGING: traj_returns =\", traj_returns)\n",
        "    print(\"DEBUGGING: actions =\", actions)\n",
        "    print(\"DEBUGGING: actions length =\", len(actions))\n",
        "\n",
        "    ## For debugging purposes, let's look at what does the model output\n",
        "    print(\"DEBUGGING: what does the model output in this round of roll-out?\")\n",
        "    obs_attention_dbg = policy.model(torch.FloatTensor(obs_matrix))\n",
        "    act_attention_dbg = policy.model(torch.FloatTensor(act_matrix))\n",
        "    logits_dbg = policy.compute_one_step_logits(obs_matrix, act_matrix)\n",
        "    print(\"DEBUGGING: obs_attention looks like:\", obs_attention_dbg)\n",
        "    print(\"DEBUGGING: act_attention looks like:\", act_attention_dbg)\n",
        "    print(\"DEBUGGING: logits looks like:\", logits_dbg)\n",
        "\n",
        "\n",
        "    assert len(traj_returns) == numtrajs\n",
        "    VAL = np.array(VAL)\n",
        "    # 1. calculate the baseline: average return of the trajectories\n",
        "    # TODO: potentially can make this into *running average*, take into account of all the previous trajectories as well\n",
        "    # TODO: potentially make the baseline the average *VALUE* of every *state*, \n",
        "    #       but I'm not sure whether that \"state\" is useful in this concept, since the first *step*\n",
        "    #       doesn't really mean the same thing among instances\n",
        "    #       I think prof says it makes sense in the OH, also it would make more sense if you engineered the reward\n",
        "    baseline = np.mean(traj_returns)\n",
        "    baseline_2 = VAL.mean(axis=0, keepdims=True)\n",
        "\n",
        "    # assert baseline_2.shape == VAL.shape\n",
        "\n",
        "    # 2. Update the policy\n",
        "    ADS = (VAL - baseline).flatten()\n",
        "    print(\"DEBUGGING: baseline2 looks like:\", baseline_2)\n",
        "    print(\"DEBUGGING: baseline2 looks like:\", baseline)\n",
        "    print(\"DEBUGGING: ADS looks like:\", ADS)\n",
        "\n",
        "\n",
        "    # Train the agent using the batch\n",
        "    # obs_batch = np.concatenate(OBS_MAT)\n",
        "    # act_batch = np.concatenate(ACT_MAT)\n",
        "\n",
        "    assert ADS.shape[0] == len(actions)\n",
        "\n",
        "    # scaling up the rewards to artificially make bigger loss, improve learning\n",
        "\n",
        "    policy.train(OBS_MAT, ACT_MAT, actions, ADS*10)\n",
        "\n",
        "\n",
        "    # TODO: wandb logging\n",
        "    wandb.log({\"Training reward (easy config)\" : rrecord[-1]})\n",
        "    #make sure to use the correct tag in wandb.init in the initialization on top\n"
      ],
      "metadata": {
        "id": "aeQHQnp1-8fR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b2d3df5-d4b3-4488-9829-0c4daf4e76f7"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<wandb.jupyter.IFrame at 0x7f3164c89810>"
            ],
            "text/html": [
              "<iframe src=\"https://wandb.ai/ieor4575-spring2022/finalproject/runs/3rwptiep?jupyter=true\" style=\"border:none;width:100%;height:420px;\"></iframe>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading training instances, dir instances/train_10_n60_m60 idx 0\n",
            "==========================================================================================================\n",
            "Outer iteration no 0\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [49]\n",
            "DEBUGGING: the action_prob is: tensor([0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [56]\n",
            "DEBUGGING: the action_prob is: tensor([0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [69]\n",
            "DEBUGGING: the action_prob is: tensor([0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0119, 0.0119], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [76]\n",
            "DEBUGGING: the action_prob is: tensor([0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [70]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.8867413289835895 and immediate rewards look like: [0.017973709444504493, 0.0034592290662658343, 0.0017804693370635505, 0.00012150896372986608, 0.02160850212931109, 0.00032132201704371255, 0.006512451654089091, 0.8349641363606679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 0.0, 0.0, 1.3642420526593924e-12, 1.3642420526593924e-12, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 9.094947017729282e-13]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [28]\n",
            "DEBUGGING: the action_prob is: tensor([0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [61]\n",
            "DEBUGGING: the action_prob is: tensor([0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [3]\n",
            "DEBUGGING: the action_prob is: tensor([0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [63]\n",
            "DEBUGGING: the action_prob is: tensor([0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [8]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.8867413289872275 and immediate rewards look like: [0.005821298033424682, 0.030556887696093327, 0.0003346033590787556, 0.8500285398840788, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 9.094947017729282e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 1.3642420526593924e-12, 4.547473508864641e-13, 1.3642420526593924e-12, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [29]\n",
            "DEBUGGING: the action_prob is: tensor([0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [13]\n",
            "DEBUGGING: the action_prob is: tensor([0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [69]\n",
            "DEBUGGING: the action_prob is: tensor([0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [0]\n",
            "DEBUGGING: the action_prob is: tensor([0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [25]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.886741328986318 and immediate rewards look like: [0.027870058938333386, 0.019888057122898317, 0.032454694090574776, 0.8065285188204143, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 9.094947017729282e-13, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13, 1.3642420526593924e-12, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 0.0, 0.0]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 150 number of matrices\n",
            "DEBUGGING: ACT_MAT has 150 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.8286963814228684, 0.8189117898771353, 0.8236894553645147, 0.8302110969974253, 0.8384743313471672, 0.8251169992099556, 0.833126946659507, 0.834964136369109, 8.526444706050109e-12, 8.612570410151625e-12, 8.699566070860226e-12, 8.787440475616391e-12, 8.876202500622618e-12, 8.965861111740019e-12, 9.056425365393959e-12, 9.147904409488848e-12, 9.24030748433217e-12, 8.874303165096673e-12, 8.504601832535565e-12, 8.590506901551075e-12, 8.677279698536439e-12, 8.30558822994947e-12, 7.930142302083844e-12, 8.010244749579641e-12, 7.631815554235533e-12, 7.249563841766736e-12, 7.32279175936034e-12, 7.396759352889232e-12, 7.012133335356332e-12, 7.082962965006396e-12, 7.154508045461006e-12, 5.848753528082438e-12, 4.52980957113439e-12, 4.1162244648968945e-12, 4.157802489794843e-12, 4.199800494742266e-12, 4.242222721961884e-12, 4.285073456527156e-12, 3.869016268323931e-12, 3.448756482260068e-12, 3.0242516478521253e-12, 2.5954588858239003e-12, 2.621675642246364e-12, 2.6481572143902667e-12, 2.2155655186907096e-12, 1.7786042099032782e-12, 1.3372291505220344e-12, 8.913957572076469e-13, 9.003997547551989e-13, 9.094947017729282e-13], [0.8611824038367416, 0.8640011169730474, 0.8418628578555092, 0.8500285398953843, 1.1419668555007645e-11, 1.1075677983960789e-11, 1.1187553519152312e-11, 1.0841218351783685e-11, 1.0950725607862307e-11, 1.1061338997840714e-11, 1.11730696947886e-11, 1.128592898463495e-11, 1.139992826730803e-11, 1.15150790578869e-11, 1.117205222929337e-11, 1.0825560483239299e-11, 1.0475568820558419e-11, 1.012204188855753e-11, 9.305603218974345e-12, 9.399599211085197e-12, 9.035203899190638e-12, 8.667127826569874e-12, 8.295333813821625e-12, 7.919784305995113e-12, 7.999782127267791e-12, 8.080588007341203e-12, 8.162210108425458e-12, 8.244656675177231e-12, 8.327936035532557e-12, 7.952715843076861e-12, 7.573705547667067e-12, 7.650207623906128e-12, 6.808800931447678e-12, 5.958895181489646e-12, 5.559745283437558e-12, 5.156563568233428e-12, 5.208650068922655e-12, 5.2612626958814694e-12, 4.85506600504546e-12, 4.444766317332319e-12, 3.1116406713867946e-12, 2.6837306267680104e-12, 1.3328167415238567e-12, 8.869387784216087e-13, 8.958977559814229e-13, 9.049472282640636e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0], [0.8619418964616238, 0.8424968055790812, 0.8309179277335181, 0.806528518831256, 1.0951127396941616e-11, 1.0602404086924395e-11, 1.0709499077701409e-11, 1.081767583606203e-11, 1.046760453048037e-11, 1.0113997151104956e-11, 1.0216158738489855e-11, 1.0319352261100863e-11, 1.0423588142526124e-11, 1.052887691164255e-11, 1.0635229203679343e-11, 1.0742655761292266e-11, 1.0391826677177577e-11, 1.0496794623411693e-11, 1.0602822851931003e-11, 1.0709922072657578e-11, 1.0358762345223349e-11, 1.0463396308306413e-11, 1.0109746421636313e-11, 9.29318355541756e-12, 8.927713337910198e-12, 8.558551502044176e-12, 8.64500151721634e-12, 8.272984006393813e-12, 7.437867984465541e-12, 7.512997964106608e-12, 7.129546073959742e-12, 7.201561690868427e-12, 6.81496397977976e-12, 5.965120482835184e-12, 5.106692708143693e-12, 5.158275462771407e-12, 4.751038496853478e-12, 3.880347267758131e-12, 3.4602019362340074e-12, 3.4951534709434417e-12, 3.530458051458022e-12, 3.566119243896992e-12, 3.1427998919298263e-12, 1.796523069970135e-12, 8.959882507042493e-13, 4.4569787860382347e-13, 4.5019987737759947e-13, 4.547473508864641e-13, 0.0, 0.0]]\n",
            "DEBUGGING: traj_returns = [0.8286963814228684, 0.8611824038367416, 0.8619418964616238]\n",
            "DEBUGGING: actions = [[49], [56], [47], [11], [7], [57], [25], [0], [53], [36], [56], [50], [17], [49], [51], [15], [29], [49], [3], [19], [69], [64], [38], [61], [65], [73], [30], [30], [11], [8], [76], [71], [89], [47], [48], [77], [63], [26], [3], [8], [70], [65], [80], [71], [51], [90], [54], [77], [63], [10], [28], [30], [32], [0], [35], [35], [28], [49], [31], [6], [61], [68], [54], [72], [20], [46], [6], [23], [69], [64], [3], [36], [63], [40], [28], [24], [71], [23], [16], [60], [63], [42], [11], [36], [14], [34], [41], [5], [60], [26], [8], [2], [81], [23], [3], [11], [12], [23], [30], [98], [29], [56], [42], [0], [30], [19], [4], [29], [5], [64], [13], [20], [52], [48], [55], [10], [70], [51], [35], [61], [69], [47], [11], [33], [74], [12], [38], [82], [13], [31], [0], [72], [62], [78], [7], [12], [57], [50], [23], [30], [25], [70], [69], [1], [10], [7], [13], [39], [49], [17]]\n",
            "DEBUGGING: actions length = 150\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-0.0872, -0.1266, -0.0283,  ..., -0.2056,  0.1254, -0.2124],\n",
            "        [-0.0872, -0.1264, -0.0286,  ..., -0.2056,  0.1254, -0.2121],\n",
            "        [-0.0871, -0.1266, -0.0285,  ..., -0.2058,  0.1254, -0.2124],\n",
            "        ...,\n",
            "        [-0.0872, -0.1266, -0.0284,  ..., -0.2057,  0.1254, -0.2124],\n",
            "        [-0.0872, -0.1266, -0.0284,  ..., -0.2057,  0.1254, -0.2124],\n",
            "        [-0.0872, -0.1266, -0.0284,  ..., -0.2057,  0.1254, -0.2124]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-0.0872, -0.1266, -0.0284,  ..., -0.2057,  0.1254, -0.2124],\n",
            "        [-0.0872, -0.1266, -0.0284,  ..., -0.2057,  0.1254, -0.2124],\n",
            "        [-0.0872, -0.1267, -0.0284,  ..., -0.2057,  0.1254, -0.2124],\n",
            "        ...,\n",
            "        [-0.0872, -0.1267, -0.0284,  ..., -0.2057,  0.1254, -0.2124],\n",
            "        [-0.0872, -0.1266, -0.0284,  ..., -0.2057,  0.1254, -0.2124],\n",
            "        [-0.0872, -0.1266, -0.0284,  ..., -0.2057,  0.1254, -0.2124]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417,\n",
            "        0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417,\n",
            "        0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417,\n",
            "        0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417,\n",
            "        0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417,\n",
            "        0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417,\n",
            "        0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417,\n",
            "        0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417,\n",
            "        0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417,\n",
            "        0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417,\n",
            "        0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417,\n",
            "        0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[8.50606894e-01 8.41803237e-01 8.32156747e-01 8.28922719e-01\n",
            "  2.79491444e-01 2.75039000e-01 2.77708982e-01 2.78321379e-01\n",
            "  9.98159161e-12 9.92930219e-12 1.00295982e-11 1.01309072e-11\n",
            "  1.02332396e-11 1.03366057e-11 1.02879023e-11 1.02387069e-11\n",
            "  1.00359010e-11 9.83104656e-12 9.47100930e-12 9.56667606e-12\n",
            "  9.35708198e-12 9.14537079e-12 8.77840751e-12 8.40773754e-12\n",
            "  8.18643701e-12 7.96290112e-12 8.04333446e-12 7.97146668e-12\n",
            "  7.59264579e-12 7.51622559e-12 7.28591989e-12 6.90017428e-12\n",
            "  6.05119149e-12 5.34674671e-12 4.94141349e-12 4.83821318e-12\n",
            "  4.73397043e-12 4.47556114e-12 4.06142807e-12 3.79622542e-12\n",
            "  3.22211679e-12 2.94843625e-12 2.36576409e-12 1.77720635e-12\n",
            "  1.33581718e-12 1.04308311e-12 7.47392126e-13 4.48714369e-13\n",
            "  3.00133252e-13 3.03164901e-13]]\n",
            "DEBUGGING: baseline2 looks like: 0.850606893907078\n",
            "DEBUGGING: ADS looks like: [-2.19105125e-02 -3.16951040e-02 -2.69174385e-02 -2.03957969e-02\n",
            " -1.21325626e-02 -2.54898947e-02 -1.74799472e-02 -1.56427575e-02\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01  1.05755099e-02  1.33942231e-02\n",
            " -8.74403605e-03 -5.78354012e-04 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            "  1.13350026e-02 -8.11008833e-03 -1.96889662e-02 -4.40783751e-02\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01 -8.50606894e-01 -8.50606894e-01\n",
            " -8.50606894e-01 -8.50606894e-01]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(-33.5325, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: training for one iteration takes 0.003738 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 1\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [39]\n",
            "DEBUGGING: the action_prob is: tensor([0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [4]\n",
            "DEBUGGING: the action_prob is: tensor([0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [59]\n",
            "DEBUGGING: the action_prob is: tensor([0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [24]\n",
            "DEBUGGING: the action_prob is: tensor([0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [26]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.12308581332627 and immediate rewards look like: [0.009627757028283668, 0.007802788751178014, 0.006339572735669208, 0.008394001936721907, 0.004065769363478466, 0.004323258146087028, 0.023835169775338727, 0.0004577645995595958, 0.0021442656038743735, 0.0005950171903350565, 0.00020467766216825112, 0.0002796951716845797, 0.0004618310499608924, 0.00045060251250106376, 0.0013755889544881938, 0.014602099828152859, 0.0021723601757912547, 0.0030028206720089656, 0.005419866174179333, 0.0016214397373914835, 0.007480556165774033, 0.0015568895282740414, 0.0026244623277307255, 2.2129477656562813e-05, 0.004367860450201988, 0.00014968103050705395, 0.003276392060342914, 0.0026846401933653397, 0.00027325000201017247, 0.0011732474308701057, 9.253397411157493e-05, 0.00030049520546526765, 0.0006111340148891031, 7.020715202088468e-06, 7.7318615240074e-05, 0.00028143051349616144, 0.0003875143552249938, 2.9912683658039896e-05, 0.00011315957135593635, 9.113006899497123e-05, 6.289296834438574e-07, 3.4091654015355743e-06, 2.4932855922088493e-06, 2.3171514840214513e-07, 6.48090431241144e-05, 0.00022532529237651033, 4.551240635919385e-06, 5.89943283557659e-07, 2.3607922230439726e-06, 4.30846557719633e-06]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [40]\n",
            "DEBUGGING: the action_prob is: tensor([0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [27]\n",
            "DEBUGGING: the action_prob is: tensor([0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [66]\n",
            "DEBUGGING: the action_prob is: tensor([0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [58]\n",
            "DEBUGGING: the action_prob is: tensor([0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [35]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.12242679811788548 and immediate rewards look like: [0.0008041198188948329, 0.020053188300153124, 0.005035369856159377, 0.020964518654636777, 0.015383307571482874, 0.0002881797204281611, 0.0009920818129103282, 0.0007656423072148755, 0.0005175961882741831, 0.0030168304751896358, 0.007446484883985249, 0.0021850922807971074, 0.0006872035010019317, 0.0016678373881404696, 0.007707651153396, 7.903774621809134e-05, 0.0008263208901553298, 0.005049858605161717, 0.0012345404138613958, 0.002783531059321831, 0.001427045243417524, 0.0018621027975314064, 0.0037713092988269636, 0.005552154000270093, 0.0014611775009143457, 4.858694865106372e-05, 0.003973574041538086, 0.00020219840735080652, 3.2593889045529068e-06, 0.00026851721304410603, 0.00047909124396028346, 4.2326859784225235e-05, 0.0004957084324814787, 0.0004029805804748321, 0.00013560142042479129, 3.174760877300287e-05, 9.369886720378418e-05, 0.0008000315724530083, 0.0007134352172215586, 0.0007162309220802854, 1.0714988093241118e-05, 0.0012660324568969372, 0.000699156194968964, 0.00019476018724162714, 5.581857067227247e-05, 6.057702421458089e-05, 1.137375102189253e-05, 3.927404850401217e-05, 7.418523455271497e-05, 4.5735469029750675e-05]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [30]\n",
            "DEBUGGING: the action_prob is: tensor([0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [11]\n",
            "DEBUGGING: the action_prob is: tensor([0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [76]\n",
            "DEBUGGING: the action_prob is: tensor([0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [52]\n",
            "DEBUGGING: the action_prob is: tensor([0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [95]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.14878802919838563 and immediate rewards look like: [0.0050582781823322875, 0.002705383163629449, 0.01782081358032883, 0.0007468443900506827, 0.03525173931120662, 0.024350161978418328, 0.002014792133195442, 0.0034006142909674963, 0.029162371846723545, 0.0012623583029380825, 0.009101863136038446, 0.00020035623310832307, 0.0007954304587656225, 0.005866101654646627, 0.0001466066769353347, 0.0018762648064694076, 0.0009556057443660393, 0.0024200270463552442, 0.0006761694253327732, 6.67861600049946e-05, 0.0002611068548503681, 5.7163935707649216e-05, 0.0007367489019998175, 6.485736275863019e-05, 0.00039245435073098633, 0.0002248238251922885, 0.0003888699629897019, 2.045058727162541e-05, 0.00012731592960335547, 0.000414441647080821, 0.0003948190756091208, 0.0003677416389109567, 0.0003846472022814851, 6.31696289019601e-05, 7.049447958706878e-05, 0.00020678371993199107, 0.0003544125997905212, 3.667864484668826e-05, 5.817282089992659e-05, 5.0491564252297394e-05, 4.125824943912448e-06, 2.7161060643265955e-05, 1.2609722034540027e-05, 9.816321198741207e-05, 6.099910478951642e-06, 3.136789518976002e-05, 1.7810962162911892e-06, 1.8056142380373785e-05, 2.249435056000948e-05, 1.1956728940276662e-05]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 150 number of matrices\n",
            "DEBUGGING: ACT_MAT has 150 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.11067080245303572, 0.1020636822472243, 0.09521302373338009, 0.08977116262395038, 0.0821991522093217, 0.07892260893519518, 0.07535287958495773, 0.05203809071678687, 0.05210133951235078, 0.05046169081664284, 0.050370377400310896, 0.05067242397792186, 0.05090174626892655, 0.050949409312086524, 0.05100889575715703, 0.05013465333602913, 0.03589146818977401, 0.03405970506462905, 0.03137059029557584, 0.02621285264787526, 0.02483981102069068, 0.01753460086355217, 0.0161391023588668, 0.013651151546602095, 0.01376668895853084, 0.009493766170029144, 0.0094384698379011, 0.006224320987432513, 0.003575435145522397, 0.003335540549002247, 0.002184134462759739, 0.0021127277663112766, 0.0018305379402484942, 0.0012317211367266576, 0.0012370711328531002, 0.0011714671895081074, 0.0008990269454666122, 0.0005166793840824429, 0.0004916835357822251, 0.0003823474388144332, 0.000294158959413598, 0.0002964949795254082, 0.0002960462768928007, 0.00029651817303090087, 0.00029927925038636237, 0.00023683859319418988, 1.1629596785534886e-05, 7.149854696581314e-06, 6.626173144468339e-06, 4.30846557719633e-06], [0.11074196304280993, 0.11104832648880314, 0.09191428099863638, 0.08775647590149192, 0.06746662348167187, 0.05260941001029191, 0.052849727565518936, 0.05238146035617031, 0.052137189948439834, 0.05214100379814712, 0.04962037709389645, 0.042599891121122425, 0.040823029131641736, 0.04054123801074728, 0.03926606123495637, 0.03187718190056603, 0.03211933752964438, 0.03160910771665561, 0.02682752435504433, 0.025851498930487812, 0.023300977647642405, 0.022094881216388768, 0.020437149918037738, 0.016834182443647247, 0.011395988326643592, 0.010035162450231562, 0.010087450001596464, 0.0061756322828872496, 0.006033771591450953, 0.0060914264672185855, 0.005881726519368161, 0.005457207348896846, 0.00546957625162891, 0.005024108908229729, 0.004667806391671613, 0.004577984819441234, 0.004592158798654778, 0.00454389892065757, 0.0037816841901056187, 0.003099241386751576, 0.0024070812774457484, 0.0024205720094469772, 0.001166201568232364, 0.00047176300329636355, 0.00027980082429771355, 0.00022624470063175868, 0.00016734108729007856, 0.00015754276390725862, 0.00011946334889216814, 4.5735469029750675e-05], [0.13923686122590892, 0.13553392226623903, 0.13417024151778745, 0.11752467468430165, 0.11795740433762725, 0.08354107578426326, 0.059788801824085795, 0.05835758554635389, 0.055512092177157975, 0.026615879121650938, 0.025609616988598843, 0.0166744988409701, 0.01664054808874927, 0.016005169323215804, 0.01024148249350422, 0.01019684425916049, 0.008404625709788973, 0.007524262591336296, 0.0051557934797788404, 0.004524872782268755, 0.004503117800266424, 0.0042848595408243, 0.00427039960112793, 0.0035693441405334463, 0.003539885634115976, 0.0031792235185706965, 0.0029842421145236443, 0.002621588031852467, 0.0026274115601826686, 0.002525349121797286, 0.0021322297724408737, 0.0017549602998300534, 0.0014012309706253503, 0.0010268522912564295, 0.0009734168306610803, 0.000912042778862638, 0.0007123828878087342, 0.00036158614951334653, 0.00032818939865319016, 0.00027274401793258944, 0.00022449742795989096, 0.0002225975788040187, 0.00019741062440480075, 0.00018666757815177852, 8.939834966097621e-05, 8.413983755760057e-05, 5.3304992290748034e-05, 5.2044339469148326e-05, 3.4331512210883375e-05, 1.1956728940276662e-05]]\n",
            "DEBUGGING: traj_returns = [0.11067080245303572, 0.11074196304280993, 0.13923686122590892]\n",
            "DEBUGGING: actions = [[39], [18], [48], [16], [37], [65], [10], [58], [43], [26], [4], [24], [60], [43], [39], [50], [65], [12], [33], [8], [59], [7], [26], [30], [8], [57], [7], [67], [53], [69], [24], [80], [70], [30], [69], [32], [13], [84], [71], [75], [26], [62], [71], [78], [3], [58], [23], [72], [99], [22], [40], [52], [27], [42], [37], [11], [21], [38], [55], [13], [27], [18], [43], [67], [1], [46], [51], [54], [58], [22], [66], [2], [43], [76], [68], [77], [38], [13], [45], [11], [58], [88], [40], [29], [79], [37], [70], [44], [70], [44], [35], [46], [70], [95], [43], [71], [35], [90], [77], [41], [30], [43], [55], [28], [19], [10], [14], [31], [7], [49], [11], [12], [32], [30], [31], [73], [32], [26], [36], [33], [76], [69], [30], [34], [66], [34], [54], [16], [61], [77], [52], [53], [69], [54], [47], [63], [59], [79], [69], [38], [95], [91], [27], [30], [58], [101], [53], [23], [93], [21]]\n",
            "DEBUGGING: actions length = 150\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-0.1093, -0.1413, -0.0645,  ..., -0.2194,  0.1315, -0.1629],\n",
            "        [-0.1093, -0.1411, -0.0647,  ..., -0.2194,  0.1316, -0.1627],\n",
            "        [-0.1092, -0.1413, -0.0646,  ..., -0.2196,  0.1314, -0.1629],\n",
            "        ...,\n",
            "        [-0.1092, -0.1413, -0.0646,  ..., -0.2195,  0.1316, -0.1629],\n",
            "        [-0.1092, -0.1413, -0.0646,  ..., -0.2195,  0.1316, -0.1629],\n",
            "        [-0.1092, -0.1413, -0.0646,  ..., -0.2195,  0.1316, -0.1629]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-0.1092, -0.1413, -0.0645,  ..., -0.2195,  0.1316, -0.1629],\n",
            "        [-0.1092, -0.1413, -0.0646,  ..., -0.2195,  0.1316, -0.1629],\n",
            "        [-0.1092, -0.1413, -0.0646,  ..., -0.2195,  0.1316, -0.1629],\n",
            "        ...,\n",
            "        [-0.1092, -0.1413, -0.0646,  ..., -0.2195,  0.1316, -0.1629],\n",
            "        [-0.1092, -0.1413, -0.0646,  ..., -0.2195,  0.1316, -0.1629],\n",
            "        [-0.1092, -0.1413, -0.0646,  ..., -0.2195,  0.1316, -0.1629]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417,\n",
            "        0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417,\n",
            "        0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417,\n",
            "        0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417,\n",
            "        0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417,\n",
            "        0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417,\n",
            "        0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417,\n",
            "        0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417,\n",
            "        0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417,\n",
            "        0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417,\n",
            "        0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417,\n",
            "        0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417, 0.1417,\n",
            "        0.1417], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[1.20216542e-01 1.16215310e-01 1.07099182e-01 9.83507711e-02\n",
            "  8.92077267e-02 7.16910316e-02 6.26638030e-02 5.42590455e-02\n",
            "  5.32502072e-02 4.30728579e-02 4.18667905e-02 3.66489380e-02\n",
            "  3.61217745e-02 3.58319389e-02 3.35054798e-02 3.07362265e-02\n",
            "  2.54718105e-02 2.43976918e-02 2.11179694e-02 1.88630748e-02\n",
            "  1.75479688e-02 1.46381139e-02 1.36155506e-02 1.13515594e-02\n",
            "  9.56752097e-03 7.56938405e-03 7.50338732e-03 5.00718043e-03\n",
            "  4.07887277e-03 3.98410538e-03 3.39936358e-03 3.10829847e-03\n",
            "  2.90044839e-03 2.42756078e-03 2.29276479e-03 2.22049826e-03\n",
            "  2.06785621e-03 1.80738815e-03 1.53385237e-03 1.25144428e-03\n",
            "  9.75245888e-04 9.79888189e-04 5.53219490e-04 3.18316251e-04\n",
            "  2.22826141e-04 1.82407710e-04 7.74252255e-05 7.22456527e-05\n",
            "  5.34736781e-05 2.06668878e-05]]\n",
            "DEBUGGING: baseline2 looks like: 0.12021654224058487\n",
            "DEBUGGING: ADS looks like: [-0.00954574 -0.01815286 -0.02500352 -0.03044538 -0.03801739 -0.04129393\n",
            " -0.04486366 -0.06817845 -0.0681152  -0.06975485 -0.06984616 -0.06954412\n",
            " -0.0693148  -0.06926713 -0.06920765 -0.07008189 -0.08432507 -0.08615684\n",
            " -0.08884595 -0.09400369 -0.09537673 -0.10268194 -0.10407744 -0.10656539\n",
            " -0.10644985 -0.11072278 -0.11077807 -0.11399222 -0.11664111 -0.116881\n",
            " -0.11803241 -0.11810381 -0.118386   -0.11898482 -0.11897947 -0.11904508\n",
            " -0.11931752 -0.11969986 -0.11972486 -0.11983419 -0.11992238 -0.11992005\n",
            " -0.1199205  -0.11992002 -0.11991726 -0.1199797  -0.12020491 -0.12020939\n",
            " -0.12020992 -0.12021223 -0.00947458 -0.00916822 -0.02830226 -0.03246007\n",
            " -0.05274992 -0.06760713 -0.06736681 -0.06783508 -0.06807935 -0.06807554\n",
            " -0.07059617 -0.07761665 -0.07939351 -0.0796753  -0.08095048 -0.08833936\n",
            " -0.0880972  -0.08860743 -0.09338902 -0.09436504 -0.09691556 -0.09812166\n",
            " -0.09977939 -0.10338236 -0.10882055 -0.11018138 -0.11012909 -0.11404091\n",
            " -0.11418277 -0.11412512 -0.11433482 -0.11475933 -0.11474697 -0.11519243\n",
            " -0.11554874 -0.11563856 -0.11562438 -0.11567264 -0.11643486 -0.1171173\n",
            " -0.11780946 -0.11779597 -0.11905034 -0.11974478 -0.11993674 -0.1199903\n",
            " -0.1200492  -0.120059   -0.12009708 -0.12017081  0.01902032  0.01531738\n",
            "  0.0139537  -0.00269187 -0.00225914 -0.03667547 -0.06042774 -0.06185896\n",
            " -0.06470445 -0.09360066 -0.09460693 -0.10354204 -0.10357599 -0.10421137\n",
            " -0.10997506 -0.1100197  -0.11181192 -0.11269228 -0.11506075 -0.11569167\n",
            " -0.11571342 -0.11593168 -0.11594614 -0.1166472  -0.11667666 -0.11703732\n",
            " -0.1172323  -0.11759495 -0.11758913 -0.11769119 -0.11808431 -0.11846158\n",
            " -0.11881531 -0.11918969 -0.11924313 -0.1193045  -0.11950416 -0.11985496\n",
            " -0.11988835 -0.1199438  -0.11999204 -0.11999394 -0.12001913 -0.12002987\n",
            " -0.12012714 -0.1201324  -0.12016324 -0.1201645  -0.12018221 -0.12020459]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(-4.2276, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: training for one iteration takes 0.005983 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 2\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [11]\n",
            "DEBUGGING: the action_prob is: tensor([0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [14]\n",
            "DEBUGGING: the action_prob is: tensor([0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [62]\n",
            "DEBUGGING: the action_prob is: tensor([0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [15]\n",
            "DEBUGGING: the action_prob is: tensor([0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [73]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.886741328977223 and immediate rewards look like: [0.004239090522787592, 0.01976025750673216, 0.0014005744096721173, 0.0005072115059192583, 0.006203060317602649, 0.013416363732630998, 0.0001980130386982637, 0.0010678054877644172, 0.016091520172267337, 0.0005787606569356285, 0.009231809482571407, 0.001863937839971186, 0.00025172747518809047, 0.0007546316232946992, 0.0031491523909608077, 0.001163138786523632, 0.0007097101511135406, 0.005362057890579308, 0.00014948641046430566, 0.00208247258387928, 4.158645515417447e-05, 0.000825971177619067, 0.0007336236849369016, 0.7969593656694087, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 0.0, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [42]\n",
            "DEBUGGING: the action_prob is: tensor([0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [57]\n",
            "DEBUGGING: the action_prob is: tensor([0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [2]\n",
            "DEBUGGING: the action_prob is: tensor([0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [12]\n",
            "DEBUGGING: the action_prob is: tensor([0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [65]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.1307836539613163 and immediate rewards look like: [0.04205345764512458, 0.014118794975729543, 0.008886211802746402, 0.03839916070228355, 4.4729258661391214e-05, 0.0009494499349784746, 0.000648620063202543, 0.0059912254941991705, 0.00012182133559690556, 0.0004604297455443884, 0.000738484307021281, 5.1687771247088676e-05, 8.220632389566163e-05, 0.0006111824081926898, 0.0015048708096401242, 0.0006327447322291846, 0.004905238050469052, 0.0026667294846447476, 0.00029382678167166887, 3.1685572139394935e-06, 0.0001971797432815947, 0.0027408022956478817, 4.184323552181013e-05, 0.001114448892622022, 0.000313219376039342, 0.0002881440332203056, 0.0002477826369613467, 0.000728841142517922, 1.1939447631448274e-05, 0.0005397600157266425, 1.3161191418475937e-05, 0.00029318337783479365, 8.426169961239793e-05, 0.00011135860268041142, 8.220508789236192e-07, 9.464215509069618e-07, 2.5607612315070583e-05, 0.00022935564993531443, 8.846398623063578e-05, 0.00014441029361478286, 5.440609720608336e-05, 1.4840379662928171e-06, 1.6704694644431584e-05, 2.6068016723002074e-05, 7.723286353211734e-05, 9.468179359828355e-05, 8.62193337525241e-06, 0.00011931502467632527, 3.313504294055747e-06, 2.254105766041903e-06]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [45]\n",
            "DEBUGGING: the action_prob is: tensor([0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [51]\n",
            "DEBUGGING: the action_prob is: tensor([0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [68]\n",
            "DEBUGGING: the action_prob is: tensor([0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [54]\n",
            "DEBUGGING: the action_prob is: tensor([0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [54]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.8867413289854085 and immediate rewards look like: [0.0007409532217934611, 0.020378480659474008, 0.033726147376910376, 0.0007745124698885775, 0.004787460887200723, 0.003348555280808796, 0.8229852190765996, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 0.0, 1.3642420526593924e-12, 4.547473508864641e-13, 0.0, 1.3642420526593924e-12, 9.094947017729282e-13, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 150 number of matrices\n",
            "DEBUGGING: ACT_MAT has 150 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.7163209491942267, 0.7192746047186254, 0.7065801486988821, 0.7123026002921313, 0.7189852411981941, 0.7199820008894863, 0.7137026637948033, 0.7207117684405101, 0.7269130938916624, 0.7180015896155505, 0.724669524200621, 0.7226643583010602, 0.7280812327889788, 0.7351813184987785, 0.7418451382580645, 0.7461575614819229, 0.7525196188842417, 0.759403948215281, 0.7616584750754563, 0.769200998651507, 0.7748671980481089, 0.7826521329221765, 0.7897233957015731, 0.7969593656733699, 4.001195445298982e-12, 4.0416115609080625e-12, 4.082435920109154e-12, 4.1236726465749034e-12, 4.1653259056312154e-12, 4.2073999046779955e-12, 4.249898893614137e-12, 4.292827165266805e-12, 3.876848297353879e-12, 3.916008381165534e-12, 3.036882504436976e-12, 2.6082173268186988e-12, 2.634562956382524e-12, 2.661174703416691e-12, 2.2287144975052797e-12, 1.7918860066856722e-12, 1.350645106867887e-12, 9.049472282640636e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12540660814782534, 0.08419510151787955, 0.07078414802237375, 0.06252316789861348, 0.024367684036696896, 0.024568641189934855, 0.023857768944400386, 0.02344358472848267, 0.017628645691195453, 0.01768366096525106, 0.01739720325222896, 0.01682698883354311, 0.01694474854777376, 0.0170328709332102, 0.016587564166684353, 0.015235043794994173, 0.014749797033095948, 0.0099439989723504, 0.007350777260308742, 0.007128232806704114, 0.00719703459544462, 0.007070560456730329, 0.004373493092002472, 0.004375403895435012, 0.003293893942235343, 0.003010782390096971, 0.002750139754420874, 0.0025276334519793203, 0.0018169619287488872, 0.0018232550314317565, 0.0012964596118233474, 0.0012962610307119915, 0.001013209750381008, 0.0009383313644127372, 0.0008353260219518443, 0.0008429333041140612, 0.0008504918005688426, 0.000833216351771487, 0.0006099603048850227, 0.0005267639582367545, 0.00038621582285047637, 0.00033516133903474044, 0.0003370477788570178, 0.0003235788729420063, 0.00030051601638283256, 0.00022553853823304568, 0.00013217852993410316, 0.00012480464298873813, 5.545069002437231e-06, 2.254105766041903e-06], [0.8373296603940392, 0.8450390981537836, 0.8329905227215249, 0.807337752873348, 0.8147103438418781, 0.8181039221764418, 0.822985219086498, 9.998401536437155e-12, 9.640054732879485e-12, 9.278088264639416e-12, 9.371806327918602e-12, 9.466471038301618e-12, 9.562091957880423e-12, 9.658678745333761e-12, 9.756241156902789e-12, 9.854789047376554e-12, 9.954332371087429e-12, 9.595540424445419e-12, 9.233124316726216e-12, 8.867047440242174e-12, 8.497272817531021e-12, 8.58310385609194e-12, 8.210461116369169e-12, 8.293395067039565e-12, 7.917825975912223e-12, 7.997804016072953e-12, 8.078589915225206e-12, 7.700851075089638e-12, 7.319296691114317e-12, 7.393228980923554e-12, 7.008567303067768e-12, 6.620020153718489e-12, 6.686889044160089e-12, 6.295092619468308e-12, 6.358679413604352e-12, 6.422908498590254e-12, 6.028445603741202e-12, 6.089338993677982e-12, 4.7728251929480704e-12, 4.361694789961218e-12, 4.405752313092139e-12, 3.072232586295704e-12, 2.184583721740178e-12, 2.2066502239799775e-12, 1.7695988617106195e-12, 1.7874735976874945e-12, 1.3461881280818489e-12, 9.004452294902876e-13, 4.5019987737759947e-13, 4.547473508864641e-13]]\n",
            "DEBUGGING: traj_returns = [0.7163209491942267, 0.12540660814782534, 0.8373296603940392]\n",
            "DEBUGGING: actions = [[11], [15], [15], [26], [31], [39], [52], [2], [6], [41], [14], [11], [58], [66], [63], [18], [39], [11], [62], [34], [62], [72], [66], [0], [64], [49], [35], [32], [48], [21], [15], [47], [46], [47], [90], [89], [41], [3], [63], [47], [73], [60], [56], [27], [76], [13], [14], [4], [20], [16], [42], [17], [35], [3], [25], [9], [34], [15], [19], [56], [57], [1], [39], [19], [4], [40], [10], [40], [65], [61], [2], [2], [30], [9], [81], [51], [25], [55], [72], [48], [12], [52], [50], [26], [47], [71], [74], [32], [70], [14], [65], [62], [37], [66], [65], [94], [64], [51], [23], [81], [45], [22], [8], [18], [54], [34], [0], [1], [12], [4], [51], [9], [13], [2], [41], [8], [28], [37], [9], [74], [68], [27], [24], [54], [33], [56], [78], [71], [47], [87], [54], [69], [44], [0], [45], [20], [24], [10], [40], [51], [54], [48], [66], [33], [61], [62], [22], [100], [71], [103]]\n",
            "DEBUGGING: actions length = 150\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-0.1278, -0.1544, -0.0879,  ..., -0.2317,  0.1393, -0.1345],\n",
            "        [-0.1278, -0.1543, -0.0881,  ..., -0.2318,  0.1394, -0.1344],\n",
            "        [-0.1277, -0.1543, -0.0881,  ..., -0.2317,  0.1391, -0.1347],\n",
            "        ...,\n",
            "        [-0.1277, -0.1544, -0.0880,  ..., -0.2318,  0.1394, -0.1345],\n",
            "        [-0.1277, -0.1544, -0.0880,  ..., -0.2318,  0.1394, -0.1345],\n",
            "        [-0.1277, -0.1544, -0.0880,  ..., -0.2318,  0.1394, -0.1345]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-0.1277, -0.1544, -0.0880,  ..., -0.2318,  0.1394, -0.1345],\n",
            "        [-0.1277, -0.1544, -0.0880,  ..., -0.2318,  0.1393, -0.1345],\n",
            "        [-0.1277, -0.1544, -0.0880,  ..., -0.2318,  0.1394, -0.1345],\n",
            "        ...,\n",
            "        [-0.1277, -0.1544, -0.0880,  ..., -0.2318,  0.1393, -0.1345],\n",
            "        [-0.1277, -0.1544, -0.0880,  ..., -0.2318,  0.1394, -0.1345],\n",
            "        [-0.1277, -0.1544, -0.0880,  ..., -0.2318,  0.1393, -0.1345]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574,\n",
            "        0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574,\n",
            "        0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574,\n",
            "        0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574,\n",
            "        0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574,\n",
            "        0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574,\n",
            "        0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574,\n",
            "        0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574,\n",
            "        0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574,\n",
            "        0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574,\n",
            "        0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574,\n",
            "        0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574, 0.1574],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[5.59685739e-01 5.49502935e-01 5.36784940e-01 5.27387840e-01\n",
            "  5.19354423e-01 5.20884855e-01 5.20181884e-01 2.48051784e-01\n",
            "  2.48180580e-01 2.45228417e-01 2.47355576e-01 2.46497116e-01\n",
            "  2.48341994e-01 2.50738063e-01 2.52810901e-01 2.53797535e-01\n",
            "  2.55756472e-01 2.56449316e-01 2.56336417e-01 2.58776410e-01\n",
            "  2.60688078e-01 2.63240898e-01 2.64698963e-01 2.67111590e-01\n",
            "  1.09796465e-03 1.00359413e-03 9.16713256e-04 8.42544488e-04\n",
            "  6.05653980e-04 6.07751681e-04 4.32153208e-04 4.32087014e-04\n",
            "  3.37736587e-04 3.12777125e-04 2.78442010e-04 2.80977771e-04\n",
            "  2.83497270e-04 2.77738787e-04 2.03320104e-04 1.75587988e-04\n",
            "  1.28738610e-04 1.11720448e-04 1.12349260e-04 1.07859625e-04\n",
            "  1.00172006e-04 7.51795133e-05 4.40595104e-05 4.16015480e-05\n",
            "  1.84835648e-06 7.51368740e-07]]\n",
            "DEBUGGING: baseline2 looks like: 0.5596857392453637\n",
            "DEBUGGING: ADS looks like: [ 0.15663521  0.15958887  0.14689441  0.15261686  0.1592995   0.16029626\n",
            "  0.15401692  0.16102603  0.16722735  0.15831585  0.16498378  0.16297862\n",
            "  0.16839549  0.17549558  0.1821594   0.18647182  0.19283388  0.19971821\n",
            "  0.20197274  0.20951526  0.21518146  0.22296639  0.23003766  0.23727363\n",
            " -0.55968574 -0.55968574 -0.55968574 -0.55968574 -0.55968574 -0.55968574\n",
            " -0.55968574 -0.55968574 -0.55968574 -0.55968574 -0.55968574 -0.55968574\n",
            " -0.55968574 -0.55968574 -0.55968574 -0.55968574 -0.55968574 -0.55968574\n",
            " -0.55968574 -0.55968574 -0.55968574 -0.55968574 -0.55968574 -0.55968574\n",
            " -0.55968574 -0.55968574 -0.43427913 -0.47549064 -0.48890159 -0.49716257\n",
            " -0.53531806 -0.5351171  -0.53582797 -0.53624215 -0.54205709 -0.54200208\n",
            " -0.54228854 -0.54285875 -0.54274099 -0.54265287 -0.54309818 -0.5444507\n",
            " -0.54493594 -0.54974174 -0.55233496 -0.55255751 -0.5524887  -0.55261518\n",
            " -0.55531225 -0.55531034 -0.55639185 -0.55667496 -0.5569356  -0.55715811\n",
            " -0.55786878 -0.55786248 -0.55838928 -0.55838948 -0.55867253 -0.55874741\n",
            " -0.55885041 -0.55884281 -0.55883525 -0.55885252 -0.55907578 -0.55915898\n",
            " -0.55929952 -0.55935058 -0.55934869 -0.55936216 -0.55938522 -0.5594602\n",
            " -0.55955356 -0.55956093 -0.55968019 -0.55968349  0.27764392  0.28535336\n",
            "  0.27330478  0.24765201  0.2550246   0.25841818  0.26329948 -0.55968574\n",
            " -0.55968574 -0.55968574 -0.55968574 -0.55968574 -0.55968574 -0.55968574\n",
            " -0.55968574 -0.55968574 -0.55968574 -0.55968574 -0.55968574 -0.55968574\n",
            " -0.55968574 -0.55968574 -0.55968574 -0.55968574 -0.55968574 -0.55968574\n",
            " -0.55968574 -0.55968574 -0.55968574 -0.55968574 -0.55968574 -0.55968574\n",
            " -0.55968574 -0.55968574 -0.55968574 -0.55968574 -0.55968574 -0.55968574\n",
            " -0.55968574 -0.55968574 -0.55968574 -0.55968574 -0.55968574 -0.55968574\n",
            " -0.55968574 -0.55968574 -0.55968574 -0.55968574 -0.55968574 -0.55968574]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(-17.7314, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: training for one iteration takes 0.002964 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 3\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [45]\n",
            "DEBUGGING: the action_prob is: tensor([0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [17]\n",
            "DEBUGGING: the action_prob is: tensor([0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [15]\n",
            "DEBUGGING: the action_prob is: tensor([0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [87]\n",
            "DEBUGGING: the action_prob is: tensor([0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [46]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.8867413289867727 and immediate rewards look like: [0.0007409532217934611, 0.005502258344222355, 0.020438564382857294, 0.014261383737903088, 0.0015795362655808276, 0.008525774427653232, 0.8356928585926653, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 9.094947017729282e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13, 0.0]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [0]\n",
            "DEBUGGING: the action_prob is: tensor([0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [23]\n",
            "DEBUGGING: the action_prob is: tensor([0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [42]\n",
            "DEBUGGING: the action_prob is: tensor([0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [62]\n",
            "DEBUGGING: the action_prob is: tensor([0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [9]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.8867413289885917 and immediate rewards look like: [0.8867413289726755, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 9.094947017729282e-13, 0.0, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [22]\n",
            "DEBUGGING: the action_prob is: tensor([0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [23]\n",
            "DEBUGGING: the action_prob is: tensor([0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [4]\n",
            "DEBUGGING: the action_prob is: tensor([0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [32]\n",
            "DEBUGGING: the action_prob is: tensor([0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [64]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.12835151601166217 and immediate rewards look like: [0.02111943388126747, 0.0007217761976789916, 0.005506333790890494, 0.020624783707717143, 0.0257880863587161, 0.0008910678097890923, 0.0009788858883439389, 0.0016556247583139339, 0.0028793502015105332, 0.0008615129745521699, 0.00838490431306127, 0.0038355382039299, 0.012823743988974456, 0.0006629005147260614, 0.009612760437448742, 0.004309047120386822, 0.0003921166694453859, 4.51407963737438e-05, 3.29788017552346e-05, 0.0010385218797637208, 6.525842809423921e-05, 0.0003887023267452605, 4.596891312758089e-05, 0.0005024996044085128, 9.73332635112456e-06, 1.9426739982009167e-05, 0.00011464996578069986, 1.9105545561615145e-05, 0.0005762478676842875, 0.00012902563912575715, 0.0004620918502951099, 0.0002220519345428329, 0.0010868759327422595, 4.4348379105940694e-05, 0.0004993042084606714, 0.00149307889296324, 0.00013909005019741016, 1.1176944099133834e-06, 5.375742603064282e-06, 9.711072607387905e-05, 1.1214243841095595e-05, 0.00018362534865445923, 1.9464787328615785e-06, 5.670494829246309e-07, 1.1569290563784307e-05, 1.7787359411158832e-05, 5.27026577401557e-06, 4.562110916594975e-06, 1.8650170204637107e-05, 1.0751631180028198e-05]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 150 number of matrices\n",
            "DEBUGGING: ACT_MAT has 150 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.836471292568507, 0.8441720599461754, 0.8471412137393465, 0.835053181168171, 0.8290826236669373, 0.8358617044458146, 0.8356928586042035, 1.1654719110684033e-11, 1.1772443546145487e-11, 1.1432016358847498e-11, 1.1088150513091953e-11, 1.1200152033426215e-11, 1.0853944123777525e-11, 1.0963579923007601e-11, 1.0155641637610781e-11, 9.339542359432175e-12, 8.974540412672436e-12, 9.065192336032764e-12, 8.697419176915455e-12, 8.32593113740302e-12, 8.410031451922244e-12, 8.035640506096747e-12, 7.65746783354574e-12, 7.734815993480547e-12, 7.353604689488973e-12, 6.509202007793984e-12, 6.115610764553051e-12, 5.718043852188471e-12, 5.31646111242627e-12, 5.3701627398245154e-12, 5.424406807903551e-12, 5.019858037390996e-12, 4.611222915661144e-12, 4.657800924910246e-12, 4.704849419101259e-12, 4.293032392136156e-12, 3.877055597221911e-12, 3.4568770165004514e-12, 3.491794966162072e-12, 3.0677248639147553e-12, 2.6393712252811023e-12, 2.206690782216806e-12, 2.2289805880977838e-12, 1.792154785061939e-12, 8.915758417060715e-13, 4.412408998177852e-13, 4.4569787860382347e-13, 4.5019987737759947e-13, 4.547473508864641e-13, 0.0], [0.8867413289852495, 1.2701004516816568e-11, 1.2829297491733907e-11, 1.24995455968156e-11, 1.1707122116204719e-11, 1.1366035116483085e-11, 1.102150279353194e-11, 1.067349034610654e-11, 1.0321962621434421e-11, 9.507543353193428e-12, 8.684897627697475e-12, 8.772623866361086e-12, 8.401895470176386e-12, 8.48676310118827e-12, 8.572487980998252e-12, 8.659078768685102e-12, 8.746544210793032e-12, 8.375552383744008e-12, 8.00081316450257e-12, 8.081629459093507e-12, 8.163262079892432e-12, 8.245719272618619e-12, 7.869668607810257e-12, 7.489819451438175e-12, 7.565474193371895e-12, 7.1825523661469e-12, 6.7957626416772076e-12, 6.864406708764856e-12, 6.474403391796356e-12, 6.080460647383729e-12, 5.68253868333057e-12, 5.280597305499097e-12, 4.874595913750134e-12, 4.464493497842091e-12, 4.509589391759688e-12, 4.55514079975726e-12, 4.601152322987131e-12, 4.647628609077911e-12, 4.69457435260395e-12, 4.28265353708837e-12, 4.3259126637256264e-12, 4.369608751238006e-12, 3.954405454900548e-12, 3.535008185872812e-12, 3.1113745807942904e-12, 2.683461848391744e-12, 1.7918860066856722e-12, 1.350645106867887e-12, 9.049472282640636e-13, 4.547473508864641e-13], [0.11958186392113473, 0.09945700004026997, 0.09973254933595048, 0.09517799550006059, 0.07530627453772065, 0.050018371897984394, 0.04962353948302556, 0.04913601373200164, 0.047959988862310815, 0.045535998647273014, 0.04512574310375843, 0.037111958374441575, 0.03361254562677947, 0.020998789533136372, 0.020541302038798295, 0.011038930910454093, 0.006797862414209364, 0.006470450247236342, 0.006490211566527877, 0.006522457338154184, 0.0055393287458489525, 0.005529363957327994, 0.0051925875056391246, 0.0051986046389005496, 0.004743540438880845, 0.004781623345989617, 0.004810299602027886, 0.004743080440653724, 0.004771691813224352, 0.004237822167212186, 0.004150299523319625, 0.0037254622959843582, 0.0035387983448904297, 0.002476689305200172, 0.0024569100263578092, 0.001977379614037513, 0.000489192647549771, 0.000353638987224607, 0.000356082113954236, 0.00035424885995067847, 0.0002597354887644439, 0.00025103156052863466, 6.808708270118732e-05, 6.68086908770967e-05, 6.69107488830021e-05, 5.590046294870484e-05, 3.84980843813596e-05, 3.356345313873135e-05, 2.929428507286502e-05, 1.0751631180028198e-05]]\n",
            "DEBUGGING: traj_returns = [0.836471292568507, 0.8867413289852495, 0.11958186392113473]\n",
            "DEBUGGING: actions = [[45], [21], [6], [23], [43], [50], [0], [2], [48], [24], [17], [47], [5], [7], [9], [72], [61], [24], [66], [16], [15], [25], [51], [59], [10], [60], [56], [22], [63], [48], [87], [29], [61], [52], [24], [25], [79], [70], [38], [96], [46], [22], [62], [26], [42], [86], [88], [94], [44], [9], [0], [13], [43], [44], [36], [22], [10], [10], [63], [25], [23], [44], [62], [50], [12], [64], [22], [39], [7], [72], [42], [80], [45], [1], [25], [5], [43], [75], [12], [55], [62], [19], [20], [43], [29], [86], [53], [52], [75], [17], [9], [93], [64], [76], [45], [80], [96], [86], [45], [42], [22], [40], [1], [56], [41], [24], [4], [12], [34], [63], [23], [68], [61], [52], [68], [55], [61], [34], [52], [67], [4], [3], [56], [75], [82], [14], [79], [33], [46], [12], [32], [13], [30], [55], [44], [53], [56], [59], [81], [4], [64], [89], [31], [78], [41], [43], [21], [53], [95], [45]]\n",
            "DEBUGGING: actions length = 150\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-0.1406, -0.1440, -0.1122,  ..., -0.2287,  0.1351, -0.1240],\n",
            "        [-0.1406, -0.1439, -0.1123,  ..., -0.2289,  0.1352, -0.1239],\n",
            "        [-0.1406, -0.1439, -0.1124,  ..., -0.2286,  0.1349, -0.1241],\n",
            "        ...,\n",
            "        [-0.1406, -0.1440, -0.1122,  ..., -0.2288,  0.1351, -0.1240],\n",
            "        [-0.1406, -0.1440, -0.1122,  ..., -0.2288,  0.1351, -0.1240],\n",
            "        [-0.1406, -0.1440, -0.1122,  ..., -0.2288,  0.1351, -0.1240]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-0.1406, -0.1440, -0.1122,  ..., -0.2288,  0.1351, -0.1240],\n",
            "        [-0.1406, -0.1440, -0.1122,  ..., -0.2288,  0.1351, -0.1240],\n",
            "        [-0.1406, -0.1440, -0.1122,  ..., -0.2288,  0.1351, -0.1240],\n",
            "        ...,\n",
            "        [-0.1406, -0.1440, -0.1122,  ..., -0.2288,  0.1351, -0.1240],\n",
            "        [-0.1406, -0.1440, -0.1122,  ..., -0.2288,  0.1351, -0.1240],\n",
            "        [-0.1406, -0.1440, -0.1122,  ..., -0.2288,  0.1351, -0.1240]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639,\n",
            "        0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639,\n",
            "        0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639,\n",
            "        0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639,\n",
            "        0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639,\n",
            "        0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639,\n",
            "        0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639,\n",
            "        0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639,\n",
            "        0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639,\n",
            "        0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639,\n",
            "        0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639,\n",
            "        0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639,\n",
            "        0.1639, 0.1639], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[6.14264828e-01 3.14543020e-01 3.15624588e-01 3.10077059e-01\n",
            "  3.01462966e-01 2.95293359e-01 2.95105466e-01 1.63786713e-02\n",
            "  1.59866630e-02 1.51786662e-02 1.50419144e-02 1.23706528e-02\n",
            "  1.12041819e-02 6.99959652e-03 6.84710069e-03 3.67964364e-03\n",
            "  2.26595414e-03 2.15681675e-03 2.16340386e-03 2.17415245e-03\n",
            "  1.84644292e-03 1.84312132e-03 1.73086251e-03 1.73286822e-03\n",
            "  1.58118015e-03 1.59387445e-03 1.60343320e-03 1.58102682e-03\n",
            "  1.59056394e-03 1.41260739e-03 1.38343318e-03 1.24182077e-03\n",
            "  1.17959945e-03 8.25563105e-04 8.18970012e-04 6.59126541e-04\n",
            "  1.63064219e-04 1.17879665e-04 1.18694041e-04 1.18082956e-04\n",
            "  8.65784986e-05 8.36771890e-05 2.26956963e-05 2.22695654e-05\n",
            "  2.23035843e-05 1.86334887e-05 1.28326955e-05 1.11878183e-05\n",
            "  9.76476214e-06 3.58387721e-06]]\n",
            "DEBUGGING: baseline2 looks like: 0.6142648284916304\n",
            "DEBUGGING: ADS looks like: [ 0.22220646  0.22990723  0.23287639  0.22078835  0.2148178   0.22159688\n",
            "  0.22142803 -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483\n",
            " -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483\n",
            " -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483\n",
            " -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483\n",
            " -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483\n",
            " -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483\n",
            " -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483\n",
            " -0.61426483 -0.61426483  0.2724765  -0.61426483 -0.61426483 -0.61426483\n",
            " -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483\n",
            " -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483\n",
            " -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483\n",
            " -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483\n",
            " -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483\n",
            " -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483\n",
            " -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.61426483\n",
            " -0.61426483 -0.61426483 -0.61426483 -0.61426483 -0.49468296 -0.51480783\n",
            " -0.51453228 -0.51908683 -0.53895855 -0.56424646 -0.56464129 -0.56512881\n",
            " -0.56630484 -0.56872883 -0.56913909 -0.57715287 -0.58065228 -0.59326604\n",
            " -0.59372353 -0.6032259  -0.60746697 -0.60779438 -0.60777462 -0.60774237\n",
            " -0.6087255  -0.60873546 -0.60907224 -0.60906622 -0.60952129 -0.60948321\n",
            " -0.60945453 -0.60952175 -0.60949314 -0.61002701 -0.61011453 -0.61053937\n",
            " -0.61072603 -0.61178814 -0.61180792 -0.61228745 -0.61377564 -0.61391119\n",
            " -0.61390875 -0.61391058 -0.61400509 -0.6140138  -0.61419674 -0.61419802\n",
            " -0.61419792 -0.61420893 -0.61422633 -0.61423127 -0.61423553 -0.61425408]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(-24.7262, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: training for one iteration takes 0.003967 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 4\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [56]\n",
            "DEBUGGING: the action_prob is: tensor([0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [41]\n",
            "DEBUGGING: the action_prob is: tensor([0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [22]\n",
            "DEBUGGING: the action_prob is: tensor([0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [66]\n",
            "DEBUGGING: the action_prob is: tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [86]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.1024816892754643 and immediate rewards look like: [0.0010383661865489557, 0.002474004746090941, 0.044536717812206916, 0.0012830904306611046, 6.055251469661016e-05, 0.004565308326164086, 0.000703617590261274, 0.0019422613308961445, 0.002085836725655099, 9.30416354094632e-06, 9.469451924815075e-06, 0.0012456119466150994, 0.00670484568718166, 0.0014777386595596909, 0.004793658633843734, 0.000633239270428021, 5.3316282901505474e-05, 0.00010428182986288448, 0.00012685576530202525, 0.0001212471161124995, 0.00100326843039511, 1.590780902915867e-05, 3.9989302877074806e-05, 7.71734426052717e-05, 2.1735108475695597e-05, 0.00501447460919735, 0.0011568848526621878, 0.0006445595663535642, 0.0013136685656718328, 0.00023710625418971176, 5.283575774228666e-06, 0.00047922390194798936, 0.004156027717272082, 0.0007627980753568409, 0.0008822884324217739, 0.00011442692539276322, 0.00023912882898002863, 0.00271799562005981, 0.003504371859889943, 0.0009096682306335424, 0.00016809073076728964, 0.0001985518797482655, 0.00031651673180022044, 0.002882704839521466, 8.904687274480239e-05, 9.589606179360999e-05, 4.024670442959177e-05, 0.0002833850717252062, 1.5809409887879156e-05, 0.0011261353934060025]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [54]\n",
            "DEBUGGING: the action_prob is: tensor([0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [25]\n",
            "DEBUGGING: the action_prob is: tensor([0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [44]\n",
            "DEBUGGING: the action_prob is: tensor([0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [36]\n",
            "DEBUGGING: the action_prob is: tensor([0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [26]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.88674132898268 and immediate rewards look like: [0.04913616336898485, 0.00198800599491733, 0.010246731111692498, 0.0027612629428404034, 0.010011128750193166, 0.00013923819687988725, 0.0018059554281535384, 0.000819622213839466, 0.002734203173986316, 0.0007250025569192076, 0.0013559002395595599, 0.004858565805534454, 0.0013686512315871369, 0.007542509812992648, 0.001466108491513296, 0.0025724871106831415, 0.02647291842777122, 0.005155121955795039, 0.0011908013138963724, 0.0011139183034174494, 0.0012094827811779396, 4.1633779346739175e-05, 0.0011131665523862466, 0.7509127494286076, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 9.094947017729282e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 9.094947017729282e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 0.0, 4.547473508864641e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [14]\n",
            "DEBUGGING: the action_prob is: tensor([0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [51]\n",
            "DEBUGGING: the action_prob is: tensor([0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [52]\n",
            "DEBUGGING: the action_prob is: tensor([0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
            "        0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
            "        0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
            "        0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
            "        0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
            "        0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
            "        0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
            "        0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
            "        0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
            "        0.0116, 0.0116, 0.0116, 0.0116, 0.0116], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [23]\n",
            "DEBUGGING: the action_prob is: tensor([0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [38]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.1602890641479462 and immediate rewards look like: [0.008577051877182384, 0.03907940943690846, 0.017928202858456643, 0.04407331123184122, 0.006651036188486614, 0.008449084486983338, 0.0003186704061590717, 0.0045316182913666125, 0.004817960435957502, 0.003949363824176544, 0.014180703871261358, 0.0020265079001546837, 0.001103532894376258, 0.0001785410304364632, 0.00024231747056546737, 0.00012465659483495983, 0.00037712984976678854, 3.758161710720742e-06, 0.0004104424315301003, 0.00030034599603823153, 0.0002141887252946617, 0.0001974673236873059, 1.73514808921027e-06, 9.81898529062164e-06, 0.00022826956865173997, 0.0001419196746610396, 0.0004837835022044601, 2.9980363251524977e-05, 3.081704790020012e-05, 1.715670350677101e-06, 0.00027378054983273614, 0.00013924913764640223, 3.966670192312449e-07, 8.504454399371753e-06, 2.3959176360222045e-05, 0.0001501646738688578, 7.99418203314417e-05, 6.8509589254972525e-06, 9.112983798331697e-05, 3.008064413734246e-05, 7.621533723067841e-05, 1.643208406676422e-05, 8.69915356815909e-06, 8.73127155500697e-05, 8.333840469276765e-05, 0.00023930386942083715, 7.782238753861748e-06, 3.16294067488343e-05, 0.00015524796754107228, 0.00011570380229386501]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 150 number of matrices\n",
            "DEBUGGING: ACT_MAT has 150 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.09100654788549913, 0.09087695121106078, 0.08929590552017155, 0.04521130071511579, 0.044371929580257256, 0.044758966732889545, 0.040599654956288346, 0.040299027642451586, 0.03874420839551055, 0.03702865825237924, 0.03739328695842252, 0.03776143182474516, 0.03688466654356572, 0.030484667531701072, 0.029299928153678163, 0.024753807595792352, 0.02436421042966094, 0.02455645873410044, 0.024699168590138945, 0.02482051800488578, 0.024948758473508362, 0.024187363679912376, 0.02441561199079113, 0.024621841098903086, 0.024792593592220014, 0.025021069175499313, 0.020208681380102993, 0.019244238916606874, 0.018787554899245766, 0.017650390235933268, 0.017589175739134905, 0.017761507235717854, 0.017456851852292794, 0.013435175893960318, 0.012800381634953006, 0.01203847798235478, 0.01204449601713335, 0.011924613321366991, 0.009299613839704223, 0.005853779777590181, 0.004994052067632968, 0.004874708421076443, 0.0047233904457860375, 0.004451387589884664, 0.0015845280306698967, 0.001510587028207166, 0.0014289807741551073, 0.0014027616865914297, 0.0011306834493598217, 0.0011261353934060025], [0.7227697110815151, 0.6804379269823538, 0.6853029504923601, 0.6818749690713815, 0.6859734405338799, 0.6827902139229158, 0.689546440127309, 0.6946873582819753, 0.7008765010789251, 0.7051942403080189, 0.7115850886374745, 0.7174032206039545, 0.7197420755539596, 0.72562972153775, 0.7253406179037952, 0.7311863731437191, 0.7359736222555919, 0.7166673776038593, 0.7186992481293579, 0.7247560068843046, 0.7309516046271588, 0.7371132543898796, 0.7445167884954877, 0.7509127494374763, 8.958231916426722e-12, 9.048719107501739e-12, 8.680779552136642e-12, 8.30912343560624e-12, 7.474372458417487e-12, 6.631189653176323e-12, 6.69817136684477e-12, 6.765829663479566e-12, 6.374830618780911e-12, 5.97988208878227e-12, 5.580944179692733e-12, 4.718635836282631e-12, 3.847617307585558e-12, 3.886482128874301e-12, 3.4663987656442803e-12, 3.0420721361190064e-12, 2.61345937902277e-12, 2.639857958608859e-12, 2.207182432042823e-12, 1.310795687141308e-12, 1.3240360476174828e-12, 8.780693906373926e-13, 8.869387784216087e-13, 8.958977559814229e-13, 9.049472282640636e-13, 4.547473508864641e-13], [0.15376095680833643, 0.14665040902136772, 0.10865757533783763, 0.09164583078725352, 0.04805305005597202, 0.041820216027763035, 0.03370821367755525, 0.03372681138524866, 0.029490094034224288, 0.02492134706895635, 0.02118382145937354, 0.007073856149608267, 0.005098331565104629, 0.0040351501724529, 0.0038955647899155932, 0.0036901488074243694, 0.003601507285443848, 0.0032569469047243026, 0.0032860492353672544, 0.0029046533372092467, 0.0026306134759303186, 0.0024408330814501586, 0.0022660260179422752, 0.0022871624948010757, 0.002300346979303489, 0.002093007485506817, 0.0019707957685310886, 0.00150203259224912, 0.001486921443431914, 0.001470812520739105, 0.0014839362125135635, 0.001222379457253361, 0.001094071029906019, 0.0011047215786735229, 0.0011072900245193446, 0.0010942735839991136, 0.0009536453637679353, 0.000882528831754034, 0.0008845231038672088, 0.0008014073392766584, 0.0007791178738780969, 0.0007100025622701197, 0.0007005762406094501, 0.0006988657444861525, 0.0006177303322586695, 0.0005397898258241433, 0.00030352116808414763, 0.000298726191242713, 0.00026979473181199865, 0.00011570380229386501]]\n",
            "DEBUGGING: traj_returns = [0.09100654788549913, 0.7227697110815151, 0.15376095680833643]\n",
            "DEBUGGING: actions = [[56], [55], [27], [5], [58], [57], [9], [47], [25], [12], [41], [60], [4], [3], [47], [19], [54], [45], [40], [66], [22], [52], [71], [72], [44], [75], [54], [11], [76], [1], [66], [6], [41], [83], [88], [7], [43], [66], [9], [55], [86], [67], [37], [10], [23], [11], [52], [18], [24], [50], [54], [11], [57], [59], [1], [47], [54], [20], [35], [38], [25], [1], [44], [44], [70], [22], [39], [54], [44], [25], [44], [16], [71], [0], [36], [13], [50], [18], [23], [61], [36], [67], [29], [47], [72], [79], [68], [27], [76], [34], [26], [97], [78], [60], [5], [27], [10], [48], [64], [97], [14], [32], [7], [35], [5], [39], [53], [64], [56], [3], [51], [66], [28], [47], [49], [64], [29], [18], [43], [57], [52], [60], [26], [43], [46], [26], [26], [52], [47], [13], [23], [41], [74], [24], [30], [69], [63], [94], [33], [88], [38], [68], [100], [95], [8], [4], [22], [42], [1], [79]]\n",
            "DEBUGGING: actions length = 150\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-0.1351, -0.1521, -0.1059,  ..., -0.2351,  0.1470, -0.1215],\n",
            "        [-0.1352, -0.1520, -0.1059,  ..., -0.2351,  0.1470, -0.1215],\n",
            "        [-0.1351, -0.1520, -0.1061,  ..., -0.2350,  0.1467, -0.1216],\n",
            "        ...,\n",
            "        [-0.1351, -0.1521, -0.1059,  ..., -0.2351,  0.1469, -0.1215],\n",
            "        [-0.1351, -0.1521, -0.1059,  ..., -0.2351,  0.1469, -0.1215],\n",
            "        [-0.1351, -0.1521, -0.1059,  ..., -0.2351,  0.1469, -0.1215]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-0.1351, -0.1522, -0.1059,  ..., -0.2351,  0.1470, -0.1215],\n",
            "        [-0.1351, -0.1521, -0.1059,  ..., -0.2351,  0.1469, -0.1215],\n",
            "        [-0.1351, -0.1521, -0.1059,  ..., -0.2351,  0.1469, -0.1215],\n",
            "        ...,\n",
            "        [-0.1351, -0.1521, -0.1059,  ..., -0.2351,  0.1469, -0.1215],\n",
            "        [-0.1351, -0.1521, -0.1059,  ..., -0.2351,  0.1469, -0.1215],\n",
            "        [-0.1351, -0.1521, -0.1059,  ..., -0.2351,  0.1469, -0.1215]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716,\n",
            "        0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716,\n",
            "        0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716,\n",
            "        0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716,\n",
            "        0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716,\n",
            "        0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716,\n",
            "        0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716,\n",
            "        0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716,\n",
            "        0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716,\n",
            "        0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716,\n",
            "        0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716,\n",
            "        0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716, 0.1716],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[0.32251241 0.30598843 0.29441881 0.2729107  0.25946614 0.25645647\n",
            "  0.2546181  0.25623773 0.25637027 0.25571475 0.25672073 0.2540795\n",
            "  0.25390836 0.25338318 0.25284537 0.25321011 0.25464645 0.24816026\n",
            "  0.24889482 0.25082706 0.25284366 0.25458048 0.25706614 0.25927392\n",
            "  0.00903098 0.00903803 0.00739316 0.00691542 0.00675816 0.00637373\n",
            "  0.0063577  0.00632796 0.00618364 0.00484663 0.00463589 0.00437758\n",
            "  0.00433271 0.00426905 0.00339471 0.0022184  0.00192439 0.00186157\n",
            "  0.00180799 0.00171675 0.00073409 0.00068346 0.0005775  0.00056716\n",
            "  0.00046683 0.00041395]]\n",
            "DEBUGGING: baseline2 looks like: 0.3225124052584502\n",
            "DEBUGGING: ADS looks like: [-0.23150586 -0.23163545 -0.2332165  -0.2773011  -0.27814048 -0.27775344\n",
            " -0.28191275 -0.28221338 -0.2837682  -0.28548375 -0.28511912 -0.28475097\n",
            " -0.28562774 -0.29202774 -0.29321248 -0.2977586  -0.29814819 -0.29795595\n",
            " -0.29781324 -0.29769189 -0.29756365 -0.29832504 -0.29809679 -0.29789056\n",
            " -0.29771981 -0.29749134 -0.30230372 -0.30326817 -0.30372485 -0.30486202\n",
            " -0.30492323 -0.3047509  -0.30505555 -0.30907723 -0.30971202 -0.31047393\n",
            " -0.31046791 -0.31058779 -0.31321279 -0.31665863 -0.31751835 -0.3176377\n",
            " -0.31778901 -0.31806102 -0.32092788 -0.32100182 -0.32108342 -0.32110964\n",
            " -0.32138172 -0.32138627  0.40025731  0.35792552  0.36279055  0.35936256\n",
            "  0.36346104  0.36027781  0.36703403  0.37217495  0.3783641   0.38268184\n",
            "  0.38907268  0.39489082  0.39722967  0.40311732  0.40282821  0.40867397\n",
            "  0.41346122  0.39415497  0.39618684  0.4022436   0.4084392   0.41460085\n",
            "  0.42200438  0.42840034 -0.32251241 -0.32251241 -0.32251241 -0.32251241\n",
            " -0.32251241 -0.32251241 -0.32251241 -0.32251241 -0.32251241 -0.32251241\n",
            " -0.32251241 -0.32251241 -0.32251241 -0.32251241 -0.32251241 -0.32251241\n",
            " -0.32251241 -0.32251241 -0.32251241 -0.32251241 -0.32251241 -0.32251241\n",
            " -0.32251241 -0.32251241 -0.32251241 -0.32251241 -0.16875145 -0.175862\n",
            " -0.21385483 -0.23086657 -0.27445936 -0.28069219 -0.28880419 -0.28878559\n",
            " -0.29302231 -0.29759106 -0.30132858 -0.31543855 -0.31741407 -0.31847726\n",
            " -0.31861684 -0.31882226 -0.3189109  -0.31925546 -0.31922636 -0.31960775\n",
            " -0.31988179 -0.32007157 -0.32024638 -0.32022524 -0.32021206 -0.3204194\n",
            " -0.32054161 -0.32101037 -0.32102548 -0.32104159 -0.32102847 -0.32129003\n",
            " -0.32141833 -0.32140768 -0.32140512 -0.32141813 -0.32155876 -0.32162988\n",
            " -0.32162788 -0.321711   -0.32173329 -0.3218024  -0.32181183 -0.32181354\n",
            " -0.32189467 -0.32197262 -0.32220888 -0.32221368 -0.32224261 -0.3223967 ]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(-8.7234, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: training for one iteration takes 0.005825 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 5\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [44]\n",
            "DEBUGGING: the action_prob is: tensor([0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [51]\n",
            "DEBUGGING: the action_prob is: tensor([0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [37]\n",
            "DEBUGGING: the action_prob is: tensor([0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [43]\n",
            "DEBUGGING: the action_prob is: tensor([0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [37]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.13369968426013656 and immediate rewards look like: [0.021271290186177794, 0.026960676236740255, 0.0269462764963464, 0.019241406676428596, 0.00021496373756235698, 0.0028992635902795882, 0.002348156799598655, 0.0013342084284886369, 0.0005420828611022444, 0.00032006539640860865, 0.0007443000881721673, 0.0024650679592923552, 0.0007218225250653632, 0.0024045332311288803, 0.002398355111836281, 0.001034675090068049, 0.002498811526947975, 0.002081926561004366, 0.0003965713572142704, 0.004103867983303644, 5.110575057187816e-05, 0.00017172451043734327, 0.00015861352994761546, 0.000505936989611655, 0.002971299703403929, 0.004956494549787749, 0.000945004613186029, 0.000545524852896051, 0.0006171908589749364, 3.636411020124797e-05, 6.282724871198297e-05, 6.800866231060354e-06, 3.937106930607115e-05, 0.00027559206955629634, 2.1020526673964923e-05, 1.18602551992808e-05, 7.067101523716701e-05, 6.15199815001688e-06, 0.0006218024491317919, 0.0004026732594866189, 1.1755992090911604e-07, 9.001541911857203e-07, 4.0080660255625844e-05, 1.3111725820635911e-06, 4.84791598864831e-06, 5.943206861047656e-05, 0.00015638046761523583, 1.3594047231890727e-05, 8.522091320628533e-06, 8.1460525507282e-06]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [9]\n",
            "DEBUGGING: the action_prob is: tensor([0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [21]\n",
            "DEBUGGING: the action_prob is: tensor([0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [31]\n",
            "DEBUGGING: the action_prob is: tensor([0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [39]\n",
            "DEBUGGING: the action_prob is: tensor([0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [47]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.886741328988137 and immediate rewards look like: [0.02985904561865027, 0.002829482493325486, 0.001709058574761002, 0.004525367322457896, 0.026924762702492444, 0.005483379938141297, 0.0232922987420352, 0.029535552048855607, 0.00029935978000139585, 0.0002558496125857346, 0.00020979076407456887, 0.015618497168816248, 0.0069140883460931946, 0.0007244244011417322, 0.0042673331172409235, 0.7342930383420025, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 1.3642420526593924e-12, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 9.094947017729282e-13, 0.0, 4.547473508864641e-13, 1.3642420526593924e-12, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 0.0, 9.094947017729282e-13, 9.094947017729282e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [16]\n",
            "DEBUGGING: the action_prob is: tensor([0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [48]\n",
            "DEBUGGING: the action_prob is: tensor([0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [1]\n",
            "DEBUGGING: the action_prob is: tensor([0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [45]\n",
            "DEBUGGING: the action_prob is: tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [38]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.10867580417652789 and immediate rewards look like: [0.021653127912486525, 0.0025029993162206665, 0.012079464429916698, 0.000399208505768911, 0.00022924587165107368, 0.0003157146475132322, 0.011925693627745204, 0.017635969747971103, 0.0005550751498049067, 0.0034265763902112667, 0.0008009880771169264, 3.591319818951888e-05, 0.0015568182111564965, 0.0032454355859954376, 0.0022426832169912814, 0.0001813900530578394, 8.608143116362044e-05, 0.0003943624114981503, 0.0019261745910625905, 0.00011282873174423003, 0.001939921102803055, 1.2187952506792499e-05, 3.461691767370212e-05, 5.590242381003918e-06, 0.00019893181433872087, 7.358414177360828e-05, 0.00019283567371530808, 0.0008879754336703627, 0.00854504681092294, 0.0007556967580057972, 0.0034137719603677397, 0.004924356241645, 0.002785553056128265, 0.00019161111822540988, 4.8566958867013454e-05, 3.7900431379966903e-06, 4.0633550725033274e-05, 1.1062738394684857e-05, 7.336621820286382e-06, 5.9182602853979915e-05, 0.0003925231517314387, 0.00019367670347492094, 2.103694032484782e-05, 3.053108866879484e-05, 9.713007921163808e-05, 0.0006364827963807329, 6.169685093482258e-06, 0.0008956666756603227, 0.0009336154412267206, 3.096876753261313e-05]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 150 number of matrices\n",
            "DEBUGGING: ACT_MAT has 150 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.1262331849383116, 0.10602211591124627, 0.0798600400752586, 0.05344824603930526, 0.03455236299280471, 0.03468424167196197, 0.03210603846634584, 0.03005846633004766, 0.029014401920766687, 0.028759918242085295, 0.028727124086542107, 0.028265478786232263, 0.026061021037313038, 0.02559515001237139, 0.023424865435598494, 0.021238899316931528, 0.0204083072998621, 0.018090399770620328, 0.016170174959208042, 0.01593293293130684, 0.01194855045252848, 0.012017620911067275, 0.011965551919828215, 0.011926200393818788, 0.011535619600209225, 0.008650828178591207, 0.003731650130104503, 0.002814793451432802, 0.0022921905035724758, 0.0016919188329268073, 0.0016722774977025853, 0.0016257073222127297, 0.0016352590464461306, 0.0016120080577172319, 0.0013499151395565006, 0.00134231779079044, 0.0013438965005971306, 0.001286086348848448, 0.0012928629805034658, 0.0006778389205774483, 0.00027794511221295896, 0.00028063389120409074, 0.00028255933031606567, 0.00024492794955599984, 0.0002460775524989255, 0.0002436662995053305, 0.0001860951827220747, 3.0014863744281683e-05, 1.658668334584945e-05, 8.1460525507282e-06], [0.7759528576636855, 0.7536301131768033, 0.7583844754378564, 0.7643186028920155, 0.7674679147167248, 0.748023385872962, 0.7500404100351724, 0.7340890013061993, 0.7116701507649936, 0.7185563545302952, 0.7255560655734439, 0.7326730048579488, 0.7242974825142754, 0.7246296910789719, 0.7312174410887173, 0.7342930383550267, 1.3155684284081791e-11, 1.3288569983921002e-11, 1.3422797963556568e-11, 1.3099041022899095e-11, 1.2772013810113769e-11, 1.2441683292148793e-11, 1.2567356860756357e-11, 1.1316277583936328e-11, 1.0971242659646327e-11, 1.1082063292572048e-11, 1.0734662567359176e-11, 9.924411985440656e-12, 9.105977054209825e-12, 9.197956620413964e-12, 8.831524514674242e-12, 7.542709557590758e-12, 6.7002170260786154e-12, 6.30855522746682e-12, 5.912937249071066e-12, 5.972663887950572e-12, 6.032993826212699e-12, 5.634592399319429e-12, 4.772825957117677e-12, 4.821036320320886e-12, 3.951052139947432e-12, 3.0722802405803076e-12, 2.184631857381191e-12, 1.7473580873684113e-12, 1.7650081690590014e-12, 1.7828365344030317e-12, 1.8008449842454866e-12, 1.3596945791505278e-12, 4.547473508864641e-13, 0.0], [0.09751843729679444, 0.07663162564071506, 0.07487740032777211, 0.06343225848268223, 0.0636697474514276, 0.06408131472704699, 0.0644096970500341, 0.0530141448709989, 0.0357355304273008, 0.035535813411612015, 0.032433572748889646, 0.03195210572906335, 0.03223857831401397, 0.03099167687157321, 0.028026506349068454, 0.026044265789976942, 0.026124116905978892, 0.02630104593415684, 0.026168367194604737, 0.024487063235901156, 0.024620438893087803, 0.022909613929580554, 0.023128713108155313, 0.023327369889375366, 0.023557353178782183, 0.023594365014589357, 0.023758364517995706, 0.02380356448917212, 0.023147059652021976, 0.014749507920302055, 0.01413516279019824, 0.010829687706899495, 0.005964981278034843, 0.0032115436584914925, 0.0030504369093596797, 0.0030321918691845113, 0.0030589917434813278, 0.0030488466593497927, 0.0030684686070253614, 0.0030920525103081565, 0.003063504957024421, 0.0026979614194878607, 0.0025295805212251915, 0.002533882404949842, 0.0025286376932131794, 0.002456068296971254, 0.0018379651521116375, 0.00185029845153349, 0.0009642745210840075, 3.096876753261313e-05]]\n",
            "DEBUGGING: traj_returns = [0.1262331849383116, 0.7759528576636855, 0.09751843729679444]\n",
            "DEBUGGING: actions = [[44], [47], [38], [14], [5], [10], [40], [15], [41], [50], [51], [21], [24], [47], [51], [45], [27], [17], [50], [51], [37], [72], [74], [52], [24], [47], [56], [63], [64], [79], [43], [38], [17], [77], [40], [8], [61], [76], [44], [34], [37], [7], [65], [30], [28], [80], [73], [36], [13], [97], [9], [43], [41], [59], [19], [10], [2], [33], [13], [25], [21], [10], [33], [70], [25], [0], [3], [63], [25], [0], [31], [20], [32], [70], [68], [48], [70], [26], [50], [49], [39], [51], [16], [64], [53], [26], [2], [63], [31], [16], [47], [64], [79], [38], [73], [76], [75], [39], [68], [57], [16], [61], [31], [54], [44], [34], [45], [30], [58], [9], [48], [52], [38], [58], [20], [62], [16], [64], [5], [12], [1], [16], [65], [35], [5], [25], [50], [84], [56], [9], [45], [66], [20], [20], [39], [71], [69], [88], [21], [89], [38], [8], [16], [50], [30], [2], [56], [58], [68], [46]]\n",
            "DEBUGGING: actions length = 150\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-0.1275, -0.1636, -0.0977,  ..., -0.2440,  0.1599, -0.1199],\n",
            "        [-0.1275, -0.1635, -0.0977,  ..., -0.2440,  0.1600, -0.1199],\n",
            "        [-0.1275, -0.1635, -0.0980,  ..., -0.2439,  0.1596, -0.1200],\n",
            "        ...,\n",
            "        [-0.1275, -0.1636, -0.0978,  ..., -0.2440,  0.1599, -0.1199],\n",
            "        [-0.1275, -0.1636, -0.0978,  ..., -0.2440,  0.1599, -0.1199],\n",
            "        [-0.1275, -0.1636, -0.0978,  ..., -0.2440,  0.1599, -0.1199]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-0.1275, -0.1637, -0.0977,  ..., -0.2441,  0.1599, -0.1199],\n",
            "        [-0.1275, -0.1636, -0.0978,  ..., -0.2440,  0.1599, -0.1199],\n",
            "        [-0.1275, -0.1636, -0.0978,  ..., -0.2440,  0.1599, -0.1199],\n",
            "        ...,\n",
            "        [-0.1275, -0.1636, -0.0978,  ..., -0.2440,  0.1599, -0.1199],\n",
            "        [-0.1275, -0.1636, -0.0978,  ..., -0.2440,  0.1599, -0.1199],\n",
            "        [-0.1275, -0.1636, -0.0978,  ..., -0.2440,  0.1599, -0.1199]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816,\n",
            "        0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816,\n",
            "        0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816,\n",
            "        0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816,\n",
            "        0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816,\n",
            "        0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816,\n",
            "        0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816,\n",
            "        0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816,\n",
            "        0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816,\n",
            "        0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816,\n",
            "        0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816,\n",
            "        0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816, 0.1816],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[3.33234827e-01 3.12094618e-01 3.04373972e-01 2.93733036e-01\n",
            "  2.88563342e-01 2.82262981e-01 2.82185382e-01 2.72387204e-01\n",
            "  2.58806694e-01 2.60950695e-01 2.62238921e-01 2.64296863e-01\n",
            "  2.60865694e-01 2.60405506e-01 2.60889604e-01 2.60525401e-01\n",
            "  1.55108081e-02 1.47971486e-02 1.41128474e-02 1.34733321e-02\n",
            "  1.21896631e-02 1.16424116e-02 1.16980883e-02 1.17511901e-02\n",
            "  1.16976576e-02 1.07483977e-02 9.16333822e-03 8.87278598e-03\n",
            "  8.47975005e-03 5.48047559e-03 5.26914677e-03 4.15179835e-03\n",
            "  2.53341344e-03 1.60785057e-03 1.46678402e-03 1.45816989e-03\n",
            "  1.46762942e-03 1.44497767e-03 1.45377720e-03 1.25663048e-03\n",
            "  1.11381669e-03 9.92865105e-04 9.37379951e-04 9.26270119e-04\n",
            "  9.24905082e-04 8.99911533e-04 6.74686779e-04 6.26771106e-04\n",
            "  3.26953735e-04 1.30382734e-05]]\n",
            "DEBUGGING: baseline2 looks like: 0.3332348266329305\n",
            "DEBUGGING: ADS looks like: [-0.20700164 -0.22721271 -0.25337479 -0.27978658 -0.29868246 -0.29855058\n",
            " -0.30112879 -0.30317636 -0.30422042 -0.30447491 -0.3045077  -0.30496935\n",
            " -0.30717381 -0.30763968 -0.30980996 -0.31199593 -0.31282652 -0.31514443\n",
            " -0.31706465 -0.31730189 -0.32128628 -0.32121721 -0.32126927 -0.32130863\n",
            " -0.32169921 -0.324584   -0.32950318 -0.33042003 -0.33094264 -0.33154291\n",
            " -0.33156255 -0.33160912 -0.33159957 -0.33162282 -0.33188491 -0.33189251\n",
            " -0.33189093 -0.33194874 -0.33194196 -0.33255699 -0.33295688 -0.33295419\n",
            " -0.33295227 -0.3329899  -0.33298875 -0.33299116 -0.33304873 -0.33320481\n",
            " -0.33321824 -0.33322668  0.44271803  0.42039529  0.42514965  0.43108378\n",
            "  0.43423309  0.41478856  0.41680558  0.40085417  0.37843532  0.38532153\n",
            "  0.39232124  0.39943818  0.39106266  0.39139486  0.39798261  0.40105821\n",
            " -0.33323483 -0.33323483 -0.33323483 -0.33323483 -0.33323483 -0.33323483\n",
            " -0.33323483 -0.33323483 -0.33323483 -0.33323483 -0.33323483 -0.33323483\n",
            " -0.33323483 -0.33323483 -0.33323483 -0.33323483 -0.33323483 -0.33323483\n",
            " -0.33323483 -0.33323483 -0.33323483 -0.33323483 -0.33323483 -0.33323483\n",
            " -0.33323483 -0.33323483 -0.33323483 -0.33323483 -0.33323483 -0.33323483\n",
            " -0.33323483 -0.33323483 -0.33323483 -0.33323483 -0.23571639 -0.2566032\n",
            " -0.25835743 -0.26980257 -0.26956508 -0.26915351 -0.26882513 -0.28022068\n",
            " -0.2974993  -0.29769901 -0.30080125 -0.30128272 -0.30099625 -0.30224315\n",
            " -0.30520832 -0.30719056 -0.30711071 -0.30693378 -0.30706646 -0.30874776\n",
            " -0.30861439 -0.31032521 -0.31010611 -0.30990746 -0.30967747 -0.30964046\n",
            " -0.30947646 -0.30943126 -0.31008777 -0.31848532 -0.31909966 -0.32240514\n",
            " -0.32726985 -0.33002328 -0.33018439 -0.33020263 -0.33017583 -0.33018598\n",
            " -0.33016636 -0.33014277 -0.33017132 -0.33053687 -0.33070525 -0.33070094\n",
            " -0.33070619 -0.33077876 -0.33139686 -0.33138453 -0.33227055 -0.33320386]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(-10.7124, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: training for one iteration takes 0.003274 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 6\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [46]\n",
            "DEBUGGING: the action_prob is: tensor([0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [51]\n",
            "DEBUGGING: the action_prob is: tensor([0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [45]\n",
            "DEBUGGING: the action_prob is: tensor([0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [79]\n",
            "DEBUGGING: the action_prob is: tensor([0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [39]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.1365974023005947 and immediate rewards look like: [0.010429303515593347, 0.001389515828122967, 0.004982242841833795, 0.05114364760538592, 0.00037553037009274703, 0.02501271921892112, 0.0023046822120704746, 0.0021585818658422795, 0.01467039379031121, 0.006377930485541583, 0.0020909669769935135, 0.00040478807022736873, 0.0002901020379795227, 0.0008434731635134085, 0.0002870039306799299, 0.006110907607762783, 0.00018648476179805584, 0.001123356921652885, 0.0003223999492547591, 0.0008498899155711115, 2.7570765723794466e-05, 0.00040754107203611056, 0.00042870790593951824, 0.0005332125901986728, 0.0001449143051104329, 7.657884225409362e-05, 0.000684868743064726, 4.611803342413623e-05, 8.200531101465458e-05, 0.00012420227358234115, 0.000480517337564379, 0.00038795838054284104, 0.00013573902106145397, 4.416650881466921e-06, 2.394455805188045e-06, 2.4900506559788482e-05, 4.081982115167193e-05, 1.326980873272987e-05, 0.0005779311200058146, 0.00030855927707307274, 9.706703622214263e-05, 3.695041459650383e-05, 6.192795444803778e-05, 0.00028922387537022587, 2.2480870484287152e-05, 9.294977189711062e-06, 1.3535030120692682e-05, 1.8477527646609815e-05, 5.401255839387886e-05, 0.00010828476524693542]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [20]\n",
            "DEBUGGING: the action_prob is: tensor([0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [34]\n",
            "DEBUGGING: the action_prob is: tensor([0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [29]\n",
            "DEBUGGING: the action_prob is: tensor([0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [52]\n",
            "DEBUGGING: the action_prob is: tensor([0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [62]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.14371706847668975 and immediate rewards look like: [0.020044006158059346, 0.02587518172640557, 0.003973272460825683, 0.004185314913684124, 0.03716426293885888, 0.0014400086029127124, 0.00012022147711832076, 0.00011425115553720389, 0.0006429518425647984, 0.001467218825382588, 0.004348173358266649, 0.002587289982329821, 0.0035834765158142545, 0.005381592571211513, 0.020408003089414706, 0.002391120744505315, 0.0042551625433588924, 0.00037132856141397497, 0.0007655329459339555, 0.0013266778996694484, 8.877732716428e-05, 2.7389212846173905e-05, 0.00028102843043598114, 4.419734659677488e-05, 0.0007710486611358647, 2.9446590360748814e-05, 0.00010840857066796161, 7.333093981287675e-05, 0.0003537384127412224, 0.00041120778678305214, 0.0001204919676638383, 0.0004697219228546601, 3.2549810384807643e-06, 1.897655693028355e-05, 4.364446795079857e-07, 2.8927545827173162e-05, 3.5357179513084702e-06, 2.960060282930499e-05, 7.618438303325092e-06, 3.892941595040611e-05, 2.3132888600230217e-06, 1.0950545402010903e-05, 3.743536854017293e-05, 0.000139937935728085, 1.3340392797545064e-05, 2.736449505391647e-05, 9.076087735593319e-06, 0.00010413139580123243, 1.024876337396563e-05, 7.155017556215171e-06]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [0]\n",
            "DEBUGGING: the action_prob is: tensor([0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [18]\n",
            "DEBUGGING: the action_prob is: tensor([0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132,\n",
            "        0.0132, 0.0132, 0.0132, 0.0132], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [13]\n",
            "DEBUGGING: the action_prob is: tensor([0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [40]\n",
            "DEBUGGING: the action_prob is: tensor([0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0105, 0.0105, 0.0105, 0.0105], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [7]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.8867413289913202 and immediate rewards look like: [0.8867413289726755, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 9.094947017729282e-13, 9.094947017729282e-13, 9.094947017729282e-13, 0.0, 9.094947017729282e-13, 1.3642420526593924e-12, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 150 number of matrices\n",
            "DEBUGGING: ACT_MAT has 150 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.12870668371867927, 0.11947210121523832, 0.119275338774864, 0.11544757164952546, 0.06495345863044398, 0.06523023056601135, 0.040623748835444666, 0.03870612790239818, 0.03691671316823828, 0.02247102967467381, 0.016255655746598208, 0.014307766433944135, 0.014043412488602795, 0.013892232778407346, 0.013180565267569635, 0.013023799330191621, 0.006982718911544281, 0.0068648829795416415, 0.005799521270594704, 0.005532445779131257, 0.004729854407636511, 0.004749781456477491, 0.004386101398425637, 0.003997367164127393, 0.0034991460342714345, 0.0033881128577383855, 0.0033449838540245374, 0.002686984960565466, 0.002667542350647808, 0.0026116535753870234, 0.0025125770725299822, 0.002052585590874347, 0.0016814416265974808, 0.001561315763167704, 0.001572625365945694, 0.0015860918284247535, 0.0015769609311767324, 0.0015516576868940006, 0.0015539271496578491, 0.0009858545754060956, 0.0006841366649828515, 0.000592999625010817, 0.0005616658691053668, 0.0005047857723811405, 0.0002177392899100148, 0.0001972307266926542, 0.00018983409040701325, 0.00017807985887507128, 0.00016121447598834492, 0.00010828476524693542], [0.13489586890141242, 0.11601198256904349, 0.09104727357842214, 0.08795353648242067, 0.08461436522094601, 0.04792939624453245, 0.046958977415777516, 0.04731187468551434, 0.0476743672019971, 0.04750648016104273, 0.04650430437945469, 0.04258195052645257, 0.040398647014265404, 0.03718704090752641, 0.03212671549122717, 0.011837083234154007, 0.009541376252170396, 0.005339609806880306, 0.0050184659045114455, 0.0042958918773510005, 0.0029992060380621744, 0.002939826980704944, 0.002941856331170475, 0.0026877049502368622, 0.0026702097006465527, 0.0019183444843542305, 0.0019079776707004866, 0.0018177465656894192, 0.0017620359857338814, 0.001422522801002686, 0.0010215303173935694, 0.0009101397472017487, 0.00044486648923948354, 0.0004460722305060634, 0.0004314097712886665, 0.00043532659253450354, 0.000410504087583162, 0.00041107916124429646, 0.0003853318771868601, 0.00038152872614498486, 0.0003460599092874533, 0.00034721880851255584, 0.00033966491223287365, 0.00030528236736636436, 0.00016701457741240337, 0.00015522644910591747, 0.00012915348894141515, 0.0001212903042483049, 1.733223075461865e-05, 7.155017556215171e-06], [0.8867413289865075, 1.3971671250066022e-11, 1.4112799242490933e-11, 1.4255352770192862e-11, 1.3940005474046867e-11, 1.3621472851677174e-11, 1.3299722728071424e-11, 1.3434063361688308e-11, 1.3110420212931155e-11, 1.2783507941459283e-11, 1.2453293525831131e-11, 1.2119743611055219e-11, 1.1782824505220964e-11, 1.190184293456663e-11, 1.2022063570269324e-11, 1.21434985558276e-11, 1.2266160157401617e-11, 1.1930720006580963e-11, 1.1591891571408585e-11, 1.1708981385261197e-11, 1.1827253924506259e-11, 1.1946721135864907e-11, 1.206739508673223e-11, 1.2189287966396193e-11, 1.2312412087268881e-11, 1.1977439127659007e-11, 1.163908260280055e-11, 1.1297308335266753e-11, 1.0952081802404333e-11, 1.0603368132846332e-11, 1.0710472861460942e-11, 9.899977939078802e-12, 9.540636957770038e-12, 9.177666269579368e-12, 8.811029210800913e-12, 8.440688747388333e-12, 8.525948229685185e-12, 8.612068918873924e-12, 7.780377997071712e-12, 6.94028615686746e-12, 6.091708540499527e-12, 6.153240949999522e-12, 5.296713382047064e-12, 3.972193262007749e-12, 3.093634909328102e-12, 2.665542988324887e-12, 2.2331269065034576e-12, 1.7963429854717106e-12, 1.355147105641663e-12, 9.094947017729282e-13]]\n",
            "DEBUGGING: traj_returns = [0.12870668371867927, 0.13489586890141242, 0.8867413289865075]\n",
            "DEBUGGING: actions = [[46], [15], [41], [51], [13], [58], [24], [5], [50], [25], [51], [58], [1], [67], [26], [51], [53], [14], [71], [52], [45], [75], [41], [30], [55], [3], [5], [13], [26], [1], [79], [78], [37], [55], [68], [45], [3], [38], [65], [37], [39], [81], [7], [5], [17], [87], [94], [93], [42], [38], [20], [27], [46], [12], [32], [2], [10], [63], [4], [32], [34], [8], [17], [6], [67], [66], [61], [13], [30], [19], [29], [18], [73], [40], [11], [1], [32], [40], [19], [19], [52], [51], [30], [13], [33], [1], [91], [13], [71], [38], [62], [25], [1], [50], [86], [99], [99], [33], [10], [52], [0], [16], [30], [30], [10], [5], [3], [59], [42], [5], [18], [60], [47], [61], [50], [57], [66], [37], [75], [21], [13], [44], [27], [27], [39], [45], [37], [68], [26], [60], [40], [9], [38], [63], [12], [29], [46], [67], [72], [54], [7], [19], [24], [37], [94], [56], [12], [55], [46], [91]]\n",
            "DEBUGGING: actions length = 150\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-0.1228, -0.1797, -0.0870,  ..., -0.2596,  0.1767, -0.1192],\n",
            "        [-0.1226, -0.1796, -0.0870,  ..., -0.2595,  0.1767, -0.1191],\n",
            "        [-0.1225, -0.1795, -0.0872,  ..., -0.2591,  0.1764, -0.1190],\n",
            "        ...,\n",
            "        [-0.1227, -0.1796, -0.0871,  ..., -0.2595,  0.1766, -0.1191],\n",
            "        [-0.1226, -0.1796, -0.0871,  ..., -0.2595,  0.1766, -0.1191],\n",
            "        [-0.1227, -0.1797, -0.0871,  ..., -0.2595,  0.1766, -0.1191]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-0.1227, -0.1797, -0.0871,  ..., -0.2595,  0.1766, -0.1191],\n",
            "        [-0.1227, -0.1797, -0.0871,  ..., -0.2595,  0.1766, -0.1191],\n",
            "        [-0.1227, -0.1797, -0.0871,  ..., -0.2595,  0.1766, -0.1191],\n",
            "        ...,\n",
            "        [-0.1227, -0.1796, -0.0871,  ..., -0.2595,  0.1766, -0.1191],\n",
            "        [-0.1227, -0.1797, -0.0871,  ..., -0.2595,  0.1766, -0.1191],\n",
            "        [-0.1227, -0.1797, -0.0871,  ..., -0.2595,  0.1766, -0.1191]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([0.2006, 0.2007, 0.2007, 0.2006, 0.2007, 0.2007, 0.2007, 0.2007, 0.2007,\n",
            "        0.2006, 0.2007, 0.2006, 0.2006, 0.2006, 0.2006, 0.2006, 0.2007, 0.2007,\n",
            "        0.2007, 0.2007, 0.2006, 0.2006, 0.2006, 0.2006, 0.2007, 0.2007, 0.2007,\n",
            "        0.2007, 0.2006, 0.2006, 0.2006, 0.2006, 0.2007, 0.2006, 0.2006, 0.2007,\n",
            "        0.2006, 0.2007, 0.2006, 0.2007, 0.2007, 0.2007, 0.2007, 0.2006, 0.2006,\n",
            "        0.2007, 0.2007, 0.2006, 0.2007, 0.2007, 0.2006, 0.2007, 0.2007, 0.2007,\n",
            "        0.2007, 0.2007, 0.2007, 0.2006, 0.2006, 0.2007, 0.2006, 0.2007, 0.2007,\n",
            "        0.2007, 0.2007, 0.2007, 0.2007, 0.2007, 0.2007, 0.2006, 0.2007, 0.2007,\n",
            "        0.2007, 0.2006, 0.2007, 0.2007, 0.2006, 0.2007, 0.2007, 0.2006, 0.2007,\n",
            "        0.2007, 0.2007, 0.2006, 0.2007, 0.2007, 0.2007, 0.2007, 0.2007, 0.2006,\n",
            "        0.2007, 0.2006, 0.2006, 0.2007, 0.2006, 0.2007, 0.2007, 0.2006, 0.2006,\n",
            "        0.2006, 0.2007, 0.2006, 0.2007, 0.2007], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[3.83447961e-01 7.84946946e-02 7.01075375e-02 6.78003694e-02\n",
            "  4.98559413e-02 3.77198756e-02 2.91942421e-02 2.86726675e-02\n",
            "  2.81970268e-02 2.33258366e-02 2.09199867e-02 1.89632390e-02\n",
            "  1.81473532e-02 1.70264246e-02 1.51024269e-02 8.28696086e-03\n",
            "  5.50803173e-03 4.06816427e-03 3.60599573e-03 3.27611256e-03\n",
            "  2.57635349e-03 2.56320282e-03 2.44265258e-03 2.22835738e-03\n",
            "  2.05645192e-03 1.76881912e-03 1.75098718e-03 1.50157718e-03\n",
            "  1.47652612e-03 1.34472546e-03 1.17803580e-03 9.87575116e-04\n",
            "  7.08769375e-04 6.69129334e-04 6.68011715e-04 6.73806143e-04\n",
            "  6.62488342e-04 6.54245619e-04 6.46419678e-04 4.55794436e-04\n",
            "  3.43398860e-04 3.13406147e-04 3.00443596e-04 2.70022715e-04\n",
            "  1.28251290e-04 1.17485726e-04 1.06329194e-04 9.97900550e-05\n",
            "  5.95155694e-05 3.84799279e-05]]\n",
            "DEBUGGING: baseline2 looks like: 0.38344796053553304\n",
            "DEBUGGING: ADS looks like: [-0.25474128 -0.26397586 -0.26417262 -0.26800039 -0.3184945  -0.31821773\n",
            " -0.34282421 -0.34474183 -0.34653125 -0.36097693 -0.3671923  -0.36914019\n",
            " -0.36940455 -0.36955573 -0.3702674  -0.37042416 -0.37646524 -0.37658308\n",
            " -0.37764844 -0.37791551 -0.37871811 -0.37869818 -0.37906186 -0.37945059\n",
            " -0.37994881 -0.38005985 -0.38010298 -0.38076098 -0.38078042 -0.38083631\n",
            " -0.38093538 -0.38139537 -0.38176652 -0.38188664 -0.38187534 -0.38186187\n",
            " -0.381871   -0.3818963  -0.38189403 -0.38246211 -0.38276382 -0.38285496\n",
            " -0.38288629 -0.38294317 -0.38323022 -0.38325073 -0.38325813 -0.38326988\n",
            " -0.38328675 -0.38333968 -0.24855209 -0.26743598 -0.29240069 -0.29549442\n",
            " -0.2988336  -0.33551856 -0.33648898 -0.33613609 -0.33577359 -0.33594148\n",
            " -0.33694366 -0.34086601 -0.34304931 -0.34626092 -0.35132125 -0.37161088\n",
            " -0.37390658 -0.37810835 -0.37842949 -0.37915207 -0.38044875 -0.38050813\n",
            " -0.3805061  -0.38076026 -0.38077775 -0.38152962 -0.38153998 -0.38163021\n",
            " -0.38168592 -0.38202544 -0.38242643 -0.38253782 -0.38300309 -0.38300189\n",
            " -0.38301655 -0.38301263 -0.38303746 -0.38303688 -0.38306263 -0.38306643\n",
            " -0.3831019  -0.38310074 -0.3831083  -0.38314268 -0.38328095 -0.38329273\n",
            " -0.38331881 -0.38332667 -0.38343063 -0.38344081  0.50329337 -0.38344796\n",
            " -0.38344796 -0.38344796 -0.38344796 -0.38344796 -0.38344796 -0.38344796\n",
            " -0.38344796 -0.38344796 -0.38344796 -0.38344796 -0.38344796 -0.38344796\n",
            " -0.38344796 -0.38344796 -0.38344796 -0.38344796 -0.38344796 -0.38344796\n",
            " -0.38344796 -0.38344796 -0.38344796 -0.38344796 -0.38344796 -0.38344796\n",
            " -0.38344796 -0.38344796 -0.38344796 -0.38344796 -0.38344796 -0.38344796\n",
            " -0.38344796 -0.38344796 -0.38344796 -0.38344796 -0.38344796 -0.38344796\n",
            " -0.38344796 -0.38344796 -0.38344796 -0.38344796 -0.38344796 -0.38344796\n",
            " -0.38344796 -0.38344796 -0.38344796 -0.38344796 -0.38344796 -0.38344796]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(-16.0079, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: training for one iteration takes 0.003265 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 7\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [51]\n",
            "DEBUGGING: the action_prob is: tensor([0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [37]\n",
            "DEBUGGING: the action_prob is: tensor([0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [58]\n",
            "DEBUGGING: the action_prob is: tensor([0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [86]\n",
            "DEBUGGING: the action_prob is: tensor([0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [7]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.8867413289854085 and immediate rewards look like: [0.013212290489263978, 0.04726740458590939, 0.00032137894959305413, 0.007063641655804531, 0.027337688202351274, 0.001803032350835565, 0.019164806106800825, 0.001545344468922849, 0.0027050883081756183, 0.0014769807007724012, 0.00010835584180313163, 0.0012425061549947714, 0.0041367571693626815, 8.694352482052636e-05, 0.0009103591492021224, 0.7583587513140628, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 0.0, 9.094947017729282e-13, 9.094947017729282e-13, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 0.0, 9.094947017729282e-13, 1.8189894035458565e-12, 0.0, 9.094947017729282e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 0.0, 9.094947017729282e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 4.547473508864641e-13]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [30]\n",
            "DEBUGGING: the action_prob is: tensor([0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [23]\n",
            "DEBUGGING: the action_prob is: tensor([0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [65]\n",
            "DEBUGGING: the action_prob is: tensor([0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [22]\n",
            "DEBUGGING: the action_prob is: tensor([0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [8]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.1338862261450231 and immediate rewards look like: [0.0050582781823322875, 0.045842889800951525, 0.0002440379944346205, 0.001681902224845544, 0.004729656118342973, 0.017227602214461513, 2.3202002921607345e-05, 0.0073784177252491645, 0.0022726126503584965, 0.013708251473872224, 0.0001593982933627558, 0.0008252080760939862, 3.7288014482328435e-05, 0.00918738624022808, 0.0014748692756256787, 4.3391970848460915e-05, 0.003998042955117853, 1.3206129551690537e-05, 0.00016218066230067052, 0.00313189019652782, 1.4723580534337088e-05, 2.3184918973129243e-05, 0.0001127103182625433, 0.003273670060934819, 0.003334715090204554, 0.0007576719112876162, 0.004004916152553051, 3.1050105917529436e-05, 0.00023454333677364048, 0.0017832688126873109, 0.0006057771893210884, 9.141084456132376e-05, 4.109617839276325e-05, 0.00018903400314229657, 2.9546793484769296e-05, 0.001095487263683026, 8.829191165204975e-05, 1.9897393485734938e-05, 4.4302185415290296e-05, 4.70834675070364e-06, 0.00011311569687677547, 4.25587404606631e-06, 1.6706207588867983e-05, 8.458763659291435e-06, 0.00011450969259385602, 9.426165206605219e-05, 0.00013153972122381674, 0.00011376592010492459, 0.0003079397934016015, 1.954223534994526e-06]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [38]\n",
            "DEBUGGING: the action_prob is: tensor([0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152,\n",
            "        0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152,\n",
            "        0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152,\n",
            "        0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152,\n",
            "        0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152,\n",
            "        0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152,\n",
            "        0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152, 0.0152,\n",
            "        0.0152, 0.0152, 0.0152], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [16]\n",
            "DEBUGGING: the action_prob is: tensor([0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [58]\n",
            "DEBUGGING: the action_prob is: tensor([0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112,\n",
            "        0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [36]\n",
            "DEBUGGING: the action_prob is: tensor([0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [60]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.1166016103079528 and immediate rewards look like: [0.01469255895653987, 0.0003601927705858543, 0.006388721220901061, 0.05588330771752226, 0.0005808178470942948, 0.002355518635795306, 0.0007691364767197229, 0.0006773192644686787, 0.002532420240186184, 0.004034306370613194, 0.0008764013236941537, 0.003577967695036932, 0.0006410119772226608, 0.002827773766966857, 1.218494207932963e-05, 0.00029583905552499346, 0.00023464759442504146, 0.0010110818570865376, 3.9308492432610365e-05, 0.0011275107094661507, 0.0003713828668878705, 0.0013613313121823012, 0.00021414379898487823, 2.0954607862222474e-06, 0.000859257659612922, 0.005778918181931658, 2.6084218916366808e-05, 1.567095114296535e-05, 8.97437266758061e-05, 0.0009555977699164941, 0.002002130218443199, 0.00014322459946924937, 0.00018255396753374953, 0.0006764801260032982, 4.801147633770597e-05, 7.976067672643694e-05, 0.00013807941013510572, 1.3798868621961446e-05, 0.0005778318209195277, 0.00029792409850415424, 0.0010019093110713584, 0.0004551220245048171, 5.522827677850728e-05, 0.0007572791346319718, 0.0003347356873746321, 2.2139111479191342e-05, 0.0001102528140108916, 0.0003952504735025286, 0.00024451027684335713, 0.0004731350736619788]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 150 number of matrices\n",
            "DEBUGGING: ACT_MAT has 150 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.7764571514909673, 0.7709544050522256, 0.7309969701679961, 0.7380561527458617, 0.7383762738283406, 0.7182207935616054, 0.7236543042533029, 0.7116055536833354, 0.7172325345600127, 0.7217448952038759, 0.7275433479829327, 0.7347828203445753, 0.7409498123127076, 0.744255611255904, 0.7516855229606904, 0.7583587513247356, 1.0780559647939637e-11, 1.0430113431366841e-11, 1.0076127354020584e-11, 1.0177906418202609e-11, 1.0280713553740009e-11, 1.0384559145191928e-11, 1.0489453682012048e-11, 1.0136067001136954e-11, 1.0238451516299954e-11, 9.42318870154245e-12, 8.599690908858103e-12, 7.767874956651693e-12, 7.386997581581039e-12, 7.461613718768725e-12, 7.536983554311843e-12, 6.694433184382741e-12, 4.924690687714025e-12, 4.9744350380949754e-12, 4.1060003397192394e-12, 4.147475090625495e-12, 4.1893687784095905e-12, 4.2316856347571624e-12, 4.274429934098144e-12, 4.31760599403853e-12, 4.3612181757964946e-12, 3.945930126171748e-12, 3.526447247762913e-12, 3.1027271685620695e-12, 2.2153863300900416e-12, 2.2377639697879206e-12, 1.3416861293080728e-12, 8.958977559814229e-13, 9.049472282640636e-13, 4.547473508864641e-13], [0.1238939928555365, 0.120036075427479, 0.07494261174396714, 0.0754531047975076, 0.07451636623501218, 0.07049162638047395, 0.053802044612133774, 0.05432206324162845, 0.047417823753918474, 0.04560122333692927, 0.03221512309399702, 0.03237952000064067, 0.03187304234802695, 0.03215732760964103, 0.023201960979205, 0.02194655727634275, 0.02212440939948918, 0.018309461054920532, 0.01848106558118065, 0.01850392416048483, 0.015527307034300008, 0.01566927621592492, 0.015804132623183628, 0.01584992152012231, 0.012703284302209586, 0.009463201224247507, 0.008793463952484738, 0.004836916969627968, 0.004854410973444888, 0.004666532966334593, 0.0029123880339871535, 0.0023299099441071365, 0.002261110201561427, 0.0022424384072410744, 0.002074145862726038, 0.0020652515850921906, 0.000979559920615318, 0.0009002707161245134, 0.0008892659824634126, 0.0008534987848970932, 0.000857364078935747, 0.0007517660424838097, 0.0007550607761997409, 0.0007458126955665383, 0.0007448019514214615, 0.0006366588473006116, 0.0005478759547823833, 0.00042054165005915817, 0.00030987447470124605, 1.954223534994526e-06], [0.108080157917878, 0.09433090804175569, 0.09491991441532306, 0.08942544767113333, 0.03388094944809199, 0.033636496566664335, 0.03159694740491821, 0.031139202957776245, 0.030769579488189463, 0.02852238307879119, 0.024735431018361613, 0.024100029994613595, 0.020729355858158244, 0.02029125644538948, 0.017639881493356185, 0.017805754092198846, 0.017686782865327124, 0.017628419465557658, 0.016785189503506184, 0.01691503132431674, 0.0159469905200511, 0.015732937023397203, 0.014516773445671618, 0.01444710065321893, 0.014590914335790614, 0.013870360278967367, 0.008173173835389605, 0.008229383450983068, 0.008296679292767782, 0.00828983390514341, 0.007408319328512036, 0.0054607970808776135, 0.0053712853347559235, 0.005241142795173913, 0.004610770372899611, 0.00460884737026455, 0.004574835043977892, 0.004481571347315945, 0.0045129014936302865, 0.003974817851222989, 0.0037140340936553887, 0.0027395199824081116, 0.0023074726847508026, 0.002274994351487167, 0.001533045673591106, 0.0012104141274913878, 0.0012002777939517136, 0.0011010353332735576, 0.0007129139997687162, 0.0004731350736619788]]\n",
            "DEBUGGING: traj_returns = [0.7764571514909673, 0.1238939928555365, 0.108080157917878]\n",
            "DEBUGGING: actions = [[51], [34], [45], [16], [38], [6], [42], [6], [54], [16], [37], [24], [61], [26], [47], [0], [56], [5], [25], [49], [58], [46], [72], [23], [13], [33], [41], [3], [65], [78], [86], [28], [33], [52], [62], [78], [7], [0], [12], [52], [7], [4], [96], [70], [22], [98], [2], [82], [83], [73], [30], [17], [44], [11], [53], [20], [22], [35], [46], [64], [23], [71], [13], [72], [29], [30], [11], [29], [26], [26], [65], [39], [3], [47], [15], [70], [80], [39], [75], [37], [22], [40], [55], [52], [86], [10], [92], [26], [60], [37], [8], [75], [3], [89], [81], [77], [70], [61], [40], [58], [38], [52], [28], [39], [11], [39], [17], [57], [61], [13], [16], [7], [6], [43], [40], [65], [33], [26], [25], [67], [58], [62], [77], [80], [71], [43], [48], [26], [32], [17], [36], [68], [12], [20], [38], [6], [38], [14], [43], [86], [60], [78], [50], [52], [82], [70], [56], [31], [95], [74]]\n",
            "DEBUGGING: actions length = 150\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-0.1294, -0.2046, -0.0768,  ..., -0.2876,  0.1947, -0.1218],\n",
            "        [-0.1292, -0.2044, -0.0769,  ..., -0.2874,  0.1947, -0.1216],\n",
            "        [-0.1290, -0.2042, -0.0770,  ..., -0.2869,  0.1944, -0.1215],\n",
            "        ...,\n",
            "        [-0.1292, -0.2045, -0.0769,  ..., -0.2874,  0.1947, -0.1217],\n",
            "        [-0.1292, -0.2045, -0.0769,  ..., -0.2874,  0.1947, -0.1217],\n",
            "        [-0.1292, -0.2045, -0.0769,  ..., -0.2874,  0.1947, -0.1217]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-0.1293, -0.2046, -0.0769,  ..., -0.2875,  0.1947, -0.1217],\n",
            "        [-0.1292, -0.2045, -0.0769,  ..., -0.2874,  0.1947, -0.1217],\n",
            "        [-0.1292, -0.2045, -0.0769,  ..., -0.2874,  0.1947, -0.1217],\n",
            "        ...,\n",
            "        [-0.1292, -0.2045, -0.0769,  ..., -0.2874,  0.1947, -0.1217],\n",
            "        [-0.1292, -0.2045, -0.0769,  ..., -0.2874,  0.1947, -0.1217],\n",
            "        [-0.1292, -0.2045, -0.0769,  ..., -0.2874,  0.1947, -0.1217]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([0.2391, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390,\n",
            "        0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390,\n",
            "        0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390,\n",
            "        0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390,\n",
            "        0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390,\n",
            "        0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390,\n",
            "        0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390,\n",
            "        0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390,\n",
            "        0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390,\n",
            "        0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390,\n",
            "        0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390,\n",
            "        0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390, 0.2390],\n",
            "       grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[3.36143767e-01 3.28440463e-01 3.00286499e-01 3.00978235e-01\n",
            "  2.82257863e-01 2.74116306e-01 2.69684432e-01 2.65688940e-01\n",
            "  2.65139979e-01 2.65289501e-01 2.61497967e-01 2.63754123e-01\n",
            "  2.64517404e-01 2.65568065e-01 2.64175788e-01 2.66037021e-01\n",
            "  1.32703974e-02 1.19792935e-02 1.17554184e-02 1.18063185e-02\n",
            "  1.04914325e-02 1.04674044e-02 1.01069687e-02 1.00990074e-02\n",
            "  9.09806622e-03 7.77785384e-03 5.65554593e-03 4.35543348e-03\n",
            "  4.38369676e-03 4.31878896e-03 3.44023579e-03 2.59690234e-03\n",
            "  2.54413185e-03 2.49452707e-03 2.22830541e-03 2.22469965e-03\n",
            "  1.85146499e-03 1.79394736e-03 1.80072249e-03 1.60943888e-03\n",
            "  1.52379939e-03 1.16376201e-03 1.02084449e-03 1.00693568e-03\n",
            "  7.59282542e-04 6.15690992e-04 5.82717917e-04 5.07192328e-04\n",
            "  3.40929492e-04 1.58363099e-04]]\n",
            "DEBUGGING: baseline2 looks like: 0.3361437674214606\n",
            "DEBUGGING: ADS looks like: [ 0.44031338  0.43481064  0.3948532   0.40191239  0.40223251  0.38207703\n",
            "  0.38751054  0.37546179  0.38108877  0.38560113  0.39139958  0.39863905\n",
            "  0.40480604  0.40811184  0.41554176  0.42221498 -0.33614377 -0.33614377\n",
            " -0.33614377 -0.33614377 -0.33614377 -0.33614377 -0.33614377 -0.33614377\n",
            " -0.33614377 -0.33614377 -0.33614377 -0.33614377 -0.33614377 -0.33614377\n",
            " -0.33614377 -0.33614377 -0.33614377 -0.33614377 -0.33614377 -0.33614377\n",
            " -0.33614377 -0.33614377 -0.33614377 -0.33614377 -0.33614377 -0.33614377\n",
            " -0.33614377 -0.33614377 -0.33614377 -0.33614377 -0.33614377 -0.33614377\n",
            " -0.33614377 -0.33614377 -0.21224977 -0.21610769 -0.26120116 -0.26069066\n",
            " -0.2616274  -0.26565214 -0.28234172 -0.2818217  -0.28872594 -0.29054254\n",
            " -0.30392864 -0.30376425 -0.30427073 -0.30398644 -0.31294181 -0.31419721\n",
            " -0.31401936 -0.31783431 -0.3176627  -0.31763984 -0.32061646 -0.32047449\n",
            " -0.32033963 -0.32029385 -0.32344048 -0.32668057 -0.3273503  -0.33130685\n",
            " -0.33128936 -0.33147723 -0.33323138 -0.33381386 -0.33388266 -0.33390133\n",
            " -0.33406962 -0.33407852 -0.33516421 -0.3352435  -0.3352545  -0.33529027\n",
            " -0.3352864  -0.335392   -0.33538871 -0.33539795 -0.33539897 -0.33550711\n",
            " -0.33559589 -0.33572323 -0.33583389 -0.33614181 -0.22806361 -0.24181286\n",
            " -0.24122385 -0.24671832 -0.30226282 -0.30250727 -0.30454682 -0.30500456\n",
            " -0.30537419 -0.30762138 -0.31140834 -0.31204374 -0.31541441 -0.31585251\n",
            " -0.31850389 -0.31833801 -0.31845698 -0.31851535 -0.31935858 -0.31922874\n",
            " -0.32019678 -0.32041083 -0.32162699 -0.32169667 -0.32155285 -0.32227341\n",
            " -0.32797059 -0.32791438 -0.32784709 -0.32785393 -0.32873545 -0.33068297\n",
            " -0.33077248 -0.33090262 -0.331533   -0.33153492 -0.33156893 -0.3316622\n",
            " -0.33163087 -0.33216895 -0.33242973 -0.33340425 -0.33383629 -0.33386877\n",
            " -0.33461072 -0.33493335 -0.33494349 -0.33504273 -0.33543085 -0.33567063]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(-10.8516, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: training for one iteration takes 0.003063 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 8\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [10]\n",
            "DEBUGGING: the action_prob is: tensor([0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141, 0.0141],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [21]\n",
            "DEBUGGING: the action_prob is: tensor([0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [10]\n",
            "DEBUGGING: the action_prob is: tensor([0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [47]\n",
            "DEBUGGING: the action_prob is: tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [26]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.1308188924372189 and immediate rewards look like: [0.06463332901012109, 0.003588245790979272, 0.015970173320511094, 0.0026518009904066275, 0.007611073330281215, 0.001646585231355857, 0.004214973759189888, 0.004158644773724518, 0.005714372874990659, 0.0006806491064708098, 0.00433416521491381, 0.00023435233288182644, 0.0011036309756491391, 9.093530252357596e-05, 0.00019176738351234235, 0.00017749175003700657, 0.0006179902452458919, 8.486075148539385e-05, 0.0013036627851761295, 0.0008646586220493191, 3.943639421777334e-05, 0.0009218166906066472, 0.00013509242262443877, 0.0013217045006967965, 2.7574136311159236e-05, 0.0010148140677301853, 0.001137156015374785, 0.0005881440210941946, 7.347576774918707e-05, 0.00030832256879875786, 0.00011569932712518494, 0.0001058888333318464, 7.124808553271578e-05, 0.00029683366210520035, 1.3512107670976548e-05, 0.00043555574848141987, 0.0005284714725348749, 7.889661810622783e-05, 0.00016777261271272437, 0.0005970847191747453, 5.073874581285054e-05, 0.0009454103342250164, 8.33328081171203e-05, 0.0005917363318985736, 0.000622169222424418, 0.00010503670046091429, 2.6449215056345565e-05, 0.00011038101820304291, 0.0004292823577998206, 2.4923797354858834e-06]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [28]\n",
            "DEBUGGING: the action_prob is: tensor([0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0145, 0.0145],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [18]\n",
            "DEBUGGING: the action_prob is: tensor([0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [75]\n",
            "DEBUGGING: the action_prob is: tensor([0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [32]\n",
            "DEBUGGING: the action_prob is: tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n",
            "        0.0100], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [69]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.18915722488236497 and immediate rewards look like: [0.005821298033424682, 0.014369683422046364, 0.0028823351508435735, 0.015361686832420673, 0.033173514092595724, 0.001537582014407235, 0.006460807707753702, 0.007290329319857847, 0.027069357996879262, 0.0033319754506919708, 0.003244283880576404, 0.006391016496309021, 0.02672454666071644, 0.003378214231815946, 0.002561080392297299, 0.0021175248439249117, 0.0003870528166771692, 2.803089000735781e-05, 0.0018880439388340164, 0.0023225570898830483, 0.0009552472542964097, 0.0009897673971863696, 0.0007693876550547429, 0.013508499430372467, 0.0015164656301749346, 0.00011016638063665596, 1.891555712063564e-06, 6.672594281553756e-07, 0.0006855079873275827, 0.00037381832635219325, 0.00011233259601794998, 0.0007078413782437565, 0.0012942759781253699, 7.885613695179927e-05, 4.7970269861252746e-05, 2.389264864177676e-05, 1.1284150787105318e-05, 1.998910192924086e-06, 4.9290270453639096e-05, 5.211793541093357e-05, 4.55470858469198e-05, 5.279218385112472e-06, 0.00010936457101706765, 0.000862164614318317, 7.146030657168012e-05, 0.00018785781912811217, 5.361008970794501e-05, 8.130459400490508e-05, 0.00010800138261402026, 4.3478758016135544e-07]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [43]\n",
            "DEBUGGING: the action_prob is: tensor([0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0147,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [21]\n",
            "DEBUGGING: the action_prob is: tensor([0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
            "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [11]\n",
            "DEBUGGING: the action_prob is: tensor([0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0119, 0.0119], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [67]\n",
            "DEBUGGING: the action_prob is: tensor([0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [19]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.8867413289840442 and immediate rewards look like: [0.006313351470453199, 0.0024895147112147242, 0.016221851917180174, 0.008386325745505019, 0.0010185419810113672, 0.011715361663391377, 0.024625314480545057, 0.016820369017750636, 0.006063707103294291, 8.745205877858098e-05, 0.004396127514155523, 0.003517427448059607, 0.003711575982379145, 0.007689881941132626, 0.0007224399846563756, 0.0007911141656222753, 0.0012140400372118165, 4.744971920445096e-05, 0.00013794914502796018, 0.7707715328861013, 0.0, 4.547473508864641e-13, 0.0, 4.547473508864641e-13, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 0.0, 0.0, 4.547473508864641e-13, 4.547473508864641e-13, 9.094947017729282e-13, 9.094947017729282e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 4.547473508864641e-13, 0.0, 4.547473508864641e-13]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 150 number of matrices\n",
            "DEBUGGING: ACT_MAT has 150 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.12489256825069403, 0.06086791842482116, 0.05785825518569888, 0.04231119380321998, 0.04005999274021551, 0.03277668627266091, 0.031444546506368735, 0.02750461893654429, 0.023581792083656336, 0.01804789819057139, 0.01754267584252584, 0.01334192992688084, 0.013239977367675772, 0.012258935749521851, 0.0122909095424225, 0.012221355716070867, 0.012165519157609961, 0.011664170618549565, 0.011696272592994113, 0.01049758566446261, 0.009730229335771001, 0.009788679738942655, 0.008956427321551523, 0.00891043929184554, 0.007665388677928024, 0.007714964183451379, 0.006767828399718377, 0.005687547862973326, 0.005150912971595082, 0.005128724448329187, 0.004869092807606494, 0.004801407556041726, 0.004742948204757454, 0.0047188890093179175, 0.004466722572942138, 0.0044981923891627895, 0.004103673374425626, 0.003611315052414901, 0.0035680994285946194, 0.0034346735513958535, 0.002866251345677887, 0.0028439521210757946, 0.0019177189766169476, 0.0018529153217169973, 0.0012739181715337613, 0.0006583322718276195, 0.0005588844155219244, 0.000537813333803615, 0.00043174981373795163, 2.4923797354858834e-06], [0.17291190714459997, 0.1687783930415912, 0.15596839355509579, 0.1546323822265174, 0.1406774700950472, 0.10858985454793077, 0.10813360861972074, 0.1026997989009768, 0.09637320159708986, 0.07000388242445514, 0.06734536057955876, 0.0647485623222044, 0.05894701598575291, 0.03254794881316815, 0.029464378365002233, 0.027175048457277712, 0.025310629912477577, 0.02517533039979839, 0.02540131263615256, 0.023750776461937923, 0.021644666032378662, 0.02089840280614369, 0.020109732736320527, 0.019535702102288673, 0.0060880835069860645, 0.004617795835162758, 0.0045531610651778805, 0.0045972419287533504, 0.0046430047164900965, 0.003997471443598499, 0.0036602556739861675, 0.003583760684816381, 0.002904968996538005, 0.0016269626448612475, 0.001563743947383281, 0.0015310845227495236, 0.0015224160344522696, 0.0015263958420860244, 0.0015397948807001013, 0.001505560212370164, 0.0014681235120800307, 0.0014369458850839504, 0.0014461279461604423, 0.0013502660354983583, 0.0004930317385656983, 0.000425829729286887, 0.00024037566682704527, 0.00018865209810010127, 0.00010843182231838, 4.3478758016135544e-07], [0.7459180052257842, 0.7470754078336677, 0.7521069627499525, 0.7433182937704771, 0.7423555232575475, 0.7488252336126627, 0.7445554262113853, 0.7272021330614548, 0.717557337417883, 0.7186804346612007, 0.7258514975782041, 0.7287427980444935, 0.7325508793903373, 0.736201316573695, 0.7358701359924873, 0.7425734303109404, 0.7492750668134526, 0.7556171987638796, 0.7632017667117931, 0.7707715328957223, 9.718185020956361e-12, 9.816348506016526e-12, 9.456162782959658e-12, 9.55167957874713e-12, 9.188820432182491e-12, 9.281636800184334e-12, 9.375390707256903e-12, 9.010750865020645e-12, 8.64242779205473e-12, 8.270384284008348e-12, 7.894582760729176e-12, 7.514985262467386e-12, 6.6722126875701595e-12, 6.280268016852218e-12, 5.884364309056317e-12, 5.484461573908943e-12, 5.080519417194423e-12, 4.672497036674706e-12, 4.719693976439097e-12, 4.767367652968784e-12, 4.815522881786651e-12, 4.404823768586047e-12, 3.9899761794945284e-12, 3.111597452244041e-12, 2.2243462125970836e-12, 1.7874735976874945e-12, 1.3461881280818489e-12, 9.004452294902876e-13, 4.5019987737759947e-13, 4.547473508864641e-13]]\n",
            "DEBUGGING: traj_returns = [0.12489256825069403, 0.17291190714459997, 0.7459180052257842]\n",
            "DEBUGGING: actions = [[10], [37], [40], [45], [29], [31], [21], [37], [45], [32], [21], [26], [41], [21], [16], [48], [38], [24], [41], [25], [10], [5], [10], [71], [6], [48], [4], [65], [26], [59], [47], [73], [88], [12], [51], [52], [40], [41], [71], [7], [26], [29], [37], [58], [103], [26], [32], [51], [91], [37], [28], [40], [20], [16], [10], [15], [23], [8], [7], [56], [18], [28], [66], [4], [59], [39], [16], [32], [43], [5], [75], [27], [77], [40], [77], [68], [60], [25], [10], [35], [32], [31], [48], [35], [49], [79], [89], [45], [82], [58], [69], [16], [46], [16], [11], [94], [39], [76], [87], [20], [43], [40], [20], [13], [63], [18], [4], [23], [19], [35], [21], [61], [16], [41], [7], [35], [18], [23], [76], [0], [11], [72], [64], [28], [44], [62], [31], [14], [46], [68], [67], [31], [11], [69], [37], [8], [76], [60], [5], [43], [19], [51], [42], [20], [96], [49], [60], [62], [48], [74]]\n",
            "DEBUGGING: actions length = 150\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-0.1497, -0.2353, -0.0708,  ..., -0.3231,  0.2123, -0.1278],\n",
            "        [-0.1495, -0.2351, -0.0708,  ..., -0.3227,  0.2122, -0.1277],\n",
            "        [-0.1492, -0.2348, -0.0710,  ..., -0.3224,  0.2121, -0.1277],\n",
            "        ...,\n",
            "        [-0.1496, -0.2352, -0.0708,  ..., -0.3229,  0.2123, -0.1278],\n",
            "        [-0.1496, -0.2352, -0.0708,  ..., -0.3229,  0.2123, -0.1278],\n",
            "        [-0.1496, -0.2352, -0.0708,  ..., -0.3229,  0.2123, -0.1278]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-0.1496, -0.2352, -0.0708,  ..., -0.3229,  0.2123, -0.1278],\n",
            "        [-0.1496, -0.2352, -0.0708,  ..., -0.3229,  0.2123, -0.1278],\n",
            "        [-0.1496, -0.2352, -0.0708,  ..., -0.3229,  0.2123, -0.1278],\n",
            "        ...,\n",
            "        [-0.1496, -0.2352, -0.0708,  ..., -0.3229,  0.2123, -0.1278],\n",
            "        [-0.1496, -0.2352, -0.0708,  ..., -0.3229,  0.2123, -0.1278],\n",
            "        [-0.1496, -0.2352, -0.0708,  ..., -0.3229,  0.2123, -0.1278]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972,\n",
            "        0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972,\n",
            "        0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972,\n",
            "        0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972,\n",
            "        0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972,\n",
            "        0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972,\n",
            "        0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972,\n",
            "        0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972,\n",
            "        0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2971,\n",
            "        0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972,\n",
            "        0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972, 0.2972,\n",
            "        0.2972, 0.2972, 0.2972, 0.2972], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[3.47907494e-01 3.25573906e-01 3.21977870e-01 3.13420623e-01\n",
            "  3.07697662e-01 2.96730591e-01 2.94711194e-01 2.85802184e-01\n",
            "  2.79170777e-01 2.68910738e-01 2.70246511e-01 2.68944430e-01\n",
            "  2.68245958e-01 2.60336067e-01 2.59208475e-01 2.60656611e-01\n",
            "  2.62250405e-01 2.64152233e-01 2.66766451e-01 2.68339965e-01\n",
            "  1.04582985e-02 1.02290275e-02 9.68872002e-03 9.48204713e-03\n",
            "  4.58449073e-03 4.11092001e-03 3.77366316e-03 3.42826327e-03\n",
            "  3.26463923e-03 3.04206530e-03 2.84311616e-03 2.79505608e-03\n",
            "  2.54930574e-03 2.11528389e-03 2.01015551e-03 2.00975897e-03\n",
            "  1.87536314e-03 1.71257030e-03 1.70263144e-03 1.64674459e-03\n",
            "  1.44479162e-03 1.42696600e-03 1.12128231e-03 1.06772712e-03\n",
            "  5.88983304e-04 3.61387334e-04 2.66420028e-04 2.42155144e-04\n",
            "  1.80060546e-04 9.75722590e-07]]\n",
            "DEBUGGING: baseline2 looks like: 0.34790749354035944\n",
            "DEBUGGING: ADS looks like: [-0.22301493 -0.28703958 -0.29004924 -0.3055963  -0.3078475  -0.31513081\n",
            " -0.31646295 -0.32040287 -0.3243257  -0.3298596  -0.33036482 -0.33456556\n",
            " -0.33466752 -0.33564856 -0.33561658 -0.33568614 -0.33574197 -0.33624332\n",
            " -0.33621122 -0.33740991 -0.33817726 -0.33811881 -0.33895107 -0.33899705\n",
            " -0.3402421  -0.34019253 -0.34113967 -0.34221995 -0.34275658 -0.34277877\n",
            " -0.3430384  -0.34310609 -0.34316455 -0.3431886  -0.34344077 -0.3434093\n",
            " -0.34380382 -0.34429618 -0.34433939 -0.34447282 -0.34504124 -0.34506354\n",
            " -0.34598977 -0.34605458 -0.34663358 -0.34724916 -0.34734861 -0.34736968\n",
            " -0.34747574 -0.347905   -0.17499559 -0.1791291  -0.1919391  -0.19327511\n",
            " -0.20723002 -0.23931764 -0.23977388 -0.24520769 -0.25153429 -0.27790361\n",
            " -0.28056213 -0.28315893 -0.28896048 -0.31535954 -0.31844312 -0.32073245\n",
            " -0.32259686 -0.32273216 -0.32250618 -0.32415672 -0.32626283 -0.32700909\n",
            " -0.32779776 -0.32837179 -0.34181941 -0.3432897  -0.34335433 -0.34331025\n",
            " -0.34326449 -0.34391002 -0.34424724 -0.34432373 -0.34500252 -0.34628053\n",
            " -0.34634375 -0.34637641 -0.34638508 -0.3463811  -0.3463677  -0.34640193\n",
            " -0.34643937 -0.34647055 -0.34646137 -0.34655723 -0.34741446 -0.34748166\n",
            " -0.34766712 -0.34771884 -0.34779906 -0.34790706  0.39801051  0.39916791\n",
            "  0.40419947  0.3954108   0.39444803  0.40091774  0.39664793  0.37929464\n",
            "  0.36964984  0.37077294  0.377944    0.3808353   0.38464339  0.38829382\n",
            "  0.38796264  0.39466594  0.40136757  0.40770971  0.41529427  0.42286404\n",
            " -0.34790749 -0.34790749 -0.34790749 -0.34790749 -0.34790749 -0.34790749\n",
            " -0.34790749 -0.34790749 -0.34790749 -0.34790749 -0.34790749 -0.34790749\n",
            " -0.34790749 -0.34790749 -0.34790749 -0.34790749 -0.34790749 -0.34790749\n",
            " -0.34790749 -0.34790749 -0.34790749 -0.34790749 -0.34790749 -0.34790749\n",
            " -0.34790749 -0.34790749 -0.34790749 -0.34790749 -0.34790749 -0.34790749]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(-10.3825, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: training for one iteration takes 0.003067 min:\n",
            "==========================================================================================================\n",
            "Outer iteration no 9\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 0\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [36]\n",
            "DEBUGGING: the action_prob is: tensor([0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [8]\n",
            "DEBUGGING: the action_prob is: tensor([0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [74]\n",
            "DEBUGGING: the action_prob is: tensor([0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [63]\n",
            "DEBUGGING: the action_prob is: tensor([0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099,\n",
            "        0.0099, 0.0099], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [35]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.12959968762561402 and immediate rewards look like: [0.003548779389802803, 0.016495226768256543, 0.05127050408236755, 0.006075236154629238, 0.0006789818648940127, 0.0009522006566839991, 0.010854149567876448, 0.000980563761004305, 0.00036003925515615265, 0.0005304035639710492, 0.0019246386386839731, 0.010555909835147759, 0.0006760541382391239, 0.00032139576842382667, 0.001479757499510015, 0.0012626181191990327, 0.003215232282400393, 0.003945807691707159, 0.003941147804198408, 0.00047758088066984783, 4.966331016476033e-05, 0.00015243364896377898, 0.0004730975456368469, 0.000910718917111808, 0.0014572075183423294, 0.0007346723200498673, 0.0009685440463726991, 0.0001656503359299677, 0.0002307272893631307, 0.000549056870113418, 3.019063342435402e-05, 7.446888366757776e-05, 1.727816925267689e-05, 3.1177387427305803e-07, 5.170934991838294e-05, 0.0003177083754053456, 1.6488315850438084e-05, 1.8279303731105756e-05, 0.00026280390738975257, 0.00019025502342628897, 1.9351088212715695e-05, 0.0004620779309334466, 0.00034910549629785237, 0.001787476634945051, 7.139491071939119e-05, 0.00022096233033153112, 1.675548310231534e-05, 0.0004487804221753322, 5.631755357171642e-06, 6.583127287740353e-07]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 1\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [16]\n",
            "DEBUGGING: the action_prob is: tensor([0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149,\n",
            "        0.0149, 0.0149, 0.0149, 0.0149], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [56]\n",
            "DEBUGGING: the action_prob is: tensor([0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130,\n",
            "        0.0130, 0.0130, 0.0130, 0.0130, 0.0130], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [28]\n",
            "DEBUGGING: the action_prob is: tensor([0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [38]\n",
            "DEBUGGING: the action_prob is: tensor([0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101, 0.0101],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [61]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.21940445054815427 and immediate rewards look like: [0.021653127912486525, 0.0025029993162206665, 0.017005671455535776, 0.15453049591042145, 0.0022638088853454974, 0.00357382307493026, 7.546442475359072e-05, 0.0007141262117329461, 0.0002641637674969388, 6.125315712779411e-05, 0.0031507109270023648, 0.000638726346551266, 9.446884860153659e-05, 0.0023599765422659402, 1.4216034742275951e-05, 3.058974061787012e-05, 0.0001758402781888435, 0.00047303820656452444, 0.0013015236991122947, 0.00349072429116859, 2.0171114556433167e-05, 0.00040319045592696057, 6.6040629462804645e-06, 0.0006716712555316917, 0.00025414370338694425, 9.520172443444608e-05, 0.0005008756511415413, 7.929328330646968e-05, 9.641305086915963e-05, 0.000944532199355308, 0.00018894033200922422, 0.00014714266171722556, 0.0003184719857927121, 2.9352881938393693e-06, 1.2689520644926233e-05, 3.7279369280440733e-07, 8.440119927399792e-07, 5.609968866338022e-06, 1.6644607967464253e-06, 1.9782723938988056e-06, 0.00011385040488676168, 4.7606005864508916e-05, 0.0004634777742467122, 3.466319003564422e-05, 0.00011352674482623115, 2.7822990887216292e-05, 0.00010525614743528422, 4.2048814066220075e-05, 0.00017104980724980123, 0.00015765384023325169]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Running trajectories: 2\n",
            "DEBUGGING: the action_prob is: tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
            "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [39]\n",
            "DEBUGGING: the action_prob is: tensor([0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [23]\n",
            "DEBUGGING: the action_prob is: tensor([0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0128],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [72]\n",
            "DEBUGGING: the action_prob is: tensor([0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110, 0.0110,\n",
            "        0.0110], grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [3]\n",
            "DEBUGGING: the action_prob is: tensor([0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "DEBUGGING: the actual action to take is: [9]\n",
            "-------------------------------------------------------------------------------------------\n",
            "DEBUGGING: the total reward of the trajectory = 0.14466785802233062 and immediate rewards look like: [0.009627757028283668, 0.006286869567247777, 0.006085816833547142, 0.005634319026739831, 0.017333954397145135, 0.03724115157001506, 0.0021953847144686733, 0.01878023399012818, 0.0016190678097700584, 0.00025025303420989076, 0.012432171448836016, 0.002495051213827537, 0.007659532260731794, 0.0005866477940799086, 0.0018134243159693142, 0.0020933799551130505, 0.00034199362471554196, 0.00124346178927226, 0.002893940605190437, 1.188513488159515e-05, 0.00010600317500575329, 0.00080952748294294, 9.591142406861763e-05, 0.00042525274329818785, 1.173335112980567e-05, 0.00011535905150594772, 1.6597353805991588e-05, 0.00026765193115352304, 0.00017242234525838285, 0.00010526974392632837, 0.0005026548806199571, 0.0002262412172058248, 0.0024855606438904942, 0.0001728817296680063, 0.0005625128906103782, 0.00016506844303876278, 0.0006939459685781912, 3.1250074698618846e-05, 6.332060638669645e-05, 4.124794213566929e-05, 6.349144086925662e-05, 0.00045514310249927803, 9.05492261153995e-05, 8.042461422519409e-05, 3.384751380508533e-05, 2.1823171209689463e-05, 0.00011594485795285436, 7.431611766151036e-05, 3.520168047543848e-05, 4.0718441596254706e-07]\n",
            "+++++++++++++++++++ The policy roll-out has finished! ++++++++++++++++++++++++++++++++\n",
            "DEBUGGING: OBS_MAT has 150 number of matrices\n",
            "DEBUGGING: ACT_MAT has 150 number of matrices\n",
            "DEBUGGING: VAL looks like: [[0.12103276825151008, 0.11867069581990634, 0.10320754449661595, 0.052461656984089305, 0.04685497053480815, 0.04664241279789307, 0.04615172943556472, 0.035654121078473, 0.0350237952701704, 0.03501389496466086, 0.03483180949564627, 0.03323956652218414, 0.022912784532359984, 0.022461343832445312, 0.022363583903052008, 0.02109477414499191, 0.020032480834134222, 0.01698711974922609, 0.013173042482342353, 0.009325146139539338, 0.008936934604918676, 0.008977041711872642, 0.008913745518089761, 0.008525907042881732, 0.007692109217949417, 0.006297880504653624, 0.005619402206670462, 0.004697836525553296, 0.004577965848104372, 0.0043911500593345875, 0.003880902211334515, 0.003889607654454708, 0.0038536755260476065, 0.003875148845247404, 0.003913976839770839, 0.0039012802927802593, 0.0036197696135100137, 0.003639678078444016, 0.003657978560316071, 0.003429469346390221, 0.003271933659559527, 0.003285436940754355, 0.0028518777876978874, 0.002528052819595995, 0.0007480567521726707, 0.0006834968095487672, 0.0004672065446638749, 0.0004550010722844036, 6.283484958657937e-06, 6.583127287740353e-07], [0.2111775291261539, 0.1914387891047145, 0.19084423210958976, 0.17559450571116564, 0.021276777576509268, 0.019205018879963405, 0.015789086671750652, 0.015872345704037435, 0.015311332820509588, 0.015199160659608737, 0.01529081565907166, 0.012262732052595248, 0.01174141990509493, 0.011764597026761005, 0.009499616651005115, 0.009581212743699838, 0.009647093942507038, 0.009566922893250701, 0.009185742107763815, 0.00796385697843588, 0.004518315845724534, 0.004543580536533435, 0.004182212202632802, 0.004217785999683355, 0.00358193408500168, 0.0033614044258734703, 0.003299194647918206, 0.0028265848452289546, 0.0027750419817398836, 0.002705685788758307, 0.001778943019598989, 0.0016060633207977421, 0.0014736572313944613, 0.0011668537834361103, 0.0011756752477194655, 0.0011747330576510498, 0.0011862224888467125, 0.0011973519968221944, 0.0012037798262180368, 0.0012142579448699902, 0.0012245249216930217, 0.0011218934513194546, 0.0010851388337928744, 0.0006279404641880425, 0.0005992699738913114, 0.0004906497263283639, 0.0004675017529708562, 0.0003659046520561333, 0.0003271271090807204, 0.00015765384023325169], [0.1344345911183759, 0.12606750918191134, 0.12099054506531673, 0.11606538205229251, 0.11154652830863908, 0.0951642160722161, 0.05850814596181923, 0.05688157701752582, 0.03848620507817943, 0.0372395325943529, 0.0373629086466091, 0.02518256282603342, 0.02291667839616756, 0.015411258722662392, 0.01497435447331564, 0.01329386884580437, 0.011313625142112444, 0.011082456078178688, 0.009938378069602453, 0.007115593398395975, 0.0071754628924387675, 0.007140868401447489, 0.006395293857075302, 0.006363012558592611, 0.005997737187166084, 0.0060464685212487665, 0.005991019666406887, 0.006034770012728178, 0.005825371799570359, 0.005710049953850481, 0.005661394151438538, 0.005210847748301597, 0.0050349560920159315, 0.002575146917298422, 0.0024265304925559757, 0.0018828460625713108, 0.0017351289086187354, 0.001051699939434893, 0.0010307574391275496, 0.0009772089219604577, 0.0009454151311361499, 0.0008908320103705993, 0.0004400898059306275, 0.00035307129274265455, 0.00027540068537117217, 0.0002439931025920069, 0.00022441407210335094, 0.00010956486277827936, 3.56047930472414e-05, 4.0718441596254706e-07]]\n",
            "DEBUGGING: traj_returns = [0.12103276825151008, 0.2111775291261539, 0.1344345911183759]\n",
            "DEBUGGING: actions = [[36], [20], [45], [19], [39], [25], [26], [48], [23], [68], [8], [5], [17], [62], [41], [31], [69], [74], [41], [61], [74], [80], [27], [17], [9], [5], [3], [12], [73], [25], [63], [78], [46], [73], [67], [60], [8], [9], [59], [3], [35], [60], [99], [41], [84], [37], [83], [58], [104], [56], [16], [61], [28], [11], [29], [16], [11], [36], [40], [15], [56], [46], [15], [26], [24], [7], [21], [13], [51], [51], [28], [75], [57], [5], [54], [38], [21], [69], [24], [15], [38], [50], [40], [20], [18], [87], [5], [25], [92], [42], [61], [48], [92], [58], [101], [73], [28], [2], [89], [79], [39], [5], [51], [50], [3], [8], [53], [4], [41], [6], [23], [30], [50], [68], [15], [5], [54], [30], [16], [52], [72], [3], [52], [39], [70], [2], [28], [63], [7], [60], [3], [50], [78], [91], [36], [48], [30], [44], [77], [57], [9], [41], [7], [30], [65], [52], [26], [50], [25], [57]]\n",
            "DEBUGGING: actions length = 150\n",
            "DEBUGGING: what does the model output in this round of roll-out?\n",
            "DEBUGGING: obs_attention looks like: tensor([[-0.1778, -0.2688, -0.0682,  ..., -0.3625,  0.2301, -0.1377],\n",
            "        [-0.1775, -0.2685, -0.0681,  ..., -0.3621,  0.2300, -0.1375],\n",
            "        [-0.1772, -0.2681, -0.0683,  ..., -0.3616,  0.2298, -0.1374],\n",
            "        ...,\n",
            "        [-0.1776, -0.2686, -0.0682,  ..., -0.3622,  0.2300, -0.1376],\n",
            "        [-0.1776, -0.2686, -0.0682,  ..., -0.3622,  0.2300, -0.1376],\n",
            "        [-0.1776, -0.2686, -0.0682,  ..., -0.3622,  0.2300, -0.1376]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: act_attention looks like: tensor([[-0.1778, -0.2688, -0.0682,  ..., -0.3625,  0.2301, -0.1376],\n",
            "        [-0.1776, -0.2686, -0.0682,  ..., -0.3622,  0.2300, -0.1376],\n",
            "        [-0.1776, -0.2686, -0.0682,  ..., -0.3622,  0.2300, -0.1376],\n",
            "        ...,\n",
            "        [-0.1776, -0.2686, -0.0682,  ..., -0.3622,  0.2300, -0.1376],\n",
            "        [-0.1776, -0.2686, -0.0682,  ..., -0.3622,  0.2300, -0.1376],\n",
            "        [-0.1776, -0.2686, -0.0682,  ..., -0.3622,  0.2300, -0.1376]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "DEBUGGING: logits looks like: tensor([0.3738, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736,\n",
            "        0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736,\n",
            "        0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736,\n",
            "        0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736,\n",
            "        0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736,\n",
            "        0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736,\n",
            "        0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736,\n",
            "        0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736,\n",
            "        0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736,\n",
            "        0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736,\n",
            "        0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736,\n",
            "        0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736,\n",
            "        0.3736], grad_fn=<MeanBackward1>)\n",
            "DEBUGGING: baseline2 looks like: [[1.55548296e-01 1.45392331e-01 1.38347441e-01 1.14707182e-01\n",
            "  5.98927588e-02 5.36705493e-02 4.01496540e-02 3.61360146e-02\n",
            "  2.96071111e-02 2.91508627e-02 2.91618446e-02 2.35616205e-02\n",
            "  1.91902943e-02 1.65457332e-02 1.56125183e-02 1.46566186e-02\n",
            "  1.36644000e-02 1.25454996e-02 1.07657209e-02 8.13486551e-03\n",
            "  6.87690445e-03 6.88716355e-03 6.49708386e-03 6.36890187e-03\n",
            "  5.75726016e-03 5.23525115e-03 4.96987217e-03 4.51973046e-03\n",
            "  4.39279321e-03 4.26896193e-03 3.77374646e-03 3.56883957e-03\n",
            "  3.45409628e-03 2.53904985e-03 2.50539419e-03 2.31961980e-03\n",
            "  2.18037367e-03 1.96291000e-03 1.96417194e-03 1.87364540e-03\n",
            "  1.81395790e-03 1.76605413e-03 1.45903548e-03 1.16968819e-03\n",
            "  5.40909137e-04 4.72713213e-04 3.86374123e-04 3.10156862e-04\n",
            "  1.23005129e-04 5.29064458e-05]]\n",
            "DEBUGGING: baseline2 looks like: 0.15554829616534663\n",
            "DEBUGGING: ADS looks like: [-0.03451553 -0.0368776  -0.05234075 -0.10308664 -0.10869333 -0.10890588\n",
            " -0.10939657 -0.11989418 -0.1205245  -0.1205344  -0.12071649 -0.12230873\n",
            " -0.13263551 -0.13308695 -0.13318471 -0.13445352 -0.13551582 -0.13856118\n",
            " -0.14237525 -0.14622315 -0.14661136 -0.14657125 -0.14663455 -0.14702239\n",
            " -0.14785619 -0.14925042 -0.14992889 -0.15085046 -0.15097033 -0.15115715\n",
            " -0.15166739 -0.15165869 -0.15169462 -0.15167315 -0.15163432 -0.15164702\n",
            " -0.15192853 -0.15190862 -0.15189032 -0.15211883 -0.15227636 -0.15226286\n",
            " -0.15269642 -0.15302024 -0.15480024 -0.1548648  -0.15508109 -0.1550933\n",
            " -0.15554201 -0.15554764  0.05562923  0.03589049  0.03529594  0.02004621\n",
            " -0.13427152 -0.13634328 -0.13975921 -0.13967595 -0.14023696 -0.14034914\n",
            " -0.14025748 -0.14328556 -0.14380688 -0.1437837  -0.14604868 -0.14596708\n",
            " -0.1459012  -0.14598137 -0.14636255 -0.14758444 -0.15102998 -0.15100472\n",
            " -0.15136608 -0.15133051 -0.15196636 -0.15218689 -0.1522491  -0.15272171\n",
            " -0.15277325 -0.15284261 -0.15376935 -0.15394223 -0.15407464 -0.15438144\n",
            " -0.15437262 -0.15437356 -0.15436207 -0.15435094 -0.15434452 -0.15433404\n",
            " -0.15432377 -0.1544264  -0.15446316 -0.15492036 -0.15494903 -0.15505765\n",
            " -0.15508079 -0.15518239 -0.15522117 -0.15539064 -0.02111371 -0.02948079\n",
            " -0.03455775 -0.03948291 -0.04400177 -0.06038408 -0.09704015 -0.09866672\n",
            " -0.11706209 -0.11830876 -0.11818539 -0.13036573 -0.13263162 -0.14013704\n",
            " -0.14057394 -0.14225443 -0.14423467 -0.14446584 -0.14560992 -0.1484327\n",
            " -0.14837283 -0.14840743 -0.149153   -0.14918528 -0.14955056 -0.14950183\n",
            " -0.14955728 -0.14951353 -0.14972292 -0.14983825 -0.1498869  -0.15033745\n",
            " -0.15051334 -0.15297315 -0.15312177 -0.15366545 -0.15381317 -0.1544966\n",
            " -0.15451754 -0.15457109 -0.15460288 -0.15465746 -0.15510821 -0.15519522\n",
            " -0.1552729  -0.1553043  -0.15532388 -0.15543873 -0.15551269 -0.15554789]\n",
            "DEBUGGING: I'm inside the training now!\n",
            "DEBUGGING: the loss = tensor(-5.9389, grad_fn=<NegBackward0>)\n",
            "DEBUGGING: training for one iteration takes 0.003109 min:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256,
          "referenced_widgets": [
            "67502f850aa0440d99224a3c7ae9bcba",
            "32b2a0d3060c4c0a9b85112f194d9c73",
            "de2f282e3e9f49d58aa4b6c135cbe29d",
            "712ba6cace53471ab81cfbd326eb9517",
            "f0da138971674feca76c536de92b2e8e",
            "7113ab569bcb4a058583df5abbd031b3",
            "79a46285cbd14a499e611b013668cefc",
            "22da20e3ebb040d59d0e4556e7c0dc1a"
          ]
        },
        "id": "Oe4fCU1V-kAs",
        "outputId": "3b5458eb-4173-48c5-a502-e94387aec73c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67502f850aa0440d99224a3c7ae9bcba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training reward (easy config)</td><td>â–ˆâ–â–ˆâ–â–â–â–ˆâ–â–ˆâ–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training reward (easy config)</td><td>0.14467</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">sandy-dawn-2077</strong>: <a href=\"https://wandb.ai/ieor4575-spring2022/finalproject/runs/3rwptiep\" target=\"_blank\">https://wandb.ai/ieor4575-spring2022/finalproject/runs/3rwptiep</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220507_205056-3rwptiep/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = [   54246.9141, 27220988.0000,  3948314.5000,  3902082.5000,\n",
        "        27219886.0000,  3816622.7500, 27175632.0000,  3976903.5000,\n",
        "         3977309.0000,  3961905.7500,  3904790.7500, 23420360.0000,\n",
        "        27218672.0000, 27206276.0000, 27190054.0000,  3976984.7500,\n",
        "         3931598.0000,  3963138.5000, 19419398.0000,  3931559.5000,\n",
        "         7862683.0000, 19299310.0000,  3915878.7500, 15562425.0000,\n",
        "        27141808.0000, 27323768.0000, 11622510.0000, 15451909.0000,\n",
        "        23389744.0000,  7781165.5000, 11871555.0000, 11657742.0000,\n",
        "        27473456.0000, 11592561.0000,  4054315.2500,  7718911.0000,\n",
        "        27136936.0000,  3914171.2500, 23255876.0000, 15599427.0000,\n",
        "        19661078.0000,  3915166.7500,  7554840.5000, 27196042.0000,\n",
        "        11791202.0000, 23477022.0000, 15708443.0000, 23297932.0000,\n",
        "        23511102.0000,  7689591.5000, 27405178.0000,  4021836.7500,\n",
        "        15558291.0000,  3948567.5000,  7915618.5000,  3864714.5000,\n",
        "         3995934.2500, 19536616.0000,  3890406.7500, 27284138.0000,\n",
        "        11731948.0000, 23232848.0000, 19200360.0000, 22949674.0000,\n",
        "        26724900.0000,  3268551.2500, 22647966.0000, 25925040.0000,\n",
        "        25380838.0000,  5669201.5000,  7810961.0000, 13092715.0000,\n",
        "        14364957.0000, 15622157.0000, 27473452.0000]"
      ],
      "metadata": {
        "id": "R5xFgG-ZGZkL"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_t = torch.FloatTensor(a)\n",
        "print(a_t)\n",
        "b_t = a_t /1000000\n",
        "print(b_t)\n",
        "print(torch.nn.functional.softmax(b_t, dim=0))\n",
        "b_t = a_t / a_t.max()\n",
        "print(b_t)\n",
        "torch.nn.functional.softmax(b_t, dim=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5GfEt5DGbcN",
        "outputId": "dc840de9-01b6-4211-8548-a0e697703d9d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([   54246.9141, 27220988.0000,  3948314.5000,  3902082.5000,\n",
            "        27219886.0000,  3816622.7500, 27175632.0000,  3976903.5000,\n",
            "         3977309.0000,  3961905.7500,  3904790.7500, 23420360.0000,\n",
            "        27218672.0000, 27206276.0000, 27190054.0000,  3976984.7500,\n",
            "         3931598.0000,  3963138.5000, 19419398.0000,  3931559.5000,\n",
            "         7862683.0000, 19299310.0000,  3915878.7500, 15562425.0000,\n",
            "        27141808.0000, 27323768.0000, 11622510.0000, 15451909.0000,\n",
            "        23389744.0000,  7781165.5000, 11871555.0000, 11657742.0000,\n",
            "        27473456.0000, 11592561.0000,  4054315.2500,  7718911.0000,\n",
            "        27136936.0000,  3914171.2500, 23255876.0000, 15599427.0000,\n",
            "        19661078.0000,  3915166.7500,  7554840.5000, 27196042.0000,\n",
            "        11791202.0000, 23477022.0000, 15708443.0000, 23297932.0000,\n",
            "        23511102.0000,  7689591.5000, 27405178.0000,  4021836.7500,\n",
            "        15558291.0000,  3948567.5000,  7915618.5000,  3864714.5000,\n",
            "         3995934.2500, 19536616.0000,  3890406.7500, 27284138.0000,\n",
            "        11731948.0000, 23232848.0000, 19200360.0000, 22949674.0000,\n",
            "        26724900.0000,  3268551.2500, 22647966.0000, 25925040.0000,\n",
            "        25380838.0000,  5669201.5000,  7810961.0000, 13092715.0000,\n",
            "        14364957.0000, 15622157.0000, 27473452.0000])\n",
            "tensor([ 0.0542, 27.2210,  3.9483,  3.9021, 27.2199,  3.8166, 27.1756,  3.9769,\n",
            "         3.9773,  3.9619,  3.9048, 23.4204, 27.2187, 27.2063, 27.1901,  3.9770,\n",
            "         3.9316,  3.9631, 19.4194,  3.9316,  7.8627, 19.2993,  3.9159, 15.5624,\n",
            "        27.1418, 27.3238, 11.6225, 15.4519, 23.3897,  7.7812, 11.8716, 11.6577,\n",
            "        27.4735, 11.5926,  4.0543,  7.7189, 27.1369,  3.9142, 23.2559, 15.5994,\n",
            "        19.6611,  3.9152,  7.5548, 27.1960, 11.7912, 23.4770, 15.7084, 23.2979,\n",
            "        23.5111,  7.6896, 27.4052,  4.0218, 15.5583,  3.9486,  7.9156,  3.8647,\n",
            "         3.9959, 19.5366,  3.8904, 27.2841, 11.7319, 23.2328, 19.2004, 22.9497,\n",
            "        26.7249,  3.2686, 22.6480, 25.9250, 25.3808,  5.6692,  7.8110, 13.0927,\n",
            "        14.3650, 15.6222, 27.4735])\n",
            "tensor([1.0010e-13, 6.2920e-02, 4.9158e-12, 4.6937e-12, 6.2851e-02, 4.3093e-12,\n",
            "        6.0130e-02, 5.0584e-12, 5.0605e-12, 4.9831e-12, 4.7065e-12, 1.4067e-03,\n",
            "        6.2775e-02, 6.2001e-02, 6.1004e-02, 5.0588e-12, 4.8344e-12, 4.9893e-12,\n",
            "        2.5740e-05, 4.8342e-12, 2.4637e-10, 2.2827e-05, 4.7589e-12, 5.4393e-07,\n",
            "        5.8130e-02, 6.9731e-02, 1.0579e-08, 4.8702e-07, 1.3643e-03, 2.2708e-10,\n",
            "        1.3571e-08, 1.0959e-08, 8.0991e-02, 1.0267e-08, 5.4655e-12, 2.1338e-10,\n",
            "        5.7848e-02, 4.7508e-12, 1.1933e-03, 5.6443e-07, 3.2777e-05, 4.7556e-12,\n",
            "        1.8109e-10, 6.1370e-02, 1.2523e-08, 1.4887e-03, 6.2944e-07, 1.2446e-03,\n",
            "        1.5403e-03, 2.0721e-10, 7.5646e-02, 5.2909e-12, 5.4168e-07, 4.9171e-12,\n",
            "        2.5976e-10, 4.5216e-12, 5.1556e-12, 2.8941e-05, 4.6393e-12, 6.7022e-02,\n",
            "        1.1803e-08, 1.1662e-03, 2.0676e-05, 8.7858e-04, 3.8313e-02, 2.4910e-12,\n",
            "        6.4976e-04, 1.7217e-02, 9.9914e-03, 2.7477e-11, 2.3395e-10, 4.6021e-08,\n",
            "        1.6424e-07, 5.7741e-07, 8.0991e-02])\n",
            "tensor([0.0020, 0.9908, 0.1437, 0.1420, 0.9908, 0.1389, 0.9892, 0.1448, 0.1448,\n",
            "        0.1442, 0.1421, 0.8525, 0.9907, 0.9903, 0.9897, 0.1448, 0.1431, 0.1443,\n",
            "        0.7068, 0.1431, 0.2862, 0.7025, 0.1425, 0.5665, 0.9879, 0.9946, 0.4230,\n",
            "        0.5624, 0.8514, 0.2832, 0.4321, 0.4243, 1.0000, 0.4220, 0.1476, 0.2810,\n",
            "        0.9878, 0.1425, 0.8465, 0.5678, 0.7156, 0.1425, 0.2750, 0.9899, 0.4292,\n",
            "        0.8545, 0.5718, 0.8480, 0.8558, 0.2799, 0.9975, 0.1464, 0.5663, 0.1437,\n",
            "        0.2881, 0.1407, 0.1454, 0.7111, 0.1416, 0.9931, 0.4270, 0.8456, 0.6989,\n",
            "        0.8353, 0.9728, 0.1190, 0.8244, 0.9436, 0.9238, 0.2064, 0.2843, 0.4766,\n",
            "        0.5229, 0.5686, 1.0000])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0074, 0.0199, 0.0085, 0.0085, 0.0199, 0.0085, 0.0199, 0.0085, 0.0085,\n",
              "        0.0085, 0.0085, 0.0173, 0.0199, 0.0199, 0.0199, 0.0085, 0.0085, 0.0085,\n",
              "        0.0150, 0.0085, 0.0098, 0.0149, 0.0085, 0.0130, 0.0198, 0.0200, 0.0113,\n",
              "        0.0130, 0.0173, 0.0098, 0.0114, 0.0113, 0.0201, 0.0113, 0.0086, 0.0098,\n",
              "        0.0198, 0.0085, 0.0172, 0.0130, 0.0151, 0.0085, 0.0097, 0.0199, 0.0113,\n",
              "        0.0174, 0.0131, 0.0172, 0.0174, 0.0098, 0.0200, 0.0085, 0.0130, 0.0085,\n",
              "        0.0099, 0.0085, 0.0085, 0.0150, 0.0085, 0.0199, 0.0113, 0.0172, 0.0149,\n",
              "        0.0170, 0.0195, 0.0083, 0.0168, 0.0190, 0.0186, 0.0091, 0.0098, 0.0119,\n",
              "        0.0125, 0.0130, 0.0201])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYdGyAom2SMj",
        "outputId": "9f8c98de-1def-4960-f196-fc6076cfdb29"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2., 0., 1., 3., 0., 0., 0., 3., 2., 3., 1., 1., 2., 0., 4., 4., 0.,\n",
              "       2., 1., 2., 2., 2., 4., 1., 3., 2., 0., 1., 2., 0., 3., 0., 3., 1.,\n",
              "       3., 0., 4., 1., 4., 4., 0., 0., 1., 2., 4., 0., 0., 1., 1., 1., 2.,\n",
              "       3., 4., 4., 3., 3., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s[1][-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WUaedhX198a",
        "outputId": "50351e05-14c3-46fe-ca0e-3b715952a000"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2869251.0"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s[-2] / np.max(s[-2], axis=1, keepdims=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmmYGIMyEQNl",
        "outputId": "11716c3f-ff3f-4a44-86da-fecfa16f7bd3"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.54545455, 0.63636364, 0.81818182, ..., 0.63636364, 0.81818182,\n",
              "        0.63636364],\n",
              "       [0.63556371, 0.76335792, 0.79889168, ..., 0.69172876, 0.76305837,\n",
              "        0.59486277],\n",
              "       [0.63554116, 0.76326371, 0.79906177, ..., 0.69172344, 0.76309617,\n",
              "        0.59482855],\n",
              "       ...,\n",
              "       [0.63561063, 0.76310384, 0.79895302, ..., 0.69220065, 0.76323637,\n",
              "        0.59465907],\n",
              "       [0.63559503, 0.76331184, 0.79909475, ..., 0.69183142, 0.76305564,\n",
              "        0.59481617],\n",
              "       [0.63570424, 0.76319295, 0.79897861, ..., 0.69210273, 0.76308193,\n",
              "        0.59484864]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(s[-2]), len(s[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GEP-MOCEZKu",
        "outputId": "4d64edd6-f82f-4a58-d565-949bb70d8a2b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(109, 110)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OH notes:\n",
        "\n",
        "baseline is what you use to interprete the reward\n",
        "\n",
        "don't use nn, use \"mean\" of rewards in this episode (I think so)\n",
        "\n",
        "Maybe look in to the instances, and look at how your agent is solving them\n",
        "\n",
        "Cutting off early, but only after solved some LP's\n",
        "- counterfactual exploration\n",
        "  - more than to do it more random\n",
        "- all prob entirely the same\n",
        "- activation function ?\n",
        "\n",
        "not learning? \n",
        "\n",
        "reward shaping?\n",
        "- someone: amplify the reward (shouldn't, since every step wil GIVE a reward), compare it to other POSSIBLE states\n",
        "- pre-trained on the instances and pre-trained the baseline?\n",
        "- get the max reward from the LP solver (isn't this cheating lol)\n",
        "- recalcluate the max-gap-to-go (lol remaining max gap) every step\n",
        "- go in the environment to make if return the shaped new-gap reward\n",
        "- the original reward doesn't help you across LPs?\n",
        "- moving average of *returns* from all *previous* episodes\n",
        "  - return is the discounted sum of the reward\n",
        "- advantage? Q - running averaged RETURN (but different states are mixed in, how do you deal with that)\n",
        "- Think about what's wrong with this base line, and write it down\n",
        "  - something about the states\n",
        "\n",
        "\n",
        "differences between LPs\n",
        "\n",
        "mode: \n",
        "- standardize the constraints, and the b vector by itself\n",
        "  - do you normalize by row or columns?\n",
        "  - He thinks it's more sense to normalize by rows\n",
        "  - Normalize it twice? LOL\n",
        "- or a normalization layer\\\n",
        "\n",
        "\n",
        "When doing softmax you can use a lamda (parameter) to mitigate large score difference\n",
        "- softmax comes with a scalar parameter\n",
        "\n",
        "Professor didn't think normalizing the input is necessary & and it shouldn't make a difference anyways (but come numerical value can be lost after normalization)\n",
        "\n",
        "iteration: \n",
        "\n",
        "plot random policy together, (as a bseline to see if your model is really learning)\n",
        "\n",
        "For baseline, Prof is suggesting Q network can work too.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ha4GPGL8Dduc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sKnhC54KYNNL"
      },
      "execution_count": 52,
      "outputs": []
    }
  ]
}